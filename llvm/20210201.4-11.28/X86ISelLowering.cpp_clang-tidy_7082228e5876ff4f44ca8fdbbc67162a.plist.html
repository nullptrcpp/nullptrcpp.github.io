<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"4": {"id": 4, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp", "content": "//===-- X86ISelLowering.cpp - X86 DAG Lowering Implementation -------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the interfaces that X86 uses to lower LLVM code into a\n// selection DAG.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"X86ISelLowering.h\"\n#include \"MCTargetDesc/X86ShuffleDecode.h\"\n#include \"X86.h\"\n#include \"X86CallingConv.h\"\n#include \"X86FrameLowering.h\"\n#include \"X86InstrBuilder.h\"\n#include \"X86IntrinsicsInfo.h\"\n#include \"X86MachineFunctionInfo.h\"\n#include \"X86TargetMachine.h\"\n#include \"X86TargetObjectFile.h\"\n#include \"llvm/ADT/SmallBitVector.h\"\n#include \"llvm/ADT/SmallSet.h\"\n#include \"llvm/ADT/Statistic.h\"\n#include \"llvm/ADT/StringExtras.h\"\n#include \"llvm/ADT/StringSwitch.h\"\n#include \"llvm/Analysis/BlockFrequencyInfo.h\"\n#include \"llvm/Analysis/EHPersonalities.h\"\n#include \"llvm/Analysis/ProfileSummaryInfo.h\"\n#include \"llvm/Analysis/VectorUtils.h\"\n#include \"llvm/CodeGen/IntrinsicLowering.h\"\n#include \"llvm/CodeGen/MachineFrameInfo.h\"\n#include \"llvm/CodeGen/MachineFunction.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineJumpTableInfo.h\"\n#include \"llvm/CodeGen/MachineLoopInfo.h\"\n#include \"llvm/CodeGen/MachineModuleInfo.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/TargetLowering.h\"\n#include \"llvm/CodeGen/WinEHFuncInfo.h\"\n#include \"llvm/IR/CallingConv.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/DiagnosticInfo.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/GlobalAlias.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/MC/MCAsmInfo.h\"\n#include \"llvm/MC/MCContext.h\"\n#include \"llvm/MC/MCExpr.h\"\n#include \"llvm/MC/MCSymbol.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/KnownBits.h\"\n#include \"llvm/Support/MathExtras.h\"\n#include \"llvm/Target/TargetOptions.h\"\n#include <algorithm>\n#include <bitset>\n#include <cctype>\n#include <numeric>\nusing namespace llvm;\n\n#define DEBUG_TYPE \"x86-isel\"\n\nSTATISTIC(NumTailCalls, \"Number of tail calls\");\n\nstatic cl::opt<int> ExperimentalPrefLoopAlignment(\n    \"x86-experimental-pref-loop-alignment\", cl::init(4),\n    cl::desc(\n        \"Sets the preferable loop alignment for experiments (as log2 bytes)\"\n        \"(the last x86-experimental-pref-loop-alignment bits\"\n        \" of the loop header PC will be 0).\"),\n    cl::Hidden);\n\nstatic cl::opt<int> ExperimentalPrefInnermostLoopAlignment(\n    \"x86-experimental-pref-innermost-loop-alignment\", cl::init(4),\n    cl::desc(\n        \"Sets the preferable loop alignment for experiments (as log2 bytes) \"\n        \"for innermost loops only. If specified, this option overrides \"\n        \"alignment set by x86-experimental-pref-loop-alignment.\"),\n    cl::Hidden);\n\nstatic cl::opt<bool> MulConstantOptimization(\n    \"mul-constant-optimization\", cl::init(true),\n    cl::desc(\"Replace 'mul x, Const' with more effective instructions like \"\n             \"SHIFT, LEA, etc.\"),\n    cl::Hidden);\n\nstatic cl::opt<bool> ExperimentalUnorderedISEL(\n    \"x86-experimental-unordered-atomic-isel\", cl::init(false),\n    cl::desc(\"Use LoadSDNode and StoreSDNode instead of \"\n             \"AtomicSDNode for unordered atomic loads and \"\n             \"stores respectively.\"),\n    cl::Hidden);\n\n/// Call this when the user attempts to do something unsupported, like\n/// returning a double without SSE2 enabled on x86_64. This is not fatal, unlike\n/// report_fatal_error, so calling code should attempt to recover without\n/// crashing.\nstatic void errorUnsupported(SelectionDAG &DAG, const SDLoc &dl,\n                             const char *Msg) {\n  MachineFunction &MF = DAG.getMachineFunction();\n  DAG.getContext()->diagnose(\n      DiagnosticInfoUnsupported(MF.getFunction(), Msg, dl.getDebugLoc()));\n}\n\nX86TargetLowering::X86TargetLowering(const X86TargetMachine &TM,\n                                     const X86Subtarget &STI)\n    : TargetLowering(TM), Subtarget(STI) {\n  bool UseX87 = !Subtarget.useSoftFloat() && Subtarget.hasX87();\n  X86ScalarSSEf64 = Subtarget.hasSSE2();\n  X86ScalarSSEf32 = Subtarget.hasSSE1();\n  MVT PtrVT = MVT::getIntegerVT(TM.getPointerSizeInBits(0));\n\n  // Set up the TargetLowering object.\n\n  // X86 is weird. It always uses i8 for shift amounts and setcc results.\n  setBooleanContents(ZeroOrOneBooleanContent);\n  // X86-SSE is even stranger. It uses -1 or 0 for vector masks.\n  setBooleanVectorContents(ZeroOrNegativeOneBooleanContent);\n\n  // For 64-bit, since we have so many registers, use the ILP scheduler.\n  // For 32-bit, use the register pressure specific scheduling.\n  // For Atom, always use ILP scheduling.\n  if (Subtarget.isAtom())\n    setSchedulingPreference(Sched::ILP);\n  else if (Subtarget.is64Bit())\n    setSchedulingPreference(Sched::ILP);\n  else\n    setSchedulingPreference(Sched::RegPressure);\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  setStackPointerRegisterToSaveRestore(RegInfo->getStackRegister());\n\n  // Bypass expensive divides and use cheaper ones.\n  if (TM.getOptLevel() >= CodeGenOpt::Default) {\n    if (Subtarget.hasSlowDivide32())\n      addBypassSlowDiv(32, 8);\n    if (Subtarget.hasSlowDivide64() && Subtarget.is64Bit())\n      addBypassSlowDiv(64, 32);\n  }\n\n  // Setup Windows compiler runtime calls.\n  if (Subtarget.isTargetWindowsMSVC() || Subtarget.isTargetWindowsItanium()) {\n    static const struct {\n      const RTLIB::Libcall Op;\n      const char * const Name;\n      const CallingConv::ID CC;\n    } LibraryCalls[] = {\n      { RTLIB::SDIV_I64, \"_alldiv\", CallingConv::X86_StdCall },\n      { RTLIB::UDIV_I64, \"_aulldiv\", CallingConv::X86_StdCall },\n      { RTLIB::SREM_I64, \"_allrem\", CallingConv::X86_StdCall },\n      { RTLIB::UREM_I64, \"_aullrem\", CallingConv::X86_StdCall },\n      { RTLIB::MUL_I64, \"_allmul\", CallingConv::X86_StdCall },\n    };\n\n    for (const auto &LC : LibraryCalls) {\n      setLibcallName(LC.Op, LC.Name);\n      setLibcallCallingConv(LC.Op, LC.CC);\n    }\n  }\n\n  if (Subtarget.getTargetTriple().isOSMSVCRT()) {\n    // MSVCRT doesn't have powi; fall back to pow\n    setLibcallName(RTLIB::POWI_F32, nullptr);\n    setLibcallName(RTLIB::POWI_F64, nullptr);\n  }\n\n  // If we don't have cmpxchg8b(meaing this is a 386/486), limit atomic size to\n  // 32 bits so the AtomicExpandPass will expand it so we don't need cmpxchg8b.\n  // FIXME: Should we be limiting the atomic size on other configs? Default is\n  // 1024.\n  if (!Subtarget.hasCmpxchg8b())\n    setMaxAtomicSizeInBitsSupported(32);\n\n  // Set up the register classes.\n  addRegisterClass(MVT::i8, &X86::GR8RegClass);\n  addRegisterClass(MVT::i16, &X86::GR16RegClass);\n  addRegisterClass(MVT::i32, &X86::GR32RegClass);\n  if (Subtarget.is64Bit())\n    addRegisterClass(MVT::i64, &X86::GR64RegClass);\n\n  for (MVT VT : MVT::integer_valuetypes())\n    setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i1, Promote);\n\n  // We don't accept any truncstore of integer registers.\n  setTruncStoreAction(MVT::i64, MVT::i32, Expand);\n  setTruncStoreAction(MVT::i64, MVT::i16, Expand);\n  setTruncStoreAction(MVT::i64, MVT::i8 , Expand);\n  setTruncStoreAction(MVT::i32, MVT::i16, Expand);\n  setTruncStoreAction(MVT::i32, MVT::i8 , Expand);\n  setTruncStoreAction(MVT::i16, MVT::i8,  Expand);\n\n  setTruncStoreAction(MVT::f64, MVT::f32, Expand);\n\n  // SETOEQ and SETUNE require checking two conditions.\n  for (auto VT : {MVT::f32, MVT::f64, MVT::f80}) {\n    setCondCodeAction(ISD::SETOEQ, VT, Expand);\n    setCondCodeAction(ISD::SETUNE, VT, Expand);\n  }\n\n  // Integer absolute.\n  if (Subtarget.hasCMov()) {\n    setOperationAction(ISD::ABS            , MVT::i16  , Custom);\n    setOperationAction(ISD::ABS            , MVT::i32  , Custom);\n    if (Subtarget.is64Bit())\n      setOperationAction(ISD::ABS          , MVT::i64  , Custom);\n  }\n\n  // Funnel shifts.\n  for (auto ShiftOp : {ISD::FSHL, ISD::FSHR}) {\n    // For slow shld targets we only lower for code size.\n    LegalizeAction ShiftDoubleAction = Subtarget.isSHLDSlow() ? Custom : Legal;\n\n    setOperationAction(ShiftOp             , MVT::i8   , Custom);\n    setOperationAction(ShiftOp             , MVT::i16  , Custom);\n    setOperationAction(ShiftOp             , MVT::i32  , ShiftDoubleAction);\n    if (Subtarget.is64Bit())\n      setOperationAction(ShiftOp           , MVT::i64  , ShiftDoubleAction);\n  }\n\n  if (!Subtarget.useSoftFloat()) {\n    // Promote all UINT_TO_FP to larger SINT_TO_FP's, as X86 doesn't have this\n    // operation.\n    setOperationAction(ISD::UINT_TO_FP,        MVT::i8, Promote);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::i8, Promote);\n    setOperationAction(ISD::UINT_TO_FP,        MVT::i16, Promote);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::i16, Promote);\n    // We have an algorithm for SSE2, and we turn this into a 64-bit\n    // FILD or VCVTUSI2SS/SD for other targets.\n    setOperationAction(ISD::UINT_TO_FP,        MVT::i32, Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::i32, Custom);\n    // We have an algorithm for SSE2->double, and we turn this into a\n    // 64-bit FILD followed by conditional FADD for other targets.\n    setOperationAction(ISD::UINT_TO_FP,        MVT::i64, Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::i64, Custom);\n\n    // Promote i8 SINT_TO_FP to larger SINT_TO_FP's, as X86 doesn't have\n    // this operation.\n    setOperationAction(ISD::SINT_TO_FP,        MVT::i8, Promote);\n    setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::i8, Promote);\n    // SSE has no i16 to fp conversion, only i32. We promote in the handler\n    // to allow f80 to use i16 and f64 to use i16 with sse1 only\n    setOperationAction(ISD::SINT_TO_FP,        MVT::i16, Custom);\n    setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::i16, Custom);\n    // f32 and f64 cases are Legal with SSE1/SSE2, f80 case is not\n    setOperationAction(ISD::SINT_TO_FP,        MVT::i32, Custom);\n    setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::i32, Custom);\n    // In 32-bit mode these are custom lowered.  In 64-bit mode F32 and F64\n    // are Legal, f80 is custom lowered.\n    setOperationAction(ISD::SINT_TO_FP,        MVT::i64, Custom);\n    setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::i64, Custom);\n\n    // Promote i8 FP_TO_SINT to larger FP_TO_SINTS's, as X86 doesn't have\n    // this operation.\n    setOperationAction(ISD::FP_TO_SINT,        MVT::i8,  Promote);\n    // FIXME: This doesn't generate invalid exception when it should. PR44019.\n    setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::i8,  Promote);\n    setOperationAction(ISD::FP_TO_SINT,        MVT::i16, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::i16, Custom);\n    setOperationAction(ISD::FP_TO_SINT,        MVT::i32, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::i32, Custom);\n    // In 32-bit mode these are custom lowered.  In 64-bit mode F32 and F64\n    // are Legal, f80 is custom lowered.\n    setOperationAction(ISD::FP_TO_SINT,        MVT::i64, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::i64, Custom);\n\n    // Handle FP_TO_UINT by promoting the destination to a larger signed\n    // conversion.\n    setOperationAction(ISD::FP_TO_UINT,        MVT::i8,  Promote);\n    // FIXME: This doesn't generate invalid exception when it should. PR44019.\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::i8,  Promote);\n    setOperationAction(ISD::FP_TO_UINT,        MVT::i16, Promote);\n    // FIXME: This doesn't generate invalid exception when it should. PR44019.\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::i16, Promote);\n    setOperationAction(ISD::FP_TO_UINT,        MVT::i32, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::i32, Custom);\n    setOperationAction(ISD::FP_TO_UINT,        MVT::i64, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::i64, Custom);\n\n    setOperationAction(ISD::LRINT,             MVT::f32, Custom);\n    setOperationAction(ISD::LRINT,             MVT::f64, Custom);\n    setOperationAction(ISD::LLRINT,            MVT::f32, Custom);\n    setOperationAction(ISD::LLRINT,            MVT::f64, Custom);\n\n    if (!Subtarget.is64Bit()) {\n      setOperationAction(ISD::LRINT,  MVT::i64, Custom);\n      setOperationAction(ISD::LLRINT, MVT::i64, Custom);\n    }\n  }\n\n  if (Subtarget.hasSSE2()) {\n    // Custom lowering for saturating float to int conversions.\n    // We handle promotion to larger result types manually.\n    for (MVT VT : { MVT::i8, MVT::i16, MVT::i32 }) {\n      setOperationAction(ISD::FP_TO_UINT_SAT, VT, Custom);\n      setOperationAction(ISD::FP_TO_SINT_SAT, VT, Custom);\n    }\n    if (Subtarget.is64Bit()) {\n      setOperationAction(ISD::FP_TO_UINT_SAT, MVT::i64, Custom);\n      setOperationAction(ISD::FP_TO_SINT_SAT, MVT::i64, Custom);\n    }\n  }\n\n  // Handle address space casts between mixed sized pointers.\n  setOperationAction(ISD::ADDRSPACECAST, MVT::i32, Custom);\n  setOperationAction(ISD::ADDRSPACECAST, MVT::i64, Custom);\n\n  // TODO: when we have SSE, these could be more efficient, by using movd/movq.\n  if (!X86ScalarSSEf64) {\n    setOperationAction(ISD::BITCAST        , MVT::f32  , Expand);\n    setOperationAction(ISD::BITCAST        , MVT::i32  , Expand);\n    if (Subtarget.is64Bit()) {\n      setOperationAction(ISD::BITCAST      , MVT::f64  , Expand);\n      // Without SSE, i64->f64 goes through memory.\n      setOperationAction(ISD::BITCAST      , MVT::i64  , Expand);\n    }\n  } else if (!Subtarget.is64Bit())\n    setOperationAction(ISD::BITCAST      , MVT::i64  , Custom);\n\n  // Scalar integer divide and remainder are lowered to use operations that\n  // produce two results, to match the available instructions. This exposes\n  // the two-result form to trivial CSE, which is able to combine x/y and x%y\n  // into a single instruction.\n  //\n  // Scalar integer multiply-high is also lowered to use two-result\n  // operations, to match the available instructions. However, plain multiply\n  // (low) operations are left as Legal, as there are single-result\n  // instructions for this in x86. Using the two-result multiply instructions\n  // when both high and low results are needed must be arranged by dagcombine.\n  for (auto VT : { MVT::i8, MVT::i16, MVT::i32, MVT::i64 }) {\n    setOperationAction(ISD::MULHS, VT, Expand);\n    setOperationAction(ISD::MULHU, VT, Expand);\n    setOperationAction(ISD::SDIV, VT, Expand);\n    setOperationAction(ISD::UDIV, VT, Expand);\n    setOperationAction(ISD::SREM, VT, Expand);\n    setOperationAction(ISD::UREM, VT, Expand);\n  }\n\n  setOperationAction(ISD::BR_JT            , MVT::Other, Expand);\n  setOperationAction(ISD::BRCOND           , MVT::Other, Custom);\n  for (auto VT : { MVT::f32, MVT::f64, MVT::f80, MVT::f128,\n                   MVT::i8,  MVT::i16, MVT::i32, MVT::i64 }) {\n    setOperationAction(ISD::BR_CC,     VT, Expand);\n    setOperationAction(ISD::SELECT_CC, VT, Expand);\n  }\n  if (Subtarget.is64Bit())\n    setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i32, Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i16  , Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i8   , Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i1   , Expand);\n\n  setOperationAction(ISD::FREM             , MVT::f32  , Expand);\n  setOperationAction(ISD::FREM             , MVT::f64  , Expand);\n  setOperationAction(ISD::FREM             , MVT::f80  , Expand);\n  setOperationAction(ISD::FREM             , MVT::f128 , Expand);\n  setOperationAction(ISD::FLT_ROUNDS_      , MVT::i32  , Custom);\n\n  // Promote the i8 variants and force them on up to i32 which has a shorter\n  // encoding.\n  setOperationPromotedToType(ISD::CTTZ           , MVT::i8   , MVT::i32);\n  setOperationPromotedToType(ISD::CTTZ_ZERO_UNDEF, MVT::i8   , MVT::i32);\n  if (!Subtarget.hasBMI()) {\n    setOperationAction(ISD::CTTZ           , MVT::i16  , Custom);\n    setOperationAction(ISD::CTTZ           , MVT::i32  , Custom);\n    setOperationAction(ISD::CTTZ_ZERO_UNDEF, MVT::i16  , Legal);\n    setOperationAction(ISD::CTTZ_ZERO_UNDEF, MVT::i32  , Legal);\n    if (Subtarget.is64Bit()) {\n      setOperationAction(ISD::CTTZ         , MVT::i64  , Custom);\n      setOperationAction(ISD::CTTZ_ZERO_UNDEF, MVT::i64, Legal);\n    }\n  }\n\n  if (Subtarget.hasLZCNT()) {\n    // When promoting the i8 variants, force them to i32 for a shorter\n    // encoding.\n    setOperationPromotedToType(ISD::CTLZ           , MVT::i8   , MVT::i32);\n    setOperationPromotedToType(ISD::CTLZ_ZERO_UNDEF, MVT::i8   , MVT::i32);\n  } else {\n    for (auto VT : {MVT::i8, MVT::i16, MVT::i32, MVT::i64}) {\n      if (VT == MVT::i64 && !Subtarget.is64Bit())\n        continue;\n      setOperationAction(ISD::CTLZ           , VT, Custom);\n      setOperationAction(ISD::CTLZ_ZERO_UNDEF, VT, Custom);\n    }\n  }\n\n  for (auto Op : {ISD::FP16_TO_FP, ISD::STRICT_FP16_TO_FP, ISD::FP_TO_FP16,\n                  ISD::STRICT_FP_TO_FP16}) {\n    // Special handling for half-precision floating point conversions.\n    // If we don't have F16C support, then lower half float conversions\n    // into library calls.\n    setOperationAction(\n        Op, MVT::f32,\n        (!Subtarget.useSoftFloat() && Subtarget.hasF16C()) ? Custom : Expand);\n    // There's never any support for operations beyond MVT::f32.\n    setOperationAction(Op, MVT::f64, Expand);\n    setOperationAction(Op, MVT::f80, Expand);\n    setOperationAction(Op, MVT::f128, Expand);\n  }\n\n  setLoadExtAction(ISD::EXTLOAD, MVT::f32, MVT::f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::f80, MVT::f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f32, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f64, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f80, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f128, MVT::f16, Expand);\n\n  setOperationAction(ISD::PARITY, MVT::i8, Custom);\n  if (Subtarget.hasPOPCNT()) {\n    setOperationPromotedToType(ISD::CTPOP, MVT::i8, MVT::i32);\n  } else {\n    setOperationAction(ISD::CTPOP          , MVT::i8   , Expand);\n    setOperationAction(ISD::CTPOP          , MVT::i16  , Expand);\n    setOperationAction(ISD::CTPOP          , MVT::i32  , Expand);\n    if (Subtarget.is64Bit())\n      setOperationAction(ISD::CTPOP        , MVT::i64  , Expand);\n    else\n      setOperationAction(ISD::CTPOP        , MVT::i64  , Custom);\n\n    setOperationAction(ISD::PARITY, MVT::i16, Custom);\n    setOperationAction(ISD::PARITY, MVT::i32, Custom);\n    if (Subtarget.is64Bit())\n      setOperationAction(ISD::PARITY, MVT::i64, Custom);\n  }\n\n  setOperationAction(ISD::READCYCLECOUNTER , MVT::i64  , Custom);\n\n  if (!Subtarget.hasMOVBE())\n    setOperationAction(ISD::BSWAP          , MVT::i16  , Expand);\n\n  // X86 wants to expand cmov itself.\n  for (auto VT : { MVT::f32, MVT::f64, MVT::f80, MVT::f128 }) {\n    setOperationAction(ISD::SELECT, VT, Custom);\n    setOperationAction(ISD::SETCC, VT, Custom);\n    setOperationAction(ISD::STRICT_FSETCC, VT, Custom);\n    setOperationAction(ISD::STRICT_FSETCCS, VT, Custom);\n  }\n  for (auto VT : { MVT::i8, MVT::i16, MVT::i32, MVT::i64 }) {\n    if (VT == MVT::i64 && !Subtarget.is64Bit())\n      continue;\n    setOperationAction(ISD::SELECT, VT, Custom);\n    setOperationAction(ISD::SETCC,  VT, Custom);\n  }\n\n  // Custom action for SELECT MMX and expand action for SELECT_CC MMX\n  setOperationAction(ISD::SELECT, MVT::x86mmx, Custom);\n  setOperationAction(ISD::SELECT_CC, MVT::x86mmx, Expand);\n\n  setOperationAction(ISD::EH_RETURN       , MVT::Other, Custom);\n  // NOTE: EH_SJLJ_SETJMP/_LONGJMP are not recommended, since\n  // LLVM/Clang supports zero-cost DWARF and SEH exception handling.\n  setOperationAction(ISD::EH_SJLJ_SETJMP, MVT::i32, Custom);\n  setOperationAction(ISD::EH_SJLJ_LONGJMP, MVT::Other, Custom);\n  setOperationAction(ISD::EH_SJLJ_SETUP_DISPATCH, MVT::Other, Custom);\n  if (TM.Options.ExceptionModel == ExceptionHandling::SjLj)\n    setLibcallName(RTLIB::UNWIND_RESUME, \"_Unwind_SjLj_Resume\");\n\n  // Darwin ABI issue.\n  for (auto VT : { MVT::i32, MVT::i64 }) {\n    if (VT == MVT::i64 && !Subtarget.is64Bit())\n      continue;\n    setOperationAction(ISD::ConstantPool    , VT, Custom);\n    setOperationAction(ISD::JumpTable       , VT, Custom);\n    setOperationAction(ISD::GlobalAddress   , VT, Custom);\n    setOperationAction(ISD::GlobalTLSAddress, VT, Custom);\n    setOperationAction(ISD::ExternalSymbol  , VT, Custom);\n    setOperationAction(ISD::BlockAddress    , VT, Custom);\n  }\n\n  // 64-bit shl, sra, srl (iff 32-bit x86)\n  for (auto VT : { MVT::i32, MVT::i64 }) {\n    if (VT == MVT::i64 && !Subtarget.is64Bit())\n      continue;\n    setOperationAction(ISD::SHL_PARTS, VT, Custom);\n    setOperationAction(ISD::SRA_PARTS, VT, Custom);\n    setOperationAction(ISD::SRL_PARTS, VT, Custom);\n  }\n\n  if (Subtarget.hasSSEPrefetch() || Subtarget.has3DNow())\n    setOperationAction(ISD::PREFETCH      , MVT::Other, Legal);\n\n  setOperationAction(ISD::ATOMIC_FENCE  , MVT::Other, Custom);\n\n  // Expand certain atomics\n  for (auto VT : { MVT::i8, MVT::i16, MVT::i32, MVT::i64 }) {\n    setOperationAction(ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS, VT, Custom);\n    setOperationAction(ISD::ATOMIC_LOAD_SUB, VT, Custom);\n    setOperationAction(ISD::ATOMIC_LOAD_ADD, VT, Custom);\n    setOperationAction(ISD::ATOMIC_LOAD_OR, VT, Custom);\n    setOperationAction(ISD::ATOMIC_LOAD_XOR, VT, Custom);\n    setOperationAction(ISD::ATOMIC_LOAD_AND, VT, Custom);\n    setOperationAction(ISD::ATOMIC_STORE, VT, Custom);\n  }\n\n  if (!Subtarget.is64Bit())\n    setOperationAction(ISD::ATOMIC_LOAD, MVT::i64, Custom);\n\n  if (Subtarget.hasCmpxchg16b()) {\n    setOperationAction(ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS, MVT::i128, Custom);\n  }\n\n  // FIXME - use subtarget debug flags\n  if (!Subtarget.isTargetDarwin() && !Subtarget.isTargetELF() &&\n      !Subtarget.isTargetCygMing() && !Subtarget.isTargetWin64() &&\n      TM.Options.ExceptionModel != ExceptionHandling::SjLj) {\n    setOperationAction(ISD::EH_LABEL, MVT::Other, Expand);\n  }\n\n  setOperationAction(ISD::FRAME_TO_ARGS_OFFSET, MVT::i32, Custom);\n  setOperationAction(ISD::FRAME_TO_ARGS_OFFSET, MVT::i64, Custom);\n\n  setOperationAction(ISD::INIT_TRAMPOLINE, MVT::Other, Custom);\n  setOperationAction(ISD::ADJUST_TRAMPOLINE, MVT::Other, Custom);\n\n  setOperationAction(ISD::TRAP, MVT::Other, Legal);\n  setOperationAction(ISD::DEBUGTRAP, MVT::Other, Legal);\n  setOperationAction(ISD::UBSANTRAP, MVT::Other, Legal);\n\n  // VASTART needs to be custom lowered to use the VarArgsFrameIndex\n  setOperationAction(ISD::VASTART           , MVT::Other, Custom);\n  setOperationAction(ISD::VAEND             , MVT::Other, Expand);\n  bool Is64Bit = Subtarget.is64Bit();\n  setOperationAction(ISD::VAARG,  MVT::Other, Is64Bit ? Custom : Expand);\n  setOperationAction(ISD::VACOPY, MVT::Other, Is64Bit ? Custom : Expand);\n\n  setOperationAction(ISD::STACKSAVE,          MVT::Other, Expand);\n  setOperationAction(ISD::STACKRESTORE,       MVT::Other, Expand);\n\n  setOperationAction(ISD::DYNAMIC_STACKALLOC, PtrVT, Custom);\n\n  // GC_TRANSITION_START and GC_TRANSITION_END need custom lowering.\n  setOperationAction(ISD::GC_TRANSITION_START, MVT::Other, Custom);\n  setOperationAction(ISD::GC_TRANSITION_END, MVT::Other, Custom);\n\n  if (!Subtarget.useSoftFloat() && X86ScalarSSEf64) {\n    // f32 and f64 use SSE.\n    // Set up the FP register classes.\n    addRegisterClass(MVT::f32, Subtarget.hasAVX512() ? &X86::FR32XRegClass\n                                                     : &X86::FR32RegClass);\n    addRegisterClass(MVT::f64, Subtarget.hasAVX512() ? &X86::FR64XRegClass\n                                                     : &X86::FR64RegClass);\n\n    // Disable f32->f64 extload as we can only generate this in one instruction\n    // under optsize. So its easier to pattern match (fpext (load)) for that\n    // case instead of needing to emit 2 instructions for extload in the\n    // non-optsize case.\n    setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand);\n\n    for (auto VT : { MVT::f32, MVT::f64 }) {\n      // Use ANDPD to simulate FABS.\n      setOperationAction(ISD::FABS, VT, Custom);\n\n      // Use XORP to simulate FNEG.\n      setOperationAction(ISD::FNEG, VT, Custom);\n\n      // Use ANDPD and ORPD to simulate FCOPYSIGN.\n      setOperationAction(ISD::FCOPYSIGN, VT, Custom);\n\n      // These might be better off as horizontal vector ops.\n      setOperationAction(ISD::FADD, VT, Custom);\n      setOperationAction(ISD::FSUB, VT, Custom);\n\n      // We don't support sin/cos/fmod\n      setOperationAction(ISD::FSIN   , VT, Expand);\n      setOperationAction(ISD::FCOS   , VT, Expand);\n      setOperationAction(ISD::FSINCOS, VT, Expand);\n    }\n\n    // Lower this to MOVMSK plus an AND.\n    setOperationAction(ISD::FGETSIGN, MVT::i64, Custom);\n    setOperationAction(ISD::FGETSIGN, MVT::i32, Custom);\n\n  } else if (!Subtarget.useSoftFloat() && X86ScalarSSEf32 &&\n             (UseX87 || Is64Bit)) {\n    // Use SSE for f32, x87 for f64.\n    // Set up the FP register classes.\n    addRegisterClass(MVT::f32, &X86::FR32RegClass);\n    if (UseX87)\n      addRegisterClass(MVT::f64, &X86::RFP64RegClass);\n\n    // Use ANDPS to simulate FABS.\n    setOperationAction(ISD::FABS , MVT::f32, Custom);\n\n    // Use XORP to simulate FNEG.\n    setOperationAction(ISD::FNEG , MVT::f32, Custom);\n\n    if (UseX87)\n      setOperationAction(ISD::UNDEF, MVT::f64, Expand);\n\n    // Use ANDPS and ORPS to simulate FCOPYSIGN.\n    if (UseX87)\n      setOperationAction(ISD::FCOPYSIGN, MVT::f64, Expand);\n    setOperationAction(ISD::FCOPYSIGN, MVT::f32, Custom);\n\n    // We don't support sin/cos/fmod\n    setOperationAction(ISD::FSIN   , MVT::f32, Expand);\n    setOperationAction(ISD::FCOS   , MVT::f32, Expand);\n    setOperationAction(ISD::FSINCOS, MVT::f32, Expand);\n\n    if (UseX87) {\n      // Always expand sin/cos functions even though x87 has an instruction.\n      setOperationAction(ISD::FSIN, MVT::f64, Expand);\n      setOperationAction(ISD::FCOS, MVT::f64, Expand);\n      setOperationAction(ISD::FSINCOS, MVT::f64, Expand);\n    }\n  } else if (UseX87) {\n    // f32 and f64 in x87.\n    // Set up the FP register classes.\n    addRegisterClass(MVT::f64, &X86::RFP64RegClass);\n    addRegisterClass(MVT::f32, &X86::RFP32RegClass);\n\n    for (auto VT : { MVT::f32, MVT::f64 }) {\n      setOperationAction(ISD::UNDEF,     VT, Expand);\n      setOperationAction(ISD::FCOPYSIGN, VT, Expand);\n\n      // Always expand sin/cos functions even though x87 has an instruction.\n      setOperationAction(ISD::FSIN   , VT, Expand);\n      setOperationAction(ISD::FCOS   , VT, Expand);\n      setOperationAction(ISD::FSINCOS, VT, Expand);\n    }\n  }\n\n  // Expand FP32 immediates into loads from the stack, save special cases.\n  if (isTypeLegal(MVT::f32)) {\n    if (UseX87 && (getRegClassFor(MVT::f32) == &X86::RFP32RegClass)) {\n      addLegalFPImmediate(APFloat(+0.0f)); // FLD0\n      addLegalFPImmediate(APFloat(+1.0f)); // FLD1\n      addLegalFPImmediate(APFloat(-0.0f)); // FLD0/FCHS\n      addLegalFPImmediate(APFloat(-1.0f)); // FLD1/FCHS\n    } else // SSE immediates.\n      addLegalFPImmediate(APFloat(+0.0f)); // xorps\n  }\n  // Expand FP64 immediates into loads from the stack, save special cases.\n  if (isTypeLegal(MVT::f64)) {\n    if (UseX87 && getRegClassFor(MVT::f64) == &X86::RFP64RegClass) {\n      addLegalFPImmediate(APFloat(+0.0)); // FLD0\n      addLegalFPImmediate(APFloat(+1.0)); // FLD1\n      addLegalFPImmediate(APFloat(-0.0)); // FLD0/FCHS\n      addLegalFPImmediate(APFloat(-1.0)); // FLD1/FCHS\n    } else // SSE immediates.\n      addLegalFPImmediate(APFloat(+0.0)); // xorpd\n  }\n  // Handle constrained floating-point operations of scalar.\n  setOperationAction(ISD::STRICT_FADD,      MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FADD,      MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FSUB,      MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FSUB,      MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FMUL,      MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FMUL,      MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FDIV,      MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FDIV,      MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FP_EXTEND, MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FP_ROUND,  MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FP_ROUND,  MVT::f64, Legal);\n  setOperationAction(ISD::STRICT_FSQRT,     MVT::f32, Legal);\n  setOperationAction(ISD::STRICT_FSQRT,     MVT::f64, Legal);\n\n  // We don't support FMA.\n  setOperationAction(ISD::FMA, MVT::f64, Expand);\n  setOperationAction(ISD::FMA, MVT::f32, Expand);\n\n  // f80 always uses X87.\n  if (UseX87) {\n    addRegisterClass(MVT::f80, &X86::RFP80RegClass);\n    setOperationAction(ISD::UNDEF,     MVT::f80, Expand);\n    setOperationAction(ISD::FCOPYSIGN, MVT::f80, Expand);\n    {\n      APFloat TmpFlt = APFloat::getZero(APFloat::x87DoubleExtended());\n      addLegalFPImmediate(TmpFlt);  // FLD0\n      TmpFlt.changeSign();\n      addLegalFPImmediate(TmpFlt);  // FLD0/FCHS\n\n      bool ignored;\n      APFloat TmpFlt2(+1.0);\n      TmpFlt2.convert(APFloat::x87DoubleExtended(), APFloat::rmNearestTiesToEven,\n                      &ignored);\n      addLegalFPImmediate(TmpFlt2);  // FLD1\n      TmpFlt2.changeSign();\n      addLegalFPImmediate(TmpFlt2);  // FLD1/FCHS\n    }\n\n    // Always expand sin/cos functions even though x87 has an instruction.\n    setOperationAction(ISD::FSIN   , MVT::f80, Expand);\n    setOperationAction(ISD::FCOS   , MVT::f80, Expand);\n    setOperationAction(ISD::FSINCOS, MVT::f80, Expand);\n\n    setOperationAction(ISD::FFLOOR, MVT::f80, Expand);\n    setOperationAction(ISD::FCEIL,  MVT::f80, Expand);\n    setOperationAction(ISD::FTRUNC, MVT::f80, Expand);\n    setOperationAction(ISD::FRINT,  MVT::f80, Expand);\n    setOperationAction(ISD::FNEARBYINT, MVT::f80, Expand);\n    setOperationAction(ISD::FMA, MVT::f80, Expand);\n    setOperationAction(ISD::LROUND, MVT::f80, Expand);\n    setOperationAction(ISD::LLROUND, MVT::f80, Expand);\n    setOperationAction(ISD::LRINT, MVT::f80, Custom);\n    setOperationAction(ISD::LLRINT, MVT::f80, Custom);\n\n    // Handle constrained floating-point operations of scalar.\n    setOperationAction(ISD::STRICT_FADD     , MVT::f80, Legal);\n    setOperationAction(ISD::STRICT_FSUB     , MVT::f80, Legal);\n    setOperationAction(ISD::STRICT_FMUL     , MVT::f80, Legal);\n    setOperationAction(ISD::STRICT_FDIV     , MVT::f80, Legal);\n    setOperationAction(ISD::STRICT_FSQRT    , MVT::f80, Legal);\n    setOperationAction(ISD::STRICT_FP_EXTEND, MVT::f80, Legal);\n    // FIXME: When the target is 64-bit, STRICT_FP_ROUND will be overwritten\n    // as Custom.\n    setOperationAction(ISD::STRICT_FP_ROUND, MVT::f80, Legal);\n  }\n\n  // f128 uses xmm registers, but most operations require libcalls.\n  if (!Subtarget.useSoftFloat() && Subtarget.is64Bit() && Subtarget.hasSSE1()) {\n    addRegisterClass(MVT::f128, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                   : &X86::VR128RegClass);\n\n    addLegalFPImmediate(APFloat::getZero(APFloat::IEEEquad())); // xorps\n\n    setOperationAction(ISD::FADD,        MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FADD, MVT::f128, LibCall);\n    setOperationAction(ISD::FSUB,        MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FSUB, MVT::f128, LibCall);\n    setOperationAction(ISD::FDIV,        MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FDIV, MVT::f128, LibCall);\n    setOperationAction(ISD::FMUL,        MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FMUL, MVT::f128, LibCall);\n    setOperationAction(ISD::FMA,         MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FMA,  MVT::f128, LibCall);\n\n    setOperationAction(ISD::FABS, MVT::f128, Custom);\n    setOperationAction(ISD::FNEG, MVT::f128, Custom);\n    setOperationAction(ISD::FCOPYSIGN, MVT::f128, Custom);\n\n    setOperationAction(ISD::FSIN,         MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FSIN,  MVT::f128, LibCall);\n    setOperationAction(ISD::FCOS,         MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FCOS,  MVT::f128, LibCall);\n    setOperationAction(ISD::FSINCOS,      MVT::f128, LibCall);\n    // No STRICT_FSINCOS\n    setOperationAction(ISD::FSQRT,        MVT::f128, LibCall);\n    setOperationAction(ISD::STRICT_FSQRT, MVT::f128, LibCall);\n\n    setOperationAction(ISD::FP_EXTEND,        MVT::f128, Custom);\n    setOperationAction(ISD::STRICT_FP_EXTEND, MVT::f128, Custom);\n    // We need to custom handle any FP_ROUND with an f128 input, but\n    // LegalizeDAG uses the result type to know when to run a custom handler.\n    // So we have to list all legal floating point result types here.\n    if (isTypeLegal(MVT::f32)) {\n      setOperationAction(ISD::FP_ROUND, MVT::f32, Custom);\n      setOperationAction(ISD::STRICT_FP_ROUND, MVT::f32, Custom);\n    }\n    if (isTypeLegal(MVT::f64)) {\n      setOperationAction(ISD::FP_ROUND, MVT::f64, Custom);\n      setOperationAction(ISD::STRICT_FP_ROUND, MVT::f64, Custom);\n    }\n    if (isTypeLegal(MVT::f80)) {\n      setOperationAction(ISD::FP_ROUND, MVT::f80, Custom);\n      setOperationAction(ISD::STRICT_FP_ROUND, MVT::f80, Custom);\n    }\n\n    setOperationAction(ISD::SETCC, MVT::f128, Custom);\n\n    setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f32, Expand);\n    setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f64, Expand);\n    setLoadExtAction(ISD::EXTLOAD, MVT::f128, MVT::f80, Expand);\n    setTruncStoreAction(MVT::f128, MVT::f32, Expand);\n    setTruncStoreAction(MVT::f128, MVT::f64, Expand);\n    setTruncStoreAction(MVT::f128, MVT::f80, Expand);\n  }\n\n  // Always use a library call for pow.\n  setOperationAction(ISD::FPOW             , MVT::f32  , Expand);\n  setOperationAction(ISD::FPOW             , MVT::f64  , Expand);\n  setOperationAction(ISD::FPOW             , MVT::f80  , Expand);\n  setOperationAction(ISD::FPOW             , MVT::f128 , Expand);\n\n  setOperationAction(ISD::FLOG, MVT::f80, Expand);\n  setOperationAction(ISD::FLOG2, MVT::f80, Expand);\n  setOperationAction(ISD::FLOG10, MVT::f80, Expand);\n  setOperationAction(ISD::FEXP, MVT::f80, Expand);\n  setOperationAction(ISD::FEXP2, MVT::f80, Expand);\n  setOperationAction(ISD::FMINNUM, MVT::f80, Expand);\n  setOperationAction(ISD::FMAXNUM, MVT::f80, Expand);\n\n  // Some FP actions are always expanded for vector types.\n  for (auto VT : { MVT::v4f32, MVT::v8f32, MVT::v16f32,\n                   MVT::v2f64, MVT::v4f64, MVT::v8f64 }) {\n    setOperationAction(ISD::FSIN,      VT, Expand);\n    setOperationAction(ISD::FSINCOS,   VT, Expand);\n    setOperationAction(ISD::FCOS,      VT, Expand);\n    setOperationAction(ISD::FREM,      VT, Expand);\n    setOperationAction(ISD::FCOPYSIGN, VT, Expand);\n    setOperationAction(ISD::FPOW,      VT, Expand);\n    setOperationAction(ISD::FLOG,      VT, Expand);\n    setOperationAction(ISD::FLOG2,     VT, Expand);\n    setOperationAction(ISD::FLOG10,    VT, Expand);\n    setOperationAction(ISD::FEXP,      VT, Expand);\n    setOperationAction(ISD::FEXP2,     VT, Expand);\n  }\n\n  // First set operation action for all vector types to either promote\n  // (for widening) or expand (for scalarization). Then we will selectively\n  // turn on ones that can be effectively codegen'd.\n  for (MVT VT : MVT::fixedlen_vector_valuetypes()) {\n    setOperationAction(ISD::SDIV, VT, Expand);\n    setOperationAction(ISD::UDIV, VT, Expand);\n    setOperationAction(ISD::SREM, VT, Expand);\n    setOperationAction(ISD::UREM, VT, Expand);\n    setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT,Expand);\n    setOperationAction(ISD::INSERT_VECTOR_ELT, VT, Expand);\n    setOperationAction(ISD::EXTRACT_SUBVECTOR, VT,Expand);\n    setOperationAction(ISD::INSERT_SUBVECTOR, VT,Expand);\n    setOperationAction(ISD::FMA,  VT, Expand);\n    setOperationAction(ISD::FFLOOR, VT, Expand);\n    setOperationAction(ISD::FCEIL, VT, Expand);\n    setOperationAction(ISD::FTRUNC, VT, Expand);\n    setOperationAction(ISD::FRINT, VT, Expand);\n    setOperationAction(ISD::FNEARBYINT, VT, Expand);\n    setOperationAction(ISD::SMUL_LOHI, VT, Expand);\n    setOperationAction(ISD::MULHS, VT, Expand);\n    setOperationAction(ISD::UMUL_LOHI, VT, Expand);\n    setOperationAction(ISD::MULHU, VT, Expand);\n    setOperationAction(ISD::SDIVREM, VT, Expand);\n    setOperationAction(ISD::UDIVREM, VT, Expand);\n    setOperationAction(ISD::CTPOP, VT, Expand);\n    setOperationAction(ISD::CTTZ, VT, Expand);\n    setOperationAction(ISD::CTLZ, VT, Expand);\n    setOperationAction(ISD::ROTL, VT, Expand);\n    setOperationAction(ISD::ROTR, VT, Expand);\n    setOperationAction(ISD::BSWAP, VT, Expand);\n    setOperationAction(ISD::SETCC, VT, Expand);\n    setOperationAction(ISD::FP_TO_UINT, VT, Expand);\n    setOperationAction(ISD::FP_TO_SINT, VT, Expand);\n    setOperationAction(ISD::UINT_TO_FP, VT, Expand);\n    setOperationAction(ISD::SINT_TO_FP, VT, Expand);\n    setOperationAction(ISD::SIGN_EXTEND_INREG, VT,Expand);\n    setOperationAction(ISD::TRUNCATE, VT, Expand);\n    setOperationAction(ISD::SIGN_EXTEND, VT, Expand);\n    setOperationAction(ISD::ZERO_EXTEND, VT, Expand);\n    setOperationAction(ISD::ANY_EXTEND, VT, Expand);\n    setOperationAction(ISD::SELECT_CC, VT, Expand);\n    for (MVT InnerVT : MVT::fixedlen_vector_valuetypes()) {\n      setTruncStoreAction(InnerVT, VT, Expand);\n\n      setLoadExtAction(ISD::SEXTLOAD, InnerVT, VT, Expand);\n      setLoadExtAction(ISD::ZEXTLOAD, InnerVT, VT, Expand);\n\n      // N.b. ISD::EXTLOAD legality is basically ignored except for i1-like\n      // types, we have to deal with them whether we ask for Expansion or not.\n      // Setting Expand causes its own optimisation problems though, so leave\n      // them legal.\n      if (VT.getVectorElementType() == MVT::i1)\n        setLoadExtAction(ISD::EXTLOAD, InnerVT, VT, Expand);\n\n      // EXTLOAD for MVT::f16 vectors is not legal because f16 vectors are\n      // split/scalarized right now.\n      if (VT.getVectorElementType() == MVT::f16)\n        setLoadExtAction(ISD::EXTLOAD, InnerVT, VT, Expand);\n    }\n  }\n\n  // FIXME: In order to prevent SSE instructions being expanded to MMX ones\n  // with -msoft-float, disable use of MMX as well.\n  if (!Subtarget.useSoftFloat() && Subtarget.hasMMX()) {\n    addRegisterClass(MVT::x86mmx, &X86::VR64RegClass);\n    // No operations on x86mmx supported, everything uses intrinsics.\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasSSE1()) {\n    addRegisterClass(MVT::v4f32, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n\n    setOperationAction(ISD::FNEG,               MVT::v4f32, Custom);\n    setOperationAction(ISD::FABS,               MVT::v4f32, Custom);\n    setOperationAction(ISD::FCOPYSIGN,          MVT::v4f32, Custom);\n    setOperationAction(ISD::BUILD_VECTOR,       MVT::v4f32, Custom);\n    setOperationAction(ISD::VECTOR_SHUFFLE,     MVT::v4f32, Custom);\n    setOperationAction(ISD::VSELECT,            MVT::v4f32, Custom);\n    setOperationAction(ISD::EXTRACT_VECTOR_ELT, MVT::v4f32, Custom);\n    setOperationAction(ISD::SELECT,             MVT::v4f32, Custom);\n\n    setOperationAction(ISD::LOAD,               MVT::v2f32, Custom);\n    setOperationAction(ISD::STORE,              MVT::v2f32, Custom);\n\n    setOperationAction(ISD::STRICT_FADD,        MVT::v4f32, Legal);\n    setOperationAction(ISD::STRICT_FSUB,        MVT::v4f32, Legal);\n    setOperationAction(ISD::STRICT_FMUL,        MVT::v4f32, Legal);\n    setOperationAction(ISD::STRICT_FDIV,        MVT::v4f32, Legal);\n    setOperationAction(ISD::STRICT_FSQRT,       MVT::v4f32, Legal);\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasSSE2()) {\n    addRegisterClass(MVT::v2f64, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n\n    // FIXME: Unfortunately, -soft-float and -no-implicit-float mean XMM\n    // registers cannot be used even for integer operations.\n    addRegisterClass(MVT::v16i8, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n    addRegisterClass(MVT::v8i16, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n    addRegisterClass(MVT::v4i32, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n    addRegisterClass(MVT::v2i64, Subtarget.hasVLX() ? &X86::VR128XRegClass\n                                                    : &X86::VR128RegClass);\n\n    for (auto VT : { MVT::v2i8, MVT::v4i8, MVT::v8i8,\n                     MVT::v2i16, MVT::v4i16, MVT::v2i32 }) {\n      setOperationAction(ISD::SDIV, VT, Custom);\n      setOperationAction(ISD::SREM, VT, Custom);\n      setOperationAction(ISD::UDIV, VT, Custom);\n      setOperationAction(ISD::UREM, VT, Custom);\n    }\n\n    setOperationAction(ISD::MUL,                MVT::v2i8,  Custom);\n    setOperationAction(ISD::MUL,                MVT::v4i8,  Custom);\n    setOperationAction(ISD::MUL,                MVT::v8i8,  Custom);\n\n    setOperationAction(ISD::MUL,                MVT::v16i8, Custom);\n    setOperationAction(ISD::MUL,                MVT::v4i32, Custom);\n    setOperationAction(ISD::MUL,                MVT::v2i64, Custom);\n    setOperationAction(ISD::MULHU,              MVT::v4i32, Custom);\n    setOperationAction(ISD::MULHS,              MVT::v4i32, Custom);\n    setOperationAction(ISD::MULHU,              MVT::v16i8, Custom);\n    setOperationAction(ISD::MULHS,              MVT::v16i8, Custom);\n    setOperationAction(ISD::MULHU,              MVT::v8i16, Legal);\n    setOperationAction(ISD::MULHS,              MVT::v8i16, Legal);\n    setOperationAction(ISD::MUL,                MVT::v8i16, Legal);\n    setOperationAction(ISD::FNEG,               MVT::v2f64, Custom);\n    setOperationAction(ISD::FABS,               MVT::v2f64, Custom);\n    setOperationAction(ISD::FCOPYSIGN,          MVT::v2f64, Custom);\n\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32, MVT::v2i64 }) {\n      setOperationAction(ISD::SMAX, VT, VT == MVT::v8i16 ? Legal : Custom);\n      setOperationAction(ISD::SMIN, VT, VT == MVT::v8i16 ? Legal : Custom);\n      setOperationAction(ISD::UMAX, VT, VT == MVT::v16i8 ? Legal : Custom);\n      setOperationAction(ISD::UMIN, VT, VT == MVT::v16i8 ? Legal : Custom);\n    }\n\n    setOperationAction(ISD::UADDSAT,            MVT::v16i8, Legal);\n    setOperationAction(ISD::SADDSAT,            MVT::v16i8, Legal);\n    setOperationAction(ISD::USUBSAT,            MVT::v16i8, Legal);\n    setOperationAction(ISD::SSUBSAT,            MVT::v16i8, Legal);\n    setOperationAction(ISD::UADDSAT,            MVT::v8i16, Legal);\n    setOperationAction(ISD::SADDSAT,            MVT::v8i16, Legal);\n    setOperationAction(ISD::USUBSAT,            MVT::v8i16, Legal);\n    setOperationAction(ISD::SSUBSAT,            MVT::v8i16, Legal);\n    setOperationAction(ISD::USUBSAT,            MVT::v4i32, Custom);\n    setOperationAction(ISD::USUBSAT,            MVT::v2i64, Custom);\n\n    setOperationAction(ISD::INSERT_VECTOR_ELT,  MVT::v8i16, Custom);\n    setOperationAction(ISD::INSERT_VECTOR_ELT,  MVT::v4i32, Custom);\n    setOperationAction(ISD::INSERT_VECTOR_ELT,  MVT::v4f32, Custom);\n\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32, MVT::v2i64 }) {\n      setOperationAction(ISD::SETCC,              VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCC,      VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCCS,     VT, Custom);\n      setOperationAction(ISD::CTPOP,              VT, Custom);\n      setOperationAction(ISD::ABS,                VT, Custom);\n\n      // The condition codes aren't legal in SSE/AVX and under AVX512 we use\n      // setcc all the way to isel and prefer SETGT in some isel patterns.\n      setCondCodeAction(ISD::SETLT, VT, Custom);\n      setCondCodeAction(ISD::SETLE, VT, Custom);\n    }\n\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32 }) {\n      setOperationAction(ISD::SCALAR_TO_VECTOR,   VT, Custom);\n      setOperationAction(ISD::BUILD_VECTOR,       VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,     VT, Custom);\n      setOperationAction(ISD::VSELECT,            VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n    }\n\n    for (auto VT : { MVT::v2f64, MVT::v2i64 }) {\n      setOperationAction(ISD::BUILD_VECTOR,       VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,     VT, Custom);\n      setOperationAction(ISD::VSELECT,            VT, Custom);\n\n      if (VT == MVT::v2i64 && !Subtarget.is64Bit())\n        continue;\n\n      setOperationAction(ISD::INSERT_VECTOR_ELT,  VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n    }\n\n    // Custom lower v2i64 and v2f64 selects.\n    setOperationAction(ISD::SELECT,             MVT::v2f64, Custom);\n    setOperationAction(ISD::SELECT,             MVT::v2i64, Custom);\n    setOperationAction(ISD::SELECT,             MVT::v4i32, Custom);\n    setOperationAction(ISD::SELECT,             MVT::v8i16, Custom);\n    setOperationAction(ISD::SELECT,             MVT::v16i8, Custom);\n\n    setOperationAction(ISD::FP_TO_SINT,         MVT::v4i32, Legal);\n    setOperationAction(ISD::FP_TO_SINT,         MVT::v2i32, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_SINT,  MVT::v4i32, Legal);\n    setOperationAction(ISD::STRICT_FP_TO_SINT,  MVT::v2i32, Custom);\n\n    // Custom legalize these to avoid over promotion or custom promotion.\n    for (auto VT : {MVT::v2i8, MVT::v4i8, MVT::v8i8, MVT::v2i16, MVT::v4i16}) {\n      setOperationAction(ISD::FP_TO_SINT,        VT, Custom);\n      setOperationAction(ISD::FP_TO_UINT,        VT, Custom);\n      setOperationAction(ISD::STRICT_FP_TO_SINT, VT, Custom);\n      setOperationAction(ISD::STRICT_FP_TO_UINT, VT, Custom);\n    }\n\n    setOperationAction(ISD::SINT_TO_FP,         MVT::v4i32, Legal);\n    setOperationAction(ISD::STRICT_SINT_TO_FP,  MVT::v4i32, Legal);\n    setOperationAction(ISD::SINT_TO_FP,         MVT::v2i32, Custom);\n    setOperationAction(ISD::STRICT_SINT_TO_FP,  MVT::v2i32, Custom);\n\n    setOperationAction(ISD::UINT_TO_FP,         MVT::v2i32, Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP,  MVT::v2i32, Custom);\n\n    setOperationAction(ISD::UINT_TO_FP,         MVT::v4i32, Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP,  MVT::v4i32, Custom);\n\n    // Fast v2f32 UINT_TO_FP( v2i32 ) custom conversion.\n    setOperationAction(ISD::SINT_TO_FP,         MVT::v2f32, Custom);\n    setOperationAction(ISD::STRICT_SINT_TO_FP,  MVT::v2f32, Custom);\n    setOperationAction(ISD::UINT_TO_FP,         MVT::v2f32, Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP,  MVT::v2f32, Custom);\n\n    setOperationAction(ISD::FP_EXTEND,          MVT::v2f32, Custom);\n    setOperationAction(ISD::STRICT_FP_EXTEND,   MVT::v2f32, Custom);\n    setOperationAction(ISD::FP_ROUND,           MVT::v2f32, Custom);\n    setOperationAction(ISD::STRICT_FP_ROUND,    MVT::v2f32, Custom);\n\n    // We want to legalize this to an f64 load rather than an i64 load on\n    // 64-bit targets and two 32-bit loads on a 32-bit target. Similar for\n    // store.\n    setOperationAction(ISD::LOAD,               MVT::v2i32, Custom);\n    setOperationAction(ISD::LOAD,               MVT::v4i16, Custom);\n    setOperationAction(ISD::LOAD,               MVT::v8i8,  Custom);\n    setOperationAction(ISD::STORE,              MVT::v2i32, Custom);\n    setOperationAction(ISD::STORE,              MVT::v4i16, Custom);\n    setOperationAction(ISD::STORE,              MVT::v8i8,  Custom);\n\n    setOperationAction(ISD::BITCAST,            MVT::v2i32, Custom);\n    setOperationAction(ISD::BITCAST,            MVT::v4i16, Custom);\n    setOperationAction(ISD::BITCAST,            MVT::v8i8,  Custom);\n    if (!Subtarget.hasAVX512())\n      setOperationAction(ISD::BITCAST, MVT::v16i1, Custom);\n\n    setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, MVT::v2i64, Custom);\n    setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, MVT::v4i32, Custom);\n    setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, MVT::v8i16, Custom);\n\n    setOperationAction(ISD::SIGN_EXTEND, MVT::v4i64, Custom);\n\n    setOperationAction(ISD::TRUNCATE,    MVT::v2i8,  Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v2i16, Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v2i32, Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v4i8,  Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v4i16, Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v8i8,  Custom);\n\n    // In the customized shift lowering, the legal v4i32/v2i64 cases\n    // in AVX2 will be recognized.\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32, MVT::v2i64 }) {\n      setOperationAction(ISD::SRL,              VT, Custom);\n      setOperationAction(ISD::SHL,              VT, Custom);\n      setOperationAction(ISD::SRA,              VT, Custom);\n    }\n\n    setOperationAction(ISD::ROTL,               MVT::v4i32, Custom);\n    setOperationAction(ISD::ROTL,               MVT::v8i16, Custom);\n\n    // With 512-bit registers or AVX512VL+BW, expanding (and promoting the\n    // shifts) is better.\n    if (!Subtarget.useAVX512Regs() &&\n        !(Subtarget.hasBWI() && Subtarget.hasVLX()))\n      setOperationAction(ISD::ROTL,             MVT::v16i8, Custom);\n\n    setOperationAction(ISD::STRICT_FSQRT,       MVT::v2f64, Legal);\n    setOperationAction(ISD::STRICT_FADD,        MVT::v2f64, Legal);\n    setOperationAction(ISD::STRICT_FSUB,        MVT::v2f64, Legal);\n    setOperationAction(ISD::STRICT_FMUL,        MVT::v2f64, Legal);\n    setOperationAction(ISD::STRICT_FDIV,        MVT::v2f64, Legal);\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasSSSE3()) {\n    setOperationAction(ISD::ABS,                MVT::v16i8, Legal);\n    setOperationAction(ISD::ABS,                MVT::v8i16, Legal);\n    setOperationAction(ISD::ABS,                MVT::v4i32, Legal);\n    setOperationAction(ISD::BITREVERSE,         MVT::v16i8, Custom);\n    setOperationAction(ISD::CTLZ,               MVT::v16i8, Custom);\n    setOperationAction(ISD::CTLZ,               MVT::v8i16, Custom);\n    setOperationAction(ISD::CTLZ,               MVT::v4i32, Custom);\n    setOperationAction(ISD::CTLZ,               MVT::v2i64, Custom);\n\n    // These might be better off as horizontal vector ops.\n    setOperationAction(ISD::ADD,                MVT::i16, Custom);\n    setOperationAction(ISD::ADD,                MVT::i32, Custom);\n    setOperationAction(ISD::SUB,                MVT::i16, Custom);\n    setOperationAction(ISD::SUB,                MVT::i32, Custom);\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasSSE41()) {\n    for (MVT RoundedTy : {MVT::f32, MVT::f64, MVT::v4f32, MVT::v2f64}) {\n      setOperationAction(ISD::FFLOOR,            RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FFLOOR,     RoundedTy,  Legal);\n      setOperationAction(ISD::FCEIL,             RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FCEIL,      RoundedTy,  Legal);\n      setOperationAction(ISD::FTRUNC,            RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FTRUNC,     RoundedTy,  Legal);\n      setOperationAction(ISD::FRINT,             RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FRINT,      RoundedTy,  Legal);\n      setOperationAction(ISD::FNEARBYINT,        RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FNEARBYINT, RoundedTy,  Legal);\n      setOperationAction(ISD::FROUNDEVEN,        RoundedTy,  Legal);\n      setOperationAction(ISD::STRICT_FROUNDEVEN, RoundedTy,  Legal);\n\n      setOperationAction(ISD::FROUND,            RoundedTy,  Custom);\n    }\n\n    setOperationAction(ISD::SMAX,               MVT::v16i8, Legal);\n    setOperationAction(ISD::SMAX,               MVT::v4i32, Legal);\n    setOperationAction(ISD::UMAX,               MVT::v8i16, Legal);\n    setOperationAction(ISD::UMAX,               MVT::v4i32, Legal);\n    setOperationAction(ISD::SMIN,               MVT::v16i8, Legal);\n    setOperationAction(ISD::SMIN,               MVT::v4i32, Legal);\n    setOperationAction(ISD::UMIN,               MVT::v8i16, Legal);\n    setOperationAction(ISD::UMIN,               MVT::v4i32, Legal);\n\n    setOperationAction(ISD::UADDSAT,            MVT::v4i32, Custom);\n\n    // FIXME: Do we need to handle scalar-to-vector here?\n    setOperationAction(ISD::MUL,                MVT::v4i32, Legal);\n\n    // We directly match byte blends in the backend as they match the VSELECT\n    // condition form.\n    setOperationAction(ISD::VSELECT,            MVT::v16i8, Legal);\n\n    // SSE41 brings specific instructions for doing vector sign extend even in\n    // cases where we don't have SRA.\n    for (auto VT : { MVT::v8i16, MVT::v4i32, MVT::v2i64 }) {\n      setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, VT, Legal);\n      setOperationAction(ISD::ZERO_EXTEND_VECTOR_INREG, VT, Legal);\n    }\n\n    // SSE41 also has vector sign/zero extending loads, PMOV[SZ]X\n    for (auto LoadExtOp : { ISD::SEXTLOAD, ISD::ZEXTLOAD }) {\n      setLoadExtAction(LoadExtOp, MVT::v8i16, MVT::v8i8,  Legal);\n      setLoadExtAction(LoadExtOp, MVT::v4i32, MVT::v4i8,  Legal);\n      setLoadExtAction(LoadExtOp, MVT::v2i64, MVT::v2i8,  Legal);\n      setLoadExtAction(LoadExtOp, MVT::v4i32, MVT::v4i16, Legal);\n      setLoadExtAction(LoadExtOp, MVT::v2i64, MVT::v2i16, Legal);\n      setLoadExtAction(LoadExtOp, MVT::v2i64, MVT::v2i32, Legal);\n    }\n\n    // i8 vectors are custom because the source register and source\n    // source memory operand types are not the same width.\n    setOperationAction(ISD::INSERT_VECTOR_ELT,  MVT::v16i8, Custom);\n\n    if (Subtarget.is64Bit() && !Subtarget.hasAVX512()) {\n      // We need to scalarize v4i64->v432 uint_to_fp using cvtsi2ss, but we can\n      // do the pre and post work in the vector domain.\n      setOperationAction(ISD::UINT_TO_FP,        MVT::v4i64, Custom);\n      setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v4i64, Custom);\n      // We need to mark SINT_TO_FP as Custom even though we want to expand it\n      // so that DAG combine doesn't try to turn it into uint_to_fp.\n      setOperationAction(ISD::SINT_TO_FP,        MVT::v4i64, Custom);\n      setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::v4i64, Custom);\n    }\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasSSE42()) {\n    setOperationAction(ISD::UADDSAT,            MVT::v2i64, Custom);\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasXOP()) {\n    for (auto VT : { MVT::v16i8, MVT::v8i16,  MVT::v4i32, MVT::v2i64,\n                     MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64 })\n      setOperationAction(ISD::ROTL, VT, Custom);\n\n    // XOP can efficiently perform BITREVERSE with VPPERM.\n    for (auto VT : { MVT::i8, MVT::i16, MVT::i32, MVT::i64 })\n      setOperationAction(ISD::BITREVERSE, VT, Custom);\n\n    for (auto VT : { MVT::v16i8, MVT::v8i16,  MVT::v4i32, MVT::v2i64,\n                     MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64 })\n      setOperationAction(ISD::BITREVERSE, VT, Custom);\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasAVX()) {\n    bool HasInt256 = Subtarget.hasInt256();\n\n    addRegisterClass(MVT::v32i8,  Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n    addRegisterClass(MVT::v16i16, Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n    addRegisterClass(MVT::v8i32,  Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n    addRegisterClass(MVT::v8f32,  Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n    addRegisterClass(MVT::v4i64,  Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n    addRegisterClass(MVT::v4f64,  Subtarget.hasVLX() ? &X86::VR256XRegClass\n                                                     : &X86::VR256RegClass);\n\n    for (auto VT : { MVT::v8f32, MVT::v4f64 }) {\n      setOperationAction(ISD::FFLOOR,            VT, Legal);\n      setOperationAction(ISD::STRICT_FFLOOR,     VT, Legal);\n      setOperationAction(ISD::FCEIL,             VT, Legal);\n      setOperationAction(ISD::STRICT_FCEIL,      VT, Legal);\n      setOperationAction(ISD::FTRUNC,            VT, Legal);\n      setOperationAction(ISD::STRICT_FTRUNC,     VT, Legal);\n      setOperationAction(ISD::FRINT,             VT, Legal);\n      setOperationAction(ISD::STRICT_FRINT,      VT, Legal);\n      setOperationAction(ISD::FNEARBYINT,        VT, Legal);\n      setOperationAction(ISD::STRICT_FNEARBYINT, VT, Legal);\n      setOperationAction(ISD::FROUNDEVEN,        VT, Legal);\n      setOperationAction(ISD::STRICT_FROUNDEVEN, VT, Legal);\n\n      setOperationAction(ISD::FROUND,            VT, Custom);\n\n      setOperationAction(ISD::FNEG,              VT, Custom);\n      setOperationAction(ISD::FABS,              VT, Custom);\n      setOperationAction(ISD::FCOPYSIGN,         VT, Custom);\n    }\n\n    // (fp_to_int:v8i16 (v8f32 ..)) requires the result type to be promoted\n    // even though v8i16 is a legal type.\n    setOperationPromotedToType(ISD::FP_TO_SINT,        MVT::v8i16, MVT::v8i32);\n    setOperationPromotedToType(ISD::FP_TO_UINT,        MVT::v8i16, MVT::v8i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_SINT, MVT::v8i16, MVT::v8i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_UINT, MVT::v8i16, MVT::v8i32);\n    setOperationAction(ISD::FP_TO_SINT,                MVT::v8i32, Legal);\n    setOperationAction(ISD::STRICT_FP_TO_SINT,         MVT::v8i32, Legal);\n\n    setOperationAction(ISD::SINT_TO_FP,         MVT::v8i32, Legal);\n    setOperationAction(ISD::STRICT_SINT_TO_FP,  MVT::v8i32, Legal);\n\n    setOperationAction(ISD::STRICT_FP_ROUND,    MVT::v4f32, Legal);\n    setOperationAction(ISD::STRICT_FADD,        MVT::v8f32, Legal);\n    setOperationAction(ISD::STRICT_FADD,        MVT::v4f64, Legal);\n    setOperationAction(ISD::STRICT_FSUB,        MVT::v8f32, Legal);\n    setOperationAction(ISD::STRICT_FSUB,        MVT::v4f64, Legal);\n    setOperationAction(ISD::STRICT_FMUL,        MVT::v8f32, Legal);\n    setOperationAction(ISD::STRICT_FMUL,        MVT::v4f64, Legal);\n    setOperationAction(ISD::STRICT_FDIV,        MVT::v8f32, Legal);\n    setOperationAction(ISD::STRICT_FDIV,        MVT::v4f64, Legal);\n    setOperationAction(ISD::STRICT_FP_EXTEND,   MVT::v4f64, Legal);\n    setOperationAction(ISD::STRICT_FSQRT,       MVT::v8f32, Legal);\n    setOperationAction(ISD::STRICT_FSQRT,       MVT::v4f64, Legal);\n\n    if (!Subtarget.hasAVX512())\n      setOperationAction(ISD::BITCAST, MVT::v32i1, Custom);\n\n    // In the customized shift lowering, the legal v8i32/v4i64 cases\n    // in AVX2 will be recognized.\n    for (auto VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64 }) {\n      setOperationAction(ISD::SRL, VT, Custom);\n      setOperationAction(ISD::SHL, VT, Custom);\n      setOperationAction(ISD::SRA, VT, Custom);\n    }\n\n    // These types need custom splitting if their input is a 128-bit vector.\n    setOperationAction(ISD::SIGN_EXTEND,       MVT::v8i64,  Custom);\n    setOperationAction(ISD::SIGN_EXTEND,       MVT::v16i32, Custom);\n    setOperationAction(ISD::ZERO_EXTEND,       MVT::v8i64,  Custom);\n    setOperationAction(ISD::ZERO_EXTEND,       MVT::v16i32, Custom);\n\n    setOperationAction(ISD::ROTL,              MVT::v8i32,  Custom);\n    setOperationAction(ISD::ROTL,              MVT::v16i16, Custom);\n\n    // With BWI, expanding (and promoting the shifts) is the better.\n    if (!Subtarget.useBWIRegs())\n      setOperationAction(ISD::ROTL,            MVT::v32i8,  Custom);\n\n    setOperationAction(ISD::SELECT,            MVT::v4f64, Custom);\n    setOperationAction(ISD::SELECT,            MVT::v4i64, Custom);\n    setOperationAction(ISD::SELECT,            MVT::v8i32, Custom);\n    setOperationAction(ISD::SELECT,            MVT::v16i16, Custom);\n    setOperationAction(ISD::SELECT,            MVT::v32i8, Custom);\n    setOperationAction(ISD::SELECT,            MVT::v8f32, Custom);\n\n    for (auto VT : { MVT::v16i16, MVT::v8i32, MVT::v4i64 }) {\n      setOperationAction(ISD::SIGN_EXTEND,     VT, Custom);\n      setOperationAction(ISD::ZERO_EXTEND,     VT, Custom);\n      setOperationAction(ISD::ANY_EXTEND,      VT, Custom);\n    }\n\n    setOperationAction(ISD::TRUNCATE,          MVT::v16i8, Custom);\n    setOperationAction(ISD::TRUNCATE,          MVT::v8i16, Custom);\n    setOperationAction(ISD::TRUNCATE,          MVT::v4i32, Custom);\n    setOperationAction(ISD::BITREVERSE,        MVT::v32i8, Custom);\n\n    for (auto VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64 }) {\n      setOperationAction(ISD::SETCC,           VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCC,   VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCCS,  VT, Custom);\n      setOperationAction(ISD::CTPOP,           VT, Custom);\n      setOperationAction(ISD::CTLZ,            VT, Custom);\n\n      // The condition codes aren't legal in SSE/AVX and under AVX512 we use\n      // setcc all the way to isel and prefer SETGT in some isel patterns.\n      setCondCodeAction(ISD::SETLT, VT, Custom);\n      setCondCodeAction(ISD::SETLE, VT, Custom);\n    }\n\n    if (Subtarget.hasAnyFMA()) {\n      for (auto VT : { MVT::f32, MVT::f64, MVT::v4f32, MVT::v8f32,\n                       MVT::v2f64, MVT::v4f64 }) {\n        setOperationAction(ISD::FMA, VT, Legal);\n        setOperationAction(ISD::STRICT_FMA, VT, Legal);\n      }\n    }\n\n    for (auto VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64 }) {\n      setOperationAction(ISD::ADD, VT, HasInt256 ? Legal : Custom);\n      setOperationAction(ISD::SUB, VT, HasInt256 ? Legal : Custom);\n    }\n\n    setOperationAction(ISD::MUL,       MVT::v4i64,  Custom);\n    setOperationAction(ISD::MUL,       MVT::v8i32,  HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::MUL,       MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::MUL,       MVT::v32i8,  Custom);\n\n    setOperationAction(ISD::MULHU,     MVT::v8i32,  Custom);\n    setOperationAction(ISD::MULHS,     MVT::v8i32,  Custom);\n    setOperationAction(ISD::MULHU,     MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::MULHS,     MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::MULHU,     MVT::v32i8,  Custom);\n    setOperationAction(ISD::MULHS,     MVT::v32i8,  Custom);\n\n    setOperationAction(ISD::ABS,       MVT::v4i64,  Custom);\n    setOperationAction(ISD::SMAX,      MVT::v4i64,  Custom);\n    setOperationAction(ISD::UMAX,      MVT::v4i64,  Custom);\n    setOperationAction(ISD::SMIN,      MVT::v4i64,  Custom);\n    setOperationAction(ISD::UMIN,      MVT::v4i64,  Custom);\n\n    setOperationAction(ISD::UADDSAT,   MVT::v32i8,  HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::SADDSAT,   MVT::v32i8,  HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::USUBSAT,   MVT::v32i8,  HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::SSUBSAT,   MVT::v32i8,  HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::UADDSAT,   MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::SADDSAT,   MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::USUBSAT,   MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::SSUBSAT,   MVT::v16i16, HasInt256 ? Legal : Custom);\n    setOperationAction(ISD::UADDSAT,   MVT::v8i32, Custom);\n    setOperationAction(ISD::USUBSAT,   MVT::v8i32, Custom);\n    setOperationAction(ISD::UADDSAT,   MVT::v4i64, Custom);\n    setOperationAction(ISD::USUBSAT,   MVT::v4i64, Custom);\n\n    for (auto VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32 }) {\n      setOperationAction(ISD::ABS,  VT, HasInt256 ? Legal : Custom);\n      setOperationAction(ISD::SMAX, VT, HasInt256 ? Legal : Custom);\n      setOperationAction(ISD::UMAX, VT, HasInt256 ? Legal : Custom);\n      setOperationAction(ISD::SMIN, VT, HasInt256 ? Legal : Custom);\n      setOperationAction(ISD::UMIN, VT, HasInt256 ? Legal : Custom);\n    }\n\n    for (auto VT : {MVT::v16i16, MVT::v8i32, MVT::v4i64}) {\n      setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, VT, Custom);\n      setOperationAction(ISD::ZERO_EXTEND_VECTOR_INREG, VT, Custom);\n    }\n\n    if (HasInt256) {\n      // The custom lowering for UINT_TO_FP for v8i32 becomes interesting\n      // when we have a 256bit-wide blend with immediate.\n      setOperationAction(ISD::UINT_TO_FP, MVT::v8i32, Custom);\n      setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v8i32, Custom);\n\n      // AVX2 also has wider vector sign/zero extending loads, VPMOV[SZ]X\n      for (auto LoadExtOp : { ISD::SEXTLOAD, ISD::ZEXTLOAD }) {\n        setLoadExtAction(LoadExtOp, MVT::v16i16, MVT::v16i8, Legal);\n        setLoadExtAction(LoadExtOp, MVT::v8i32,  MVT::v8i8,  Legal);\n        setLoadExtAction(LoadExtOp, MVT::v4i64,  MVT::v4i8,  Legal);\n        setLoadExtAction(LoadExtOp, MVT::v8i32,  MVT::v8i16, Legal);\n        setLoadExtAction(LoadExtOp, MVT::v4i64,  MVT::v4i16, Legal);\n        setLoadExtAction(LoadExtOp, MVT::v4i64,  MVT::v4i32, Legal);\n      }\n    }\n\n    for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64,\n                     MVT::v4f32, MVT::v8f32, MVT::v2f64, MVT::v4f64 }) {\n      setOperationAction(ISD::MLOAD,  VT, Subtarget.hasVLX() ? Legal : Custom);\n      setOperationAction(ISD::MSTORE, VT, Legal);\n    }\n\n    // Extract subvector is special because the value type\n    // (result) is 128-bit but the source is 256-bit wide.\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32, MVT::v2i64,\n                     MVT::v4f32, MVT::v2f64 }) {\n      setOperationAction(ISD::EXTRACT_SUBVECTOR, VT, Legal);\n    }\n\n    // Custom lower several nodes for 256-bit types.\n    for (MVT VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64,\n                    MVT::v8f32, MVT::v4f64 }) {\n      setOperationAction(ISD::BUILD_VECTOR,       VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,     VT, Custom);\n      setOperationAction(ISD::VSELECT,            VT, Custom);\n      setOperationAction(ISD::INSERT_VECTOR_ELT,  VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n      setOperationAction(ISD::SCALAR_TO_VECTOR,   VT, Custom);\n      setOperationAction(ISD::INSERT_SUBVECTOR,   VT, Legal);\n      setOperationAction(ISD::CONCAT_VECTORS,     VT, Custom);\n      setOperationAction(ISD::STORE,              VT, Custom);\n    }\n\n    if (HasInt256) {\n      setOperationAction(ISD::VSELECT, MVT::v32i8, Legal);\n\n      // Custom legalize 2x32 to get a little better code.\n      setOperationAction(ISD::MGATHER, MVT::v2f32, Custom);\n      setOperationAction(ISD::MGATHER, MVT::v2i32, Custom);\n\n      for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64,\n                       MVT::v4f32, MVT::v8f32, MVT::v2f64, MVT::v4f64 })\n        setOperationAction(ISD::MGATHER,  VT, Custom);\n    }\n  }\n\n  // This block controls legalization of the mask vector sizes that are\n  // available with AVX512. 512-bit vectors are in a separate block controlled\n  // by useAVX512Regs.\n  if (!Subtarget.useSoftFloat() && Subtarget.hasAVX512()) {\n    addRegisterClass(MVT::v1i1,   &X86::VK1RegClass);\n    addRegisterClass(MVT::v2i1,   &X86::VK2RegClass);\n    addRegisterClass(MVT::v4i1,   &X86::VK4RegClass);\n    addRegisterClass(MVT::v8i1,   &X86::VK8RegClass);\n    addRegisterClass(MVT::v16i1,  &X86::VK16RegClass);\n\n    setOperationAction(ISD::SELECT,             MVT::v1i1, Custom);\n    setOperationAction(ISD::EXTRACT_VECTOR_ELT, MVT::v1i1, Custom);\n    setOperationAction(ISD::BUILD_VECTOR,       MVT::v1i1, Custom);\n\n    setOperationPromotedToType(ISD::FP_TO_SINT,        MVT::v8i1,  MVT::v8i32);\n    setOperationPromotedToType(ISD::FP_TO_UINT,        MVT::v8i1,  MVT::v8i32);\n    setOperationPromotedToType(ISD::FP_TO_SINT,        MVT::v4i1,  MVT::v4i32);\n    setOperationPromotedToType(ISD::FP_TO_UINT,        MVT::v4i1,  MVT::v4i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_SINT, MVT::v8i1,  MVT::v8i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_UINT, MVT::v8i1,  MVT::v8i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_SINT, MVT::v4i1,  MVT::v4i32);\n    setOperationPromotedToType(ISD::STRICT_FP_TO_UINT, MVT::v4i1,  MVT::v4i32);\n    setOperationAction(ISD::FP_TO_SINT,                MVT::v2i1,  Custom);\n    setOperationAction(ISD::FP_TO_UINT,                MVT::v2i1,  Custom);\n    setOperationAction(ISD::STRICT_FP_TO_SINT,         MVT::v2i1,  Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT,         MVT::v2i1,  Custom);\n\n    // There is no byte sized k-register load or store without AVX512DQ.\n    if (!Subtarget.hasDQI()) {\n      setOperationAction(ISD::LOAD, MVT::v1i1, Custom);\n      setOperationAction(ISD::LOAD, MVT::v2i1, Custom);\n      setOperationAction(ISD::LOAD, MVT::v4i1, Custom);\n      setOperationAction(ISD::LOAD, MVT::v8i1, Custom);\n\n      setOperationAction(ISD::STORE, MVT::v1i1, Custom);\n      setOperationAction(ISD::STORE, MVT::v2i1, Custom);\n      setOperationAction(ISD::STORE, MVT::v4i1, Custom);\n      setOperationAction(ISD::STORE, MVT::v8i1, Custom);\n    }\n\n    // Extends of v16i1/v8i1/v4i1/v2i1 to 128-bit vectors.\n    for (auto VT : { MVT::v16i8, MVT::v8i16, MVT::v4i32, MVT::v2i64 }) {\n      setOperationAction(ISD::SIGN_EXTEND, VT, Custom);\n      setOperationAction(ISD::ZERO_EXTEND, VT, Custom);\n      setOperationAction(ISD::ANY_EXTEND,  VT, Custom);\n    }\n\n    for (auto VT : { MVT::v1i1, MVT::v2i1, MVT::v4i1, MVT::v8i1, MVT::v16i1 }) {\n      setOperationAction(ISD::ADD,              VT, Custom);\n      setOperationAction(ISD::SUB,              VT, Custom);\n      setOperationAction(ISD::MUL,              VT, Custom);\n      setOperationAction(ISD::UADDSAT,          VT, Custom);\n      setOperationAction(ISD::SADDSAT,          VT, Custom);\n      setOperationAction(ISD::USUBSAT,          VT, Custom);\n      setOperationAction(ISD::SSUBSAT,          VT, Custom);\n      setOperationAction(ISD::VSELECT,          VT,  Expand);\n    }\n\n    for (auto VT : { MVT::v2i1, MVT::v4i1, MVT::v8i1, MVT::v16i1 }) {\n      setOperationAction(ISD::SETCC,            VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCC,    VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCCS,   VT, Custom);\n      setOperationAction(ISD::SELECT,           VT, Custom);\n      setOperationAction(ISD::TRUNCATE,         VT, Custom);\n\n      setOperationAction(ISD::BUILD_VECTOR,     VT, Custom);\n      setOperationAction(ISD::CONCAT_VECTORS,   VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n      setOperationAction(ISD::INSERT_SUBVECTOR, VT, Custom);\n      setOperationAction(ISD::INSERT_VECTOR_ELT, VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,   VT,  Custom);\n    }\n\n    for (auto VT : { MVT::v1i1, MVT::v2i1, MVT::v4i1, MVT::v8i1 })\n      setOperationAction(ISD::EXTRACT_SUBVECTOR, VT, Custom);\n  }\n\n  // This block controls legalization for 512-bit operations with 32/64 bit\n  // elements. 512-bits can be disabled based on prefer-vector-width and\n  // required-vector-width function attributes.\n  if (!Subtarget.useSoftFloat() && Subtarget.useAVX512Regs()) {\n    bool HasBWI = Subtarget.hasBWI();\n\n    addRegisterClass(MVT::v16i32, &X86::VR512RegClass);\n    addRegisterClass(MVT::v16f32, &X86::VR512RegClass);\n    addRegisterClass(MVT::v8i64,  &X86::VR512RegClass);\n    addRegisterClass(MVT::v8f64,  &X86::VR512RegClass);\n    addRegisterClass(MVT::v32i16, &X86::VR512RegClass);\n    addRegisterClass(MVT::v64i8,  &X86::VR512RegClass);\n\n    for (auto ExtType : {ISD::ZEXTLOAD, ISD::SEXTLOAD}) {\n      setLoadExtAction(ExtType, MVT::v16i32, MVT::v16i8,  Legal);\n      setLoadExtAction(ExtType, MVT::v16i32, MVT::v16i16, Legal);\n      setLoadExtAction(ExtType, MVT::v8i64,  MVT::v8i8,   Legal);\n      setLoadExtAction(ExtType, MVT::v8i64,  MVT::v8i16,  Legal);\n      setLoadExtAction(ExtType, MVT::v8i64,  MVT::v8i32,  Legal);\n      if (HasBWI)\n        setLoadExtAction(ExtType, MVT::v32i16, MVT::v32i8, Legal);\n    }\n\n    for (MVT VT : { MVT::v16f32, MVT::v8f64 }) {\n      setOperationAction(ISD::FNEG,  VT, Custom);\n      setOperationAction(ISD::FABS,  VT, Custom);\n      setOperationAction(ISD::FMA,   VT, Legal);\n      setOperationAction(ISD::STRICT_FMA, VT, Legal);\n      setOperationAction(ISD::FCOPYSIGN, VT, Custom);\n    }\n\n    for (MVT VT : { MVT::v16i1, MVT::v16i8, MVT::v16i16 }) {\n      setOperationPromotedToType(ISD::FP_TO_SINT       , VT, MVT::v16i32);\n      setOperationPromotedToType(ISD::FP_TO_UINT       , VT, MVT::v16i32);\n      setOperationPromotedToType(ISD::STRICT_FP_TO_SINT, VT, MVT::v16i32);\n      setOperationPromotedToType(ISD::STRICT_FP_TO_UINT, VT, MVT::v16i32);\n    }\n    setOperationAction(ISD::FP_TO_SINT,        MVT::v16i32, Legal);\n    setOperationAction(ISD::FP_TO_UINT,        MVT::v16i32, Legal);\n    setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::v16i32, Legal);\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::v16i32, Legal);\n    setOperationAction(ISD::SINT_TO_FP,        MVT::v16i32, Legal);\n    setOperationAction(ISD::UINT_TO_FP,        MVT::v16i32, Legal);\n    setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::v16i32, Legal);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v16i32, Legal);\n\n    setOperationAction(ISD::STRICT_FADD,      MVT::v16f32, Legal);\n    setOperationAction(ISD::STRICT_FADD,      MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FSUB,      MVT::v16f32, Legal);\n    setOperationAction(ISD::STRICT_FSUB,      MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FMUL,      MVT::v16f32, Legal);\n    setOperationAction(ISD::STRICT_FMUL,      MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FDIV,      MVT::v16f32, Legal);\n    setOperationAction(ISD::STRICT_FDIV,      MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FSQRT,     MVT::v16f32, Legal);\n    setOperationAction(ISD::STRICT_FSQRT,     MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FP_EXTEND, MVT::v8f64,  Legal);\n    setOperationAction(ISD::STRICT_FP_ROUND,  MVT::v8f32,  Legal);\n\n    setTruncStoreAction(MVT::v8i64,   MVT::v8i8,   Legal);\n    setTruncStoreAction(MVT::v8i64,   MVT::v8i16,  Legal);\n    setTruncStoreAction(MVT::v8i64,   MVT::v8i32,  Legal);\n    setTruncStoreAction(MVT::v16i32,  MVT::v16i8,  Legal);\n    setTruncStoreAction(MVT::v16i32,  MVT::v16i16, Legal);\n    if (HasBWI)\n      setTruncStoreAction(MVT::v32i16,  MVT::v32i8, Legal);\n\n    // With 512-bit vectors and no VLX, we prefer to widen MLOAD/MSTORE\n    // to 512-bit rather than use the AVX2 instructions so that we can use\n    // k-masks.\n    if (!Subtarget.hasVLX()) {\n      for (auto VT : {MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64,\n           MVT::v4f32, MVT::v8f32, MVT::v2f64, MVT::v4f64}) {\n        setOperationAction(ISD::MLOAD,  VT, Custom);\n        setOperationAction(ISD::MSTORE, VT, Custom);\n      }\n    }\n\n    setOperationAction(ISD::TRUNCATE,    MVT::v8i32,  Legal);\n    setOperationAction(ISD::TRUNCATE,    MVT::v16i16, Legal);\n    setOperationAction(ISD::TRUNCATE,    MVT::v32i8,  HasBWI ? Legal : Custom);\n    setOperationAction(ISD::TRUNCATE,    MVT::v16i64, Custom);\n    setOperationAction(ISD::ZERO_EXTEND, MVT::v32i16, Custom);\n    setOperationAction(ISD::ZERO_EXTEND, MVT::v16i32, Custom);\n    setOperationAction(ISD::ZERO_EXTEND, MVT::v8i64,  Custom);\n    setOperationAction(ISD::ANY_EXTEND,  MVT::v32i16, Custom);\n    setOperationAction(ISD::ANY_EXTEND,  MVT::v16i32, Custom);\n    setOperationAction(ISD::ANY_EXTEND,  MVT::v8i64,  Custom);\n    setOperationAction(ISD::SIGN_EXTEND, MVT::v32i16, Custom);\n    setOperationAction(ISD::SIGN_EXTEND, MVT::v16i32, Custom);\n    setOperationAction(ISD::SIGN_EXTEND, MVT::v8i64,  Custom);\n\n    if (HasBWI) {\n      // Extends from v64i1 masks to 512-bit vectors.\n      setOperationAction(ISD::SIGN_EXTEND,        MVT::v64i8, Custom);\n      setOperationAction(ISD::ZERO_EXTEND,        MVT::v64i8, Custom);\n      setOperationAction(ISD::ANY_EXTEND,         MVT::v64i8, Custom);\n    }\n\n    for (auto VT : { MVT::v16f32, MVT::v8f64 }) {\n      setOperationAction(ISD::FFLOOR,            VT, Legal);\n      setOperationAction(ISD::STRICT_FFLOOR,     VT, Legal);\n      setOperationAction(ISD::FCEIL,             VT, Legal);\n      setOperationAction(ISD::STRICT_FCEIL,      VT, Legal);\n      setOperationAction(ISD::FTRUNC,            VT, Legal);\n      setOperationAction(ISD::STRICT_FTRUNC,     VT, Legal);\n      setOperationAction(ISD::FRINT,             VT, Legal);\n      setOperationAction(ISD::STRICT_FRINT,      VT, Legal);\n      setOperationAction(ISD::FNEARBYINT,        VT, Legal);\n      setOperationAction(ISD::STRICT_FNEARBYINT, VT, Legal);\n      setOperationAction(ISD::FROUNDEVEN,        VT, Legal);\n      setOperationAction(ISD::STRICT_FROUNDEVEN, VT, Legal);\n\n      setOperationAction(ISD::FROUND,            VT, Custom);\n    }\n\n    for (auto VT : {MVT::v32i16, MVT::v16i32, MVT::v8i64}) {\n      setOperationAction(ISD::SIGN_EXTEND_VECTOR_INREG, VT, Custom);\n      setOperationAction(ISD::ZERO_EXTEND_VECTOR_INREG, VT, Custom);\n    }\n\n    setOperationAction(ISD::ADD, MVT::v32i16, HasBWI ? Legal : Custom);\n    setOperationAction(ISD::SUB, MVT::v32i16, HasBWI ? Legal : Custom);\n    setOperationAction(ISD::ADD, MVT::v64i8,  HasBWI ? Legal : Custom);\n    setOperationAction(ISD::SUB, MVT::v64i8,  HasBWI ? Legal : Custom);\n\n    setOperationAction(ISD::MUL, MVT::v8i64,  Custom);\n    setOperationAction(ISD::MUL, MVT::v16i32, Legal);\n    setOperationAction(ISD::MUL, MVT::v32i16, HasBWI ? Legal : Custom);\n    setOperationAction(ISD::MUL, MVT::v64i8,  Custom);\n\n    setOperationAction(ISD::MULHU, MVT::v16i32, Custom);\n    setOperationAction(ISD::MULHS, MVT::v16i32, Custom);\n    setOperationAction(ISD::MULHS, MVT::v32i16, HasBWI ? Legal : Custom);\n    setOperationAction(ISD::MULHU, MVT::v32i16, HasBWI ? Legal : Custom);\n    setOperationAction(ISD::MULHS, MVT::v64i8,  Custom);\n    setOperationAction(ISD::MULHU, MVT::v64i8,  Custom);\n\n    setOperationAction(ISD::BITREVERSE, MVT::v64i8,  Custom);\n\n    for (auto VT : { MVT::v64i8, MVT::v32i16, MVT::v16i32, MVT::v8i64 }) {\n      setOperationAction(ISD::SRL,              VT, Custom);\n      setOperationAction(ISD::SHL,              VT, Custom);\n      setOperationAction(ISD::SRA,              VT, Custom);\n      setOperationAction(ISD::SETCC,            VT, Custom);\n\n      // The condition codes aren't legal in SSE/AVX and under AVX512 we use\n      // setcc all the way to isel and prefer SETGT in some isel patterns.\n      setCondCodeAction(ISD::SETLT, VT, Custom);\n      setCondCodeAction(ISD::SETLE, VT, Custom);\n    }\n    for (auto VT : { MVT::v16i32, MVT::v8i64 }) {\n      setOperationAction(ISD::SMAX,             VT, Legal);\n      setOperationAction(ISD::UMAX,             VT, Legal);\n      setOperationAction(ISD::SMIN,             VT, Legal);\n      setOperationAction(ISD::UMIN,             VT, Legal);\n      setOperationAction(ISD::ABS,              VT, Legal);\n      setOperationAction(ISD::CTPOP,            VT, Custom);\n      setOperationAction(ISD::ROTL,             VT, Custom);\n      setOperationAction(ISD::ROTR,             VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCC,    VT, Custom);\n      setOperationAction(ISD::STRICT_FSETCCS,   VT, Custom);\n    }\n\n    for (auto VT : { MVT::v64i8, MVT::v32i16 }) {\n      setOperationAction(ISD::ABS,     VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::CTPOP,   VT, Subtarget.hasBITALG() ? Legal : Custom);\n      setOperationAction(ISD::CTLZ,    VT, Custom);\n      setOperationAction(ISD::SMAX,    VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::UMAX,    VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::SMIN,    VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::UMIN,    VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::UADDSAT, VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::SADDSAT, VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::USUBSAT, VT, HasBWI ? Legal : Custom);\n      setOperationAction(ISD::SSUBSAT, VT, HasBWI ? Legal : Custom);\n    }\n\n    if (Subtarget.hasDQI()) {\n      setOperationAction(ISD::SINT_TO_FP, MVT::v8i64, Legal);\n      setOperationAction(ISD::UINT_TO_FP, MVT::v8i64, Legal);\n      setOperationAction(ISD::STRICT_SINT_TO_FP, MVT::v8i64, Legal);\n      setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v8i64, Legal);\n      setOperationAction(ISD::FP_TO_SINT, MVT::v8i64, Legal);\n      setOperationAction(ISD::FP_TO_UINT, MVT::v8i64, Legal);\n      setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::v8i64, Legal);\n      setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::v8i64, Legal);\n\n      setOperationAction(ISD::MUL,        MVT::v8i64, Legal);\n    }\n\n    if (Subtarget.hasCDI()) {\n      // NonVLX sub-targets extend 128/256 vectors to use the 512 version.\n      for (auto VT : { MVT::v16i32, MVT::v8i64} ) {\n        setOperationAction(ISD::CTLZ,            VT, Legal);\n      }\n    } // Subtarget.hasCDI()\n\n    if (Subtarget.hasVPOPCNTDQ()) {\n      for (auto VT : { MVT::v16i32, MVT::v8i64 })\n        setOperationAction(ISD::CTPOP, VT, Legal);\n    }\n\n    // Extract subvector is special because the value type\n    // (result) is 256-bit but the source is 512-bit wide.\n    // 128-bit was made Legal under AVX1.\n    for (auto VT : { MVT::v32i8, MVT::v16i16, MVT::v8i32, MVT::v4i64,\n                     MVT::v8f32, MVT::v4f64 })\n      setOperationAction(ISD::EXTRACT_SUBVECTOR, VT, Legal);\n\n    for (auto VT : { MVT::v64i8, MVT::v32i16, MVT::v16i32, MVT::v8i64,\n                     MVT::v16f32, MVT::v8f64 }) {\n      setOperationAction(ISD::CONCAT_VECTORS,     VT, Custom);\n      setOperationAction(ISD::INSERT_SUBVECTOR,   VT, Legal);\n      setOperationAction(ISD::SELECT,             VT, Custom);\n      setOperationAction(ISD::VSELECT,            VT, Custom);\n      setOperationAction(ISD::BUILD_VECTOR,       VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,     VT, Custom);\n      setOperationAction(ISD::SCALAR_TO_VECTOR,   VT, Custom);\n      setOperationAction(ISD::INSERT_VECTOR_ELT,  VT, Custom);\n    }\n\n    for (auto VT : { MVT::v16i32, MVT::v8i64, MVT::v16f32, MVT::v8f64 }) {\n      setOperationAction(ISD::MLOAD,               VT, Legal);\n      setOperationAction(ISD::MSTORE,              VT, Legal);\n      setOperationAction(ISD::MGATHER,             VT, Custom);\n      setOperationAction(ISD::MSCATTER,            VT, Custom);\n    }\n    if (HasBWI) {\n      for (auto VT : { MVT::v64i8, MVT::v32i16 }) {\n        setOperationAction(ISD::MLOAD,        VT, Legal);\n        setOperationAction(ISD::MSTORE,       VT, Legal);\n      }\n    } else {\n      setOperationAction(ISD::STORE, MVT::v32i16, Custom);\n      setOperationAction(ISD::STORE, MVT::v64i8,  Custom);\n    }\n\n    if (Subtarget.hasVBMI2()) {\n      for (auto VT : { MVT::v8i16, MVT::v4i32, MVT::v2i64,\n                       MVT::v16i16, MVT::v8i32, MVT::v4i64,\n                       MVT::v32i16, MVT::v16i32, MVT::v8i64 }) {\n        setOperationAction(ISD::FSHL, VT, Custom);\n        setOperationAction(ISD::FSHR, VT, Custom);\n      }\n\n      setOperationAction(ISD::ROTL, MVT::v32i16, Custom);\n      setOperationAction(ISD::ROTR, MVT::v8i16,  Custom);\n      setOperationAction(ISD::ROTR, MVT::v16i16, Custom);\n      setOperationAction(ISD::ROTR, MVT::v32i16, Custom);\n    }\n  }// useAVX512Regs\n\n  // This block controls legalization for operations that don't have\n  // pre-AVX512 equivalents. Without VLX we use 512-bit operations for\n  // narrower widths.\n  if (!Subtarget.useSoftFloat() && Subtarget.hasAVX512()) {\n    // These operations are handled on non-VLX by artificially widening in\n    // isel patterns.\n\n    setOperationAction(ISD::FP_TO_UINT, MVT::v8i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::FP_TO_UINT, MVT::v4i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::FP_TO_UINT,         MVT::v2i32, Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::v8i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::v4i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::STRICT_FP_TO_UINT,  MVT::v2i32, Custom);\n    setOperationAction(ISD::UINT_TO_FP, MVT::v8i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::UINT_TO_FP, MVT::v4i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v8i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n    setOperationAction(ISD::STRICT_UINT_TO_FP, MVT::v4i32,\n                       Subtarget.hasVLX() ? Legal : Custom);\n\n    if (Subtarget.hasDQI()) {\n      // Fast v2f32 SINT_TO_FP( v2i64 ) custom conversion.\n      // v2f32 UINT_TO_FP is already custom under SSE2.\n      assert(isOperationCustom(ISD::UINT_TO_FP, MVT::v2f32) &&\n             isOperationCustom(ISD::STRICT_UINT_TO_FP, MVT::v2f32) &&\n             \"Unexpected operation action!\");\n      // v2i64 FP_TO_S/UINT(v2f32) custom conversion.\n      setOperationAction(ISD::FP_TO_SINT,        MVT::v2f32, Custom);\n      setOperationAction(ISD::FP_TO_UINT,        MVT::v2f32, Custom);\n      setOperationAction(ISD::STRICT_FP_TO_SINT, MVT::v2f32, Custom);\n      setOperationAction(ISD::STRICT_FP_TO_UINT, MVT::v2f32, Custom);\n    }\n\n    for (auto VT : { MVT::v2i64, MVT::v4i64 }) {\n      setOperationAction(ISD::SMAX, VT, Legal);\n      setOperationAction(ISD::UMAX, VT, Legal);\n      setOperationAction(ISD::SMIN, VT, Legal);\n      setOperationAction(ISD::UMIN, VT, Legal);\n      setOperationAction(ISD::ABS,  VT, Legal);\n    }\n\n    for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64 }) {\n      setOperationAction(ISD::ROTL,     VT, Custom);\n      setOperationAction(ISD::ROTR,     VT, Custom);\n    }\n\n    // Custom legalize 2x32 to get a little better code.\n    setOperationAction(ISD::MSCATTER, MVT::v2f32, Custom);\n    setOperationAction(ISD::MSCATTER, MVT::v2i32, Custom);\n\n    for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64,\n                     MVT::v4f32, MVT::v8f32, MVT::v2f64, MVT::v4f64 })\n      setOperationAction(ISD::MSCATTER, VT, Custom);\n\n    if (Subtarget.hasDQI()) {\n      for (auto VT : { MVT::v2i64, MVT::v4i64 }) {\n        setOperationAction(ISD::SINT_TO_FP, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::UINT_TO_FP, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::STRICT_SINT_TO_FP, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::STRICT_UINT_TO_FP, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::FP_TO_SINT, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::FP_TO_UINT, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::STRICT_FP_TO_SINT, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::STRICT_FP_TO_UINT, VT,\n                           Subtarget.hasVLX() ? Legal : Custom);\n        setOperationAction(ISD::MUL,               VT, Legal);\n      }\n    }\n\n    if (Subtarget.hasCDI()) {\n      for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64 }) {\n        setOperationAction(ISD::CTLZ,            VT, Legal);\n      }\n    } // Subtarget.hasCDI()\n\n    if (Subtarget.hasVPOPCNTDQ()) {\n      for (auto VT : { MVT::v4i32, MVT::v8i32, MVT::v2i64, MVT::v4i64 })\n        setOperationAction(ISD::CTPOP, VT, Legal);\n    }\n  }\n\n  // This block control legalization of v32i1/v64i1 which are available with\n  // AVX512BW. 512-bit v32i16 and v64i8 vector legalization is controlled with\n  // useBWIRegs.\n  if (!Subtarget.useSoftFloat() && Subtarget.hasBWI()) {\n    addRegisterClass(MVT::v32i1,  &X86::VK32RegClass);\n    addRegisterClass(MVT::v64i1,  &X86::VK64RegClass);\n\n    for (auto VT : { MVT::v32i1, MVT::v64i1 }) {\n      setOperationAction(ISD::ADD,                VT, Custom);\n      setOperationAction(ISD::SUB,                VT, Custom);\n      setOperationAction(ISD::MUL,                VT, Custom);\n      setOperationAction(ISD::VSELECT,            VT, Expand);\n      setOperationAction(ISD::UADDSAT,            VT, Custom);\n      setOperationAction(ISD::SADDSAT,            VT, Custom);\n      setOperationAction(ISD::USUBSAT,            VT, Custom);\n      setOperationAction(ISD::SSUBSAT,            VT, Custom);\n\n      setOperationAction(ISD::TRUNCATE,           VT, Custom);\n      setOperationAction(ISD::SETCC,              VT, Custom);\n      setOperationAction(ISD::EXTRACT_VECTOR_ELT, VT, Custom);\n      setOperationAction(ISD::INSERT_VECTOR_ELT,  VT, Custom);\n      setOperationAction(ISD::SELECT,             VT, Custom);\n      setOperationAction(ISD::BUILD_VECTOR,       VT, Custom);\n      setOperationAction(ISD::VECTOR_SHUFFLE,     VT, Custom);\n      setOperationAction(ISD::CONCAT_VECTORS,     VT, Custom);\n      setOperationAction(ISD::INSERT_SUBVECTOR,   VT, Custom);\n    }\n\n    for (auto VT : { MVT::v16i1, MVT::v32i1 })\n      setOperationAction(ISD::EXTRACT_SUBVECTOR, VT, Custom);\n\n    // Extends from v32i1 masks to 256-bit vectors.\n    setOperationAction(ISD::SIGN_EXTEND,        MVT::v32i8, Custom);\n    setOperationAction(ISD::ZERO_EXTEND,        MVT::v32i8, Custom);\n    setOperationAction(ISD::ANY_EXTEND,         MVT::v32i8, Custom);\n\n    for (auto VT : { MVT::v32i8, MVT::v16i8, MVT::v16i16, MVT::v8i16 }) {\n      setOperationAction(ISD::MLOAD,  VT, Subtarget.hasVLX() ? Legal : Custom);\n      setOperationAction(ISD::MSTORE, VT, Subtarget.hasVLX() ? Legal : Custom);\n    }\n\n    // These operations are handled on non-VLX by artificially widening in\n    // isel patterns.\n    // TODO: Custom widen in lowering on non-VLX and drop the isel patterns?\n\n    if (Subtarget.hasBITALG()) {\n      for (auto VT : { MVT::v16i8, MVT::v32i8, MVT::v8i16, MVT::v16i16 })\n        setOperationAction(ISD::CTPOP, VT, Legal);\n    }\n  }\n\n  if (!Subtarget.useSoftFloat() && Subtarget.hasVLX()) {\n    setTruncStoreAction(MVT::v4i64, MVT::v4i8,  Legal);\n    setTruncStoreAction(MVT::v4i64, MVT::v4i16, Legal);\n    setTruncStoreAction(MVT::v4i64, MVT::v4i32, Legal);\n    setTruncStoreAction(MVT::v8i32, MVT::v8i8,  Legal);\n    setTruncStoreAction(MVT::v8i32, MVT::v8i16, Legal);\n\n    setTruncStoreAction(MVT::v2i64, MVT::v2i8,  Legal);\n    setTruncStoreAction(MVT::v2i64, MVT::v2i16, Legal);\n    setTruncStoreAction(MVT::v2i64, MVT::v2i32, Legal);\n    setTruncStoreAction(MVT::v4i32, MVT::v4i8,  Legal);\n    setTruncStoreAction(MVT::v4i32, MVT::v4i16, Legal);\n\n    if (Subtarget.hasBWI()) {\n      setTruncStoreAction(MVT::v16i16,  MVT::v16i8, Legal);\n      setTruncStoreAction(MVT::v8i16,   MVT::v8i8,  Legal);\n    }\n\n    setOperationAction(ISD::TRUNCATE, MVT::v16i32, Custom);\n    setOperationAction(ISD::TRUNCATE, MVT::v8i64, Custom);\n    setOperationAction(ISD::TRUNCATE, MVT::v16i64, Custom);\n  }\n\n  if (Subtarget.hasAMXTILE()) {\n    addRegisterClass(MVT::x86amx, &X86::TILERegClass);\n  }\n\n  // We want to custom lower some of our intrinsics.\n  setOperationAction(ISD::INTRINSIC_WO_CHAIN, MVT::Other, Custom);\n  setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::Other, Custom);\n  setOperationAction(ISD::INTRINSIC_VOID, MVT::Other, Custom);\n  if (!Subtarget.is64Bit()) {\n    setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::i64, Custom);\n  }\n\n  // Only custom-lower 64-bit SADDO and friends on 64-bit because we don't\n  // handle type legalization for these operations here.\n  //\n  // FIXME: We really should do custom legalization for addition and\n  // subtraction on x86-32 once PR3203 is fixed.  We really can't do much better\n  // than generic legalization for 64-bit multiplication-with-overflow, though.\n  for (auto VT : { MVT::i8, MVT::i16, MVT::i32, MVT::i64 }) {\n    if (VT == MVT::i64 && !Subtarget.is64Bit())\n      continue;\n    // Add/Sub/Mul with overflow operations are custom lowered.\n    setOperationAction(ISD::SADDO, VT, Custom);\n    setOperationAction(ISD::UADDO, VT, Custom);\n    setOperationAction(ISD::SSUBO, VT, Custom);\n    setOperationAction(ISD::USUBO, VT, Custom);\n    setOperationAction(ISD::SMULO, VT, Custom);\n    setOperationAction(ISD::UMULO, VT, Custom);\n\n    // Support carry in as value rather than glue.\n    setOperationAction(ISD::ADDCARRY, VT, Custom);\n    setOperationAction(ISD::SUBCARRY, VT, Custom);\n    setOperationAction(ISD::SETCCCARRY, VT, Custom);\n    setOperationAction(ISD::SADDO_CARRY, VT, Custom);\n    setOperationAction(ISD::SSUBO_CARRY, VT, Custom);\n  }\n\n  if (!Subtarget.is64Bit()) {\n    // These libcalls are not available in 32-bit.\n    setLibcallName(RTLIB::SHL_I128, nullptr);\n    setLibcallName(RTLIB::SRL_I128, nullptr);\n    setLibcallName(RTLIB::SRA_I128, nullptr);\n    setLibcallName(RTLIB::MUL_I128, nullptr);\n  }\n\n  // Combine sin / cos into _sincos_stret if it is available.\n  if (getLibcallName(RTLIB::SINCOS_STRET_F32) != nullptr &&\n      getLibcallName(RTLIB::SINCOS_STRET_F64) != nullptr) {\n    setOperationAction(ISD::FSINCOS, MVT::f64, Custom);\n    setOperationAction(ISD::FSINCOS, MVT::f32, Custom);\n  }\n\n  if (Subtarget.isTargetWin64()) {\n    setOperationAction(ISD::SDIV, MVT::i128, Custom);\n    setOperationAction(ISD::UDIV, MVT::i128, Custom);\n    setOperationAction(ISD::SREM, MVT::i128, Custom);\n    setOperationAction(ISD::UREM, MVT::i128, Custom);\n  }\n\n  // On 32 bit MSVC, `fmodf(f32)` is not defined - only `fmod(f64)`\n  // is. We should promote the value to 64-bits to solve this.\n  // This is what the CRT headers do - `fmodf` is an inline header\n  // function casting to f64 and calling `fmod`.\n  if (Subtarget.is32Bit() &&\n      (Subtarget.isTargetWindowsMSVC() || Subtarget.isTargetWindowsItanium()))\n    for (ISD::NodeType Op :\n         {ISD::FCEIL,  ISD::STRICT_FCEIL,\n          ISD::FCOS,   ISD::STRICT_FCOS,\n          ISD::FEXP,   ISD::STRICT_FEXP,\n          ISD::FFLOOR, ISD::STRICT_FFLOOR,\n          ISD::FREM,   ISD::STRICT_FREM,\n          ISD::FLOG,   ISD::STRICT_FLOG,\n          ISD::FLOG10, ISD::STRICT_FLOG10,\n          ISD::FPOW,   ISD::STRICT_FPOW,\n          ISD::FSIN,   ISD::STRICT_FSIN})\n      if (isOperationExpand(Op, MVT::f32))\n        setOperationAction(Op, MVT::f32, Promote);\n\n  // We have target-specific dag combine patterns for the following nodes:\n  setTargetDAGCombine(ISD::VECTOR_SHUFFLE);\n  setTargetDAGCombine(ISD::SCALAR_TO_VECTOR);\n  setTargetDAGCombine(ISD::INSERT_VECTOR_ELT);\n  setTargetDAGCombine(ISD::EXTRACT_VECTOR_ELT);\n  setTargetDAGCombine(ISD::CONCAT_VECTORS);\n  setTargetDAGCombine(ISD::INSERT_SUBVECTOR);\n  setTargetDAGCombine(ISD::EXTRACT_SUBVECTOR);\n  setTargetDAGCombine(ISD::BITCAST);\n  setTargetDAGCombine(ISD::VSELECT);\n  setTargetDAGCombine(ISD::SELECT);\n  setTargetDAGCombine(ISD::SHL);\n  setTargetDAGCombine(ISD::SRA);\n  setTargetDAGCombine(ISD::SRL);\n  setTargetDAGCombine(ISD::OR);\n  setTargetDAGCombine(ISD::AND);\n  setTargetDAGCombine(ISD::ADD);\n  setTargetDAGCombine(ISD::FADD);\n  setTargetDAGCombine(ISD::FSUB);\n  setTargetDAGCombine(ISD::FNEG);\n  setTargetDAGCombine(ISD::FMA);\n  setTargetDAGCombine(ISD::STRICT_FMA);\n  setTargetDAGCombine(ISD::FMINNUM);\n  setTargetDAGCombine(ISD::FMAXNUM);\n  setTargetDAGCombine(ISD::SUB);\n  setTargetDAGCombine(ISD::LOAD);\n  setTargetDAGCombine(ISD::MLOAD);\n  setTargetDAGCombine(ISD::STORE);\n  setTargetDAGCombine(ISD::MSTORE);\n  setTargetDAGCombine(ISD::TRUNCATE);\n  setTargetDAGCombine(ISD::ZERO_EXTEND);\n  setTargetDAGCombine(ISD::ANY_EXTEND);\n  setTargetDAGCombine(ISD::SIGN_EXTEND);\n  setTargetDAGCombine(ISD::SIGN_EXTEND_INREG);\n  setTargetDAGCombine(ISD::ANY_EXTEND_VECTOR_INREG);\n  setTargetDAGCombine(ISD::SIGN_EXTEND_VECTOR_INREG);\n  setTargetDAGCombine(ISD::ZERO_EXTEND_VECTOR_INREG);\n  setTargetDAGCombine(ISD::SINT_TO_FP);\n  setTargetDAGCombine(ISD::UINT_TO_FP);\n  setTargetDAGCombine(ISD::STRICT_SINT_TO_FP);\n  setTargetDAGCombine(ISD::STRICT_UINT_TO_FP);\n  setTargetDAGCombine(ISD::SETCC);\n  setTargetDAGCombine(ISD::MUL);\n  setTargetDAGCombine(ISD::XOR);\n  setTargetDAGCombine(ISD::MSCATTER);\n  setTargetDAGCombine(ISD::MGATHER);\n  setTargetDAGCombine(ISD::FP16_TO_FP);\n  setTargetDAGCombine(ISD::FP_EXTEND);\n  setTargetDAGCombine(ISD::STRICT_FP_EXTEND);\n  setTargetDAGCombine(ISD::FP_ROUND);\n\n  computeRegisterProperties(Subtarget.getRegisterInfo());\n\n  MaxStoresPerMemset = 16; // For @llvm.memset -> sequence of stores\n  MaxStoresPerMemsetOptSize = 8;\n  MaxStoresPerMemcpy = 8; // For @llvm.memcpy -> sequence of stores\n  MaxStoresPerMemcpyOptSize = 4;\n  MaxStoresPerMemmove = 8; // For @llvm.memmove -> sequence of stores\n  MaxStoresPerMemmoveOptSize = 4;\n\n  // TODO: These control memcmp expansion in CGP and could be raised higher, but\n  // that needs to benchmarked and balanced with the potential use of vector\n  // load/store types (PR33329, PR33914).\n  MaxLoadsPerMemcmp = 2;\n  MaxLoadsPerMemcmpOptSize = 2;\n\n  // Set loop alignment to 2^ExperimentalPrefLoopAlignment bytes (default: 2^4).\n  setPrefLoopAlignment(Align(1ULL << ExperimentalPrefLoopAlignment));\n\n  // An out-of-order CPU can speculatively execute past a predictable branch,\n  // but a conditional move could be stalled by an expensive earlier operation.\n  PredictableSelectIsExpensive = Subtarget.getSchedModel().isOutOfOrder();\n  EnableExtLdPromotion = true;\n  setPrefFunctionAlignment(Align(16));\n\n  verifyIntrinsicTables();\n\n  // Default to having -disable-strictnode-mutation on\n  IsStrictFPEnabled = true;\n}\n\n// This has so far only been implemented for 64-bit MachO.\nbool X86TargetLowering::useLoadStackGuardNode() const {\n  return Subtarget.isTargetMachO() && Subtarget.is64Bit();\n}\n\nbool X86TargetLowering::useStackGuardXorFP() const {\n  // Currently only MSVC CRTs XOR the frame pointer into the stack guard value.\n  return Subtarget.getTargetTriple().isOSMSVCRT() && !Subtarget.isTargetMachO();\n}\n\nSDValue X86TargetLowering::emitStackGuardXorFP(SelectionDAG &DAG, SDValue Val,\n                                               const SDLoc &DL) const {\n  EVT PtrTy = getPointerTy(DAG.getDataLayout());\n  unsigned XorOp = Subtarget.is64Bit() ? X86::XOR64_FP : X86::XOR32_FP;\n  MachineSDNode *Node = DAG.getMachineNode(XorOp, DL, PtrTy, Val);\n  return SDValue(Node, 0);\n}\n\nTargetLoweringBase::LegalizeTypeAction\nX86TargetLowering::getPreferredVectorAction(MVT VT) const {\n  if ((VT == MVT::v32i1 || VT == MVT::v64i1) && Subtarget.hasAVX512() &&\n      !Subtarget.hasBWI())\n    return TypeSplitVector;\n\n  if (VT.getVectorNumElements() != 1 &&\n      VT.getVectorElementType() != MVT::i1)\n    return TypeWidenVector;\n\n  return TargetLoweringBase::getPreferredVectorAction(VT);\n}\n\nstatic std::pair<MVT, unsigned>\nhandleMaskRegisterForCallingConv(unsigned NumElts, CallingConv::ID CC,\n                                 const X86Subtarget &Subtarget) {\n  // v2i1/v4i1/v8i1/v16i1 all pass in xmm registers unless the calling\n  // convention is one that uses k registers.\n  if (NumElts == 2)\n    return {MVT::v2i64, 1};\n  if (NumElts == 4)\n    return {MVT::v4i32, 1};\n  if (NumElts == 8 && CC != CallingConv::X86_RegCall &&\n      CC != CallingConv::Intel_OCL_BI)\n    return {MVT::v8i16, 1};\n  if (NumElts == 16 && CC != CallingConv::X86_RegCall &&\n      CC != CallingConv::Intel_OCL_BI)\n    return {MVT::v16i8, 1};\n  // v32i1 passes in ymm unless we have BWI and the calling convention is\n  // regcall.\n  if (NumElts == 32 && (!Subtarget.hasBWI() || CC != CallingConv::X86_RegCall))\n    return {MVT::v32i8, 1};\n  // Split v64i1 vectors if we don't have v64i8 available.\n  if (NumElts == 64 && Subtarget.hasBWI() && CC != CallingConv::X86_RegCall) {\n    if (Subtarget.useAVX512Regs())\n      return {MVT::v64i8, 1};\n    return {MVT::v32i8, 2};\n  }\n\n  // Break wide or odd vXi1 vectors into scalars to match avx2 behavior.\n  if (!isPowerOf2_32(NumElts) || (NumElts == 64 && !Subtarget.hasBWI()) ||\n      NumElts > 64)\n    return {MVT::i8, NumElts};\n\n  return {MVT::INVALID_SIMPLE_VALUE_TYPE, 0};\n}\n\nMVT X86TargetLowering::getRegisterTypeForCallingConv(LLVMContext &Context,\n                                                     CallingConv::ID CC,\n                                                     EVT VT) const {\n  if (VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      Subtarget.hasAVX512()) {\n    unsigned NumElts = VT.getVectorNumElements();\n\n    MVT RegisterVT;\n    unsigned NumRegisters;\n    std::tie(RegisterVT, NumRegisters) =\n        handleMaskRegisterForCallingConv(NumElts, CC, Subtarget);\n    if (RegisterVT != MVT::INVALID_SIMPLE_VALUE_TYPE)\n      return RegisterVT;\n  }\n\n  return TargetLowering::getRegisterTypeForCallingConv(Context, CC, VT);\n}\n\nunsigned X86TargetLowering::getNumRegistersForCallingConv(LLVMContext &Context,\n                                                          CallingConv::ID CC,\n                                                          EVT VT) const {\n  if (VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      Subtarget.hasAVX512()) {\n    unsigned NumElts = VT.getVectorNumElements();\n\n    MVT RegisterVT;\n    unsigned NumRegisters;\n    std::tie(RegisterVT, NumRegisters) =\n        handleMaskRegisterForCallingConv(NumElts, CC, Subtarget);\n    if (RegisterVT != MVT::INVALID_SIMPLE_VALUE_TYPE)\n      return NumRegisters;\n  }\n\n  return TargetLowering::getNumRegistersForCallingConv(Context, CC, VT);\n}\n\nunsigned X86TargetLowering::getVectorTypeBreakdownForCallingConv(\n    LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,\n    unsigned &NumIntermediates, MVT &RegisterVT) const {\n  // Break wide or odd vXi1 vectors into scalars to match avx2 behavior.\n  if (VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      Subtarget.hasAVX512() &&\n      (!isPowerOf2_32(VT.getVectorNumElements()) ||\n       (VT.getVectorNumElements() == 64 && !Subtarget.hasBWI()) ||\n       VT.getVectorNumElements() > 64)) {\n    RegisterVT = MVT::i8;\n    IntermediateVT = MVT::i1;\n    NumIntermediates = VT.getVectorNumElements();\n    return NumIntermediates;\n  }\n\n  // Split v64i1 vectors if we don't have v64i8 available.\n  if (VT == MVT::v64i1 && Subtarget.hasBWI() && !Subtarget.useAVX512Regs() &&\n      CC != CallingConv::X86_RegCall) {\n    RegisterVT = MVT::v32i8;\n    IntermediateVT = MVT::v32i1;\n    NumIntermediates = 2;\n    return 2;\n  }\n\n  return TargetLowering::getVectorTypeBreakdownForCallingConv(Context, CC, VT, IntermediateVT,\n                                              NumIntermediates, RegisterVT);\n}\n\nEVT X86TargetLowering::getSetCCResultType(const DataLayout &DL,\n                                          LLVMContext& Context,\n                                          EVT VT) const {\n  if (!VT.isVector())\n    return MVT::i8;\n\n  if (Subtarget.hasAVX512()) {\n    const unsigned NumElts = VT.getVectorNumElements();\n\n    // Figure out what this type will be legalized to.\n    EVT LegalVT = VT;\n    while (getTypeAction(Context, LegalVT) != TypeLegal)\n      LegalVT = getTypeToTransformTo(Context, LegalVT);\n\n    // If we got a 512-bit vector then we'll definitely have a vXi1 compare.\n    if (LegalVT.getSimpleVT().is512BitVector())\n      return EVT::getVectorVT(Context, MVT::i1, NumElts);\n\n    if (LegalVT.getSimpleVT().isVector() && Subtarget.hasVLX()) {\n      // If we legalized to less than a 512-bit vector, then we will use a vXi1\n      // compare for vXi32/vXi64 for sure. If we have BWI we will also support\n      // vXi16/vXi8.\n      MVT EltVT = LegalVT.getSimpleVT().getVectorElementType();\n      if (Subtarget.hasBWI() || EltVT.getSizeInBits() >= 32)\n        return EVT::getVectorVT(Context, MVT::i1, NumElts);\n    }\n  }\n\n  return VT.changeVectorElementTypeToInteger();\n}\n\n/// Helper for getByValTypeAlignment to determine\n/// the desired ByVal argument alignment.\nstatic void getMaxByValAlign(Type *Ty, Align &MaxAlign) {\n  if (MaxAlign == 16)\n    return;\n  if (VectorType *VTy = dyn_cast<VectorType>(Ty)) {\n    if (VTy->getPrimitiveSizeInBits().getFixedSize() == 128)\n      MaxAlign = Align(16);\n  } else if (ArrayType *ATy = dyn_cast<ArrayType>(Ty)) {\n    Align EltAlign;\n    getMaxByValAlign(ATy->getElementType(), EltAlign);\n    if (EltAlign > MaxAlign)\n      MaxAlign = EltAlign;\n  } else if (StructType *STy = dyn_cast<StructType>(Ty)) {\n    for (auto *EltTy : STy->elements()) {\n      Align EltAlign;\n      getMaxByValAlign(EltTy, EltAlign);\n      if (EltAlign > MaxAlign)\n        MaxAlign = EltAlign;\n      if (MaxAlign == 16)\n        break;\n    }\n  }\n}\n\n/// Return the desired alignment for ByVal aggregate\n/// function arguments in the caller parameter area. For X86, aggregates\n/// that contain SSE vectors are placed at 16-byte boundaries while the rest\n/// are at 4-byte boundaries.\nunsigned X86TargetLowering::getByValTypeAlignment(Type *Ty,\n                                                  const DataLayout &DL) const {\n  if (Subtarget.is64Bit()) {\n    // Max of 8 and alignment of type.\n    Align TyAlign = DL.getABITypeAlign(Ty);\n    if (TyAlign > 8)\n      return TyAlign.value();\n    return 8;\n  }\n\n  Align Alignment(4);\n  if (Subtarget.hasSSE1())\n    getMaxByValAlign(Ty, Alignment);\n  return Alignment.value();\n}\n\n/// It returns EVT::Other if the type should be determined using generic\n/// target-independent logic.\n/// For vector ops we check that the overall size isn't larger than our\n/// preferred vector width.\nEVT X86TargetLowering::getOptimalMemOpType(\n    const MemOp &Op, const AttributeList &FuncAttributes) const {\n  if (!FuncAttributes.hasFnAttribute(Attribute::NoImplicitFloat)) {\n    if (Op.size() >= 16 &&\n        (!Subtarget.isUnalignedMem16Slow() || Op.isAligned(Align(16)))) {\n      // FIXME: Check if unaligned 64-byte accesses are slow.\n      if (Op.size() >= 64 && Subtarget.hasAVX512() &&\n          (Subtarget.getPreferVectorWidth() >= 512)) {\n        return Subtarget.hasBWI() ? MVT::v64i8 : MVT::v16i32;\n      }\n      // FIXME: Check if unaligned 32-byte accesses are slow.\n      if (Op.size() >= 32 && Subtarget.hasAVX() &&\n          (Subtarget.getPreferVectorWidth() >= 256)) {\n        // Although this isn't a well-supported type for AVX1, we'll let\n        // legalization and shuffle lowering produce the optimal codegen. If we\n        // choose an optimal type with a vector element larger than a byte,\n        // getMemsetStores() may create an intermediate splat (using an integer\n        // multiply) before we splat as a vector.\n        return MVT::v32i8;\n      }\n      if (Subtarget.hasSSE2() && (Subtarget.getPreferVectorWidth() >= 128))\n        return MVT::v16i8;\n      // TODO: Can SSE1 handle a byte vector?\n      // If we have SSE1 registers we should be able to use them.\n      if (Subtarget.hasSSE1() && (Subtarget.is64Bit() || Subtarget.hasX87()) &&\n          (Subtarget.getPreferVectorWidth() >= 128))\n        return MVT::v4f32;\n    } else if (((Op.isMemcpy() && !Op.isMemcpyStrSrc()) || Op.isZeroMemset()) &&\n               Op.size() >= 8 && !Subtarget.is64Bit() && Subtarget.hasSSE2()) {\n      // Do not use f64 to lower memcpy if source is string constant. It's\n      // better to use i32 to avoid the loads.\n      // Also, do not use f64 to lower memset unless this is a memset of zeros.\n      // The gymnastics of splatting a byte value into an XMM register and then\n      // only using 8-byte stores (because this is a CPU with slow unaligned\n      // 16-byte accesses) makes that a loser.\n      return MVT::f64;\n    }\n  }\n  // This is a compromise. If we reach here, unaligned accesses may be slow on\n  // this target. However, creating smaller, aligned accesses could be even\n  // slower and would certainly be a lot more code.\n  if (Subtarget.is64Bit() && Op.size() >= 8)\n    return MVT::i64;\n  return MVT::i32;\n}\n\nbool X86TargetLowering::isSafeMemOpType(MVT VT) const {\n  if (VT == MVT::f32)\n    return X86ScalarSSEf32;\n  else if (VT == MVT::f64)\n    return X86ScalarSSEf64;\n  return true;\n}\n\nbool X86TargetLowering::allowsMisalignedMemoryAccesses(\n    EVT VT, unsigned, unsigned Align, MachineMemOperand::Flags Flags,\n    bool *Fast) const {\n  if (Fast) {\n    switch (VT.getSizeInBits()) {\n    default:\n      // 8-byte and under are always assumed to be fast.\n      *Fast = true;\n      break;\n    case 128:\n      *Fast = !Subtarget.isUnalignedMem16Slow();\n      break;\n    case 256:\n      *Fast = !Subtarget.isUnalignedMem32Slow();\n      break;\n    // TODO: What about AVX-512 (512-bit) accesses?\n    }\n  }\n  // NonTemporal vector memory ops must be aligned.\n  if (!!(Flags & MachineMemOperand::MONonTemporal) && VT.isVector()) {\n    // NT loads can only be vector aligned, so if its less aligned than the\n    // minimum vector size (which we can split the vector down to), we might as\n    // well use a regular unaligned vector load.\n    // We don't have any NT loads pre-SSE41.\n    if (!!(Flags & MachineMemOperand::MOLoad))\n      return (Align < 16 || !Subtarget.hasSSE41());\n    return false;\n  }\n  // Misaligned accesses of any size are always allowed.\n  return true;\n}\n\n/// Return the entry encoding for a jump table in the\n/// current function.  The returned value is a member of the\n/// MachineJumpTableInfo::JTEntryKind enum.\nunsigned X86TargetLowering::getJumpTableEncoding() const {\n  // In GOT pic mode, each entry in the jump table is emitted as a @GOTOFF\n  // symbol.\n  if (isPositionIndependent() && Subtarget.isPICStyleGOT())\n    return MachineJumpTableInfo::EK_Custom32;\n\n  // Otherwise, use the normal jump table encoding heuristics.\n  return TargetLowering::getJumpTableEncoding();\n}\n\nbool X86TargetLowering::useSoftFloat() const {\n  return Subtarget.useSoftFloat();\n}\n\nvoid X86TargetLowering::markLibCallAttributes(MachineFunction *MF, unsigned CC,\n                                              ArgListTy &Args) const {\n\n  // Only relabel X86-32 for C / Stdcall CCs.\n  if (Subtarget.is64Bit())\n    return;\n  if (CC != CallingConv::C && CC != CallingConv::X86_StdCall)\n    return;\n  unsigned ParamRegs = 0;\n  if (auto *M = MF->getFunction().getParent())\n    ParamRegs = M->getNumberRegisterParameters();\n\n  // Mark the first N int arguments as having reg\n  for (unsigned Idx = 0; Idx < Args.size(); Idx++) {\n    Type *T = Args[Idx].Ty;\n    if (T->isIntOrPtrTy())\n      if (MF->getDataLayout().getTypeAllocSize(T) <= 8) {\n        unsigned numRegs = 1;\n        if (MF->getDataLayout().getTypeAllocSize(T) > 4)\n          numRegs = 2;\n        if (ParamRegs < numRegs)\n          return;\n        ParamRegs -= numRegs;\n        Args[Idx].IsInReg = true;\n      }\n  }\n}\n\nconst MCExpr *\nX86TargetLowering::LowerCustomJumpTableEntry(const MachineJumpTableInfo *MJTI,\n                                             const MachineBasicBlock *MBB,\n                                             unsigned uid,MCContext &Ctx) const{\n  assert(isPositionIndependent() && Subtarget.isPICStyleGOT());\n  // In 32-bit ELF systems, our jump table entries are formed with @GOTOFF\n  // entries.\n  return MCSymbolRefExpr::create(MBB->getSymbol(),\n                                 MCSymbolRefExpr::VK_GOTOFF, Ctx);\n}\n\n/// Returns relocation base for the given PIC jumptable.\nSDValue X86TargetLowering::getPICJumpTableRelocBase(SDValue Table,\n                                                    SelectionDAG &DAG) const {\n  if (!Subtarget.is64Bit())\n    // This doesn't have SDLoc associated with it, but is not really the\n    // same as a Register.\n    return DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(),\n                       getPointerTy(DAG.getDataLayout()));\n  return Table;\n}\n\n/// This returns the relocation base for the given PIC jumptable,\n/// the same as getPICJumpTableRelocBase, but as an MCExpr.\nconst MCExpr *X86TargetLowering::\ngetPICJumpTableRelocBaseExpr(const MachineFunction *MF, unsigned JTI,\n                             MCContext &Ctx) const {\n  // X86-64 uses RIP relative addressing based on the jump table label.\n  if (Subtarget.isPICStyleRIPRel())\n    return TargetLowering::getPICJumpTableRelocBaseExpr(MF, JTI, Ctx);\n\n  // Otherwise, the reference is relative to the PIC base.\n  return MCSymbolRefExpr::create(MF->getPICBaseSymbol(), Ctx);\n}\n\nstd::pair<const TargetRegisterClass *, uint8_t>\nX86TargetLowering::findRepresentativeClass(const TargetRegisterInfo *TRI,\n                                           MVT VT) const {\n  const TargetRegisterClass *RRC = nullptr;\n  uint8_t Cost = 1;\n  switch (VT.SimpleTy) {\n  default:\n    return TargetLowering::findRepresentativeClass(TRI, VT);\n  case MVT::i8: case MVT::i16: case MVT::i32: case MVT::i64:\n    RRC = Subtarget.is64Bit() ? &X86::GR64RegClass : &X86::GR32RegClass;\n    break;\n  case MVT::x86mmx:\n    RRC = &X86::VR64RegClass;\n    break;\n  case MVT::f32: case MVT::f64:\n  case MVT::v16i8: case MVT::v8i16: case MVT::v4i32: case MVT::v2i64:\n  case MVT::v4f32: case MVT::v2f64:\n  case MVT::v32i8: case MVT::v16i16: case MVT::v8i32: case MVT::v4i64:\n  case MVT::v8f32: case MVT::v4f64:\n  case MVT::v64i8: case MVT::v32i16: case MVT::v16i32: case MVT::v8i64:\n  case MVT::v16f32: case MVT::v8f64:\n    RRC = &X86::VR128XRegClass;\n    break;\n  }\n  return std::make_pair(RRC, Cost);\n}\n\nunsigned X86TargetLowering::getAddressSpace() const {\n  if (Subtarget.is64Bit())\n    return (getTargetMachine().getCodeModel() == CodeModel::Kernel) ? 256 : 257;\n  return 256;\n}\n\nstatic bool hasStackGuardSlotTLS(const Triple &TargetTriple) {\n  return TargetTriple.isOSGlibc() || TargetTriple.isOSFuchsia() ||\n         (TargetTriple.isAndroid() && !TargetTriple.isAndroidVersionLT(17));\n}\n\nstatic Constant* SegmentOffset(IRBuilder<> &IRB,\n                               unsigned Offset, unsigned AddressSpace) {\n  return ConstantExpr::getIntToPtr(\n      ConstantInt::get(Type::getInt32Ty(IRB.getContext()), Offset),\n      Type::getInt8PtrTy(IRB.getContext())->getPointerTo(AddressSpace));\n}\n\nValue *X86TargetLowering::getIRStackGuard(IRBuilder<> &IRB) const {\n  // glibc, bionic, and Fuchsia have a special slot for the stack guard in\n  // tcbhead_t; use it instead of the usual global variable (see\n  // sysdeps/{i386,x86_64}/nptl/tls.h)\n  if (hasStackGuardSlotTLS(Subtarget.getTargetTriple())) {\n    if (Subtarget.isTargetFuchsia()) {\n      // <zircon/tls.h> defines ZX_TLS_STACK_GUARD_OFFSET with this value.\n      return SegmentOffset(IRB, 0x10, getAddressSpace());\n    } else {\n      unsigned AddressSpace = getAddressSpace();\n      // Specially, some users may customize the base reg and offset.\n      unsigned Offset = getTargetMachine().Options.StackProtectorGuardOffset;\n      // If we don't set -stack-protector-guard-offset value:\n      // %fs:0x28, unless we're using a Kernel code model, in which case\n      // it's %gs:0x28.  gs:0x14 on i386.\n      if (Offset == (unsigned)-1)\n        Offset = (Subtarget.is64Bit()) ? 0x28 : 0x14;\n\n      const auto &GuardReg = getTargetMachine().Options.StackProtectorGuardReg;\n      if (GuardReg == \"fs\")\n        AddressSpace = X86AS::FS;\n      else if (GuardReg == \"gs\")\n        AddressSpace = X86AS::GS;\n      return SegmentOffset(IRB, Offset, AddressSpace);\n    }\n  }\n  return TargetLowering::getIRStackGuard(IRB);\n}\n\nvoid X86TargetLowering::insertSSPDeclarations(Module &M) const {\n  // MSVC CRT provides functionalities for stack protection.\n  if (Subtarget.getTargetTriple().isWindowsMSVCEnvironment() ||\n      Subtarget.getTargetTriple().isWindowsItaniumEnvironment()) {\n    // MSVC CRT has a global variable holding security cookie.\n    M.getOrInsertGlobal(\"__security_cookie\",\n                        Type::getInt8PtrTy(M.getContext()));\n\n    // MSVC CRT has a function to validate security cookie.\n    FunctionCallee SecurityCheckCookie = M.getOrInsertFunction(\n        \"__security_check_cookie\", Type::getVoidTy(M.getContext()),\n        Type::getInt8PtrTy(M.getContext()));\n    if (Function *F = dyn_cast<Function>(SecurityCheckCookie.getCallee())) {\n      F->setCallingConv(CallingConv::X86_FastCall);\n      F->addAttribute(1, Attribute::AttrKind::InReg);\n    }\n    return;\n  }\n\n  auto GuardMode = getTargetMachine().Options.StackProtectorGuard;\n\n  // glibc, bionic, and Fuchsia have a special slot for the stack guard.\n  if ((GuardMode == llvm::StackProtectorGuards::TLS ||\n       GuardMode == llvm::StackProtectorGuards::None)\n      && hasStackGuardSlotTLS(Subtarget.getTargetTriple()))\n    return;\n  TargetLowering::insertSSPDeclarations(M);\n}\n\nValue *X86TargetLowering::getSDagStackGuard(const Module &M) const {\n  // MSVC CRT has a global variable holding security cookie.\n  if (Subtarget.getTargetTriple().isWindowsMSVCEnvironment() ||\n      Subtarget.getTargetTriple().isWindowsItaniumEnvironment()) {\n    return M.getGlobalVariable(\"__security_cookie\");\n  }\n  return TargetLowering::getSDagStackGuard(M);\n}\n\nFunction *X86TargetLowering::getSSPStackGuardCheck(const Module &M) const {\n  // MSVC CRT has a function to validate security cookie.\n  if (Subtarget.getTargetTriple().isWindowsMSVCEnvironment() ||\n      Subtarget.getTargetTriple().isWindowsItaniumEnvironment()) {\n    return M.getFunction(\"__security_check_cookie\");\n  }\n  return TargetLowering::getSSPStackGuardCheck(M);\n}\n\nValue *X86TargetLowering::getSafeStackPointerLocation(IRBuilder<> &IRB) const {\n  if (Subtarget.getTargetTriple().isOSContiki())\n    return getDefaultSafeStackPointerLocation(IRB, false);\n\n  // Android provides a fixed TLS slot for the SafeStack pointer. See the\n  // definition of TLS_SLOT_SAFESTACK in\n  // https://android.googlesource.com/platform/bionic/+/master/libc/private/bionic_tls.h\n  if (Subtarget.isTargetAndroid()) {\n    // %fs:0x48, unless we're using a Kernel code model, in which case it's %gs:\n    // %gs:0x24 on i386\n    unsigned Offset = (Subtarget.is64Bit()) ? 0x48 : 0x24;\n    return SegmentOffset(IRB, Offset, getAddressSpace());\n  }\n\n  // Fuchsia is similar.\n  if (Subtarget.isTargetFuchsia()) {\n    // <zircon/tls.h> defines ZX_TLS_UNSAFE_SP_OFFSET with this value.\n    return SegmentOffset(IRB, 0x18, getAddressSpace());\n  }\n\n  return TargetLowering::getSafeStackPointerLocation(IRB);\n}\n\n//===----------------------------------------------------------------------===//\n//               Return Value Calling Convention Implementation\n//===----------------------------------------------------------------------===//\n\nbool X86TargetLowering::CanLowerReturn(\n    CallingConv::ID CallConv, MachineFunction &MF, bool isVarArg,\n    const SmallVectorImpl<ISD::OutputArg> &Outs, LLVMContext &Context) const {\n  SmallVector<CCValAssign, 16> RVLocs;\n  CCState CCInfo(CallConv, isVarArg, MF, RVLocs, Context);\n  return CCInfo.CheckReturn(Outs, RetCC_X86);\n}\n\nconst MCPhysReg *X86TargetLowering::getScratchRegisters(CallingConv::ID) const {\n  static const MCPhysReg ScratchRegs[] = { X86::R11, 0 };\n  return ScratchRegs;\n}\n\n/// Lowers masks values (v*i1) to the local register values\n/// \\returns DAG node after lowering to register type\nstatic SDValue lowerMasksToReg(const SDValue &ValArg, const EVT &ValLoc,\n                               const SDLoc &Dl, SelectionDAG &DAG) {\n  EVT ValVT = ValArg.getValueType();\n\n  if (ValVT == MVT::v1i1)\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, Dl, ValLoc, ValArg,\n                       DAG.getIntPtrConstant(0, Dl));\n\n  if ((ValVT == MVT::v8i1 && (ValLoc == MVT::i8 || ValLoc == MVT::i32)) ||\n      (ValVT == MVT::v16i1 && (ValLoc == MVT::i16 || ValLoc == MVT::i32))) {\n    // Two stage lowering might be required\n    // bitcast:   v8i1 -> i8 / v16i1 -> i16\n    // anyextend: i8   -> i32 / i16   -> i32\n    EVT TempValLoc = ValVT == MVT::v8i1 ? MVT::i8 : MVT::i16;\n    SDValue ValToCopy = DAG.getBitcast(TempValLoc, ValArg);\n    if (ValLoc == MVT::i32)\n      ValToCopy = DAG.getNode(ISD::ANY_EXTEND, Dl, ValLoc, ValToCopy);\n    return ValToCopy;\n  }\n\n  if ((ValVT == MVT::v32i1 && ValLoc == MVT::i32) ||\n      (ValVT == MVT::v64i1 && ValLoc == MVT::i64)) {\n    // One stage lowering is required\n    // bitcast:   v32i1 -> i32 / v64i1 -> i64\n    return DAG.getBitcast(ValLoc, ValArg);\n  }\n\n  return DAG.getNode(ISD::ANY_EXTEND, Dl, ValLoc, ValArg);\n}\n\n/// Breaks v64i1 value into two registers and adds the new node to the DAG\nstatic void Passv64i1ArgInRegs(\n    const SDLoc &Dl, SelectionDAG &DAG, SDValue &Arg,\n    SmallVectorImpl<std::pair<Register, SDValue>> &RegsToPass, CCValAssign &VA,\n    CCValAssign &NextVA, const X86Subtarget &Subtarget) {\n  assert(Subtarget.hasBWI() && \"Expected AVX512BW target!\");\n  assert(Subtarget.is32Bit() && \"Expecting 32 bit target\");\n  assert(Arg.getValueType() == MVT::i64 && \"Expecting 64 bit value\");\n  assert(VA.isRegLoc() && NextVA.isRegLoc() &&\n         \"The value should reside in two registers\");\n\n  // Before splitting the value we cast it to i64\n  Arg = DAG.getBitcast(MVT::i64, Arg);\n\n  // Splitting the value into two i32 types\n  SDValue Lo, Hi;\n  Lo = DAG.getNode(ISD::EXTRACT_ELEMENT, Dl, MVT::i32, Arg,\n                   DAG.getConstant(0, Dl, MVT::i32));\n  Hi = DAG.getNode(ISD::EXTRACT_ELEMENT, Dl, MVT::i32, Arg,\n                   DAG.getConstant(1, Dl, MVT::i32));\n\n  // Attach the two i32 types into corresponding registers\n  RegsToPass.push_back(std::make_pair(VA.getLocReg(), Lo));\n  RegsToPass.push_back(std::make_pair(NextVA.getLocReg(), Hi));\n}\n\nSDValue\nX86TargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,\n                               bool isVarArg,\n                               const SmallVectorImpl<ISD::OutputArg> &Outs,\n                               const SmallVectorImpl<SDValue> &OutVals,\n                               const SDLoc &dl, SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();\n\n  // In some cases we need to disable registers from the default CSR list.\n  // For example, when they are used for argument passing.\n  bool ShouldDisableCalleeSavedRegister =\n      CallConv == CallingConv::X86_RegCall ||\n      MF.getFunction().hasFnAttribute(\"no_caller_saved_registers\");\n\n  if (CallConv == CallingConv::X86_INTR && !Outs.empty())\n    report_fatal_error(\"X86 interrupts may not return any value\");\n\n  SmallVector<CCValAssign, 16> RVLocs;\n  CCState CCInfo(CallConv, isVarArg, MF, RVLocs, *DAG.getContext());\n  CCInfo.AnalyzeReturn(Outs, RetCC_X86);\n\n  SmallVector<std::pair<Register, SDValue>, 4> RetVals;\n  for (unsigned I = 0, OutsIndex = 0, E = RVLocs.size(); I != E;\n       ++I, ++OutsIndex) {\n    CCValAssign &VA = RVLocs[I];\n    assert(VA.isRegLoc() && \"Can only return in registers!\");\n\n    // Add the register to the CalleeSaveDisableRegs list.\n    if (ShouldDisableCalleeSavedRegister)\n      MF.getRegInfo().disableCalleeSavedRegister(VA.getLocReg());\n\n    SDValue ValToCopy = OutVals[OutsIndex];\n    EVT ValVT = ValToCopy.getValueType();\n\n    // Promote values to the appropriate types.\n    if (VA.getLocInfo() == CCValAssign::SExt)\n      ValToCopy = DAG.getNode(ISD::SIGN_EXTEND, dl, VA.getLocVT(), ValToCopy);\n    else if (VA.getLocInfo() == CCValAssign::ZExt)\n      ValToCopy = DAG.getNode(ISD::ZERO_EXTEND, dl, VA.getLocVT(), ValToCopy);\n    else if (VA.getLocInfo() == CCValAssign::AExt) {\n      if (ValVT.isVector() && ValVT.getVectorElementType() == MVT::i1)\n        ValToCopy = lowerMasksToReg(ValToCopy, VA.getLocVT(), dl, DAG);\n      else\n        ValToCopy = DAG.getNode(ISD::ANY_EXTEND, dl, VA.getLocVT(), ValToCopy);\n    }\n    else if (VA.getLocInfo() == CCValAssign::BCvt)\n      ValToCopy = DAG.getBitcast(VA.getLocVT(), ValToCopy);\n\n    assert(VA.getLocInfo() != CCValAssign::FPExt &&\n           \"Unexpected FP-extend for return value.\");\n\n    // Report an error if we have attempted to return a value via an XMM\n    // register and SSE was disabled.\n    if (!Subtarget.hasSSE1() && X86::FR32XRegClass.contains(VA.getLocReg())) {\n      errorUnsupported(DAG, dl, \"SSE register return with SSE disabled\");\n      VA.convertToReg(X86::FP0); // Set reg to FP0, avoid hitting asserts.\n    } else if (!Subtarget.hasSSE2() &&\n               X86::FR64XRegClass.contains(VA.getLocReg()) &&\n               ValVT == MVT::f64) {\n      // When returning a double via an XMM register, report an error if SSE2 is\n      // not enabled.\n      errorUnsupported(DAG, dl, \"SSE2 register return with SSE2 disabled\");\n      VA.convertToReg(X86::FP0); // Set reg to FP0, avoid hitting asserts.\n    }\n\n    // Returns in ST0/ST1 are handled specially: these are pushed as operands to\n    // the RET instruction and handled by the FP Stackifier.\n    if (VA.getLocReg() == X86::FP0 ||\n        VA.getLocReg() == X86::FP1) {\n      // If this is a copy from an xmm register to ST(0), use an FPExtend to\n      // change the value to the FP stack register class.\n      if (isScalarFPTypeInSSEReg(VA.getValVT()))\n        ValToCopy = DAG.getNode(ISD::FP_EXTEND, dl, MVT::f80, ValToCopy);\n      RetVals.push_back(std::make_pair(VA.getLocReg(), ValToCopy));\n      // Don't emit a copytoreg.\n      continue;\n    }\n\n    // 64-bit vector (MMX) values are returned in XMM0 / XMM1 except for v1i64\n    // which is returned in RAX / RDX.\n    if (Subtarget.is64Bit()) {\n      if (ValVT == MVT::x86mmx) {\n        if (VA.getLocReg() == X86::XMM0 || VA.getLocReg() == X86::XMM1) {\n          ValToCopy = DAG.getBitcast(MVT::i64, ValToCopy);\n          ValToCopy = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64,\n                                  ValToCopy);\n          // If we don't have SSE2 available, convert to v4f32 so the generated\n          // register is legal.\n          if (!Subtarget.hasSSE2())\n            ValToCopy = DAG.getBitcast(MVT::v4f32, ValToCopy);\n        }\n      }\n    }\n\n    if (VA.needsCustom()) {\n      assert(VA.getValVT() == MVT::v64i1 &&\n             \"Currently the only custom case is when we split v64i1 to 2 regs\");\n\n      Passv64i1ArgInRegs(dl, DAG, ValToCopy, RetVals, VA, RVLocs[++I],\n                         Subtarget);\n\n      // Add the second register to the CalleeSaveDisableRegs list.\n      if (ShouldDisableCalleeSavedRegister)\n        MF.getRegInfo().disableCalleeSavedRegister(RVLocs[I].getLocReg());\n    } else {\n      RetVals.push_back(std::make_pair(VA.getLocReg(), ValToCopy));\n    }\n  }\n\n  SDValue Flag;\n  SmallVector<SDValue, 6> RetOps;\n  RetOps.push_back(Chain); // Operand #0 = Chain (updated below)\n  // Operand #1 = Bytes To Pop\n  RetOps.push_back(DAG.getTargetConstant(FuncInfo->getBytesToPopOnReturn(), dl,\n                   MVT::i32));\n\n  // Copy the result values into the output registers.\n  for (auto &RetVal : RetVals) {\n    if (RetVal.first == X86::FP0 || RetVal.first == X86::FP1) {\n      RetOps.push_back(RetVal.second);\n      continue; // Don't emit a copytoreg.\n    }\n\n    Chain = DAG.getCopyToReg(Chain, dl, RetVal.first, RetVal.second, Flag);\n    Flag = Chain.getValue(1);\n    RetOps.push_back(\n        DAG.getRegister(RetVal.first, RetVal.second.getValueType()));\n  }\n\n  // Swift calling convention does not require we copy the sret argument\n  // into %rax/%eax for the return, and SRetReturnReg is not set for Swift.\n\n  // All x86 ABIs require that for returning structs by value we copy\n  // the sret argument into %rax/%eax (depending on ABI) for the return.\n  // We saved the argument into a virtual register in the entry block,\n  // so now we copy the value out and into %rax/%eax.\n  //\n  // Checking Function.hasStructRetAttr() here is insufficient because the IR\n  // may not have an explicit sret argument. If FuncInfo.CanLowerReturn is\n  // false, then an sret argument may be implicitly inserted in the SelDAG. In\n  // either case FuncInfo->setSRetReturnReg() will have been called.\n  if (Register SRetReg = FuncInfo->getSRetReturnReg()) {\n    // When we have both sret and another return value, we should use the\n    // original Chain stored in RetOps[0], instead of the current Chain updated\n    // in the above loop. If we only have sret, RetOps[0] equals to Chain.\n\n    // For the case of sret and another return value, we have\n    //   Chain_0 at the function entry\n    //   Chain_1 = getCopyToReg(Chain_0) in the above loop\n    // If we use Chain_1 in getCopyFromReg, we will have\n    //   Val = getCopyFromReg(Chain_1)\n    //   Chain_2 = getCopyToReg(Chain_1, Val) from below\n\n    // getCopyToReg(Chain_0) will be glued together with\n    // getCopyToReg(Chain_1, Val) into Unit A, getCopyFromReg(Chain_1) will be\n    // in Unit B, and we will have cyclic dependency between Unit A and Unit B:\n    //   Data dependency from Unit B to Unit A due to usage of Val in\n    //     getCopyToReg(Chain_1, Val)\n    //   Chain dependency from Unit A to Unit B\n\n    // So here, we use RetOps[0] (i.e Chain_0) for getCopyFromReg.\n    SDValue Val = DAG.getCopyFromReg(RetOps[0], dl, SRetReg,\n                                     getPointerTy(MF.getDataLayout()));\n\n    Register RetValReg\n        = (Subtarget.is64Bit() && !Subtarget.isTarget64BitILP32()) ?\n          X86::RAX : X86::EAX;\n    Chain = DAG.getCopyToReg(Chain, dl, RetValReg, Val, Flag);\n    Flag = Chain.getValue(1);\n\n    // RAX/EAX now acts like a return value.\n    RetOps.push_back(\n        DAG.getRegister(RetValReg, getPointerTy(DAG.getDataLayout())));\n\n    // Add the returned register to the CalleeSaveDisableRegs list.\n    if (ShouldDisableCalleeSavedRegister)\n      MF.getRegInfo().disableCalleeSavedRegister(RetValReg);\n  }\n\n  const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n  const MCPhysReg *I =\n      TRI->getCalleeSavedRegsViaCopy(&DAG.getMachineFunction());\n  if (I) {\n    for (; *I; ++I) {\n      if (X86::GR64RegClass.contains(*I))\n        RetOps.push_back(DAG.getRegister(*I, MVT::i64));\n      else\n        llvm_unreachable(\"Unexpected register class in CSRsViaCopy!\");\n    }\n  }\n\n  RetOps[0] = Chain;  // Update chain.\n\n  // Add the flag if we have it.\n  if (Flag.getNode())\n    RetOps.push_back(Flag);\n\n  X86ISD::NodeType opcode = X86ISD::RET_FLAG;\n  if (CallConv == CallingConv::X86_INTR)\n    opcode = X86ISD::IRET;\n  return DAG.getNode(opcode, dl, MVT::Other, RetOps);\n}\n\nbool X86TargetLowering::isUsedByReturnOnly(SDNode *N, SDValue &Chain) const {\n  if (N->getNumValues() != 1 || !N->hasNUsesOfValue(1, 0))\n    return false;\n\n  SDValue TCChain = Chain;\n  SDNode *Copy = *N->use_begin();\n  if (Copy->getOpcode() == ISD::CopyToReg) {\n    // If the copy has a glue operand, we conservatively assume it isn't safe to\n    // perform a tail call.\n    if (Copy->getOperand(Copy->getNumOperands()-1).getValueType() == MVT::Glue)\n      return false;\n    TCChain = Copy->getOperand(0);\n  } else if (Copy->getOpcode() != ISD::FP_EXTEND)\n    return false;\n\n  bool HasRet = false;\n  for (SDNode::use_iterator UI = Copy->use_begin(), UE = Copy->use_end();\n       UI != UE; ++UI) {\n    if (UI->getOpcode() != X86ISD::RET_FLAG)\n      return false;\n    // If we are returning more than one value, we can definitely\n    // not make a tail call see PR19530\n    if (UI->getNumOperands() > 4)\n      return false;\n    if (UI->getNumOperands() == 4 &&\n        UI->getOperand(UI->getNumOperands()-1).getValueType() != MVT::Glue)\n      return false;\n    HasRet = true;\n  }\n\n  if (!HasRet)\n    return false;\n\n  Chain = TCChain;\n  return true;\n}\n\nEVT X86TargetLowering::getTypeForExtReturn(LLVMContext &Context, EVT VT,\n                                           ISD::NodeType ExtendKind) const {\n  MVT ReturnMVT = MVT::i32;\n\n  bool Darwin = Subtarget.getTargetTriple().isOSDarwin();\n  if (VT == MVT::i1 || (!Darwin && (VT == MVT::i8 || VT == MVT::i16))) {\n    // The ABI does not require i1, i8 or i16 to be extended.\n    //\n    // On Darwin, there is code in the wild relying on Clang's old behaviour of\n    // always extending i8/i16 return values, so keep doing that for now.\n    // (PR26665).\n    ReturnMVT = MVT::i8;\n  }\n\n  EVT MinVT = getRegisterType(Context, ReturnMVT);\n  return VT.bitsLT(MinVT) ? MinVT : VT;\n}\n\n/// Reads two 32 bit registers and creates a 64 bit mask value.\n/// \\param VA The current 32 bit value that need to be assigned.\n/// \\param NextVA The next 32 bit value that need to be assigned.\n/// \\param Root The parent DAG node.\n/// \\param [in,out] InFlag Represents SDvalue in the parent DAG node for\n///                        glue purposes. In the case the DAG is already using\n///                        physical register instead of virtual, we should glue\n///                        our new SDValue to InFlag SDvalue.\n/// \\return a new SDvalue of size 64bit.\nstatic SDValue getv64i1Argument(CCValAssign &VA, CCValAssign &NextVA,\n                                SDValue &Root, SelectionDAG &DAG,\n                                const SDLoc &Dl, const X86Subtarget &Subtarget,\n                                SDValue *InFlag = nullptr) {\n  assert((Subtarget.hasBWI()) && \"Expected AVX512BW target!\");\n  assert(Subtarget.is32Bit() && \"Expecting 32 bit target\");\n  assert(VA.getValVT() == MVT::v64i1 &&\n         \"Expecting first location of 64 bit width type\");\n  assert(NextVA.getValVT() == VA.getValVT() &&\n         \"The locations should have the same type\");\n  assert(VA.isRegLoc() && NextVA.isRegLoc() &&\n         \"The values should reside in two registers\");\n\n  SDValue Lo, Hi;\n  SDValue ArgValueLo, ArgValueHi;\n\n  MachineFunction &MF = DAG.getMachineFunction();\n  const TargetRegisterClass *RC = &X86::GR32RegClass;\n\n  // Read a 32 bit value from the registers.\n  if (nullptr == InFlag) {\n    // When no physical register is present,\n    // create an intermediate virtual register.\n    Register Reg = MF.addLiveIn(VA.getLocReg(), RC);\n    ArgValueLo = DAG.getCopyFromReg(Root, Dl, Reg, MVT::i32);\n    Reg = MF.addLiveIn(NextVA.getLocReg(), RC);\n    ArgValueHi = DAG.getCopyFromReg(Root, Dl, Reg, MVT::i32);\n  } else {\n    // When a physical register is available read the value from it and glue\n    // the reads together.\n    ArgValueLo =\n      DAG.getCopyFromReg(Root, Dl, VA.getLocReg(), MVT::i32, *InFlag);\n    *InFlag = ArgValueLo.getValue(2);\n    ArgValueHi =\n      DAG.getCopyFromReg(Root, Dl, NextVA.getLocReg(), MVT::i32, *InFlag);\n    *InFlag = ArgValueHi.getValue(2);\n  }\n\n  // Convert the i32 type into v32i1 type.\n  Lo = DAG.getBitcast(MVT::v32i1, ArgValueLo);\n\n  // Convert the i32 type into v32i1 type.\n  Hi = DAG.getBitcast(MVT::v32i1, ArgValueHi);\n\n  // Concatenate the two values together.\n  return DAG.getNode(ISD::CONCAT_VECTORS, Dl, MVT::v64i1, Lo, Hi);\n}\n\n/// The function will lower a register of various sizes (8/16/32/64)\n/// to a mask value of the expected size (v8i1/v16i1/v32i1/v64i1)\n/// \\returns a DAG node contains the operand after lowering to mask type.\nstatic SDValue lowerRegToMasks(const SDValue &ValArg, const EVT &ValVT,\n                               const EVT &ValLoc, const SDLoc &Dl,\n                               SelectionDAG &DAG) {\n  SDValue ValReturned = ValArg;\n\n  if (ValVT == MVT::v1i1)\n    return DAG.getNode(ISD::SCALAR_TO_VECTOR, Dl, MVT::v1i1, ValReturned);\n\n  if (ValVT == MVT::v64i1) {\n    // In 32 bit machine, this case is handled by getv64i1Argument\n    assert(ValLoc == MVT::i64 && \"Expecting only i64 locations\");\n    // In 64 bit machine, There is no need to truncate the value only bitcast\n  } else {\n    MVT maskLen;\n    switch (ValVT.getSimpleVT().SimpleTy) {\n    case MVT::v8i1:\n      maskLen = MVT::i8;\n      break;\n    case MVT::v16i1:\n      maskLen = MVT::i16;\n      break;\n    case MVT::v32i1:\n      maskLen = MVT::i32;\n      break;\n    default:\n      llvm_unreachable(\"Expecting a vector of i1 types\");\n    }\n\n    ValReturned = DAG.getNode(ISD::TRUNCATE, Dl, maskLen, ValReturned);\n  }\n  return DAG.getBitcast(ValVT, ValReturned);\n}\n\n/// Lower the result values of a call into the\n/// appropriate copies out of appropriate physical registers.\n///\nSDValue X86TargetLowering::LowerCallResult(\n    SDValue Chain, SDValue InFlag, CallingConv::ID CallConv, bool isVarArg,\n    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &dl,\n    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals,\n    uint32_t *RegMask) const {\n\n  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n  // Assign locations to each value returned by this call.\n  SmallVector<CCValAssign, 16> RVLocs;\n  CCState CCInfo(CallConv, isVarArg, DAG.getMachineFunction(), RVLocs,\n                 *DAG.getContext());\n  CCInfo.AnalyzeCallResult(Ins, RetCC_X86);\n\n  // Copy all of the result registers out of their specified physreg.\n  for (unsigned I = 0, InsIndex = 0, E = RVLocs.size(); I != E;\n       ++I, ++InsIndex) {\n    CCValAssign &VA = RVLocs[I];\n    EVT CopyVT = VA.getLocVT();\n\n    // In some calling conventions we need to remove the used registers\n    // from the register mask.\n    if (RegMask) {\n      for (MCSubRegIterator SubRegs(VA.getLocReg(), TRI, /*IncludeSelf=*/true);\n           SubRegs.isValid(); ++SubRegs)\n        RegMask[*SubRegs / 32] &= ~(1u << (*SubRegs % 32));\n    }\n\n    // Report an error if there was an attempt to return FP values via XMM\n    // registers.\n    if (!Subtarget.hasSSE1() && X86::FR32XRegClass.contains(VA.getLocReg())) {\n      errorUnsupported(DAG, dl, \"SSE register return with SSE disabled\");\n      if (VA.getLocReg() == X86::XMM1)\n        VA.convertToReg(X86::FP1); // Set reg to FP1, avoid hitting asserts.\n      else\n        VA.convertToReg(X86::FP0); // Set reg to FP0, avoid hitting asserts.\n    } else if (!Subtarget.hasSSE2() &&\n               X86::FR64XRegClass.contains(VA.getLocReg()) &&\n               CopyVT == MVT::f64) {\n      errorUnsupported(DAG, dl, \"SSE2 register return with SSE2 disabled\");\n      if (VA.getLocReg() == X86::XMM1)\n        VA.convertToReg(X86::FP1); // Set reg to FP1, avoid hitting asserts.\n      else\n        VA.convertToReg(X86::FP0); // Set reg to FP0, avoid hitting asserts.\n    }\n\n    // If we prefer to use the value in xmm registers, copy it out as f80 and\n    // use a truncate to move it from fp stack reg to xmm reg.\n    bool RoundAfterCopy = false;\n    if ((VA.getLocReg() == X86::FP0 || VA.getLocReg() == X86::FP1) &&\n        isScalarFPTypeInSSEReg(VA.getValVT())) {\n      if (!Subtarget.hasX87())\n        report_fatal_error(\"X87 register return with X87 disabled\");\n      CopyVT = MVT::f80;\n      RoundAfterCopy = (CopyVT != VA.getLocVT());\n    }\n\n    SDValue Val;\n    if (VA.needsCustom()) {\n      assert(VA.getValVT() == MVT::v64i1 &&\n             \"Currently the only custom case is when we split v64i1 to 2 regs\");\n      Val =\n          getv64i1Argument(VA, RVLocs[++I], Chain, DAG, dl, Subtarget, &InFlag);\n    } else {\n      Chain = DAG.getCopyFromReg(Chain, dl, VA.getLocReg(), CopyVT, InFlag)\n                  .getValue(1);\n      Val = Chain.getValue(0);\n      InFlag = Chain.getValue(2);\n    }\n\n    if (RoundAfterCopy)\n      Val = DAG.getNode(ISD::FP_ROUND, dl, VA.getValVT(), Val,\n                        // This truncation won't change the value.\n                        DAG.getIntPtrConstant(1, dl));\n\n    if (VA.isExtInLoc()) {\n      if (VA.getValVT().isVector() &&\n          VA.getValVT().getScalarType() == MVT::i1 &&\n          ((VA.getLocVT() == MVT::i64) || (VA.getLocVT() == MVT::i32) ||\n           (VA.getLocVT() == MVT::i16) || (VA.getLocVT() == MVT::i8))) {\n        // promoting a mask type (v*i1) into a register of type i64/i32/i16/i8\n        Val = lowerRegToMasks(Val, VA.getValVT(), VA.getLocVT(), dl, DAG);\n      } else\n        Val = DAG.getNode(ISD::TRUNCATE, dl, VA.getValVT(), Val);\n    }\n\n    if (VA.getLocInfo() == CCValAssign::BCvt)\n      Val = DAG.getBitcast(VA.getValVT(), Val);\n\n    InVals.push_back(Val);\n  }\n\n  return Chain;\n}\n\n//===----------------------------------------------------------------------===//\n//                C & StdCall & Fast Calling Convention implementation\n//===----------------------------------------------------------------------===//\n//  StdCall calling convention seems to be standard for many Windows' API\n//  routines and around. It differs from C calling convention just a little:\n//  callee should clean up the stack, not caller. Symbols should be also\n//  decorated in some fancy way :) It doesn't support any vector arguments.\n//  For info on fast calling convention see Fast Calling Convention (tail call)\n//  implementation LowerX86_32FastCCCallTo.\n\n/// CallIsStructReturn - Determines whether a call uses struct return\n/// semantics.\nenum StructReturnType {\n  NotStructReturn,\n  RegStructReturn,\n  StackStructReturn\n};\nstatic StructReturnType\ncallIsStructReturn(ArrayRef<ISD::OutputArg> Outs, bool IsMCU) {\n  if (Outs.empty())\n    return NotStructReturn;\n\n  const ISD::ArgFlagsTy &Flags = Outs[0].Flags;\n  if (!Flags.isSRet())\n    return NotStructReturn;\n  if (Flags.isInReg() || IsMCU)\n    return RegStructReturn;\n  return StackStructReturn;\n}\n\n/// Determines whether a function uses struct return semantics.\nstatic StructReturnType\nargsAreStructReturn(ArrayRef<ISD::InputArg> Ins, bool IsMCU) {\n  if (Ins.empty())\n    return NotStructReturn;\n\n  const ISD::ArgFlagsTy &Flags = Ins[0].Flags;\n  if (!Flags.isSRet())\n    return NotStructReturn;\n  if (Flags.isInReg() || IsMCU)\n    return RegStructReturn;\n  return StackStructReturn;\n}\n\n/// Make a copy of an aggregate at address specified by \"Src\" to address\n/// \"Dst\" with size and alignment information specified by the specific\n/// parameter attribute. The copy will be passed as a byval function parameter.\nstatic SDValue CreateCopyOfByValArgument(SDValue Src, SDValue Dst,\n                                         SDValue Chain, ISD::ArgFlagsTy Flags,\n                                         SelectionDAG &DAG, const SDLoc &dl) {\n  SDValue SizeNode = DAG.getIntPtrConstant(Flags.getByValSize(), dl);\n\n  return DAG.getMemcpy(\n      Chain, dl, Dst, Src, SizeNode, Flags.getNonZeroByValAlign(),\n      /*isVolatile*/ false, /*AlwaysInline=*/true,\n      /*isTailCall*/ false, MachinePointerInfo(), MachinePointerInfo());\n}\n\n/// Return true if the calling convention is one that we can guarantee TCO for.\nstatic bool canGuaranteeTCO(CallingConv::ID CC) {\n  return (CC == CallingConv::Fast || CC == CallingConv::GHC ||\n          CC == CallingConv::X86_RegCall || CC == CallingConv::HiPE ||\n          CC == CallingConv::HHVM || CC == CallingConv::Tail);\n}\n\n/// Return true if we might ever do TCO for calls with this calling convention.\nstatic bool mayTailCallThisCC(CallingConv::ID CC) {\n  switch (CC) {\n  // C calling conventions:\n  case CallingConv::C:\n  case CallingConv::Win64:\n  case CallingConv::X86_64_SysV:\n  // Callee pop conventions:\n  case CallingConv::X86_ThisCall:\n  case CallingConv::X86_StdCall:\n  case CallingConv::X86_VectorCall:\n  case CallingConv::X86_FastCall:\n  // Swift:\n  case CallingConv::Swift:\n    return true;\n  default:\n    return canGuaranteeTCO(CC);\n  }\n}\n\n/// Return true if the function is being made into a tailcall target by\n/// changing its ABI.\nstatic bool shouldGuaranteeTCO(CallingConv::ID CC, bool GuaranteedTailCallOpt) {\n  return (GuaranteedTailCallOpt && canGuaranteeTCO(CC)) || CC == CallingConv::Tail;\n}\n\nbool X86TargetLowering::mayBeEmittedAsTailCall(const CallInst *CI) const {\n  if (!CI->isTailCall())\n    return false;\n\n  CallingConv::ID CalleeCC = CI->getCallingConv();\n  if (!mayTailCallThisCC(CalleeCC))\n    return false;\n\n  return true;\n}\n\nSDValue\nX86TargetLowering::LowerMemArgument(SDValue Chain, CallingConv::ID CallConv,\n                                    const SmallVectorImpl<ISD::InputArg> &Ins,\n                                    const SDLoc &dl, SelectionDAG &DAG,\n                                    const CCValAssign &VA,\n                                    MachineFrameInfo &MFI, unsigned i) const {\n  // Create the nodes corresponding to a load from this parameter slot.\n  ISD::ArgFlagsTy Flags = Ins[i].Flags;\n  bool AlwaysUseMutable = shouldGuaranteeTCO(\n      CallConv, DAG.getTarget().Options.GuaranteedTailCallOpt);\n  bool isImmutable = !AlwaysUseMutable && !Flags.isByVal();\n  EVT ValVT;\n  MVT PtrVT = getPointerTy(DAG.getDataLayout());\n\n  // If value is passed by pointer we have address passed instead of the value\n  // itself. No need to extend if the mask value and location share the same\n  // absolute size.\n  bool ExtendedInMem =\n      VA.isExtInLoc() && VA.getValVT().getScalarType() == MVT::i1 &&\n      VA.getValVT().getSizeInBits() != VA.getLocVT().getSizeInBits();\n\n  if (VA.getLocInfo() == CCValAssign::Indirect || ExtendedInMem)\n    ValVT = VA.getLocVT();\n  else\n    ValVT = VA.getValVT();\n\n  // FIXME: For now, all byval parameter objects are marked mutable. This can be\n  // changed with more analysis.\n  // In case of tail call optimization mark all arguments mutable. Since they\n  // could be overwritten by lowering of arguments in case of a tail call.\n  if (Flags.isByVal()) {\n    unsigned Bytes = Flags.getByValSize();\n    if (Bytes == 0) Bytes = 1; // Don't create zero-sized stack objects.\n\n    // FIXME: For now, all byval parameter objects are marked as aliasing. This\n    // can be improved with deeper analysis.\n    int FI = MFI.CreateFixedObject(Bytes, VA.getLocMemOffset(), isImmutable,\n                                   /*isAliased=*/true);\n    return DAG.getFrameIndex(FI, PtrVT);\n  }\n\n  EVT ArgVT = Ins[i].ArgVT;\n\n  // If this is a vector that has been split into multiple parts, and the\n  // scalar size of the parts don't match the vector element size, then we can't\n  // elide the copy. The parts will have padding between them instead of being\n  // packed like a vector.\n  bool ScalarizedAndExtendedVector =\n      ArgVT.isVector() && !VA.getLocVT().isVector() &&\n      VA.getLocVT().getSizeInBits() != ArgVT.getScalarSizeInBits();\n\n  // This is an argument in memory. We might be able to perform copy elision.\n  // If the argument is passed directly in memory without any extension, then we\n  // can perform copy elision. Large vector types, for example, may be passed\n  // indirectly by pointer.\n  if (Flags.isCopyElisionCandidate() &&\n      VA.getLocInfo() != CCValAssign::Indirect && !ExtendedInMem &&\n      !ScalarizedAndExtendedVector) {\n    SDValue PartAddr;\n    if (Ins[i].PartOffset == 0) {\n      // If this is a one-part value or the first part of a multi-part value,\n      // create a stack object for the entire argument value type and return a\n      // load from our portion of it. This assumes that if the first part of an\n      // argument is in memory, the rest will also be in memory.\n      int FI = MFI.CreateFixedObject(ArgVT.getStoreSize(), VA.getLocMemOffset(),\n                                     /*IsImmutable=*/false);\n      PartAddr = DAG.getFrameIndex(FI, PtrVT);\n      return DAG.getLoad(\n          ValVT, dl, Chain, PartAddr,\n          MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI));\n    } else {\n      // This is not the first piece of an argument in memory. See if there is\n      // already a fixed stack object including this offset. If so, assume it\n      // was created by the PartOffset == 0 branch above and create a load from\n      // the appropriate offset into it.\n      int64_t PartBegin = VA.getLocMemOffset();\n      int64_t PartEnd = PartBegin + ValVT.getSizeInBits() / 8;\n      int FI = MFI.getObjectIndexBegin();\n      for (; MFI.isFixedObjectIndex(FI); ++FI) {\n        int64_t ObjBegin = MFI.getObjectOffset(FI);\n        int64_t ObjEnd = ObjBegin + MFI.getObjectSize(FI);\n        if (ObjBegin <= PartBegin && PartEnd <= ObjEnd)\n          break;\n      }\n      if (MFI.isFixedObjectIndex(FI)) {\n        SDValue Addr =\n            DAG.getNode(ISD::ADD, dl, PtrVT, DAG.getFrameIndex(FI, PtrVT),\n                        DAG.getIntPtrConstant(Ins[i].PartOffset, dl));\n        return DAG.getLoad(\n            ValVT, dl, Chain, Addr,\n            MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI,\n                                              Ins[i].PartOffset));\n      }\n    }\n  }\n\n  int FI = MFI.CreateFixedObject(ValVT.getSizeInBits() / 8,\n                                 VA.getLocMemOffset(), isImmutable);\n\n  // Set SExt or ZExt flag.\n  if (VA.getLocInfo() == CCValAssign::ZExt) {\n    MFI.setObjectZExt(FI, true);\n  } else if (VA.getLocInfo() == CCValAssign::SExt) {\n    MFI.setObjectSExt(FI, true);\n  }\n\n  SDValue FIN = DAG.getFrameIndex(FI, PtrVT);\n  SDValue Val = DAG.getLoad(\n      ValVT, dl, Chain, FIN,\n      MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI));\n  return ExtendedInMem\n             ? (VA.getValVT().isVector()\n                    ? DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VA.getValVT(), Val)\n                    : DAG.getNode(ISD::TRUNCATE, dl, VA.getValVT(), Val))\n             : Val;\n}\n\n// FIXME: Get this from tablegen.\nstatic ArrayRef<MCPhysReg> get64BitArgumentGPRs(CallingConv::ID CallConv,\n                                                const X86Subtarget &Subtarget) {\n  assert(Subtarget.is64Bit());\n\n  if (Subtarget.isCallingConvWin64(CallConv)) {\n    static const MCPhysReg GPR64ArgRegsWin64[] = {\n      X86::RCX, X86::RDX, X86::R8,  X86::R9\n    };\n    return makeArrayRef(std::begin(GPR64ArgRegsWin64), std::end(GPR64ArgRegsWin64));\n  }\n\n  static const MCPhysReg GPR64ArgRegs64Bit[] = {\n    X86::RDI, X86::RSI, X86::RDX, X86::RCX, X86::R8, X86::R9\n  };\n  return makeArrayRef(std::begin(GPR64ArgRegs64Bit), std::end(GPR64ArgRegs64Bit));\n}\n\n// FIXME: Get this from tablegen.\nstatic ArrayRef<MCPhysReg> get64BitArgumentXMMs(MachineFunction &MF,\n                                                CallingConv::ID CallConv,\n                                                const X86Subtarget &Subtarget) {\n  assert(Subtarget.is64Bit());\n  if (Subtarget.isCallingConvWin64(CallConv)) {\n    // The XMM registers which might contain var arg parameters are shadowed\n    // in their paired GPR.  So we only need to save the GPR to their home\n    // slots.\n    // TODO: __vectorcall will change this.\n    return None;\n  }\n\n  const Function &F = MF.getFunction();\n  bool NoImplicitFloatOps = F.hasFnAttribute(Attribute::NoImplicitFloat);\n  bool isSoftFloat = Subtarget.useSoftFloat();\n  assert(!(isSoftFloat && NoImplicitFloatOps) &&\n         \"SSE register cannot be used when SSE is disabled!\");\n  if (isSoftFloat || NoImplicitFloatOps || !Subtarget.hasSSE1())\n    // Kernel mode asks for SSE to be disabled, so there are no XMM argument\n    // registers.\n    return None;\n\n  static const MCPhysReg XMMArgRegs64Bit[] = {\n    X86::XMM0, X86::XMM1, X86::XMM2, X86::XMM3,\n    X86::XMM4, X86::XMM5, X86::XMM6, X86::XMM7\n  };\n  return makeArrayRef(std::begin(XMMArgRegs64Bit), std::end(XMMArgRegs64Bit));\n}\n\n#ifndef NDEBUG\nstatic bool isSortedByValueNo(ArrayRef<CCValAssign> ArgLocs) {\n  return llvm::is_sorted(\n      ArgLocs, [](const CCValAssign &A, const CCValAssign &B) -> bool {\n        return A.getValNo() < B.getValNo();\n      });\n}\n#endif\n\nnamespace {\n/// This is a helper class for lowering variable arguments parameters.\nclass VarArgsLoweringHelper {\npublic:\n  VarArgsLoweringHelper(X86MachineFunctionInfo *FuncInfo, const SDLoc &Loc,\n                        SelectionDAG &DAG, const X86Subtarget &Subtarget,\n                        CallingConv::ID CallConv, CCState &CCInfo)\n      : FuncInfo(FuncInfo), DL(Loc), DAG(DAG), Subtarget(Subtarget),\n        TheMachineFunction(DAG.getMachineFunction()),\n        TheFunction(TheMachineFunction.getFunction()),\n        FrameInfo(TheMachineFunction.getFrameInfo()),\n        FrameLowering(*Subtarget.getFrameLowering()),\n        TargLowering(DAG.getTargetLoweringInfo()), CallConv(CallConv),\n        CCInfo(CCInfo) {}\n\n  // Lower variable arguments parameters.\n  void lowerVarArgsParameters(SDValue &Chain, unsigned StackSize);\n\nprivate:\n  void createVarArgAreaAndStoreRegisters(SDValue &Chain, unsigned StackSize);\n\n  void forwardMustTailParameters(SDValue &Chain);\n\n  bool is64Bit() const { return Subtarget.is64Bit(); }\n  bool isWin64() const { return Subtarget.isCallingConvWin64(CallConv); }\n\n  X86MachineFunctionInfo *FuncInfo;\n  const SDLoc &DL;\n  SelectionDAG &DAG;\n  const X86Subtarget &Subtarget;\n  MachineFunction &TheMachineFunction;\n  const Function &TheFunction;\n  MachineFrameInfo &FrameInfo;\n  const TargetFrameLowering &FrameLowering;\n  const TargetLowering &TargLowering;\n  CallingConv::ID CallConv;\n  CCState &CCInfo;\n};\n} // namespace\n\nvoid VarArgsLoweringHelper::createVarArgAreaAndStoreRegisters(\n    SDValue &Chain, unsigned StackSize) {\n  // If the function takes variable number of arguments, make a frame index for\n  // the start of the first vararg value... for expansion of llvm.va_start. We\n  // can skip this if there are no va_start calls.\n  if (is64Bit() || (CallConv != CallingConv::X86_FastCall &&\n                    CallConv != CallingConv::X86_ThisCall)) {\n    FuncInfo->setVarArgsFrameIndex(\n        FrameInfo.CreateFixedObject(1, StackSize, true));\n  }\n\n  // Figure out if XMM registers are in use.\n  assert(!(Subtarget.useSoftFloat() &&\n           TheFunction.hasFnAttribute(Attribute::NoImplicitFloat)) &&\n         \"SSE register cannot be used when SSE is disabled!\");\n\n  // 64-bit calling conventions support varargs and register parameters, so we\n  // have to do extra work to spill them in the prologue.\n  if (is64Bit()) {\n    // Find the first unallocated argument registers.\n    ArrayRef<MCPhysReg> ArgGPRs = get64BitArgumentGPRs(CallConv, Subtarget);\n    ArrayRef<MCPhysReg> ArgXMMs =\n        get64BitArgumentXMMs(TheMachineFunction, CallConv, Subtarget);\n    unsigned NumIntRegs = CCInfo.getFirstUnallocated(ArgGPRs);\n    unsigned NumXMMRegs = CCInfo.getFirstUnallocated(ArgXMMs);\n\n    assert(!(NumXMMRegs && !Subtarget.hasSSE1()) &&\n           \"SSE register cannot be used when SSE is disabled!\");\n\n    if (isWin64()) {\n      // Get to the caller-allocated home save location.  Add 8 to account\n      // for the return address.\n      int HomeOffset = FrameLowering.getOffsetOfLocalArea() + 8;\n      FuncInfo->setRegSaveFrameIndex(\n          FrameInfo.CreateFixedObject(1, NumIntRegs * 8 + HomeOffset, false));\n      // Fixup to set vararg frame on shadow area (4 x i64).\n      if (NumIntRegs < 4)\n        FuncInfo->setVarArgsFrameIndex(FuncInfo->getRegSaveFrameIndex());\n    } else {\n      // For X86-64, if there are vararg parameters that are passed via\n      // registers, then we must store them to their spots on the stack so\n      // they may be loaded by dereferencing the result of va_next.\n      FuncInfo->setVarArgsGPOffset(NumIntRegs * 8);\n      FuncInfo->setVarArgsFPOffset(ArgGPRs.size() * 8 + NumXMMRegs * 16);\n      FuncInfo->setRegSaveFrameIndex(FrameInfo.CreateStackObject(\n          ArgGPRs.size() * 8 + ArgXMMs.size() * 16, Align(16), false));\n    }\n\n    SmallVector<SDValue, 6>\n        LiveGPRs; // list of SDValue for GPR registers keeping live input value\n    SmallVector<SDValue, 8> LiveXMMRegs; // list of SDValue for XMM registers\n                                         // keeping live input value\n    SDValue ALVal; // if applicable keeps SDValue for %al register\n\n    // Gather all the live in physical registers.\n    for (MCPhysReg Reg : ArgGPRs.slice(NumIntRegs)) {\n      Register GPR = TheMachineFunction.addLiveIn(Reg, &X86::GR64RegClass);\n      LiveGPRs.push_back(DAG.getCopyFromReg(Chain, DL, GPR, MVT::i64));\n    }\n    const auto &AvailableXmms = ArgXMMs.slice(NumXMMRegs);\n    if (!AvailableXmms.empty()) {\n      Register AL = TheMachineFunction.addLiveIn(X86::AL, &X86::GR8RegClass);\n      ALVal = DAG.getCopyFromReg(Chain, DL, AL, MVT::i8);\n      for (MCPhysReg Reg : AvailableXmms) {\n        Register XMMReg = TheMachineFunction.addLiveIn(Reg, &X86::VR128RegClass);\n        LiveXMMRegs.push_back(\n            DAG.getCopyFromReg(Chain, DL, XMMReg, MVT::v4f32));\n      }\n    }\n\n    // Store the integer parameter registers.\n    SmallVector<SDValue, 8> MemOps;\n    SDValue RSFIN =\n        DAG.getFrameIndex(FuncInfo->getRegSaveFrameIndex(),\n                          TargLowering.getPointerTy(DAG.getDataLayout()));\n    unsigned Offset = FuncInfo->getVarArgsGPOffset();\n    for (SDValue Val : LiveGPRs) {\n      SDValue FIN = DAG.getNode(ISD::ADD, DL,\n                                TargLowering.getPointerTy(DAG.getDataLayout()),\n                                RSFIN, DAG.getIntPtrConstant(Offset, DL));\n      SDValue Store =\n          DAG.getStore(Val.getValue(1), DL, Val, FIN,\n                       MachinePointerInfo::getFixedStack(\n                           DAG.getMachineFunction(),\n                           FuncInfo->getRegSaveFrameIndex(), Offset));\n      MemOps.push_back(Store);\n      Offset += 8;\n    }\n\n    // Now store the XMM (fp + vector) parameter registers.\n    if (!LiveXMMRegs.empty()) {\n      SmallVector<SDValue, 12> SaveXMMOps;\n      SaveXMMOps.push_back(Chain);\n      SaveXMMOps.push_back(ALVal);\n      SaveXMMOps.push_back(\n          DAG.getTargetConstant(FuncInfo->getRegSaveFrameIndex(), DL, MVT::i32));\n      SaveXMMOps.push_back(\n          DAG.getTargetConstant(FuncInfo->getVarArgsFPOffset(), DL, MVT::i32));\n      llvm::append_range(SaveXMMOps, LiveXMMRegs);\n      MemOps.push_back(DAG.getNode(X86ISD::VASTART_SAVE_XMM_REGS, DL,\n                                   MVT::Other, SaveXMMOps));\n    }\n\n    if (!MemOps.empty())\n      Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOps);\n  }\n}\n\nvoid VarArgsLoweringHelper::forwardMustTailParameters(SDValue &Chain) {\n  // Find the largest legal vector type.\n  MVT VecVT = MVT::Other;\n  // FIXME: Only some x86_32 calling conventions support AVX512.\n  if (Subtarget.useAVX512Regs() &&\n      (is64Bit() || (CallConv == CallingConv::X86_VectorCall ||\n                     CallConv == CallingConv::Intel_OCL_BI)))\n    VecVT = MVT::v16f32;\n  else if (Subtarget.hasAVX())\n    VecVT = MVT::v8f32;\n  else if (Subtarget.hasSSE2())\n    VecVT = MVT::v4f32;\n\n  // We forward some GPRs and some vector types.\n  SmallVector<MVT, 2> RegParmTypes;\n  MVT IntVT = is64Bit() ? MVT::i64 : MVT::i32;\n  RegParmTypes.push_back(IntVT);\n  if (VecVT != MVT::Other)\n    RegParmTypes.push_back(VecVT);\n\n  // Compute the set of forwarded registers. The rest are scratch.\n  SmallVectorImpl<ForwardedRegister> &Forwards =\n      FuncInfo->getForwardedMustTailRegParms();\n  CCInfo.analyzeMustTailForwardedRegisters(Forwards, RegParmTypes, CC_X86);\n\n  // Forward AL for SysV x86_64 targets, since it is used for varargs.\n  if (is64Bit() && !isWin64() && !CCInfo.isAllocated(X86::AL)) {\n    Register ALVReg = TheMachineFunction.addLiveIn(X86::AL, &X86::GR8RegClass);\n    Forwards.push_back(ForwardedRegister(ALVReg, X86::AL, MVT::i8));\n  }\n\n  // Copy all forwards from physical to virtual registers.\n  for (ForwardedRegister &FR : Forwards) {\n    // FIXME: Can we use a less constrained schedule?\n    SDValue RegVal = DAG.getCopyFromReg(Chain, DL, FR.VReg, FR.VT);\n    FR.VReg = TheMachineFunction.getRegInfo().createVirtualRegister(\n        TargLowering.getRegClassFor(FR.VT));\n    Chain = DAG.getCopyToReg(Chain, DL, FR.VReg, RegVal);\n  }\n}\n\nvoid VarArgsLoweringHelper::lowerVarArgsParameters(SDValue &Chain,\n                                                   unsigned StackSize) {\n  // Set FrameIndex to the 0xAAAAAAA value to mark unset state.\n  // If necessary, it would be set into the correct value later.\n  FuncInfo->setVarArgsFrameIndex(0xAAAAAAA);\n  FuncInfo->setRegSaveFrameIndex(0xAAAAAAA);\n\n  if (FrameInfo.hasVAStart())\n    createVarArgAreaAndStoreRegisters(Chain, StackSize);\n\n  if (FrameInfo.hasMustTailInVarArgFunc())\n    forwardMustTailParameters(Chain);\n}\n\nSDValue X86TargetLowering::LowerFormalArguments(\n    SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,\n    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &dl,\n    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();\n\n  const Function &F = MF.getFunction();\n  if (F.hasExternalLinkage() && Subtarget.isTargetCygMing() &&\n      F.getName() == \"main\")\n    FuncInfo->setForceFramePointer(true);\n\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  bool Is64Bit = Subtarget.is64Bit();\n  bool IsWin64 = Subtarget.isCallingConvWin64(CallConv);\n\n  assert(\n      !(IsVarArg && canGuaranteeTCO(CallConv)) &&\n      \"Var args not supported with calling conv' regcall, fastcc, ghc or hipe\");\n\n  // Assign locations to all of the incoming arguments.\n  SmallVector<CCValAssign, 16> ArgLocs;\n  CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getContext());\n\n  // Allocate shadow area for Win64.\n  if (IsWin64)\n    CCInfo.AllocateStack(32, Align(8));\n\n  CCInfo.AnalyzeArguments(Ins, CC_X86);\n\n  // In vectorcall calling convention a second pass is required for the HVA\n  // types.\n  if (CallingConv::X86_VectorCall == CallConv) {\n    CCInfo.AnalyzeArgumentsSecondPass(Ins, CC_X86);\n  }\n\n  // The next loop assumes that the locations are in the same order of the\n  // input arguments.\n  assert(isSortedByValueNo(ArgLocs) &&\n         \"Argument Location list must be sorted before lowering\");\n\n  SDValue ArgValue;\n  for (unsigned I = 0, InsIndex = 0, E = ArgLocs.size(); I != E;\n       ++I, ++InsIndex) {\n    assert(InsIndex < Ins.size() && \"Invalid Ins index\");\n    CCValAssign &VA = ArgLocs[I];\n\n    if (VA.isRegLoc()) {\n      EVT RegVT = VA.getLocVT();\n      if (VA.needsCustom()) {\n        assert(\n            VA.getValVT() == MVT::v64i1 &&\n            \"Currently the only custom case is when we split v64i1 to 2 regs\");\n\n        // v64i1 values, in regcall calling convention, that are\n        // compiled to 32 bit arch, are split up into two registers.\n        ArgValue =\n            getv64i1Argument(VA, ArgLocs[++I], Chain, DAG, dl, Subtarget);\n      } else {\n        const TargetRegisterClass *RC;\n        if (RegVT == MVT::i8)\n          RC = &X86::GR8RegClass;\n        else if (RegVT == MVT::i16)\n          RC = &X86::GR16RegClass;\n        else if (RegVT == MVT::i32)\n          RC = &X86::GR32RegClass;\n        else if (Is64Bit && RegVT == MVT::i64)\n          RC = &X86::GR64RegClass;\n        else if (RegVT == MVT::f32)\n          RC = Subtarget.hasAVX512() ? &X86::FR32XRegClass : &X86::FR32RegClass;\n        else if (RegVT == MVT::f64)\n          RC = Subtarget.hasAVX512() ? &X86::FR64XRegClass : &X86::FR64RegClass;\n        else if (RegVT == MVT::f80)\n          RC = &X86::RFP80RegClass;\n        else if (RegVT == MVT::f128)\n          RC = &X86::VR128RegClass;\n        else if (RegVT.is512BitVector())\n          RC = &X86::VR512RegClass;\n        else if (RegVT.is256BitVector())\n          RC = Subtarget.hasVLX() ? &X86::VR256XRegClass : &X86::VR256RegClass;\n        else if (RegVT.is128BitVector())\n          RC = Subtarget.hasVLX() ? &X86::VR128XRegClass : &X86::VR128RegClass;\n        else if (RegVT == MVT::x86mmx)\n          RC = &X86::VR64RegClass;\n        else if (RegVT == MVT::v1i1)\n          RC = &X86::VK1RegClass;\n        else if (RegVT == MVT::v8i1)\n          RC = &X86::VK8RegClass;\n        else if (RegVT == MVT::v16i1)\n          RC = &X86::VK16RegClass;\n        else if (RegVT == MVT::v32i1)\n          RC = &X86::VK32RegClass;\n        else if (RegVT == MVT::v64i1)\n          RC = &X86::VK64RegClass;\n        else\n          llvm_unreachable(\"Unknown argument type!\");\n\n        Register Reg = MF.addLiveIn(VA.getLocReg(), RC);\n        ArgValue = DAG.getCopyFromReg(Chain, dl, Reg, RegVT);\n      }\n\n      // If this is an 8 or 16-bit value, it is really passed promoted to 32\n      // bits.  Insert an assert[sz]ext to capture this, then truncate to the\n      // right size.\n      if (VA.getLocInfo() == CCValAssign::SExt)\n        ArgValue = DAG.getNode(ISD::AssertSext, dl, RegVT, ArgValue,\n                               DAG.getValueType(VA.getValVT()));\n      else if (VA.getLocInfo() == CCValAssign::ZExt)\n        ArgValue = DAG.getNode(ISD::AssertZext, dl, RegVT, ArgValue,\n                               DAG.getValueType(VA.getValVT()));\n      else if (VA.getLocInfo() == CCValAssign::BCvt)\n        ArgValue = DAG.getBitcast(VA.getValVT(), ArgValue);\n\n      if (VA.isExtInLoc()) {\n        // Handle MMX values passed in XMM regs.\n        if (RegVT.isVector() && VA.getValVT().getScalarType() != MVT::i1)\n          ArgValue = DAG.getNode(X86ISD::MOVDQ2Q, dl, VA.getValVT(), ArgValue);\n        else if (VA.getValVT().isVector() &&\n                 VA.getValVT().getScalarType() == MVT::i1 &&\n                 ((VA.getLocVT() == MVT::i64) || (VA.getLocVT() == MVT::i32) ||\n                  (VA.getLocVT() == MVT::i16) || (VA.getLocVT() == MVT::i8))) {\n          // Promoting a mask type (v*i1) into a register of type i64/i32/i16/i8\n          ArgValue = lowerRegToMasks(ArgValue, VA.getValVT(), RegVT, dl, DAG);\n        } else\n          ArgValue = DAG.getNode(ISD::TRUNCATE, dl, VA.getValVT(), ArgValue);\n      }\n    } else {\n      assert(VA.isMemLoc());\n      ArgValue =\n          LowerMemArgument(Chain, CallConv, Ins, dl, DAG, VA, MFI, InsIndex);\n    }\n\n    // If value is passed via pointer - do a load.\n    if (VA.getLocInfo() == CCValAssign::Indirect && !Ins[I].Flags.isByVal())\n      ArgValue =\n          DAG.getLoad(VA.getValVT(), dl, Chain, ArgValue, MachinePointerInfo());\n\n    InVals.push_back(ArgValue);\n  }\n\n  for (unsigned I = 0, E = Ins.size(); I != E; ++I) {\n    // Swift calling convention does not require we copy the sret argument\n    // into %rax/%eax for the return. We don't set SRetReturnReg for Swift.\n    if (CallConv == CallingConv::Swift)\n      continue;\n\n    // All x86 ABIs require that for returning structs by value we copy the\n    // sret argument into %rax/%eax (depending on ABI) for the return. Save\n    // the argument into a virtual register so that we can access it from the\n    // return points.\n    if (Ins[I].Flags.isSRet()) {\n      Register Reg = FuncInfo->getSRetReturnReg();\n      if (!Reg) {\n        MVT PtrTy = getPointerTy(DAG.getDataLayout());\n        Reg = MF.getRegInfo().createVirtualRegister(getRegClassFor(PtrTy));\n        FuncInfo->setSRetReturnReg(Reg);\n      }\n      SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), dl, Reg, InVals[I]);\n      Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other, Copy, Chain);\n      break;\n    }\n  }\n\n  unsigned StackSize = CCInfo.getNextStackOffset();\n  // Align stack specially for tail calls.\n  if (shouldGuaranteeTCO(CallConv,\n                         MF.getTarget().Options.GuaranteedTailCallOpt))\n    StackSize = GetAlignedArgumentStackSize(StackSize, DAG);\n\n  if (IsVarArg)\n    VarArgsLoweringHelper(FuncInfo, dl, DAG, Subtarget, CallConv, CCInfo)\n        .lowerVarArgsParameters(Chain, StackSize);\n\n  // Some CCs need callee pop.\n  if (X86::isCalleePop(CallConv, Is64Bit, IsVarArg,\n                       MF.getTarget().Options.GuaranteedTailCallOpt)) {\n    FuncInfo->setBytesToPopOnReturn(StackSize); // Callee pops everything.\n  } else if (CallConv == CallingConv::X86_INTR && Ins.size() == 2) {\n    // X86 interrupts must pop the error code (and the alignment padding) if\n    // present.\n    FuncInfo->setBytesToPopOnReturn(Is64Bit ? 16 : 4);\n  } else {\n    FuncInfo->setBytesToPopOnReturn(0); // Callee pops nothing.\n    // If this is an sret function, the return should pop the hidden pointer.\n    if (!Is64Bit && !canGuaranteeTCO(CallConv) &&\n        !Subtarget.getTargetTriple().isOSMSVCRT() &&\n        argsAreStructReturn(Ins, Subtarget.isTargetMCU()) == StackStructReturn)\n      FuncInfo->setBytesToPopOnReturn(4);\n  }\n\n  if (!Is64Bit) {\n    // RegSaveFrameIndex is X86-64 only.\n    FuncInfo->setRegSaveFrameIndex(0xAAAAAAA);\n  }\n\n  FuncInfo->setArgumentStackSize(StackSize);\n\n  if (WinEHFuncInfo *EHInfo = MF.getWinEHFuncInfo()) {\n    EHPersonality Personality = classifyEHPersonality(F.getPersonalityFn());\n    if (Personality == EHPersonality::CoreCLR) {\n      assert(Is64Bit);\n      // TODO: Add a mechanism to frame lowering that will allow us to indicate\n      // that we'd prefer this slot be allocated towards the bottom of the frame\n      // (i.e. near the stack pointer after allocating the frame).  Every\n      // funclet needs a copy of this slot in its (mostly empty) frame, and the\n      // offset from the bottom of this and each funclet's frame must be the\n      // same, so the size of funclets' (mostly empty) frames is dictated by\n      // how far this slot is from the bottom (since they allocate just enough\n      // space to accommodate holding this slot at the correct offset).\n      int PSPSymFI = MFI.CreateStackObject(8, Align(8), /*isSpillSlot=*/false);\n      EHInfo->PSPSymFrameIdx = PSPSymFI;\n    }\n  }\n\n  if (CallConv == CallingConv::X86_RegCall ||\n      F.hasFnAttribute(\"no_caller_saved_registers\")) {\n    MachineRegisterInfo &MRI = MF.getRegInfo();\n    for (std::pair<Register, Register> Pair : MRI.liveins())\n      MRI.disableCalleeSavedRegister(Pair.first);\n  }\n\n  return Chain;\n}\n\nSDValue X86TargetLowering::LowerMemOpCallTo(SDValue Chain, SDValue StackPtr,\n                                            SDValue Arg, const SDLoc &dl,\n                                            SelectionDAG &DAG,\n                                            const CCValAssign &VA,\n                                            ISD::ArgFlagsTy Flags,\n                                            bool isByVal) const {\n  unsigned LocMemOffset = VA.getLocMemOffset();\n  SDValue PtrOff = DAG.getIntPtrConstant(LocMemOffset, dl);\n  PtrOff = DAG.getNode(ISD::ADD, dl, getPointerTy(DAG.getDataLayout()),\n                       StackPtr, PtrOff);\n  if (isByVal)\n    return CreateCopyOfByValArgument(Arg, PtrOff, Chain, Flags, DAG, dl);\n\n  return DAG.getStore(\n      Chain, dl, Arg, PtrOff,\n      MachinePointerInfo::getStack(DAG.getMachineFunction(), LocMemOffset));\n}\n\n/// Emit a load of return address if tail call\n/// optimization is performed and it is required.\nSDValue X86TargetLowering::EmitTailCallLoadRetAddr(\n    SelectionDAG &DAG, SDValue &OutRetAddr, SDValue Chain, bool IsTailCall,\n    bool Is64Bit, int FPDiff, const SDLoc &dl) const {\n  // Adjust the Return address stack slot.\n  EVT VT = getPointerTy(DAG.getDataLayout());\n  OutRetAddr = getReturnAddressFrameIndex(DAG);\n\n  // Load the \"old\" Return address.\n  OutRetAddr = DAG.getLoad(VT, dl, Chain, OutRetAddr, MachinePointerInfo());\n  return SDValue(OutRetAddr.getNode(), 1);\n}\n\n/// Emit a store of the return address if tail call\n/// optimization is performed and it is required (FPDiff!=0).\nstatic SDValue EmitTailCallStoreRetAddr(SelectionDAG &DAG, MachineFunction &MF,\n                                        SDValue Chain, SDValue RetAddrFrIdx,\n                                        EVT PtrVT, unsigned SlotSize,\n                                        int FPDiff, const SDLoc &dl) {\n  // Store the return address to the appropriate stack slot.\n  if (!FPDiff) return Chain;\n  // Calculate the new stack slot for the return address.\n  int NewReturnAddrFI =\n    MF.getFrameInfo().CreateFixedObject(SlotSize, (int64_t)FPDiff - SlotSize,\n                                         false);\n  SDValue NewRetAddrFrIdx = DAG.getFrameIndex(NewReturnAddrFI, PtrVT);\n  Chain = DAG.getStore(Chain, dl, RetAddrFrIdx, NewRetAddrFrIdx,\n                       MachinePointerInfo::getFixedStack(\n                           DAG.getMachineFunction(), NewReturnAddrFI));\n  return Chain;\n}\n\n/// Returns a vector_shuffle mask for an movs{s|d}, movd\n/// operation of specified width.\nstatic SDValue getMOVL(SelectionDAG &DAG, const SDLoc &dl, MVT VT, SDValue V1,\n                       SDValue V2) {\n  unsigned NumElems = VT.getVectorNumElements();\n  SmallVector<int, 8> Mask;\n  Mask.push_back(NumElems);\n  for (unsigned i = 1; i != NumElems; ++i)\n    Mask.push_back(i);\n  return DAG.getVectorShuffle(VT, dl, V1, V2, Mask);\n}\n\nSDValue\nX86TargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,\n                             SmallVectorImpl<SDValue> &InVals) const {\n  SelectionDAG &DAG                     = CLI.DAG;\n  SDLoc &dl                             = CLI.DL;\n  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;\n  SmallVectorImpl<SDValue> &OutVals     = CLI.OutVals;\n  SmallVectorImpl<ISD::InputArg> &Ins   = CLI.Ins;\n  SDValue Chain                         = CLI.Chain;\n  SDValue Callee                        = CLI.Callee;\n  CallingConv::ID CallConv              = CLI.CallConv;\n  bool &isTailCall                      = CLI.IsTailCall;\n  bool isVarArg                         = CLI.IsVarArg;\n\n  MachineFunction &MF = DAG.getMachineFunction();\n  bool Is64Bit        = Subtarget.is64Bit();\n  bool IsWin64        = Subtarget.isCallingConvWin64(CallConv);\n  StructReturnType SR = callIsStructReturn(Outs, Subtarget.isTargetMCU());\n  bool IsSibcall      = false;\n  bool IsGuaranteeTCO = MF.getTarget().Options.GuaranteedTailCallOpt ||\n      CallConv == CallingConv::Tail;\n  X86MachineFunctionInfo *X86Info = MF.getInfo<X86MachineFunctionInfo>();\n  const auto *CI = dyn_cast_or_null<CallInst>(CLI.CB);\n  const Function *Fn = CI ? CI->getCalledFunction() : nullptr;\n  bool HasNCSR = (CI && CI->hasFnAttr(\"no_caller_saved_registers\")) ||\n                 (Fn && Fn->hasFnAttribute(\"no_caller_saved_registers\"));\n  const auto *II = dyn_cast_or_null<InvokeInst>(CLI.CB);\n  bool HasNoCfCheck =\n      (CI && CI->doesNoCfCheck()) || (II && II->doesNoCfCheck());\n\tbool IsIndirectCall = (CI && CI->isIndirectCall());\n  const Module *M = MF.getMMI().getModule();\n  Metadata *IsCFProtectionSupported = M->getModuleFlag(\"cf-protection-branch\");\n\n  MachineFunction::CallSiteInfo CSInfo;\n  if (CallConv == CallingConv::X86_INTR)\n    report_fatal_error(\"X86 interrupts may not be called directly\");\n\n  if (Subtarget.isPICStyleGOT() && !IsGuaranteeTCO) {\n    // If we are using a GOT, disable tail calls to external symbols with\n    // default visibility. Tail calling such a symbol requires using a GOT\n    // relocation, which forces early binding of the symbol. This breaks code\n    // that require lazy function symbol resolution. Using musttail or\n    // GuaranteedTailCallOpt will override this.\n    GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee);\n    if (!G || (!G->getGlobal()->hasLocalLinkage() &&\n               G->getGlobal()->hasDefaultVisibility()))\n      isTailCall = false;\n  }\n\n  bool IsMustTail = CLI.CB && CLI.CB->isMustTailCall();\n  if (IsMustTail) {\n    // Force this to be a tail call.  The verifier rules are enough to ensure\n    // that we can lower this successfully without moving the return address\n    // around.\n    isTailCall = true;\n  } else if (isTailCall) {\n    // Check if it's really possible to do a tail call.\n    isTailCall = IsEligibleForTailCallOptimization(Callee, CallConv,\n                    isVarArg, SR != NotStructReturn,\n                    MF.getFunction().hasStructRetAttr(), CLI.RetTy,\n                    Outs, OutVals, Ins, DAG);\n\n    // Sibcalls are automatically detected tailcalls which do not require\n    // ABI changes.\n    if (!IsGuaranteeTCO && isTailCall)\n      IsSibcall = true;\n\n    if (isTailCall)\n      ++NumTailCalls;\n  }\n\n  assert(!(isVarArg && canGuaranteeTCO(CallConv)) &&\n         \"Var args not supported with calling convention fastcc, ghc or hipe\");\n\n  // Analyze operands of the call, assigning locations to each operand.\n  SmallVector<CCValAssign, 16> ArgLocs;\n  CCState CCInfo(CallConv, isVarArg, MF, ArgLocs, *DAG.getContext());\n\n  // Allocate shadow area for Win64.\n  if (IsWin64)\n    CCInfo.AllocateStack(32, Align(8));\n\n  CCInfo.AnalyzeArguments(Outs, CC_X86);\n\n  // In vectorcall calling convention a second pass is required for the HVA\n  // types.\n  if (CallingConv::X86_VectorCall == CallConv) {\n    CCInfo.AnalyzeArgumentsSecondPass(Outs, CC_X86);\n  }\n\n  // Get a count of how many bytes are to be pushed on the stack.\n  unsigned NumBytes = CCInfo.getAlignedCallFrameSize();\n  if (IsSibcall)\n    // This is a sibcall. The memory operands are available in caller's\n    // own caller's stack.\n    NumBytes = 0;\n  else if (IsGuaranteeTCO && canGuaranteeTCO(CallConv))\n    NumBytes = GetAlignedArgumentStackSize(NumBytes, DAG);\n\n  int FPDiff = 0;\n  if (isTailCall && !IsSibcall && !IsMustTail) {\n    // Lower arguments at fp - stackoffset + fpdiff.\n    unsigned NumBytesCallerPushed = X86Info->getBytesToPopOnReturn();\n\n    FPDiff = NumBytesCallerPushed - NumBytes;\n\n    // Set the delta of movement of the returnaddr stackslot.\n    // But only set if delta is greater than previous delta.\n    if (FPDiff < X86Info->getTCReturnAddrDelta())\n      X86Info->setTCReturnAddrDelta(FPDiff);\n  }\n\n  unsigned NumBytesToPush = NumBytes;\n  unsigned NumBytesToPop = NumBytes;\n\n  // If we have an inalloca argument, all stack space has already been allocated\n  // for us and be right at the top of the stack.  We don't support multiple\n  // arguments passed in memory when using inalloca.\n  if (!Outs.empty() && Outs.back().Flags.isInAlloca()) {\n    NumBytesToPush = 0;\n    if (!ArgLocs.back().isMemLoc())\n      report_fatal_error(\"cannot use inalloca attribute on a register \"\n                         \"parameter\");\n    if (ArgLocs.back().getLocMemOffset() != 0)\n      report_fatal_error(\"any parameter with the inalloca attribute must be \"\n                         \"the only memory argument\");\n  } else if (CLI.IsPreallocated) {\n    assert(ArgLocs.back().isMemLoc() &&\n           \"cannot use preallocated attribute on a register \"\n           \"parameter\");\n    SmallVector<size_t, 4> PreallocatedOffsets;\n    for (size_t i = 0; i < CLI.OutVals.size(); ++i) {\n      if (CLI.CB->paramHasAttr(i, Attribute::Preallocated)) {\n        PreallocatedOffsets.push_back(ArgLocs[i].getLocMemOffset());\n      }\n    }\n    auto *MFI = DAG.getMachineFunction().getInfo<X86MachineFunctionInfo>();\n    size_t PreallocatedId = MFI->getPreallocatedIdForCallSite(CLI.CB);\n    MFI->setPreallocatedStackSize(PreallocatedId, NumBytes);\n    MFI->setPreallocatedArgOffsets(PreallocatedId, PreallocatedOffsets);\n    NumBytesToPush = 0;\n  }\n\n  if (!IsSibcall && !IsMustTail)\n    Chain = DAG.getCALLSEQ_START(Chain, NumBytesToPush,\n                                 NumBytes - NumBytesToPush, dl);\n\n  SDValue RetAddrFrIdx;\n  // Load return address for tail calls.\n  if (isTailCall && FPDiff)\n    Chain = EmitTailCallLoadRetAddr(DAG, RetAddrFrIdx, Chain, isTailCall,\n                                    Is64Bit, FPDiff, dl);\n\n  SmallVector<std::pair<Register, SDValue>, 8> RegsToPass;\n  SmallVector<SDValue, 8> MemOpChains;\n  SDValue StackPtr;\n\n  // The next loop assumes that the locations are in the same order of the\n  // input arguments.\n  assert(isSortedByValueNo(ArgLocs) &&\n         \"Argument Location list must be sorted before lowering\");\n\n  // Walk the register/memloc assignments, inserting copies/loads.  In the case\n  // of tail call optimization arguments are handle later.\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  for (unsigned I = 0, OutIndex = 0, E = ArgLocs.size(); I != E;\n       ++I, ++OutIndex) {\n    assert(OutIndex < Outs.size() && \"Invalid Out index\");\n    // Skip inalloca/preallocated arguments, they have already been written.\n    ISD::ArgFlagsTy Flags = Outs[OutIndex].Flags;\n    if (Flags.isInAlloca() || Flags.isPreallocated())\n      continue;\n\n    CCValAssign &VA = ArgLocs[I];\n    EVT RegVT = VA.getLocVT();\n    SDValue Arg = OutVals[OutIndex];\n    bool isByVal = Flags.isByVal();\n\n    // Promote the value if needed.\n    switch (VA.getLocInfo()) {\n    default: llvm_unreachable(\"Unknown loc info!\");\n    case CCValAssign::Full: break;\n    case CCValAssign::SExt:\n      Arg = DAG.getNode(ISD::SIGN_EXTEND, dl, RegVT, Arg);\n      break;\n    case CCValAssign::ZExt:\n      Arg = DAG.getNode(ISD::ZERO_EXTEND, dl, RegVT, Arg);\n      break;\n    case CCValAssign::AExt:\n      if (Arg.getValueType().isVector() &&\n          Arg.getValueType().getVectorElementType() == MVT::i1)\n        Arg = lowerMasksToReg(Arg, RegVT, dl, DAG);\n      else if (RegVT.is128BitVector()) {\n        // Special case: passing MMX values in XMM registers.\n        Arg = DAG.getBitcast(MVT::i64, Arg);\n        Arg = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64, Arg);\n        Arg = getMOVL(DAG, dl, MVT::v2i64, DAG.getUNDEF(MVT::v2i64), Arg);\n      } else\n        Arg = DAG.getNode(ISD::ANY_EXTEND, dl, RegVT, Arg);\n      break;\n    case CCValAssign::BCvt:\n      Arg = DAG.getBitcast(RegVT, Arg);\n      break;\n    case CCValAssign::Indirect: {\n      if (isByVal) {\n        // Memcpy the argument to a temporary stack slot to prevent\n        // the caller from seeing any modifications the callee may make\n        // as guaranteed by the `byval` attribute.\n        int FrameIdx = MF.getFrameInfo().CreateStackObject(\n            Flags.getByValSize(),\n            std::max(Align(16), Flags.getNonZeroByValAlign()), false);\n        SDValue StackSlot =\n            DAG.getFrameIndex(FrameIdx, getPointerTy(DAG.getDataLayout()));\n        Chain =\n            CreateCopyOfByValArgument(Arg, StackSlot, Chain, Flags, DAG, dl);\n        // From now on treat this as a regular pointer\n        Arg = StackSlot;\n        isByVal = false;\n      } else {\n        // Store the argument.\n        SDValue SpillSlot = DAG.CreateStackTemporary(VA.getValVT());\n        int FI = cast<FrameIndexSDNode>(SpillSlot)->getIndex();\n        Chain = DAG.getStore(\n            Chain, dl, Arg, SpillSlot,\n            MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI));\n        Arg = SpillSlot;\n      }\n      break;\n    }\n    }\n\n    if (VA.needsCustom()) {\n      assert(VA.getValVT() == MVT::v64i1 &&\n             \"Currently the only custom case is when we split v64i1 to 2 regs\");\n      // Split v64i1 value into two registers\n      Passv64i1ArgInRegs(dl, DAG, Arg, RegsToPass, VA, ArgLocs[++I], Subtarget);\n    } else if (VA.isRegLoc()) {\n      RegsToPass.push_back(std::make_pair(VA.getLocReg(), Arg));\n      const TargetOptions &Options = DAG.getTarget().Options;\n      if (Options.EmitCallSiteInfo)\n        CSInfo.emplace_back(VA.getLocReg(), I);\n      if (isVarArg && IsWin64) {\n        // Win64 ABI requires argument XMM reg to be copied to the corresponding\n        // shadow reg if callee is a varargs function.\n        Register ShadowReg;\n        switch (VA.getLocReg()) {\n        case X86::XMM0: ShadowReg = X86::RCX; break;\n        case X86::XMM1: ShadowReg = X86::RDX; break;\n        case X86::XMM2: ShadowReg = X86::R8; break;\n        case X86::XMM3: ShadowReg = X86::R9; break;\n        }\n        if (ShadowReg)\n          RegsToPass.push_back(std::make_pair(ShadowReg, Arg));\n      }\n    } else if (!IsSibcall && (!isTailCall || isByVal)) {\n      assert(VA.isMemLoc());\n      if (!StackPtr.getNode())\n        StackPtr = DAG.getCopyFromReg(Chain, dl, RegInfo->getStackRegister(),\n                                      getPointerTy(DAG.getDataLayout()));\n      MemOpChains.push_back(LowerMemOpCallTo(Chain, StackPtr, Arg,\n                                             dl, DAG, VA, Flags, isByVal));\n    }\n  }\n\n  if (!MemOpChains.empty())\n    Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other, MemOpChains);\n\n  if (Subtarget.isPICStyleGOT()) {\n    // ELF / PIC requires GOT in the EBX register before function calls via PLT\n    // GOT pointer (except regcall).\n    if (!isTailCall) {\n      // Indirect call with RegCall calling convertion may use up all the\n      // general registers, so it is not suitable to bind EBX reister for\n      // GOT address, just let register allocator handle it.\n      if (CallConv != CallingConv::X86_RegCall)\n        RegsToPass.push_back(std::make_pair(\n          Register(X86::EBX), DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(),\n                                          getPointerTy(DAG.getDataLayout()))));\n    } else {\n      // If we are tail calling and generating PIC/GOT style code load the\n      // address of the callee into ECX. The value in ecx is used as target of\n      // the tail jump. This is done to circumvent the ebx/callee-saved problem\n      // for tail calls on PIC/GOT architectures. Normally we would just put the\n      // address of GOT into ebx and then call target@PLT. But for tail calls\n      // ebx would be restored (since ebx is callee saved) before jumping to the\n      // target@PLT.\n\n      // Note: The actual moving to ECX is done further down.\n      GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee);\n      if (G && !G->getGlobal()->hasLocalLinkage() &&\n          G->getGlobal()->hasDefaultVisibility())\n        Callee = LowerGlobalAddress(Callee, DAG);\n      else if (isa<ExternalSymbolSDNode>(Callee))\n        Callee = LowerExternalSymbol(Callee, DAG);\n    }\n  }\n\n  if (Is64Bit && isVarArg && !IsWin64 && !IsMustTail) {\n    // From AMD64 ABI document:\n    // For calls that may call functions that use varargs or stdargs\n    // (prototype-less calls or calls to functions containing ellipsis (...) in\n    // the declaration) %al is used as hidden argument to specify the number\n    // of SSE registers used. The contents of %al do not need to match exactly\n    // the number of registers, but must be an ubound on the number of SSE\n    // registers used and is in the range 0 - 8 inclusive.\n\n    // Count the number of XMM registers allocated.\n    static const MCPhysReg XMMArgRegs[] = {\n      X86::XMM0, X86::XMM1, X86::XMM2, X86::XMM3,\n      X86::XMM4, X86::XMM5, X86::XMM6, X86::XMM7\n    };\n    unsigned NumXMMRegs = CCInfo.getFirstUnallocated(XMMArgRegs);\n    assert((Subtarget.hasSSE1() || !NumXMMRegs)\n           && \"SSE registers cannot be used when SSE is disabled\");\n    RegsToPass.push_back(std::make_pair(Register(X86::AL),\n                                        DAG.getConstant(NumXMMRegs, dl,\n                                                        MVT::i8)));\n  }\n\n  if (isVarArg && IsMustTail) {\n    const auto &Forwards = X86Info->getForwardedMustTailRegParms();\n    for (const auto &F : Forwards) {\n      SDValue Val = DAG.getCopyFromReg(Chain, dl, F.VReg, F.VT);\n      RegsToPass.push_back(std::make_pair(F.PReg, Val));\n    }\n  }\n\n  // For tail calls lower the arguments to the 'real' stack slots.  Sibcalls\n  // don't need this because the eligibility check rejects calls that require\n  // shuffling arguments passed in memory.\n  if (!IsSibcall && isTailCall) {\n    // Force all the incoming stack arguments to be loaded from the stack\n    // before any new outgoing arguments are stored to the stack, because the\n    // outgoing stack slots may alias the incoming argument stack slots, and\n    // the alias isn't otherwise explicit. This is slightly more conservative\n    // than necessary, because it means that each store effectively depends\n    // on every argument instead of just those arguments it would clobber.\n    SDValue ArgChain = DAG.getStackArgumentTokenFactor(Chain);\n\n    SmallVector<SDValue, 8> MemOpChains2;\n    SDValue FIN;\n    int FI = 0;\n    for (unsigned I = 0, OutsIndex = 0, E = ArgLocs.size(); I != E;\n         ++I, ++OutsIndex) {\n      CCValAssign &VA = ArgLocs[I];\n\n      if (VA.isRegLoc()) {\n        if (VA.needsCustom()) {\n          assert((CallConv == CallingConv::X86_RegCall) &&\n                 \"Expecting custom case only in regcall calling convention\");\n          // This means that we are in special case where one argument was\n          // passed through two register locations - Skip the next location\n          ++I;\n        }\n\n        continue;\n      }\n\n      assert(VA.isMemLoc());\n      SDValue Arg = OutVals[OutsIndex];\n      ISD::ArgFlagsTy Flags = Outs[OutsIndex].Flags;\n      // Skip inalloca/preallocated arguments.  They don't require any work.\n      if (Flags.isInAlloca() || Flags.isPreallocated())\n        continue;\n      // Create frame index.\n      int32_t Offset = VA.getLocMemOffset()+FPDiff;\n      uint32_t OpSize = (VA.getLocVT().getSizeInBits()+7)/8;\n      FI = MF.getFrameInfo().CreateFixedObject(OpSize, Offset, true);\n      FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));\n\n      if (Flags.isByVal()) {\n        // Copy relative to framepointer.\n        SDValue Source = DAG.getIntPtrConstant(VA.getLocMemOffset(), dl);\n        if (!StackPtr.getNode())\n          StackPtr = DAG.getCopyFromReg(Chain, dl, RegInfo->getStackRegister(),\n                                        getPointerTy(DAG.getDataLayout()));\n        Source = DAG.getNode(ISD::ADD, dl, getPointerTy(DAG.getDataLayout()),\n                             StackPtr, Source);\n\n        MemOpChains2.push_back(CreateCopyOfByValArgument(Source, FIN,\n                                                         ArgChain,\n                                                         Flags, DAG, dl));\n      } else {\n        // Store relative to framepointer.\n        MemOpChains2.push_back(DAG.getStore(\n            ArgChain, dl, Arg, FIN,\n            MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI)));\n      }\n    }\n\n    if (!MemOpChains2.empty())\n      Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other, MemOpChains2);\n\n    // Store the return address to the appropriate stack slot.\n    Chain = EmitTailCallStoreRetAddr(DAG, MF, Chain, RetAddrFrIdx,\n                                     getPointerTy(DAG.getDataLayout()),\n                                     RegInfo->getSlotSize(), FPDiff, dl);\n  }\n\n  // Build a sequence of copy-to-reg nodes chained together with token chain\n  // and flag operands which copy the outgoing args into registers.\n  SDValue InFlag;\n  for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i) {\n    Chain = DAG.getCopyToReg(Chain, dl, RegsToPass[i].first,\n                             RegsToPass[i].second, InFlag);\n    InFlag = Chain.getValue(1);\n  }\n\n  if (DAG.getTarget().getCodeModel() == CodeModel::Large) {\n    assert(Is64Bit && \"Large code model is only legal in 64-bit mode.\");\n    // In the 64-bit large code model, we have to make all calls\n    // through a register, since the call instruction's 32-bit\n    // pc-relative offset may not be large enough to hold the whole\n    // address.\n  } else if (Callee->getOpcode() == ISD::GlobalAddress ||\n             Callee->getOpcode() == ISD::ExternalSymbol) {\n    // Lower direct calls to global addresses and external symbols. Setting\n    // ForCall to true here has the effect of removing WrapperRIP when possible\n    // to allow direct calls to be selected without first materializing the\n    // address into a register.\n    Callee = LowerGlobalOrExternal(Callee, DAG, /*ForCall=*/true);\n  } else if (Subtarget.isTarget64BitILP32() &&\n             Callee->getValueType(0) == MVT::i32) {\n    // Zero-extend the 32-bit Callee address into a 64-bit according to x32 ABI\n    Callee = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i64, Callee);\n  }\n\n  // Returns a chain & a flag for retval copy to use.\n  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);\n  SmallVector<SDValue, 8> Ops;\n\n  if (!IsSibcall && isTailCall && !IsMustTail) {\n    Chain = DAG.getCALLSEQ_END(Chain,\n                               DAG.getIntPtrConstant(NumBytesToPop, dl, true),\n                               DAG.getIntPtrConstant(0, dl, true), InFlag, dl);\n    InFlag = Chain.getValue(1);\n  }\n\n  Ops.push_back(Chain);\n  Ops.push_back(Callee);\n\n  if (isTailCall)\n    Ops.push_back(DAG.getTargetConstant(FPDiff, dl, MVT::i32));\n\n  // Add argument registers to the end of the list so that they are known live\n  // into the call.\n  for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i)\n    Ops.push_back(DAG.getRegister(RegsToPass[i].first,\n                                  RegsToPass[i].second.getValueType()));\n\n  // Add a register mask operand representing the call-preserved registers.\n  // If HasNCSR is asserted (attribute NoCallerSavedRegisters exists) then we\n  // set X86_INTR calling convention because it has the same CSR mask\n  // (same preserved registers).\n  const uint32_t *Mask = RegInfo->getCallPreservedMask(\n      MF, HasNCSR ? (CallingConv::ID)CallingConv::X86_INTR : CallConv);\n  assert(Mask && \"Missing call preserved mask for calling convention\");\n\n  // If this is an invoke in a 32-bit function using a funclet-based\n  // personality, assume the function clobbers all registers. If an exception\n  // is thrown, the runtime will not restore CSRs.\n  // FIXME: Model this more precisely so that we can register allocate across\n  // the normal edge and spill and fill across the exceptional edge.\n  if (!Is64Bit && CLI.CB && isa<InvokeInst>(CLI.CB)) {\n    const Function &CallerFn = MF.getFunction();\n    EHPersonality Pers =\n        CallerFn.hasPersonalityFn()\n            ? classifyEHPersonality(CallerFn.getPersonalityFn())\n            : EHPersonality::Unknown;\n    if (isFuncletEHPersonality(Pers))\n      Mask = RegInfo->getNoPreservedMask();\n  }\n\n  // Define a new register mask from the existing mask.\n  uint32_t *RegMask = nullptr;\n\n  // In some calling conventions we need to remove the used physical registers\n  // from the reg mask.\n  if (CallConv == CallingConv::X86_RegCall || HasNCSR) {\n    const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n\n    // Allocate a new Reg Mask and copy Mask.\n    RegMask = MF.allocateRegMask();\n    unsigned RegMaskSize = MachineOperand::getRegMaskSize(TRI->getNumRegs());\n    memcpy(RegMask, Mask, sizeof(RegMask[0]) * RegMaskSize);\n\n    // Make sure all sub registers of the argument registers are reset\n    // in the RegMask.\n    for (auto const &RegPair : RegsToPass)\n      for (MCSubRegIterator SubRegs(RegPair.first, TRI, /*IncludeSelf=*/true);\n           SubRegs.isValid(); ++SubRegs)\n        RegMask[*SubRegs / 32] &= ~(1u << (*SubRegs % 32));\n\n    // Create the RegMask Operand according to our updated mask.\n    Ops.push_back(DAG.getRegisterMask(RegMask));\n  } else {\n    // Create the RegMask Operand according to the static mask.\n    Ops.push_back(DAG.getRegisterMask(Mask));\n  }\n\n  if (InFlag.getNode())\n    Ops.push_back(InFlag);\n\n  if (isTailCall) {\n    // We used to do:\n    //// If this is the first return lowered for this function, add the regs\n    //// to the liveout set for the function.\n    // This isn't right, although it's probably harmless on x86; liveouts\n    // should be computed from returns not tail calls.  Consider a void\n    // function making a tail call to a function returning int.\n    MF.getFrameInfo().setHasTailCall();\n    SDValue Ret = DAG.getNode(X86ISD::TC_RETURN, dl, NodeTys, Ops);\n    DAG.addCallSiteInfo(Ret.getNode(), std::move(CSInfo));\n    return Ret;\n  }\n\n  if (HasNoCfCheck && IsCFProtectionSupported && IsIndirectCall) {\n    Chain = DAG.getNode(X86ISD::NT_CALL, dl, NodeTys, Ops);\n  } else {\n    Chain = DAG.getNode(X86ISD::CALL, dl, NodeTys, Ops);\n  }\n  InFlag = Chain.getValue(1);\n  DAG.addNoMergeSiteInfo(Chain.getNode(), CLI.NoMerge);\n  DAG.addCallSiteInfo(Chain.getNode(), std::move(CSInfo));\n\n  // Save heapallocsite metadata.\n  if (CLI.CB)\n    if (MDNode *HeapAlloc = CLI.CB->getMetadata(\"heapallocsite\"))\n      DAG.addHeapAllocSite(Chain.getNode(), HeapAlloc);\n\n  // Create the CALLSEQ_END node.\n  unsigned NumBytesForCalleeToPop;\n  if (X86::isCalleePop(CallConv, Is64Bit, isVarArg,\n                       DAG.getTarget().Options.GuaranteedTailCallOpt))\n    NumBytesForCalleeToPop = NumBytes;    // Callee pops everything\n  else if (!Is64Bit && !canGuaranteeTCO(CallConv) &&\n           !Subtarget.getTargetTriple().isOSMSVCRT() &&\n           SR == StackStructReturn)\n    // If this is a call to a struct-return function, the callee\n    // pops the hidden struct pointer, so we have to push it back.\n    // This is common for Darwin/X86, Linux & Mingw32 targets.\n    // For MSVC Win32 targets, the caller pops the hidden struct pointer.\n    NumBytesForCalleeToPop = 4;\n  else\n    NumBytesForCalleeToPop = 0;  // Callee pops nothing.\n\n  // Returns a flag for retval copy to use.\n  if (!IsSibcall) {\n    Chain = DAG.getCALLSEQ_END(Chain,\n                               DAG.getIntPtrConstant(NumBytesToPop, dl, true),\n                               DAG.getIntPtrConstant(NumBytesForCalleeToPop, dl,\n                                                     true),\n                               InFlag, dl);\n    InFlag = Chain.getValue(1);\n  }\n\n  // Handle result values, copying them out of physregs into vregs that we\n  // return.\n  return LowerCallResult(Chain, InFlag, CallConv, isVarArg, Ins, dl, DAG,\n                         InVals, RegMask);\n}\n\n//===----------------------------------------------------------------------===//\n//                Fast Calling Convention (tail call) implementation\n//===----------------------------------------------------------------------===//\n\n//  Like std call, callee cleans arguments, convention except that ECX is\n//  reserved for storing the tail called function address. Only 2 registers are\n//  free for argument passing (inreg). Tail call optimization is performed\n//  provided:\n//                * tailcallopt is enabled\n//                * caller/callee are fastcc\n//  On X86_64 architecture with GOT-style position independent code only local\n//  (within module) calls are supported at the moment.\n//  To keep the stack aligned according to platform abi the function\n//  GetAlignedArgumentStackSize ensures that argument delta is always multiples\n//  of stack alignment. (Dynamic linkers need this - Darwin's dyld for example)\n//  If a tail called function callee has more arguments than the caller the\n//  caller needs to make sure that there is room to move the RETADDR to. This is\n//  achieved by reserving an area the size of the argument delta right after the\n//  original RETADDR, but before the saved framepointer or the spilled registers\n//  e.g. caller(arg1, arg2) calls callee(arg1, arg2,arg3,arg4)\n//  stack layout:\n//    arg1\n//    arg2\n//    RETADDR\n//    [ new RETADDR\n//      move area ]\n//    (possible EBP)\n//    ESI\n//    EDI\n//    local1 ..\n\n/// Make the stack size align e.g 16n + 12 aligned for a 16-byte align\n/// requirement.\nunsigned\nX86TargetLowering::GetAlignedArgumentStackSize(const unsigned StackSize,\n                                               SelectionDAG &DAG) const {\n  const Align StackAlignment = Subtarget.getFrameLowering()->getStackAlign();\n  const uint64_t SlotSize = Subtarget.getRegisterInfo()->getSlotSize();\n  assert(StackSize % SlotSize == 0 &&\n         \"StackSize must be a multiple of SlotSize\");\n  return alignTo(StackSize + SlotSize, StackAlignment) - SlotSize;\n}\n\n/// Return true if the given stack call argument is already available in the\n/// same position (relatively) of the caller's incoming argument stack.\nstatic\nbool MatchingStackOffset(SDValue Arg, unsigned Offset, ISD::ArgFlagsTy Flags,\n                         MachineFrameInfo &MFI, const MachineRegisterInfo *MRI,\n                         const X86InstrInfo *TII, const CCValAssign &VA) {\n  unsigned Bytes = Arg.getValueSizeInBits() / 8;\n\n  for (;;) {\n    // Look through nodes that don't alter the bits of the incoming value.\n    unsigned Op = Arg.getOpcode();\n    if (Op == ISD::ZERO_EXTEND || Op == ISD::ANY_EXTEND || Op == ISD::BITCAST) {\n      Arg = Arg.getOperand(0);\n      continue;\n    }\n    if (Op == ISD::TRUNCATE) {\n      const SDValue &TruncInput = Arg.getOperand(0);\n      if (TruncInput.getOpcode() == ISD::AssertZext &&\n          cast<VTSDNode>(TruncInput.getOperand(1))->getVT() ==\n              Arg.getValueType()) {\n        Arg = TruncInput.getOperand(0);\n        continue;\n      }\n    }\n    break;\n  }\n\n  int FI = INT_MAX;\n  if (Arg.getOpcode() == ISD::CopyFromReg) {\n    Register VR = cast<RegisterSDNode>(Arg.getOperand(1))->getReg();\n    if (!VR.isVirtual())\n      return false;\n    MachineInstr *Def = MRI->getVRegDef(VR);\n    if (!Def)\n      return false;\n    if (!Flags.isByVal()) {\n      if (!TII->isLoadFromStackSlot(*Def, FI))\n        return false;\n    } else {\n      unsigned Opcode = Def->getOpcode();\n      if ((Opcode == X86::LEA32r || Opcode == X86::LEA64r ||\n           Opcode == X86::LEA64_32r) &&\n          Def->getOperand(1).isFI()) {\n        FI = Def->getOperand(1).getIndex();\n        Bytes = Flags.getByValSize();\n      } else\n        return false;\n    }\n  } else if (LoadSDNode *Ld = dyn_cast<LoadSDNode>(Arg)) {\n    if (Flags.isByVal())\n      // ByVal argument is passed in as a pointer but it's now being\n      // dereferenced. e.g.\n      // define @foo(%struct.X* %A) {\n      //   tail call @bar(%struct.X* byval %A)\n      // }\n      return false;\n    SDValue Ptr = Ld->getBasePtr();\n    FrameIndexSDNode *FINode = dyn_cast<FrameIndexSDNode>(Ptr);\n    if (!FINode)\n      return false;\n    FI = FINode->getIndex();\n  } else if (Arg.getOpcode() == ISD::FrameIndex && Flags.isByVal()) {\n    FrameIndexSDNode *FINode = cast<FrameIndexSDNode>(Arg);\n    FI = FINode->getIndex();\n    Bytes = Flags.getByValSize();\n  } else\n    return false;\n\n  assert(FI != INT_MAX);\n  if (!MFI.isFixedObjectIndex(FI))\n    return false;\n\n  if (Offset != MFI.getObjectOffset(FI))\n    return false;\n\n  // If this is not byval, check that the argument stack object is immutable.\n  // inalloca and argument copy elision can create mutable argument stack\n  // objects. Byval objects can be mutated, but a byval call intends to pass the\n  // mutated memory.\n  if (!Flags.isByVal() && !MFI.isImmutableObjectIndex(FI))\n    return false;\n\n  if (VA.getLocVT().getFixedSizeInBits() >\n      Arg.getValueSizeInBits().getFixedSize()) {\n    // If the argument location is wider than the argument type, check that any\n    // extension flags match.\n    if (Flags.isZExt() != MFI.isObjectZExt(FI) ||\n        Flags.isSExt() != MFI.isObjectSExt(FI)) {\n      return false;\n    }\n  }\n\n  return Bytes == MFI.getObjectSize(FI);\n}\n\n/// Check whether the call is eligible for tail call optimization. Targets\n/// that want to do tail call optimization should implement this function.\nbool X86TargetLowering::IsEligibleForTailCallOptimization(\n    SDValue Callee, CallingConv::ID CalleeCC, bool isVarArg,\n    bool isCalleeStructRet, bool isCallerStructRet, Type *RetTy,\n    const SmallVectorImpl<ISD::OutputArg> &Outs,\n    const SmallVectorImpl<SDValue> &OutVals,\n    const SmallVectorImpl<ISD::InputArg> &Ins, SelectionDAG &DAG) const {\n  if (!mayTailCallThisCC(CalleeCC))\n    return false;\n\n  // If -tailcallopt is specified, make fastcc functions tail-callable.\n  MachineFunction &MF = DAG.getMachineFunction();\n  const Function &CallerF = MF.getFunction();\n\n  // If the function return type is x86_fp80 and the callee return type is not,\n  // then the FP_EXTEND of the call result is not a nop. It's not safe to\n  // perform a tailcall optimization here.\n  if (CallerF.getReturnType()->isX86_FP80Ty() && !RetTy->isX86_FP80Ty())\n    return false;\n\n  CallingConv::ID CallerCC = CallerF.getCallingConv();\n  bool CCMatch = CallerCC == CalleeCC;\n  bool IsCalleeWin64 = Subtarget.isCallingConvWin64(CalleeCC);\n  bool IsCallerWin64 = Subtarget.isCallingConvWin64(CallerCC);\n  bool IsGuaranteeTCO = DAG.getTarget().Options.GuaranteedTailCallOpt ||\n      CalleeCC == CallingConv::Tail;\n\n  // Win64 functions have extra shadow space for argument homing. Don't do the\n  // sibcall if the caller and callee have mismatched expectations for this\n  // space.\n  if (IsCalleeWin64 != IsCallerWin64)\n    return false;\n\n  if (IsGuaranteeTCO) {\n    if (canGuaranteeTCO(CalleeCC) && CCMatch)\n      return true;\n    return false;\n  }\n\n  // Look for obvious safe cases to perform tail call optimization that do not\n  // require ABI changes. This is what gcc calls sibcall.\n\n  // Can't do sibcall if stack needs to be dynamically re-aligned. PEI needs to\n  // emit a special epilogue.\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  if (RegInfo->needsStackRealignment(MF))\n    return false;\n\n  // Also avoid sibcall optimization if either caller or callee uses struct\n  // return semantics.\n  if (isCalleeStructRet || isCallerStructRet)\n    return false;\n\n  // Do not sibcall optimize vararg calls unless all arguments are passed via\n  // registers.\n  LLVMContext &C = *DAG.getContext();\n  if (isVarArg && !Outs.empty()) {\n    // Optimizing for varargs on Win64 is unlikely to be safe without\n    // additional testing.\n    if (IsCalleeWin64 || IsCallerWin64)\n      return false;\n\n    SmallVector<CCValAssign, 16> ArgLocs;\n    CCState CCInfo(CalleeCC, isVarArg, MF, ArgLocs, C);\n\n    CCInfo.AnalyzeCallOperands(Outs, CC_X86);\n    for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i)\n      if (!ArgLocs[i].isRegLoc())\n        return false;\n  }\n\n  // If the call result is in ST0 / ST1, it needs to be popped off the x87\n  // stack.  Therefore, if it's not used by the call it is not safe to optimize\n  // this into a sibcall.\n  bool Unused = false;\n  for (unsigned i = 0, e = Ins.size(); i != e; ++i) {\n    if (!Ins[i].Used) {\n      Unused = true;\n      break;\n    }\n  }\n  if (Unused) {\n    SmallVector<CCValAssign, 16> RVLocs;\n    CCState CCInfo(CalleeCC, false, MF, RVLocs, C);\n    CCInfo.AnalyzeCallResult(Ins, RetCC_X86);\n    for (unsigned i = 0, e = RVLocs.size(); i != e; ++i) {\n      CCValAssign &VA = RVLocs[i];\n      if (VA.getLocReg() == X86::FP0 || VA.getLocReg() == X86::FP1)\n        return false;\n    }\n  }\n\n  // Check that the call results are passed in the same way.\n  if (!CCState::resultsCompatible(CalleeCC, CallerCC, MF, C, Ins,\n                                  RetCC_X86, RetCC_X86))\n    return false;\n  // The callee has to preserve all registers the caller needs to preserve.\n  const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n  const uint32_t *CallerPreserved = TRI->getCallPreservedMask(MF, CallerCC);\n  if (!CCMatch) {\n    const uint32_t *CalleePreserved = TRI->getCallPreservedMask(MF, CalleeCC);\n    if (!TRI->regmaskSubsetEqual(CallerPreserved, CalleePreserved))\n      return false;\n  }\n\n  unsigned StackArgsSize = 0;\n\n  // If the callee takes no arguments then go on to check the results of the\n  // call.\n  if (!Outs.empty()) {\n    // Check if stack adjustment is needed. For now, do not do this if any\n    // argument is passed on the stack.\n    SmallVector<CCValAssign, 16> ArgLocs;\n    CCState CCInfo(CalleeCC, isVarArg, MF, ArgLocs, C);\n\n    // Allocate shadow area for Win64\n    if (IsCalleeWin64)\n      CCInfo.AllocateStack(32, Align(8));\n\n    CCInfo.AnalyzeCallOperands(Outs, CC_X86);\n    StackArgsSize = CCInfo.getNextStackOffset();\n\n    if (CCInfo.getNextStackOffset()) {\n      // Check if the arguments are already laid out in the right way as\n      // the caller's fixed stack objects.\n      MachineFrameInfo &MFI = MF.getFrameInfo();\n      const MachineRegisterInfo *MRI = &MF.getRegInfo();\n      const X86InstrInfo *TII = Subtarget.getInstrInfo();\n      for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {\n        CCValAssign &VA = ArgLocs[i];\n        SDValue Arg = OutVals[i];\n        ISD::ArgFlagsTy Flags = Outs[i].Flags;\n        if (VA.getLocInfo() == CCValAssign::Indirect)\n          return false;\n        if (!VA.isRegLoc()) {\n          if (!MatchingStackOffset(Arg, VA.getLocMemOffset(), Flags,\n                                   MFI, MRI, TII, VA))\n            return false;\n        }\n      }\n    }\n\n    bool PositionIndependent = isPositionIndependent();\n    // If the tailcall address may be in a register, then make sure it's\n    // possible to register allocate for it. In 32-bit, the call address can\n    // only target EAX, EDX, or ECX since the tail call must be scheduled after\n    // callee-saved registers are restored. These happen to be the same\n    // registers used to pass 'inreg' arguments so watch out for those.\n    if (!Subtarget.is64Bit() && ((!isa<GlobalAddressSDNode>(Callee) &&\n                                  !isa<ExternalSymbolSDNode>(Callee)) ||\n                                 PositionIndependent)) {\n      unsigned NumInRegs = 0;\n      // In PIC we need an extra register to formulate the address computation\n      // for the callee.\n      unsigned MaxInRegs = PositionIndependent ? 2 : 3;\n\n      for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {\n        CCValAssign &VA = ArgLocs[i];\n        if (!VA.isRegLoc())\n          continue;\n        Register Reg = VA.getLocReg();\n        switch (Reg) {\n        default: break;\n        case X86::EAX: case X86::EDX: case X86::ECX:\n          if (++NumInRegs == MaxInRegs)\n            return false;\n          break;\n        }\n      }\n    }\n\n    const MachineRegisterInfo &MRI = MF.getRegInfo();\n    if (!parametersInCSRMatch(MRI, CallerPreserved, ArgLocs, OutVals))\n      return false;\n  }\n\n  bool CalleeWillPop =\n      X86::isCalleePop(CalleeCC, Subtarget.is64Bit(), isVarArg,\n                       MF.getTarget().Options.GuaranteedTailCallOpt);\n\n  if (unsigned BytesToPop =\n          MF.getInfo<X86MachineFunctionInfo>()->getBytesToPopOnReturn()) {\n    // If we have bytes to pop, the callee must pop them.\n    bool CalleePopMatches = CalleeWillPop && BytesToPop == StackArgsSize;\n    if (!CalleePopMatches)\n      return false;\n  } else if (CalleeWillPop && StackArgsSize > 0) {\n    // If we don't have bytes to pop, make sure the callee doesn't pop any.\n    return false;\n  }\n\n  return true;\n}\n\nFastISel *\nX86TargetLowering::createFastISel(FunctionLoweringInfo &funcInfo,\n                                  const TargetLibraryInfo *libInfo) const {\n  return X86::createFastISel(funcInfo, libInfo);\n}\n\n//===----------------------------------------------------------------------===//\n//                           Other Lowering Hooks\n//===----------------------------------------------------------------------===//\n\nstatic bool MayFoldLoad(SDValue Op) {\n  return Op.hasOneUse() && ISD::isNormalLoad(Op.getNode());\n}\n\nstatic bool MayFoldIntoStore(SDValue Op) {\n  return Op.hasOneUse() && ISD::isNormalStore(*Op.getNode()->use_begin());\n}\n\nstatic bool MayFoldIntoZeroExtend(SDValue Op) {\n  if (Op.hasOneUse()) {\n    unsigned Opcode = Op.getNode()->use_begin()->getOpcode();\n    return (ISD::ZERO_EXTEND == Opcode);\n  }\n  return false;\n}\n\nstatic bool isTargetShuffle(unsigned Opcode) {\n  switch(Opcode) {\n  default: return false;\n  case X86ISD::BLENDI:\n  case X86ISD::PSHUFB:\n  case X86ISD::PSHUFD:\n  case X86ISD::PSHUFHW:\n  case X86ISD::PSHUFLW:\n  case X86ISD::SHUFP:\n  case X86ISD::INSERTPS:\n  case X86ISD::EXTRQI:\n  case X86ISD::INSERTQI:\n  case X86ISD::VALIGN:\n  case X86ISD::PALIGNR:\n  case X86ISD::VSHLDQ:\n  case X86ISD::VSRLDQ:\n  case X86ISD::MOVLHPS:\n  case X86ISD::MOVHLPS:\n  case X86ISD::MOVSHDUP:\n  case X86ISD::MOVSLDUP:\n  case X86ISD::MOVDDUP:\n  case X86ISD::MOVSS:\n  case X86ISD::MOVSD:\n  case X86ISD::UNPCKL:\n  case X86ISD::UNPCKH:\n  case X86ISD::VBROADCAST:\n  case X86ISD::VPERMILPI:\n  case X86ISD::VPERMILPV:\n  case X86ISD::VPERM2X128:\n  case X86ISD::SHUF128:\n  case X86ISD::VPERMIL2:\n  case X86ISD::VPERMI:\n  case X86ISD::VPPERM:\n  case X86ISD::VPERMV:\n  case X86ISD::VPERMV3:\n  case X86ISD::VZEXT_MOVL:\n    return true;\n  }\n}\n\nstatic bool isTargetShuffleVariableMask(unsigned Opcode) {\n  switch (Opcode) {\n  default: return false;\n  // Target Shuffles.\n  case X86ISD::PSHUFB:\n  case X86ISD::VPERMILPV:\n  case X86ISD::VPERMIL2:\n  case X86ISD::VPPERM:\n  case X86ISD::VPERMV:\n  case X86ISD::VPERMV3:\n    return true;\n  // 'Faux' Target Shuffles.\n  case ISD::OR:\n  case ISD::AND:\n  case X86ISD::ANDNP:\n    return true;\n  }\n}\n\nstatic bool isTargetShuffleSplat(SDValue Op) {\n  unsigned Opcode = Op.getOpcode();\n  if (Opcode == ISD::EXTRACT_SUBVECTOR)\n    return isTargetShuffleSplat(Op.getOperand(0));\n  return Opcode == X86ISD::VBROADCAST || Opcode == X86ISD::VBROADCAST_LOAD;\n}\n\nSDValue X86TargetLowering::getReturnAddressFrameIndex(SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();\n  int ReturnAddrIndex = FuncInfo->getRAIndex();\n\n  if (ReturnAddrIndex == 0) {\n    // Set up a frame object for the return address.\n    unsigned SlotSize = RegInfo->getSlotSize();\n    ReturnAddrIndex = MF.getFrameInfo().CreateFixedObject(SlotSize,\n                                                          -(int64_t)SlotSize,\n                                                          false);\n    FuncInfo->setRAIndex(ReturnAddrIndex);\n  }\n\n  return DAG.getFrameIndex(ReturnAddrIndex, getPointerTy(DAG.getDataLayout()));\n}\n\nbool X86::isOffsetSuitableForCodeModel(int64_t Offset, CodeModel::Model M,\n                                       bool hasSymbolicDisplacement) {\n  // Offset should fit into 32 bit immediate field.\n  if (!isInt<32>(Offset))\n    return false;\n\n  // If we don't have a symbolic displacement - we don't have any extra\n  // restrictions.\n  if (!hasSymbolicDisplacement)\n    return true;\n\n  // FIXME: Some tweaks might be needed for medium code model.\n  if (M != CodeModel::Small && M != CodeModel::Kernel)\n    return false;\n\n  // For small code model we assume that latest object is 16MB before end of 31\n  // bits boundary. We may also accept pretty large negative constants knowing\n  // that all objects are in the positive half of address space.\n  if (M == CodeModel::Small && Offset < 16*1024*1024)\n    return true;\n\n  // For kernel code model we know that all object resist in the negative half\n  // of 32bits address space. We may not accept negative offsets, since they may\n  // be just off and we may accept pretty large positive ones.\n  if (M == CodeModel::Kernel && Offset >= 0)\n    return true;\n\n  return false;\n}\n\n/// Determines whether the callee is required to pop its own arguments.\n/// Callee pop is necessary to support tail calls.\nbool X86::isCalleePop(CallingConv::ID CallingConv,\n                      bool is64Bit, bool IsVarArg, bool GuaranteeTCO) {\n  // If GuaranteeTCO is true, we force some calls to be callee pop so that we\n  // can guarantee TCO.\n  if (!IsVarArg && shouldGuaranteeTCO(CallingConv, GuaranteeTCO))\n    return true;\n\n  switch (CallingConv) {\n  default:\n    return false;\n  case CallingConv::X86_StdCall:\n  case CallingConv::X86_FastCall:\n  case CallingConv::X86_ThisCall:\n  case CallingConv::X86_VectorCall:\n    return !is64Bit;\n  }\n}\n\n/// Return true if the condition is an signed comparison operation.\nstatic bool isX86CCSigned(unsigned X86CC) {\n  switch (X86CC) {\n  default:\n    llvm_unreachable(\"Invalid integer condition!\");\n  case X86::COND_E:\n  case X86::COND_NE:\n  case X86::COND_B:\n  case X86::COND_A:\n  case X86::COND_BE:\n  case X86::COND_AE:\n    return false;\n  case X86::COND_G:\n  case X86::COND_GE:\n  case X86::COND_L:\n  case X86::COND_LE:\n    return true;\n  }\n}\n\nstatic X86::CondCode TranslateIntegerX86CC(ISD::CondCode SetCCOpcode) {\n  switch (SetCCOpcode) {\n  default: llvm_unreachable(\"Invalid integer condition!\");\n  case ISD::SETEQ:  return X86::COND_E;\n  case ISD::SETGT:  return X86::COND_G;\n  case ISD::SETGE:  return X86::COND_GE;\n  case ISD::SETLT:  return X86::COND_L;\n  case ISD::SETLE:  return X86::COND_LE;\n  case ISD::SETNE:  return X86::COND_NE;\n  case ISD::SETULT: return X86::COND_B;\n  case ISD::SETUGT: return X86::COND_A;\n  case ISD::SETULE: return X86::COND_BE;\n  case ISD::SETUGE: return X86::COND_AE;\n  }\n}\n\n/// Do a one-to-one translation of a ISD::CondCode to the X86-specific\n/// condition code, returning the condition code and the LHS/RHS of the\n/// comparison to make.\nstatic X86::CondCode TranslateX86CC(ISD::CondCode SetCCOpcode, const SDLoc &DL,\n                               bool isFP, SDValue &LHS, SDValue &RHS,\n                               SelectionDAG &DAG) {\n  if (!isFP) {\n    if (ConstantSDNode *RHSC = dyn_cast<ConstantSDNode>(RHS)) {\n      if (SetCCOpcode == ISD::SETGT && RHSC->isAllOnesValue()) {\n        // X > -1   -> X == 0, jump !sign.\n        RHS = DAG.getConstant(0, DL, RHS.getValueType());\n        return X86::COND_NS;\n      }\n      if (SetCCOpcode == ISD::SETLT && RHSC->isNullValue()) {\n        // X < 0   -> X == 0, jump on sign.\n        return X86::COND_S;\n      }\n      if (SetCCOpcode == ISD::SETGE && RHSC->isNullValue()) {\n        // X >= 0   -> X == 0, jump on !sign.\n        return X86::COND_NS;\n      }\n      if (SetCCOpcode == ISD::SETLT && RHSC->isOne()) {\n        // X < 1   -> X <= 0\n        RHS = DAG.getConstant(0, DL, RHS.getValueType());\n        return X86::COND_LE;\n      }\n    }\n\n    return TranslateIntegerX86CC(SetCCOpcode);\n  }\n\n  // First determine if it is required or is profitable to flip the operands.\n\n  // If LHS is a foldable load, but RHS is not, flip the condition.\n  if (ISD::isNON_EXTLoad(LHS.getNode()) &&\n      !ISD::isNON_EXTLoad(RHS.getNode())) {\n    SetCCOpcode = getSetCCSwappedOperands(SetCCOpcode);\n    std::swap(LHS, RHS);\n  }\n\n  switch (SetCCOpcode) {\n  default: break;\n  case ISD::SETOLT:\n  case ISD::SETOLE:\n  case ISD::SETUGT:\n  case ISD::SETUGE:\n    std::swap(LHS, RHS);\n    break;\n  }\n\n  // On a floating point condition, the flags are set as follows:\n  // ZF  PF  CF   op\n  //  0 | 0 | 0 | X > Y\n  //  0 | 0 | 1 | X < Y\n  //  1 | 0 | 0 | X == Y\n  //  1 | 1 | 1 | unordered\n  switch (SetCCOpcode) {\n  default: llvm_unreachable(\"Condcode should be pre-legalized away\");\n  case ISD::SETUEQ:\n  case ISD::SETEQ:   return X86::COND_E;\n  case ISD::SETOLT:              // flipped\n  case ISD::SETOGT:\n  case ISD::SETGT:   return X86::COND_A;\n  case ISD::SETOLE:              // flipped\n  case ISD::SETOGE:\n  case ISD::SETGE:   return X86::COND_AE;\n  case ISD::SETUGT:              // flipped\n  case ISD::SETULT:\n  case ISD::SETLT:   return X86::COND_B;\n  case ISD::SETUGE:              // flipped\n  case ISD::SETULE:\n  case ISD::SETLE:   return X86::COND_BE;\n  case ISD::SETONE:\n  case ISD::SETNE:   return X86::COND_NE;\n  case ISD::SETUO:   return X86::COND_P;\n  case ISD::SETO:    return X86::COND_NP;\n  case ISD::SETOEQ:\n  case ISD::SETUNE:  return X86::COND_INVALID;\n  }\n}\n\n/// Is there a floating point cmov for the specific X86 condition code?\n/// Current x86 isa includes the following FP cmov instructions:\n/// fcmovb, fcomvbe, fcomve, fcmovu, fcmovae, fcmova, fcmovne, fcmovnu.\nstatic bool hasFPCMov(unsigned X86CC) {\n  switch (X86CC) {\n  default:\n    return false;\n  case X86::COND_B:\n  case X86::COND_BE:\n  case X86::COND_E:\n  case X86::COND_P:\n  case X86::COND_A:\n  case X86::COND_AE:\n  case X86::COND_NE:\n  case X86::COND_NP:\n    return true;\n  }\n}\n\n\nbool X86TargetLowering::getTgtMemIntrinsic(IntrinsicInfo &Info,\n                                           const CallInst &I,\n                                           MachineFunction &MF,\n                                           unsigned Intrinsic) const {\n  Info.flags = MachineMemOperand::MONone;\n  Info.offset = 0;\n\n  const IntrinsicData* IntrData = getIntrinsicWithChain(Intrinsic);\n  if (!IntrData) {\n    switch (Intrinsic) {\n    case Intrinsic::x86_aesenc128kl:\n    case Intrinsic::x86_aesdec128kl:\n      Info.opc = ISD::INTRINSIC_W_CHAIN;\n      Info.ptrVal = I.getArgOperand(1);\n      Info.memVT = EVT::getIntegerVT(I.getType()->getContext(), 48);\n      Info.align = Align(1);\n      Info.flags |= MachineMemOperand::MOLoad;\n      return true;\n    case Intrinsic::x86_aesenc256kl:\n    case Intrinsic::x86_aesdec256kl:\n      Info.opc = ISD::INTRINSIC_W_CHAIN;\n      Info.ptrVal = I.getArgOperand(1);\n      Info.memVT = EVT::getIntegerVT(I.getType()->getContext(), 64);\n      Info.align = Align(1);\n      Info.flags |= MachineMemOperand::MOLoad;\n      return true;\n    case Intrinsic::x86_aesencwide128kl:\n    case Intrinsic::x86_aesdecwide128kl:\n      Info.opc = ISD::INTRINSIC_W_CHAIN;\n      Info.ptrVal = I.getArgOperand(0);\n      Info.memVT = EVT::getIntegerVT(I.getType()->getContext(), 48);\n      Info.align = Align(1);\n      Info.flags |= MachineMemOperand::MOLoad;\n      return true;\n    case Intrinsic::x86_aesencwide256kl:\n    case Intrinsic::x86_aesdecwide256kl:\n      Info.opc = ISD::INTRINSIC_W_CHAIN;\n      Info.ptrVal = I.getArgOperand(0);\n      Info.memVT = EVT::getIntegerVT(I.getType()->getContext(), 64);\n      Info.align = Align(1);\n      Info.flags |= MachineMemOperand::MOLoad;\n      return true;\n    }\n    return false;\n  }\n\n  switch (IntrData->Type) {\n  case TRUNCATE_TO_MEM_VI8:\n  case TRUNCATE_TO_MEM_VI16:\n  case TRUNCATE_TO_MEM_VI32: {\n    Info.opc = ISD::INTRINSIC_VOID;\n    Info.ptrVal = I.getArgOperand(0);\n    MVT VT  = MVT::getVT(I.getArgOperand(1)->getType());\n    MVT ScalarVT = MVT::INVALID_SIMPLE_VALUE_TYPE;\n    if (IntrData->Type == TRUNCATE_TO_MEM_VI8)\n      ScalarVT = MVT::i8;\n    else if (IntrData->Type == TRUNCATE_TO_MEM_VI16)\n      ScalarVT = MVT::i16;\n    else if (IntrData->Type == TRUNCATE_TO_MEM_VI32)\n      ScalarVT = MVT::i32;\n\n    Info.memVT = MVT::getVectorVT(ScalarVT, VT.getVectorNumElements());\n    Info.align = Align(1);\n    Info.flags |= MachineMemOperand::MOStore;\n    break;\n  }\n  case GATHER:\n  case GATHER_AVX2: {\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    Info.ptrVal = nullptr;\n    MVT DataVT = MVT::getVT(I.getType());\n    MVT IndexVT = MVT::getVT(I.getArgOperand(2)->getType());\n    unsigned NumElts = std::min(DataVT.getVectorNumElements(),\n                                IndexVT.getVectorNumElements());\n    Info.memVT = MVT::getVectorVT(DataVT.getVectorElementType(), NumElts);\n    Info.align = Align(1);\n    Info.flags |= MachineMemOperand::MOLoad;\n    break;\n  }\n  case SCATTER: {\n    Info.opc = ISD::INTRINSIC_VOID;\n    Info.ptrVal = nullptr;\n    MVT DataVT = MVT::getVT(I.getArgOperand(3)->getType());\n    MVT IndexVT = MVT::getVT(I.getArgOperand(2)->getType());\n    unsigned NumElts = std::min(DataVT.getVectorNumElements(),\n                                IndexVT.getVectorNumElements());\n    Info.memVT = MVT::getVectorVT(DataVT.getVectorElementType(), NumElts);\n    Info.align = Align(1);\n    Info.flags |= MachineMemOperand::MOStore;\n    break;\n  }\n  default:\n    return false;\n  }\n\n  return true;\n}\n\n/// Returns true if the target can instruction select the\n/// specified FP immediate natively. If false, the legalizer will\n/// materialize the FP immediate as a load from a constant pool.\nbool X86TargetLowering::isFPImmLegal(const APFloat &Imm, EVT VT,\n                                     bool ForCodeSize) const {\n  for (unsigned i = 0, e = LegalFPImmediates.size(); i != e; ++i) {\n    if (Imm.bitwiseIsEqual(LegalFPImmediates[i]))\n      return true;\n  }\n  return false;\n}\n\nbool X86TargetLowering::shouldReduceLoadWidth(SDNode *Load,\n                                              ISD::LoadExtType ExtTy,\n                                              EVT NewVT) const {\n  assert(cast<LoadSDNode>(Load)->isSimple() && \"illegal to narrow\");\n\n  // \"ELF Handling for Thread-Local Storage\" specifies that R_X86_64_GOTTPOFF\n  // relocation target a movq or addq instruction: don't let the load shrink.\n  SDValue BasePtr = cast<LoadSDNode>(Load)->getBasePtr();\n  if (BasePtr.getOpcode() == X86ISD::WrapperRIP)\n    if (const auto *GA = dyn_cast<GlobalAddressSDNode>(BasePtr.getOperand(0)))\n      return GA->getTargetFlags() != X86II::MO_GOTTPOFF;\n\n  // If this is an (1) AVX vector load with (2) multiple uses and (3) all of\n  // those uses are extracted directly into a store, then the extract + store\n  // can be store-folded. Therefore, it's probably not worth splitting the load.\n  EVT VT = Load->getValueType(0);\n  if ((VT.is256BitVector() || VT.is512BitVector()) && !Load->hasOneUse()) {\n    for (auto UI = Load->use_begin(), UE = Load->use_end(); UI != UE; ++UI) {\n      // Skip uses of the chain value. Result 0 of the node is the load value.\n      if (UI.getUse().getResNo() != 0)\n        continue;\n\n      // If this use is not an extract + store, it's probably worth splitting.\n      if (UI->getOpcode() != ISD::EXTRACT_SUBVECTOR || !UI->hasOneUse() ||\n          UI->use_begin()->getOpcode() != ISD::STORE)\n        return true;\n    }\n    // All non-chain uses are extract + store.\n    return false;\n  }\n\n  return true;\n}\n\n/// Returns true if it is beneficial to convert a load of a constant\n/// to just the constant itself.\nbool X86TargetLowering::shouldConvertConstantLoadToIntImm(const APInt &Imm,\n                                                          Type *Ty) const {\n  assert(Ty->isIntegerTy());\n\n  unsigned BitSize = Ty->getPrimitiveSizeInBits();\n  if (BitSize == 0 || BitSize > 64)\n    return false;\n  return true;\n}\n\nbool X86TargetLowering::reduceSelectOfFPConstantLoads(EVT CmpOpVT) const {\n  // If we are using XMM registers in the ABI and the condition of the select is\n  // a floating-point compare and we have blendv or conditional move, then it is\n  // cheaper to select instead of doing a cross-register move and creating a\n  // load that depends on the compare result.\n  bool IsFPSetCC = CmpOpVT.isFloatingPoint() && CmpOpVT != MVT::f128;\n  return !IsFPSetCC || !Subtarget.isTarget64BitLP64() || !Subtarget.hasAVX();\n}\n\nbool X86TargetLowering::convertSelectOfConstantsToMath(EVT VT) const {\n  // TODO: It might be a win to ease or lift this restriction, but the generic\n  // folds in DAGCombiner conflict with vector folds for an AVX512 target.\n  if (VT.isVector() && Subtarget.hasAVX512())\n    return false;\n\n  return true;\n}\n\nbool X86TargetLowering::decomposeMulByConstant(LLVMContext &Context, EVT VT,\n                                               SDValue C) const {\n  // TODO: We handle scalars using custom code, but generic combining could make\n  // that unnecessary.\n  APInt MulC;\n  if (!ISD::isConstantSplatVector(C.getNode(), MulC))\n    return false;\n\n  // Find the type this will be legalized too. Otherwise we might prematurely\n  // convert this to shl+add/sub and then still have to type legalize those ops.\n  // Another choice would be to defer the decision for illegal types until\n  // after type legalization. But constant splat vectors of i64 can't make it\n  // through type legalization on 32-bit targets so we would need to special\n  // case vXi64.\n  while (getTypeAction(Context, VT) != TypeLegal)\n    VT = getTypeToTransformTo(Context, VT);\n\n  // If vector multiply is legal, assume that's faster than shl + add/sub.\n  // TODO: Multiply is a complex op with higher latency and lower throughput in\n  //       most implementations, so this check could be loosened based on type\n  //       and/or a CPU attribute.\n  if (isOperationLegal(ISD::MUL, VT))\n    return false;\n\n  // shl+add, shl+sub, shl+add+neg\n  return (MulC + 1).isPowerOf2() || (MulC - 1).isPowerOf2() ||\n         (1 - MulC).isPowerOf2() || (-(MulC + 1)).isPowerOf2();\n}\n\nbool X86TargetLowering::isExtractSubvectorCheap(EVT ResVT, EVT SrcVT,\n                                                unsigned Index) const {\n  if (!isOperationLegalOrCustom(ISD::EXTRACT_SUBVECTOR, ResVT))\n    return false;\n\n  // Mask vectors support all subregister combinations and operations that\n  // extract half of vector.\n  if (ResVT.getVectorElementType() == MVT::i1)\n    return Index == 0 || ((ResVT.getSizeInBits() == SrcVT.getSizeInBits()*2) &&\n                          (Index == ResVT.getVectorNumElements()));\n\n  return (Index % ResVT.getVectorNumElements()) == 0;\n}\n\nbool X86TargetLowering::shouldScalarizeBinop(SDValue VecOp) const {\n  unsigned Opc = VecOp.getOpcode();\n\n  // Assume target opcodes can't be scalarized.\n  // TODO - do we have any exceptions?\n  if (Opc >= ISD::BUILTIN_OP_END)\n    return false;\n\n  // If the vector op is not supported, try to convert to scalar.\n  EVT VecVT = VecOp.getValueType();\n  if (!isOperationLegalOrCustomOrPromote(Opc, VecVT))\n    return true;\n\n  // If the vector op is supported, but the scalar op is not, the transform may\n  // not be worthwhile.\n  EVT ScalarVT = VecVT.getScalarType();\n  return isOperationLegalOrCustomOrPromote(Opc, ScalarVT);\n}\n\nbool X86TargetLowering::shouldFormOverflowOp(unsigned Opcode, EVT VT,\n                                             bool) const {\n  // TODO: Allow vectors?\n  if (VT.isVector())\n    return false;\n  return VT.isSimple() || !isOperationExpand(Opcode, VT);\n}\n\nbool X86TargetLowering::isCheapToSpeculateCttz() const {\n  // Speculate cttz only if we can directly use TZCNT.\n  return Subtarget.hasBMI();\n}\n\nbool X86TargetLowering::isCheapToSpeculateCtlz() const {\n  // Speculate ctlz only if we can directly use LZCNT.\n  return Subtarget.hasLZCNT();\n}\n\nbool X86TargetLowering::isLoadBitCastBeneficial(EVT LoadVT, EVT BitcastVT,\n                                                const SelectionDAG &DAG,\n                                                const MachineMemOperand &MMO) const {\n  if (!Subtarget.hasAVX512() && !LoadVT.isVector() && BitcastVT.isVector() &&\n      BitcastVT.getVectorElementType() == MVT::i1)\n    return false;\n\n  if (!Subtarget.hasDQI() && BitcastVT == MVT::v8i1 && LoadVT == MVT::i8)\n    return false;\n\n  // If both types are legal vectors, it's always ok to convert them.\n  if (LoadVT.isVector() && BitcastVT.isVector() &&\n      isTypeLegal(LoadVT) && isTypeLegal(BitcastVT))\n    return true;\n\n  return TargetLowering::isLoadBitCastBeneficial(LoadVT, BitcastVT, DAG, MMO);\n}\n\nbool X86TargetLowering::canMergeStoresTo(unsigned AddressSpace, EVT MemVT,\n                                         const SelectionDAG &DAG) const {\n  // Do not merge to float value size (128 bytes) if no implicit\n  // float attribute is set.\n  bool NoFloat = DAG.getMachineFunction().getFunction().hasFnAttribute(\n      Attribute::NoImplicitFloat);\n\n  if (NoFloat) {\n    unsigned MaxIntSize = Subtarget.is64Bit() ? 64 : 32;\n    return (MemVT.getSizeInBits() <= MaxIntSize);\n  }\n  // Make sure we don't merge greater than our preferred vector\n  // width.\n  if (MemVT.getSizeInBits() > Subtarget.getPreferVectorWidth())\n    return false;\n\n  return true;\n}\n\nbool X86TargetLowering::isCtlzFast() const {\n  return Subtarget.hasFastLZCNT();\n}\n\nbool X86TargetLowering::isMaskAndCmp0FoldingBeneficial(\n    const Instruction &AndI) const {\n  return true;\n}\n\nbool X86TargetLowering::hasAndNotCompare(SDValue Y) const {\n  EVT VT = Y.getValueType();\n\n  if (VT.isVector())\n    return false;\n\n  if (!Subtarget.hasBMI())\n    return false;\n\n  // There are only 32-bit and 64-bit forms for 'andn'.\n  if (VT != MVT::i32 && VT != MVT::i64)\n    return false;\n\n  return !isa<ConstantSDNode>(Y);\n}\n\nbool X86TargetLowering::hasAndNot(SDValue Y) const {\n  EVT VT = Y.getValueType();\n\n  if (!VT.isVector())\n    return hasAndNotCompare(Y);\n\n  // Vector.\n\n  if (!Subtarget.hasSSE1() || VT.getSizeInBits() < 128)\n    return false;\n\n  if (VT == MVT::v4i32)\n    return true;\n\n  return Subtarget.hasSSE2();\n}\n\nbool X86TargetLowering::hasBitTest(SDValue X, SDValue Y) const {\n  return X.getValueType().isScalarInteger(); // 'bt'\n}\n\nbool X86TargetLowering::\n    shouldProduceAndByConstByHoistingConstFromShiftsLHSOfAnd(\n        SDValue X, ConstantSDNode *XC, ConstantSDNode *CC, SDValue Y,\n        unsigned OldShiftOpcode, unsigned NewShiftOpcode,\n        SelectionDAG &DAG) const {\n  // Does baseline recommend not to perform the fold by default?\n  if (!TargetLowering::shouldProduceAndByConstByHoistingConstFromShiftsLHSOfAnd(\n          X, XC, CC, Y, OldShiftOpcode, NewShiftOpcode, DAG))\n    return false;\n  // For scalars this transform is always beneficial.\n  if (X.getValueType().isScalarInteger())\n    return true;\n  // If all the shift amounts are identical, then transform is beneficial even\n  // with rudimentary SSE2 shifts.\n  if (DAG.isSplatValue(Y, /*AllowUndefs=*/true))\n    return true;\n  // If we have AVX2 with it's powerful shift operations, then it's also good.\n  if (Subtarget.hasAVX2())\n    return true;\n  // Pre-AVX2 vector codegen for this pattern is best for variant with 'shl'.\n  return NewShiftOpcode == ISD::SHL;\n}\n\nbool X86TargetLowering::shouldFoldConstantShiftPairToMask(\n    const SDNode *N, CombineLevel Level) const {\n  assert(((N->getOpcode() == ISD::SHL &&\n           N->getOperand(0).getOpcode() == ISD::SRL) ||\n          (N->getOpcode() == ISD::SRL &&\n           N->getOperand(0).getOpcode() == ISD::SHL)) &&\n         \"Expected shift-shift mask\");\n  EVT VT = N->getValueType(0);\n  if ((Subtarget.hasFastVectorShiftMasks() && VT.isVector()) ||\n      (Subtarget.hasFastScalarShiftMasks() && !VT.isVector())) {\n    // Only fold if the shift values are equal - so it folds to AND.\n    // TODO - we should fold if either is a non-uniform vector but we don't do\n    // the fold for non-splats yet.\n    return N->getOperand(1) == N->getOperand(0).getOperand(1);\n  }\n  return TargetLoweringBase::shouldFoldConstantShiftPairToMask(N, Level);\n}\n\nbool X86TargetLowering::shouldFoldMaskToVariableShiftPair(SDValue Y) const {\n  EVT VT = Y.getValueType();\n\n  // For vectors, we don't have a preference, but we probably want a mask.\n  if (VT.isVector())\n    return false;\n\n  // 64-bit shifts on 32-bit targets produce really bad bloated code.\n  if (VT == MVT::i64 && !Subtarget.is64Bit())\n    return false;\n\n  return true;\n}\n\nbool X86TargetLowering::shouldExpandShift(SelectionDAG &DAG,\n                                          SDNode *N) const {\n  if (DAG.getMachineFunction().getFunction().hasMinSize() &&\n      !Subtarget.isOSWindows())\n    return false;\n  return true;\n}\n\nbool X86TargetLowering::shouldSplatInsEltVarIndex(EVT VT) const {\n  // Any legal vector type can be splatted more efficiently than\n  // loading/spilling from memory.\n  return isTypeLegal(VT);\n}\n\nMVT X86TargetLowering::hasFastEqualityCompare(unsigned NumBits) const {\n  MVT VT = MVT::getIntegerVT(NumBits);\n  if (isTypeLegal(VT))\n    return VT;\n\n  // PMOVMSKB can handle this.\n  if (NumBits == 128 && isTypeLegal(MVT::v16i8))\n    return MVT::v16i8;\n\n  // VPMOVMSKB can handle this.\n  if (NumBits == 256 && isTypeLegal(MVT::v32i8))\n    return MVT::v32i8;\n\n  // TODO: Allow 64-bit type for 32-bit target.\n  // TODO: 512-bit types should be allowed, but make sure that those\n  // cases are handled in combineVectorSizedSetCCEquality().\n\n  return MVT::INVALID_SIMPLE_VALUE_TYPE;\n}\n\n/// Val is the undef sentinel value or equal to the specified value.\nstatic bool isUndefOrEqual(int Val, int CmpVal) {\n  return ((Val == SM_SentinelUndef) || (Val == CmpVal));\n}\n\n/// Return true if every element in Mask is the undef sentinel value or equal to\n/// the specified value..\nstatic bool isUndefOrEqual(ArrayRef<int> Mask, int CmpVal) {\n  return llvm::all_of(Mask, [CmpVal](int M) {\n    return (M == SM_SentinelUndef) || (M == CmpVal);\n  });\n}\n\n/// Val is either the undef or zero sentinel value.\nstatic bool isUndefOrZero(int Val) {\n  return ((Val == SM_SentinelUndef) || (Val == SM_SentinelZero));\n}\n\n/// Return true if every element in Mask, beginning from position Pos and ending\n/// in Pos+Size is the undef sentinel value.\nstatic bool isUndefInRange(ArrayRef<int> Mask, unsigned Pos, unsigned Size) {\n  return llvm::all_of(Mask.slice(Pos, Size),\n                      [](int M) { return M == SM_SentinelUndef; });\n}\n\n/// Return true if the mask creates a vector whose lower half is undefined.\nstatic bool isUndefLowerHalf(ArrayRef<int> Mask) {\n  unsigned NumElts = Mask.size();\n  return isUndefInRange(Mask, 0, NumElts / 2);\n}\n\n/// Return true if the mask creates a vector whose upper half is undefined.\nstatic bool isUndefUpperHalf(ArrayRef<int> Mask) {\n  unsigned NumElts = Mask.size();\n  return isUndefInRange(Mask, NumElts / 2, NumElts / 2);\n}\n\n/// Return true if Val falls within the specified range (L, H].\nstatic bool isInRange(int Val, int Low, int Hi) {\n  return (Val >= Low && Val < Hi);\n}\n\n/// Return true if the value of any element in Mask falls within the specified\n/// range (L, H].\nstatic bool isAnyInRange(ArrayRef<int> Mask, int Low, int Hi) {\n  return llvm::any_of(Mask, [Low, Hi](int M) { return isInRange(M, Low, Hi); });\n}\n\n/// Return true if the value of any element in Mask is the zero sentinel value.\nstatic bool isAnyZero(ArrayRef<int> Mask) {\n  return llvm::any_of(Mask, [](int M) { return M == SM_SentinelZero; });\n}\n\n/// Return true if the value of any element in Mask is the zero or undef\n/// sentinel values.\nstatic bool isAnyZeroOrUndef(ArrayRef<int> Mask) {\n  return llvm::any_of(Mask, [](int M) {\n    return M == SM_SentinelZero || M == SM_SentinelUndef;\n  });\n}\n\n/// Return true if Val is undef or if its value falls within the\n/// specified range (L, H].\nstatic bool isUndefOrInRange(int Val, int Low, int Hi) {\n  return (Val == SM_SentinelUndef) || isInRange(Val, Low, Hi);\n}\n\n/// Return true if every element in Mask is undef or if its value\n/// falls within the specified range (L, H].\nstatic bool isUndefOrInRange(ArrayRef<int> Mask, int Low, int Hi) {\n  return llvm::all_of(\n      Mask, [Low, Hi](int M) { return isUndefOrInRange(M, Low, Hi); });\n}\n\n/// Return true if Val is undef, zero or if its value falls within the\n/// specified range (L, H].\nstatic bool isUndefOrZeroOrInRange(int Val, int Low, int Hi) {\n  return isUndefOrZero(Val) || isInRange(Val, Low, Hi);\n}\n\n/// Return true if every element in Mask is undef, zero or if its value\n/// falls within the specified range (L, H].\nstatic bool isUndefOrZeroOrInRange(ArrayRef<int> Mask, int Low, int Hi) {\n  return llvm::all_of(\n      Mask, [Low, Hi](int M) { return isUndefOrZeroOrInRange(M, Low, Hi); });\n}\n\n/// Return true if every element in Mask, beginning\n/// from position Pos and ending in Pos + Size, falls within the specified\n/// sequence (Low, Low + Step, ..., Low + (Size - 1) * Step) or is undef.\nstatic bool isSequentialOrUndefInRange(ArrayRef<int> Mask, unsigned Pos,\n                                       unsigned Size, int Low, int Step = 1) {\n  for (unsigned i = Pos, e = Pos + Size; i != e; ++i, Low += Step)\n    if (!isUndefOrEqual(Mask[i], Low))\n      return false;\n  return true;\n}\n\n/// Return true if every element in Mask, beginning\n/// from position Pos and ending in Pos+Size, falls within the specified\n/// sequential range (Low, Low+Size], or is undef or is zero.\nstatic bool isSequentialOrUndefOrZeroInRange(ArrayRef<int> Mask, unsigned Pos,\n                                             unsigned Size, int Low,\n                                             int Step = 1) {\n  for (unsigned i = Pos, e = Pos + Size; i != e; ++i, Low += Step)\n    if (!isUndefOrZero(Mask[i]) && Mask[i] != Low)\n      return false;\n  return true;\n}\n\n/// Return true if every element in Mask, beginning\n/// from position Pos and ending in Pos+Size is undef or is zero.\nstatic bool isUndefOrZeroInRange(ArrayRef<int> Mask, unsigned Pos,\n                                 unsigned Size) {\n  return llvm::all_of(Mask.slice(Pos, Size),\n                      [](int M) { return isUndefOrZero(M); });\n}\n\n/// Helper function to test whether a shuffle mask could be\n/// simplified by widening the elements being shuffled.\n///\n/// Appends the mask for wider elements in WidenedMask if valid. Otherwise\n/// leaves it in an unspecified state.\n///\n/// NOTE: This must handle normal vector shuffle masks and *target* vector\n/// shuffle masks. The latter have the special property of a '-2' representing\n/// a zero-ed lane of a vector.\nstatic bool canWidenShuffleElements(ArrayRef<int> Mask,\n                                    SmallVectorImpl<int> &WidenedMask) {\n  WidenedMask.assign(Mask.size() / 2, 0);\n  for (int i = 0, Size = Mask.size(); i < Size; i += 2) {\n    int M0 = Mask[i];\n    int M1 = Mask[i + 1];\n\n    // If both elements are undef, its trivial.\n    if (M0 == SM_SentinelUndef && M1 == SM_SentinelUndef) {\n      WidenedMask[i / 2] = SM_SentinelUndef;\n      continue;\n    }\n\n    // Check for an undef mask and a mask value properly aligned to fit with\n    // a pair of values. If we find such a case, use the non-undef mask's value.\n    if (M0 == SM_SentinelUndef && M1 >= 0 && (M1 % 2) == 1) {\n      WidenedMask[i / 2] = M1 / 2;\n      continue;\n    }\n    if (M1 == SM_SentinelUndef && M0 >= 0 && (M0 % 2) == 0) {\n      WidenedMask[i / 2] = M0 / 2;\n      continue;\n    }\n\n    // When zeroing, we need to spread the zeroing across both lanes to widen.\n    if (M0 == SM_SentinelZero || M1 == SM_SentinelZero) {\n      if ((M0 == SM_SentinelZero || M0 == SM_SentinelUndef) &&\n          (M1 == SM_SentinelZero || M1 == SM_SentinelUndef)) {\n        WidenedMask[i / 2] = SM_SentinelZero;\n        continue;\n      }\n      return false;\n    }\n\n    // Finally check if the two mask values are adjacent and aligned with\n    // a pair.\n    if (M0 != SM_SentinelUndef && (M0 % 2) == 0 && (M0 + 1) == M1) {\n      WidenedMask[i / 2] = M0 / 2;\n      continue;\n    }\n\n    // Otherwise we can't safely widen the elements used in this shuffle.\n    return false;\n  }\n  assert(WidenedMask.size() == Mask.size() / 2 &&\n         \"Incorrect size of mask after widening the elements!\");\n\n  return true;\n}\n\nstatic bool canWidenShuffleElements(ArrayRef<int> Mask,\n                                    const APInt &Zeroable,\n                                    bool V2IsZero,\n                                    SmallVectorImpl<int> &WidenedMask) {\n  // Create an alternative mask with info about zeroable elements.\n  // Here we do not set undef elements as zeroable.\n  SmallVector<int, 64> ZeroableMask(Mask.begin(), Mask.end());\n  if (V2IsZero) {\n    assert(!Zeroable.isNullValue() && \"V2's non-undef elements are used?!\");\n    for (int i = 0, Size = Mask.size(); i != Size; ++i)\n      if (Mask[i] != SM_SentinelUndef && Zeroable[i])\n        ZeroableMask[i] = SM_SentinelZero;\n  }\n  return canWidenShuffleElements(ZeroableMask, WidenedMask);\n}\n\nstatic bool canWidenShuffleElements(ArrayRef<int> Mask) {\n  SmallVector<int, 32> WidenedMask;\n  return canWidenShuffleElements(Mask, WidenedMask);\n}\n\n// Attempt to narrow/widen shuffle mask until it matches the target number of\n// elements.\nstatic bool scaleShuffleElements(ArrayRef<int> Mask, unsigned NumDstElts,\n                                 SmallVectorImpl<int> &ScaledMask) {\n  unsigned NumSrcElts = Mask.size();\n  assert(((NumSrcElts % NumDstElts) == 0 || (NumDstElts % NumSrcElts) == 0) &&\n         \"Illegal shuffle scale factor\");\n\n  // Narrowing is guaranteed to work.\n  if (NumDstElts >= NumSrcElts) {\n    int Scale = NumDstElts / NumSrcElts;\n    llvm::narrowShuffleMaskElts(Scale, Mask, ScaledMask);\n    return true;\n  }\n\n  // We have to repeat the widening until we reach the target size, but we can\n  // split out the first widening as it sets up ScaledMask for us.\n  if (canWidenShuffleElements(Mask, ScaledMask)) {\n    while (ScaledMask.size() > NumDstElts) {\n      SmallVector<int, 16> WidenedMask;\n      if (!canWidenShuffleElements(ScaledMask, WidenedMask))\n        return false;\n      ScaledMask = std::move(WidenedMask);\n    }\n    return true;\n  }\n\n  return false;\n}\n\n/// Returns true if Elt is a constant zero or a floating point constant +0.0.\nbool X86::isZeroNode(SDValue Elt) {\n  return isNullConstant(Elt) || isNullFPConstant(Elt);\n}\n\n// Build a vector of constants.\n// Use an UNDEF node if MaskElt == -1.\n// Split 64-bit constants in the 32-bit mode.\nstatic SDValue getConstVector(ArrayRef<int> Values, MVT VT, SelectionDAG &DAG,\n                              const SDLoc &dl, bool IsMask = false) {\n\n  SmallVector<SDValue, 32>  Ops;\n  bool Split = false;\n\n  MVT ConstVecVT = VT;\n  unsigned NumElts = VT.getVectorNumElements();\n  bool In64BitMode = DAG.getTargetLoweringInfo().isTypeLegal(MVT::i64);\n  if (!In64BitMode && VT.getVectorElementType() == MVT::i64) {\n    ConstVecVT = MVT::getVectorVT(MVT::i32, NumElts * 2);\n    Split = true;\n  }\n\n  MVT EltVT = ConstVecVT.getVectorElementType();\n  for (unsigned i = 0; i < NumElts; ++i) {\n    bool IsUndef = Values[i] < 0 && IsMask;\n    SDValue OpNode = IsUndef ? DAG.getUNDEF(EltVT) :\n      DAG.getConstant(Values[i], dl, EltVT);\n    Ops.push_back(OpNode);\n    if (Split)\n      Ops.push_back(IsUndef ? DAG.getUNDEF(EltVT) :\n                    DAG.getConstant(0, dl, EltVT));\n  }\n  SDValue ConstsNode = DAG.getBuildVector(ConstVecVT, dl, Ops);\n  if (Split)\n    ConstsNode = DAG.getBitcast(VT, ConstsNode);\n  return ConstsNode;\n}\n\nstatic SDValue getConstVector(ArrayRef<APInt> Bits, APInt &Undefs,\n                              MVT VT, SelectionDAG &DAG, const SDLoc &dl) {\n  assert(Bits.size() == Undefs.getBitWidth() &&\n         \"Unequal constant and undef arrays\");\n  SmallVector<SDValue, 32> Ops;\n  bool Split = false;\n\n  MVT ConstVecVT = VT;\n  unsigned NumElts = VT.getVectorNumElements();\n  bool In64BitMode = DAG.getTargetLoweringInfo().isTypeLegal(MVT::i64);\n  if (!In64BitMode && VT.getVectorElementType() == MVT::i64) {\n    ConstVecVT = MVT::getVectorVT(MVT::i32, NumElts * 2);\n    Split = true;\n  }\n\n  MVT EltVT = ConstVecVT.getVectorElementType();\n  for (unsigned i = 0, e = Bits.size(); i != e; ++i) {\n    if (Undefs[i]) {\n      Ops.append(Split ? 2 : 1, DAG.getUNDEF(EltVT));\n      continue;\n    }\n    const APInt &V = Bits[i];\n    assert(V.getBitWidth() == VT.getScalarSizeInBits() && \"Unexpected sizes\");\n    if (Split) {\n      Ops.push_back(DAG.getConstant(V.trunc(32), dl, EltVT));\n      Ops.push_back(DAG.getConstant(V.lshr(32).trunc(32), dl, EltVT));\n    } else if (EltVT == MVT::f32) {\n      APFloat FV(APFloat::IEEEsingle(), V);\n      Ops.push_back(DAG.getConstantFP(FV, dl, EltVT));\n    } else if (EltVT == MVT::f64) {\n      APFloat FV(APFloat::IEEEdouble(), V);\n      Ops.push_back(DAG.getConstantFP(FV, dl, EltVT));\n    } else {\n      Ops.push_back(DAG.getConstant(V, dl, EltVT));\n    }\n  }\n\n  SDValue ConstsNode = DAG.getBuildVector(ConstVecVT, dl, Ops);\n  return DAG.getBitcast(VT, ConstsNode);\n}\n\n/// Returns a vector of specified type with all zero elements.\nstatic SDValue getZeroVector(MVT VT, const X86Subtarget &Subtarget,\n                             SelectionDAG &DAG, const SDLoc &dl) {\n  assert((VT.is128BitVector() || VT.is256BitVector() || VT.is512BitVector() ||\n          VT.getVectorElementType() == MVT::i1) &&\n         \"Unexpected vector type\");\n\n  // Try to build SSE/AVX zero vectors as <N x i32> bitcasted to their dest\n  // type. This ensures they get CSE'd. But if the integer type is not\n  // available, use a floating-point +0.0 instead.\n  SDValue Vec;\n  if (!Subtarget.hasSSE2() && VT.is128BitVector()) {\n    Vec = DAG.getConstantFP(+0.0, dl, MVT::v4f32);\n  } else if (VT.isFloatingPoint()) {\n    Vec = DAG.getConstantFP(+0.0, dl, VT);\n  } else if (VT.getVectorElementType() == MVT::i1) {\n    assert((Subtarget.hasBWI() || VT.getVectorNumElements() <= 16) &&\n           \"Unexpected vector type\");\n    Vec = DAG.getConstant(0, dl, VT);\n  } else {\n    unsigned Num32BitElts = VT.getSizeInBits() / 32;\n    Vec = DAG.getConstant(0, dl, MVT::getVectorVT(MVT::i32, Num32BitElts));\n  }\n  return DAG.getBitcast(VT, Vec);\n}\n\nstatic SDValue extractSubVector(SDValue Vec, unsigned IdxVal, SelectionDAG &DAG,\n                                const SDLoc &dl, unsigned vectorWidth) {\n  EVT VT = Vec.getValueType();\n  EVT ElVT = VT.getVectorElementType();\n  unsigned Factor = VT.getSizeInBits()/vectorWidth;\n  EVT ResultVT = EVT::getVectorVT(*DAG.getContext(), ElVT,\n                                  VT.getVectorNumElements()/Factor);\n\n  // Extract the relevant vectorWidth bits.  Generate an EXTRACT_SUBVECTOR\n  unsigned ElemsPerChunk = vectorWidth / ElVT.getSizeInBits();\n  assert(isPowerOf2_32(ElemsPerChunk) && \"Elements per chunk not power of 2\");\n\n  // This is the index of the first element of the vectorWidth-bit chunk\n  // we want. Since ElemsPerChunk is a power of 2 just need to clear bits.\n  IdxVal &= ~(ElemsPerChunk - 1);\n\n  // If the input is a buildvector just emit a smaller one.\n  if (Vec.getOpcode() == ISD::BUILD_VECTOR)\n    return DAG.getBuildVector(ResultVT, dl,\n                              Vec->ops().slice(IdxVal, ElemsPerChunk));\n\n  SDValue VecIdx = DAG.getIntPtrConstant(IdxVal, dl);\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, ResultVT, Vec, VecIdx);\n}\n\n/// Generate a DAG to grab 128-bits from a vector > 128 bits.  This\n/// sets things up to match to an AVX VEXTRACTF128 / VEXTRACTI128\n/// or AVX-512 VEXTRACTF32x4 / VEXTRACTI32x4\n/// instructions or a simple subregister reference. Idx is an index in the\n/// 128 bits we want.  It need not be aligned to a 128-bit boundary.  That makes\n/// lowering EXTRACT_VECTOR_ELT operations easier.\nstatic SDValue extract128BitVector(SDValue Vec, unsigned IdxVal,\n                                   SelectionDAG &DAG, const SDLoc &dl) {\n  assert((Vec.getValueType().is256BitVector() ||\n          Vec.getValueType().is512BitVector()) && \"Unexpected vector size!\");\n  return extractSubVector(Vec, IdxVal, DAG, dl, 128);\n}\n\n/// Generate a DAG to grab 256-bits from a 512-bit vector.\nstatic SDValue extract256BitVector(SDValue Vec, unsigned IdxVal,\n                                   SelectionDAG &DAG, const SDLoc &dl) {\n  assert(Vec.getValueType().is512BitVector() && \"Unexpected vector size!\");\n  return extractSubVector(Vec, IdxVal, DAG, dl, 256);\n}\n\nstatic SDValue insertSubVector(SDValue Result, SDValue Vec, unsigned IdxVal,\n                               SelectionDAG &DAG, const SDLoc &dl,\n                               unsigned vectorWidth) {\n  assert((vectorWidth == 128 || vectorWidth == 256) &&\n         \"Unsupported vector width\");\n  // Inserting UNDEF is Result\n  if (Vec.isUndef())\n    return Result;\n  EVT VT = Vec.getValueType();\n  EVT ElVT = VT.getVectorElementType();\n  EVT ResultVT = Result.getValueType();\n\n  // Insert the relevant vectorWidth bits.\n  unsigned ElemsPerChunk = vectorWidth/ElVT.getSizeInBits();\n  assert(isPowerOf2_32(ElemsPerChunk) && \"Elements per chunk not power of 2\");\n\n  // This is the index of the first element of the vectorWidth-bit chunk\n  // we want. Since ElemsPerChunk is a power of 2 just need to clear bits.\n  IdxVal &= ~(ElemsPerChunk - 1);\n\n  SDValue VecIdx = DAG.getIntPtrConstant(IdxVal, dl);\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ResultVT, Result, Vec, VecIdx);\n}\n\n/// Generate a DAG to put 128-bits into a vector > 128 bits.  This\n/// sets things up to match to an AVX VINSERTF128/VINSERTI128 or\n/// AVX-512 VINSERTF32x4/VINSERTI32x4 instructions or a\n/// simple superregister reference.  Idx is an index in the 128 bits\n/// we want.  It need not be aligned to a 128-bit boundary.  That makes\n/// lowering INSERT_VECTOR_ELT operations easier.\nstatic SDValue insert128BitVector(SDValue Result, SDValue Vec, unsigned IdxVal,\n                                  SelectionDAG &DAG, const SDLoc &dl) {\n  assert(Vec.getValueType().is128BitVector() && \"Unexpected vector size!\");\n  return insertSubVector(Result, Vec, IdxVal, DAG, dl, 128);\n}\n\n/// Widen a vector to a larger size with the same scalar type, with the new\n/// elements either zero or undef.\nstatic SDValue widenSubVector(MVT VT, SDValue Vec, bool ZeroNewElements,\n                              const X86Subtarget &Subtarget, SelectionDAG &DAG,\n                              const SDLoc &dl) {\n  assert(Vec.getValueSizeInBits().getFixedSize() < VT.getFixedSizeInBits() &&\n         Vec.getValueType().getScalarType() == VT.getScalarType() &&\n         \"Unsupported vector widening type\");\n  SDValue Res = ZeroNewElements ? getZeroVector(VT, Subtarget, DAG, dl)\n                                : DAG.getUNDEF(VT);\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, VT, Res, Vec,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\n/// Widen a vector to a larger size with the same scalar type, with the new\n/// elements either zero or undef.\nstatic SDValue widenSubVector(SDValue Vec, bool ZeroNewElements,\n                              const X86Subtarget &Subtarget, SelectionDAG &DAG,\n                              const SDLoc &dl, unsigned WideSizeInBits) {\n  assert(Vec.getValueSizeInBits() < WideSizeInBits &&\n         (WideSizeInBits % Vec.getScalarValueSizeInBits()) == 0 &&\n         \"Unsupported vector widening type\");\n  unsigned WideNumElts = WideSizeInBits / Vec.getScalarValueSizeInBits();\n  MVT SVT = Vec.getSimpleValueType().getScalarType();\n  MVT VT = MVT::getVectorVT(SVT, WideNumElts);\n  return widenSubVector(VT, Vec, ZeroNewElements, Subtarget, DAG, dl);\n}\n\n// Helper function to collect subvector ops that are concatenated together,\n// either by ISD::CONCAT_VECTORS or a ISD::INSERT_SUBVECTOR series.\n// The subvectors in Ops are guaranteed to be the same type.\nstatic bool collectConcatOps(SDNode *N, SmallVectorImpl<SDValue> &Ops) {\n  assert(Ops.empty() && \"Expected an empty ops vector\");\n\n  if (N->getOpcode() == ISD::CONCAT_VECTORS) {\n    Ops.append(N->op_begin(), N->op_end());\n    return true;\n  }\n\n  if (N->getOpcode() == ISD::INSERT_SUBVECTOR) {\n    SDValue Src = N->getOperand(0);\n    SDValue Sub = N->getOperand(1);\n    const APInt &Idx = N->getConstantOperandAPInt(2);\n    EVT VT = Src.getValueType();\n    EVT SubVT = Sub.getValueType();\n\n    // TODO - Handle more general insert_subvector chains.\n    if (VT.getSizeInBits() == (SubVT.getSizeInBits() * 2) &&\n        Idx == (VT.getVectorNumElements() / 2)) {\n      // insert_subvector(insert_subvector(undef, x, lo), y, hi)\n      if (Src.getOpcode() == ISD::INSERT_SUBVECTOR &&\n          Src.getOperand(1).getValueType() == SubVT &&\n          isNullConstant(Src.getOperand(2))) {\n        Ops.push_back(Src.getOperand(1));\n        Ops.push_back(Sub);\n        return true;\n      }\n      // insert_subvector(x, extract_subvector(x, lo), hi)\n      if (Sub.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n          Sub.getOperand(0) == Src && isNullConstant(Sub.getOperand(1))) {\n        Ops.append(2, Sub);\n        return true;\n      }\n    }\n  }\n\n  return false;\n}\n\nstatic std::pair<SDValue, SDValue> splitVector(SDValue Op, SelectionDAG &DAG,\n                                               const SDLoc &dl) {\n  EVT VT = Op.getValueType();\n  unsigned NumElems = VT.getVectorNumElements();\n  unsigned SizeInBits = VT.getSizeInBits();\n  assert((NumElems % 2) == 0 && (SizeInBits % 2) == 0 &&\n         \"Can't split odd sized vector\");\n\n  SDValue Lo = extractSubVector(Op, 0, DAG, dl, SizeInBits / 2);\n  SDValue Hi = extractSubVector(Op, NumElems / 2, DAG, dl, SizeInBits / 2);\n  return std::make_pair(Lo, Hi);\n}\n\n// Split an unary integer op into 2 half sized ops.\nstatic SDValue splitVectorIntUnary(SDValue Op, SelectionDAG &DAG) {\n  EVT VT = Op.getValueType();\n\n  // Make sure we only try to split 256/512-bit types to avoid creating\n  // narrow vectors.\n  assert((Op.getOperand(0).getValueType().is256BitVector() ||\n          Op.getOperand(0).getValueType().is512BitVector()) &&\n         (VT.is256BitVector() || VT.is512BitVector()) && \"Unsupported VT!\");\n  assert(Op.getOperand(0).getValueType().getVectorNumElements() ==\n             VT.getVectorNumElements() &&\n         \"Unexpected VTs!\");\n\n  SDLoc dl(Op);\n\n  // Extract the Lo/Hi vectors\n  SDValue Lo, Hi;\n  std::tie(Lo, Hi) = splitVector(Op.getOperand(0), DAG, dl);\n\n  EVT LoVT, HiVT;\n  std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT,\n                     DAG.getNode(Op.getOpcode(), dl, LoVT, Lo),\n                     DAG.getNode(Op.getOpcode(), dl, HiVT, Hi));\n}\n\n/// Break a binary integer operation into 2 half sized ops and then\n/// concatenate the result back.\nstatic SDValue splitVectorIntBinary(SDValue Op, SelectionDAG &DAG) {\n  EVT VT = Op.getValueType();\n\n  // Sanity check that all the types match.\n  assert(Op.getOperand(0).getValueType() == VT &&\n         Op.getOperand(1).getValueType() == VT && \"Unexpected VTs!\");\n  assert((VT.is256BitVector() || VT.is512BitVector()) && \"Unsupported VT!\");\n\n  SDLoc dl(Op);\n\n  // Extract the LHS Lo/Hi vectors\n  SDValue LHS1, LHS2;\n  std::tie(LHS1, LHS2) = splitVector(Op.getOperand(0), DAG, dl);\n\n  // Extract the RHS Lo/Hi vectors\n  SDValue RHS1, RHS2;\n  std::tie(RHS1, RHS2) = splitVector(Op.getOperand(1), DAG, dl);\n\n  EVT LoVT, HiVT;\n  std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT,\n                     DAG.getNode(Op.getOpcode(), dl, LoVT, LHS1, RHS1),\n                     DAG.getNode(Op.getOpcode(), dl, HiVT, LHS2, RHS2));\n}\n\n// Helper for splitting operands of an operation to legal target size and\n// apply a function on each part.\n// Useful for operations that are available on SSE2 in 128-bit, on AVX2 in\n// 256-bit and on AVX512BW in 512-bit. The argument VT is the type used for\n// deciding if/how to split Ops. Ops elements do *not* have to be of type VT.\n// The argument Builder is a function that will be applied on each split part:\n// SDValue Builder(SelectionDAG&G, SDLoc, ArrayRef<SDValue>)\ntemplate <typename F>\nSDValue SplitOpsAndApply(SelectionDAG &DAG, const X86Subtarget &Subtarget,\n                         const SDLoc &DL, EVT VT, ArrayRef<SDValue> Ops,\n                         F Builder, bool CheckBWI = true) {\n  assert(Subtarget.hasSSE2() && \"Target assumed to support at least SSE2\");\n  unsigned NumSubs = 1;\n  if ((CheckBWI && Subtarget.useBWIRegs()) ||\n      (!CheckBWI && Subtarget.useAVX512Regs())) {\n    if (VT.getSizeInBits() > 512) {\n      NumSubs = VT.getSizeInBits() / 512;\n      assert((VT.getSizeInBits() % 512) == 0 && \"Illegal vector size\");\n    }\n  } else if (Subtarget.hasAVX2()) {\n    if (VT.getSizeInBits() > 256) {\n      NumSubs = VT.getSizeInBits() / 256;\n      assert((VT.getSizeInBits() % 256) == 0 && \"Illegal vector size\");\n    }\n  } else {\n    if (VT.getSizeInBits() > 128) {\n      NumSubs = VT.getSizeInBits() / 128;\n      assert((VT.getSizeInBits() % 128) == 0 && \"Illegal vector size\");\n    }\n  }\n\n  if (NumSubs == 1)\n    return Builder(DAG, DL, Ops);\n\n  SmallVector<SDValue, 4> Subs;\n  for (unsigned i = 0; i != NumSubs; ++i) {\n    SmallVector<SDValue, 2> SubOps;\n    for (SDValue Op : Ops) {\n      EVT OpVT = Op.getValueType();\n      unsigned NumSubElts = OpVT.getVectorNumElements() / NumSubs;\n      unsigned SizeSub = OpVT.getSizeInBits() / NumSubs;\n      SubOps.push_back(extractSubVector(Op, i * NumSubElts, DAG, DL, SizeSub));\n    }\n    Subs.push_back(Builder(DAG, DL, SubOps));\n  }\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Subs);\n}\n\n/// Insert i1-subvector to i1-vector.\nstatic SDValue insert1BitVector(SDValue Op, SelectionDAG &DAG,\n                                const X86Subtarget &Subtarget) {\n\n  SDLoc dl(Op);\n  SDValue Vec = Op.getOperand(0);\n  SDValue SubVec = Op.getOperand(1);\n  SDValue Idx = Op.getOperand(2);\n  unsigned IdxVal = Op.getConstantOperandVal(2);\n\n  // Inserting undef is a nop. We can just return the original vector.\n  if (SubVec.isUndef())\n    return Vec;\n\n  if (IdxVal == 0 && Vec.isUndef()) // the operation is legal\n    return Op;\n\n  MVT OpVT = Op.getSimpleValueType();\n  unsigned NumElems = OpVT.getVectorNumElements();\n  SDValue ZeroIdx = DAG.getIntPtrConstant(0, dl);\n\n  // Extend to natively supported kshift.\n  MVT WideOpVT = OpVT;\n  if ((!Subtarget.hasDQI() && NumElems == 8) || NumElems < 8)\n    WideOpVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n\n  // Inserting into the lsbs of a zero vector is legal. ISel will insert shifts\n  // if necessary.\n  if (IdxVal == 0 && ISD::isBuildVectorAllZeros(Vec.getNode())) {\n    // May need to promote to a legal type.\n    Op = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT,\n                     DAG.getConstant(0, dl, WideOpVT),\n                     SubVec, Idx);\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, Op, ZeroIdx);\n  }\n\n  MVT SubVecVT = SubVec.getSimpleValueType();\n  unsigned SubVecNumElems = SubVecVT.getVectorNumElements();\n  assert(IdxVal + SubVecNumElems <= NumElems &&\n         IdxVal % SubVecVT.getSizeInBits() == 0 &&\n         \"Unexpected index value in INSERT_SUBVECTOR\");\n\n  SDValue Undef = DAG.getUNDEF(WideOpVT);\n\n  if (IdxVal == 0) {\n    // Zero lower bits of the Vec\n    SDValue ShiftBits = DAG.getTargetConstant(SubVecNumElems, dl, MVT::i8);\n    Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT, Undef, Vec,\n                      ZeroIdx);\n    Vec = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, Vec, ShiftBits);\n    Vec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, Vec, ShiftBits);\n    // Merge them together, SubVec should be zero extended.\n    SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT,\n                         DAG.getConstant(0, dl, WideOpVT),\n                         SubVec, ZeroIdx);\n    Op = DAG.getNode(ISD::OR, dl, WideOpVT, Vec, SubVec);\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, Op, ZeroIdx);\n  }\n\n  SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT,\n                       Undef, SubVec, ZeroIdx);\n\n  if (Vec.isUndef()) {\n    assert(IdxVal != 0 && \"Unexpected index\");\n    SubVec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, SubVec,\n                         DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, SubVec, ZeroIdx);\n  }\n\n  if (ISD::isBuildVectorAllZeros(Vec.getNode())) {\n    assert(IdxVal != 0 && \"Unexpected index\");\n    NumElems = WideOpVT.getVectorNumElements();\n    unsigned ShiftLeft = NumElems - SubVecNumElems;\n    unsigned ShiftRight = NumElems - SubVecNumElems - IdxVal;\n    SubVec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, SubVec,\n                         DAG.getTargetConstant(ShiftLeft, dl, MVT::i8));\n    if (ShiftRight != 0)\n      SubVec = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, SubVec,\n                           DAG.getTargetConstant(ShiftRight, dl, MVT::i8));\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, SubVec, ZeroIdx);\n  }\n\n  // Simple case when we put subvector in the upper part\n  if (IdxVal + SubVecNumElems == NumElems) {\n    SubVec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, SubVec,\n                         DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n    if (SubVecNumElems * 2 == NumElems) {\n      // Special case, use legal zero extending insert_subvector. This allows\n      // isel to optimize when bits are known zero.\n      Vec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, SubVecVT, Vec, ZeroIdx);\n      Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT,\n                        DAG.getConstant(0, dl, WideOpVT),\n                        Vec, ZeroIdx);\n    } else {\n      // Otherwise use explicit shifts to zero the bits.\n      Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT,\n                        Undef, Vec, ZeroIdx);\n      NumElems = WideOpVT.getVectorNumElements();\n      SDValue ShiftBits = DAG.getTargetConstant(NumElems - IdxVal, dl, MVT::i8);\n      Vec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, Vec, ShiftBits);\n      Vec = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, Vec, ShiftBits);\n    }\n    Op = DAG.getNode(ISD::OR, dl, WideOpVT, Vec, SubVec);\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, Op, ZeroIdx);\n  }\n\n  // Inserting into the middle is more complicated.\n\n  NumElems = WideOpVT.getVectorNumElements();\n\n  // Widen the vector if needed.\n  Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideOpVT, Undef, Vec, ZeroIdx);\n\n  unsigned ShiftLeft = NumElems - SubVecNumElems;\n  unsigned ShiftRight = NumElems - SubVecNumElems - IdxVal;\n\n  // Do an optimization for the the most frequently used types.\n  if (WideOpVT != MVT::v64i1 || Subtarget.is64Bit()) {\n    APInt Mask0 = APInt::getBitsSet(NumElems, IdxVal, IdxVal + SubVecNumElems);\n    Mask0.flipAllBits();\n    SDValue CMask0 = DAG.getConstant(Mask0, dl, MVT::getIntegerVT(NumElems));\n    SDValue VMask0 = DAG.getNode(ISD::BITCAST, dl, WideOpVT, CMask0);\n    Vec = DAG.getNode(ISD::AND, dl, WideOpVT, Vec, VMask0);\n    SubVec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, SubVec,\n                         DAG.getTargetConstant(ShiftLeft, dl, MVT::i8));\n    SubVec = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, SubVec,\n                         DAG.getTargetConstant(ShiftRight, dl, MVT::i8));\n    Op = DAG.getNode(ISD::OR, dl, WideOpVT, Vec, SubVec);\n\n    // Reduce to original width if needed.\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, Op, ZeroIdx);\n  }\n\n  // Clear the upper bits of the subvector and move it to its insert position.\n  SubVec = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, SubVec,\n                       DAG.getTargetConstant(ShiftLeft, dl, MVT::i8));\n  SubVec = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, SubVec,\n                       DAG.getTargetConstant(ShiftRight, dl, MVT::i8));\n\n  // Isolate the bits below the insertion point.\n  unsigned LowShift = NumElems - IdxVal;\n  SDValue Low = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, Vec,\n                            DAG.getTargetConstant(LowShift, dl, MVT::i8));\n  Low = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, Low,\n                    DAG.getTargetConstant(LowShift, dl, MVT::i8));\n\n  // Isolate the bits after the last inserted bit.\n  unsigned HighShift = IdxVal + SubVecNumElems;\n  SDValue High = DAG.getNode(X86ISD::KSHIFTR, dl, WideOpVT, Vec,\n                            DAG.getTargetConstant(HighShift, dl, MVT::i8));\n  High = DAG.getNode(X86ISD::KSHIFTL, dl, WideOpVT, High,\n                    DAG.getTargetConstant(HighShift, dl, MVT::i8));\n\n  // Now OR all 3 pieces together.\n  Vec = DAG.getNode(ISD::OR, dl, WideOpVT, Low, High);\n  SubVec = DAG.getNode(ISD::OR, dl, WideOpVT, SubVec, Vec);\n\n  // Reduce to original width if needed.\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OpVT, SubVec, ZeroIdx);\n}\n\nstatic SDValue concatSubVectors(SDValue V1, SDValue V2, SelectionDAG &DAG,\n                                const SDLoc &dl) {\n  assert(V1.getValueType() == V2.getValueType() && \"subvector type mismatch\");\n  EVT SubVT = V1.getValueType();\n  EVT SubSVT = SubVT.getScalarType();\n  unsigned SubNumElts = SubVT.getVectorNumElements();\n  unsigned SubVectorWidth = SubVT.getSizeInBits();\n  EVT VT = EVT::getVectorVT(*DAG.getContext(), SubSVT, 2 * SubNumElts);\n  SDValue V = insertSubVector(DAG.getUNDEF(VT), V1, 0, DAG, dl, SubVectorWidth);\n  return insertSubVector(V, V2, SubNumElts, DAG, dl, SubVectorWidth);\n}\n\n/// Returns a vector of specified type with all bits set.\n/// Always build ones vectors as <4 x i32>, <8 x i32> or <16 x i32>.\n/// Then bitcast to their original type, ensuring they get CSE'd.\nstatic SDValue getOnesVector(EVT VT, SelectionDAG &DAG, const SDLoc &dl) {\n  assert((VT.is128BitVector() || VT.is256BitVector() || VT.is512BitVector()) &&\n         \"Expected a 128/256/512-bit vector type\");\n\n  APInt Ones = APInt::getAllOnesValue(32);\n  unsigned NumElts = VT.getSizeInBits() / 32;\n  SDValue Vec = DAG.getConstant(Ones, dl, MVT::getVectorVT(MVT::i32, NumElts));\n  return DAG.getBitcast(VT, Vec);\n}\n\n// Convert *_EXTEND_VECTOR_INREG to *_EXTEND opcode.\nstatic unsigned getOpcode_EXTEND(unsigned Opcode) {\n  switch (Opcode) {\n  case ISD::ANY_EXTEND:\n  case ISD::ANY_EXTEND_VECTOR_INREG:\n    return ISD::ANY_EXTEND;\n  case ISD::ZERO_EXTEND:\n  case ISD::ZERO_EXTEND_VECTOR_INREG:\n    return ISD::ZERO_EXTEND;\n  case ISD::SIGN_EXTEND:\n  case ISD::SIGN_EXTEND_VECTOR_INREG:\n    return ISD::SIGN_EXTEND;\n  }\n  llvm_unreachable(\"Unknown opcode\");\n}\n\n// Convert *_EXTEND to *_EXTEND_VECTOR_INREG opcode.\nstatic unsigned getOpcode_EXTEND_VECTOR_INREG(unsigned Opcode) {\n  switch (Opcode) {\n  case ISD::ANY_EXTEND:\n  case ISD::ANY_EXTEND_VECTOR_INREG:\n    return ISD::ANY_EXTEND_VECTOR_INREG;\n  case ISD::ZERO_EXTEND:\n  case ISD::ZERO_EXTEND_VECTOR_INREG:\n    return ISD::ZERO_EXTEND_VECTOR_INREG;\n  case ISD::SIGN_EXTEND:\n  case ISD::SIGN_EXTEND_VECTOR_INREG:\n    return ISD::SIGN_EXTEND_VECTOR_INREG;\n  }\n  llvm_unreachable(\"Unknown opcode\");\n}\n\nstatic SDValue getEXTEND_VECTOR_INREG(unsigned Opcode, const SDLoc &DL, EVT VT,\n                                      SDValue In, SelectionDAG &DAG) {\n  EVT InVT = In.getValueType();\n  assert(VT.isVector() && InVT.isVector() && \"Expected vector VTs.\");\n  assert((ISD::ANY_EXTEND == Opcode || ISD::SIGN_EXTEND == Opcode ||\n          ISD::ZERO_EXTEND == Opcode) &&\n         \"Unknown extension opcode\");\n\n  // For 256-bit vectors, we only need the lower (128-bit) input half.\n  // For 512-bit vectors, we only need the lower input half or quarter.\n  if (InVT.getSizeInBits() > 128) {\n    assert(VT.getSizeInBits() == InVT.getSizeInBits() &&\n           \"Expected VTs to be the same size!\");\n    unsigned Scale = VT.getScalarSizeInBits() / InVT.getScalarSizeInBits();\n    In = extractSubVector(In, 0, DAG, DL,\n                          std::max(128U, (unsigned)VT.getSizeInBits() / Scale));\n    InVT = In.getValueType();\n  }\n\n  if (VT.getVectorNumElements() != InVT.getVectorNumElements())\n    Opcode = getOpcode_EXTEND_VECTOR_INREG(Opcode);\n\n  return DAG.getNode(Opcode, DL, VT, In);\n}\n\n// Match (xor X, -1) -> X.\n// Match extract_subvector(xor X, -1) -> extract_subvector(X).\n// Match concat_vectors(xor X, -1, xor Y, -1) -> concat_vectors(X, Y).\nstatic SDValue IsNOT(SDValue V, SelectionDAG &DAG, bool OneUse = false) {\n  V = OneUse ? peekThroughOneUseBitcasts(V) : peekThroughBitcasts(V);\n  if (V.getOpcode() == ISD::XOR &&\n      ISD::isBuildVectorAllOnes(V.getOperand(1).getNode()))\n    return V.getOperand(0);\n  if (V.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      (isNullConstant(V.getOperand(1)) || V.getOperand(0).hasOneUse())) {\n    if (SDValue Not = IsNOT(V.getOperand(0), DAG)) {\n      Not = DAG.getBitcast(V.getOperand(0).getValueType(), Not);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, SDLoc(Not), V.getValueType(),\n                         Not, V.getOperand(1));\n    }\n  }\n  SmallVector<SDValue, 2> CatOps;\n  if (collectConcatOps(V.getNode(), CatOps)) {\n    for (SDValue &CatOp : CatOps) {\n      SDValue NotCat = IsNOT(CatOp, DAG);\n      if (!NotCat) return SDValue();\n      CatOp = DAG.getBitcast(CatOp.getValueType(), NotCat);\n    }\n    return DAG.getNode(ISD::CONCAT_VECTORS, SDLoc(V), V.getValueType(), CatOps);\n  }\n  return SDValue();\n}\n\nvoid llvm::createUnpackShuffleMask(EVT VT, SmallVectorImpl<int> &Mask,\n                                   bool Lo, bool Unary) {\n  assert(VT.getScalarType().isSimple() && (VT.getSizeInBits() % 128) == 0 &&\n         \"Illegal vector type to unpack\");\n  assert(Mask.empty() && \"Expected an empty shuffle mask vector\");\n  int NumElts = VT.getVectorNumElements();\n  int NumEltsInLane = 128 / VT.getScalarSizeInBits();\n  for (int i = 0; i < NumElts; ++i) {\n    unsigned LaneStart = (i / NumEltsInLane) * NumEltsInLane;\n    int Pos = (i % NumEltsInLane) / 2 + LaneStart;\n    Pos += (Unary ? 0 : NumElts * (i % 2));\n    Pos += (Lo ? 0 : NumEltsInLane / 2);\n    Mask.push_back(Pos);\n  }\n}\n\n/// Similar to unpacklo/unpackhi, but without the 128-bit lane limitation\n/// imposed by AVX and specific to the unary pattern. Example:\n/// v8iX Lo --> <0, 0, 1, 1, 2, 2, 3, 3>\n/// v8iX Hi --> <4, 4, 5, 5, 6, 6, 7, 7>\nvoid llvm::createSplat2ShuffleMask(MVT VT, SmallVectorImpl<int> &Mask,\n                                   bool Lo) {\n  assert(Mask.empty() && \"Expected an empty shuffle mask vector\");\n  int NumElts = VT.getVectorNumElements();\n  for (int i = 0; i < NumElts; ++i) {\n    int Pos = i / 2;\n    Pos += (Lo ? 0 : NumElts / 2);\n    Mask.push_back(Pos);\n  }\n}\n\n/// Returns a vector_shuffle node for an unpackl operation.\nstatic SDValue getUnpackl(SelectionDAG &DAG, const SDLoc &dl, EVT VT,\n                          SDValue V1, SDValue V2) {\n  SmallVector<int, 8> Mask;\n  createUnpackShuffleMask(VT, Mask, /* Lo = */ true, /* Unary = */ false);\n  return DAG.getVectorShuffle(VT, dl, V1, V2, Mask);\n}\n\n/// Returns a vector_shuffle node for an unpackh operation.\nstatic SDValue getUnpackh(SelectionDAG &DAG, const SDLoc &dl, EVT VT,\n                          SDValue V1, SDValue V2) {\n  SmallVector<int, 8> Mask;\n  createUnpackShuffleMask(VT, Mask, /* Lo = */ false, /* Unary = */ false);\n  return DAG.getVectorShuffle(VT, dl, V1, V2, Mask);\n}\n\n/// Return a vector_shuffle of the specified vector of zero or undef vector.\n/// This produces a shuffle where the low element of V2 is swizzled into the\n/// zero/undef vector, landing at element Idx.\n/// This produces a shuffle mask like 4,1,2,3 (idx=0) or  0,1,2,4 (idx=3).\nstatic SDValue getShuffleVectorZeroOrUndef(SDValue V2, int Idx,\n                                           bool IsZero,\n                                           const X86Subtarget &Subtarget,\n                                           SelectionDAG &DAG) {\n  MVT VT = V2.getSimpleValueType();\n  SDValue V1 = IsZero\n    ? getZeroVector(VT, Subtarget, DAG, SDLoc(V2)) : DAG.getUNDEF(VT);\n  int NumElems = VT.getVectorNumElements();\n  SmallVector<int, 16> MaskVec(NumElems);\n  for (int i = 0; i != NumElems; ++i)\n    // If this is the insertion idx, put the low elt of V2 here.\n    MaskVec[i] = (i == Idx) ? NumElems : i;\n  return DAG.getVectorShuffle(VT, SDLoc(V2), V1, V2, MaskVec);\n}\n\nstatic const Constant *getTargetConstantFromBasePtr(SDValue Ptr) {\n  if (Ptr.getOpcode() == X86ISD::Wrapper ||\n      Ptr.getOpcode() == X86ISD::WrapperRIP)\n    Ptr = Ptr.getOperand(0);\n\n  auto *CNode = dyn_cast<ConstantPoolSDNode>(Ptr);\n  if (!CNode || CNode->isMachineConstantPoolEntry() || CNode->getOffset() != 0)\n    return nullptr;\n\n  return CNode->getConstVal();\n}\n\nstatic const Constant *getTargetConstantFromNode(LoadSDNode *Load) {\n  if (!Load || !ISD::isNormalLoad(Load))\n    return nullptr;\n  return getTargetConstantFromBasePtr(Load->getBasePtr());\n}\n\nstatic const Constant *getTargetConstantFromNode(SDValue Op) {\n  Op = peekThroughBitcasts(Op);\n  return getTargetConstantFromNode(dyn_cast<LoadSDNode>(Op));\n}\n\nconst Constant *\nX86TargetLowering::getTargetConstantFromLoad(LoadSDNode *LD) const {\n  assert(LD && \"Unexpected null LoadSDNode\");\n  return getTargetConstantFromNode(LD);\n}\n\n// Extract raw constant bits from constant pools.\nstatic bool getTargetConstantBitsFromNode(SDValue Op, unsigned EltSizeInBits,\n                                          APInt &UndefElts,\n                                          SmallVectorImpl<APInt> &EltBits,\n                                          bool AllowWholeUndefs = true,\n                                          bool AllowPartialUndefs = true) {\n  assert(EltBits.empty() && \"Expected an empty EltBits vector\");\n\n  Op = peekThroughBitcasts(Op);\n\n  EVT VT = Op.getValueType();\n  unsigned SizeInBits = VT.getSizeInBits();\n  assert((SizeInBits % EltSizeInBits) == 0 && \"Can't split constant!\");\n  unsigned NumElts = SizeInBits / EltSizeInBits;\n\n  // Bitcast a source array of element bits to the target size.\n  auto CastBitData = [&](APInt &UndefSrcElts, ArrayRef<APInt> SrcEltBits) {\n    unsigned NumSrcElts = UndefSrcElts.getBitWidth();\n    unsigned SrcEltSizeInBits = SrcEltBits[0].getBitWidth();\n    assert((NumSrcElts * SrcEltSizeInBits) == SizeInBits &&\n           \"Constant bit sizes don't match\");\n\n    // Don't split if we don't allow undef bits.\n    bool AllowUndefs = AllowWholeUndefs || AllowPartialUndefs;\n    if (UndefSrcElts.getBoolValue() && !AllowUndefs)\n      return false;\n\n    // If we're already the right size, don't bother bitcasting.\n    if (NumSrcElts == NumElts) {\n      UndefElts = UndefSrcElts;\n      EltBits.assign(SrcEltBits.begin(), SrcEltBits.end());\n      return true;\n    }\n\n    // Extract all the undef/constant element data and pack into single bitsets.\n    APInt UndefBits(SizeInBits, 0);\n    APInt MaskBits(SizeInBits, 0);\n\n    for (unsigned i = 0; i != NumSrcElts; ++i) {\n      unsigned BitOffset = i * SrcEltSizeInBits;\n      if (UndefSrcElts[i])\n        UndefBits.setBits(BitOffset, BitOffset + SrcEltSizeInBits);\n      MaskBits.insertBits(SrcEltBits[i], BitOffset);\n    }\n\n    // Split the undef/constant single bitset data into the target elements.\n    UndefElts = APInt(NumElts, 0);\n    EltBits.resize(NumElts, APInt(EltSizeInBits, 0));\n\n    for (unsigned i = 0; i != NumElts; ++i) {\n      unsigned BitOffset = i * EltSizeInBits;\n      APInt UndefEltBits = UndefBits.extractBits(EltSizeInBits, BitOffset);\n\n      // Only treat an element as UNDEF if all bits are UNDEF.\n      if (UndefEltBits.isAllOnesValue()) {\n        if (!AllowWholeUndefs)\n          return false;\n        UndefElts.setBit(i);\n        continue;\n      }\n\n      // If only some bits are UNDEF then treat them as zero (or bail if not\n      // supported).\n      if (UndefEltBits.getBoolValue() && !AllowPartialUndefs)\n        return false;\n\n      EltBits[i] = MaskBits.extractBits(EltSizeInBits, BitOffset);\n    }\n    return true;\n  };\n\n  // Collect constant bits and insert into mask/undef bit masks.\n  auto CollectConstantBits = [](const Constant *Cst, APInt &Mask, APInt &Undefs,\n                                unsigned UndefBitIndex) {\n    if (!Cst)\n      return false;\n    if (isa<UndefValue>(Cst)) {\n      Undefs.setBit(UndefBitIndex);\n      return true;\n    }\n    if (auto *CInt = dyn_cast<ConstantInt>(Cst)) {\n      Mask = CInt->getValue();\n      return true;\n    }\n    if (auto *CFP = dyn_cast<ConstantFP>(Cst)) {\n      Mask = CFP->getValueAPF().bitcastToAPInt();\n      return true;\n    }\n    return false;\n  };\n\n  // Handle UNDEFs.\n  if (Op.isUndef()) {\n    APInt UndefSrcElts = APInt::getAllOnesValue(NumElts);\n    SmallVector<APInt, 64> SrcEltBits(NumElts, APInt(EltSizeInBits, 0));\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n\n  // Extract scalar constant bits.\n  if (auto *Cst = dyn_cast<ConstantSDNode>(Op)) {\n    APInt UndefSrcElts = APInt::getNullValue(1);\n    SmallVector<APInt, 64> SrcEltBits(1, Cst->getAPIntValue());\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n  if (auto *Cst = dyn_cast<ConstantFPSDNode>(Op)) {\n    APInt UndefSrcElts = APInt::getNullValue(1);\n    APInt RawBits = Cst->getValueAPF().bitcastToAPInt();\n    SmallVector<APInt, 64> SrcEltBits(1, RawBits);\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n\n  // Extract constant bits from build vector.\n  if (ISD::isBuildVectorOfConstantSDNodes(Op.getNode())) {\n    unsigned SrcEltSizeInBits = VT.getScalarSizeInBits();\n    unsigned NumSrcElts = SizeInBits / SrcEltSizeInBits;\n\n    APInt UndefSrcElts(NumSrcElts, 0);\n    SmallVector<APInt, 64> SrcEltBits(NumSrcElts, APInt(SrcEltSizeInBits, 0));\n    for (unsigned i = 0, e = Op.getNumOperands(); i != e; ++i) {\n      const SDValue &Src = Op.getOperand(i);\n      if (Src.isUndef()) {\n        UndefSrcElts.setBit(i);\n        continue;\n      }\n      auto *Cst = cast<ConstantSDNode>(Src);\n      SrcEltBits[i] = Cst->getAPIntValue().zextOrTrunc(SrcEltSizeInBits);\n    }\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n  if (ISD::isBuildVectorOfConstantFPSDNodes(Op.getNode())) {\n    unsigned SrcEltSizeInBits = VT.getScalarSizeInBits();\n    unsigned NumSrcElts = SizeInBits / SrcEltSizeInBits;\n\n    APInt UndefSrcElts(NumSrcElts, 0);\n    SmallVector<APInt, 64> SrcEltBits(NumSrcElts, APInt(SrcEltSizeInBits, 0));\n    for (unsigned i = 0, e = Op.getNumOperands(); i != e; ++i) {\n      const SDValue &Src = Op.getOperand(i);\n      if (Src.isUndef()) {\n        UndefSrcElts.setBit(i);\n        continue;\n      }\n      auto *Cst = cast<ConstantFPSDNode>(Src);\n      APInt RawBits = Cst->getValueAPF().bitcastToAPInt();\n      SrcEltBits[i] = RawBits.zextOrTrunc(SrcEltSizeInBits);\n    }\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n\n  // Extract constant bits from constant pool vector.\n  if (auto *Cst = getTargetConstantFromNode(Op)) {\n    Type *CstTy = Cst->getType();\n    unsigned CstSizeInBits = CstTy->getPrimitiveSizeInBits();\n    if (!CstTy->isVectorTy() || (CstSizeInBits % SizeInBits) != 0)\n      return false;\n\n    unsigned SrcEltSizeInBits = CstTy->getScalarSizeInBits();\n    unsigned NumSrcElts = SizeInBits / SrcEltSizeInBits;\n\n    APInt UndefSrcElts(NumSrcElts, 0);\n    SmallVector<APInt, 64> SrcEltBits(NumSrcElts, APInt(SrcEltSizeInBits, 0));\n    for (unsigned i = 0; i != NumSrcElts; ++i)\n      if (!CollectConstantBits(Cst->getAggregateElement(i), SrcEltBits[i],\n                               UndefSrcElts, i))\n        return false;\n\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n\n  // Extract constant bits from a broadcasted constant pool scalar.\n  if (Op.getOpcode() == X86ISD::VBROADCAST_LOAD &&\n      EltSizeInBits <= VT.getScalarSizeInBits()) {\n    auto *MemIntr = cast<MemIntrinsicSDNode>(Op);\n    if (MemIntr->getMemoryVT().getScalarSizeInBits() != VT.getScalarSizeInBits())\n      return false;\n\n    SDValue Ptr = MemIntr->getBasePtr();\n    if (const Constant *C = getTargetConstantFromBasePtr(Ptr)) {\n      unsigned SrcEltSizeInBits = C->getType()->getScalarSizeInBits();\n      unsigned NumSrcElts = SizeInBits / SrcEltSizeInBits;\n\n      APInt UndefSrcElts(NumSrcElts, 0);\n      SmallVector<APInt, 64> SrcEltBits(1, APInt(SrcEltSizeInBits, 0));\n      if (CollectConstantBits(C, SrcEltBits[0], UndefSrcElts, 0)) {\n        if (UndefSrcElts[0])\n          UndefSrcElts.setBits(0, NumSrcElts);\n        SrcEltBits.append(NumSrcElts - 1, SrcEltBits[0]);\n        return CastBitData(UndefSrcElts, SrcEltBits);\n      }\n    }\n  }\n\n  // Extract constant bits from a subvector broadcast.\n  if (Op.getOpcode() == X86ISD::SUBV_BROADCAST_LOAD) {\n    auto *MemIntr = cast<MemIntrinsicSDNode>(Op);\n    SDValue Ptr = MemIntr->getBasePtr();\n    if (const Constant *Cst = getTargetConstantFromBasePtr(Ptr)) {\n      Type *CstTy = Cst->getType();\n      unsigned CstSizeInBits = CstTy->getPrimitiveSizeInBits();\n      if (!CstTy->isVectorTy() || (SizeInBits % CstSizeInBits) != 0)\n        return false;\n      unsigned SubEltSizeInBits = CstTy->getScalarSizeInBits();\n      unsigned NumSubElts = CstSizeInBits / SubEltSizeInBits;\n      unsigned NumSubVecs = SizeInBits / CstSizeInBits;\n      APInt UndefSubElts(NumSubElts, 0);\n      SmallVector<APInt, 64> SubEltBits(NumSubElts * NumSubVecs,\n                                        APInt(SubEltSizeInBits, 0));\n      for (unsigned i = 0; i != NumSubElts; ++i) {\n        if (!CollectConstantBits(Cst->getAggregateElement(i), SubEltBits[i],\n                                 UndefSubElts, i))\n          return false;\n        for (unsigned j = 1; j != NumSubVecs; ++j)\n          SubEltBits[i + (j * NumSubElts)] = SubEltBits[i];\n      }\n      UndefSubElts = APInt::getSplat(NumSubVecs * UndefSubElts.getBitWidth(),\n                                     UndefSubElts);\n      return CastBitData(UndefSubElts, SubEltBits);\n    }\n  }\n\n  // Extract a rematerialized scalar constant insertion.\n  if (Op.getOpcode() == X86ISD::VZEXT_MOVL &&\n      Op.getOperand(0).getOpcode() == ISD::SCALAR_TO_VECTOR &&\n      isa<ConstantSDNode>(Op.getOperand(0).getOperand(0))) {\n    unsigned SrcEltSizeInBits = VT.getScalarSizeInBits();\n    unsigned NumSrcElts = SizeInBits / SrcEltSizeInBits;\n\n    APInt UndefSrcElts(NumSrcElts, 0);\n    SmallVector<APInt, 64> SrcEltBits;\n    auto *CN = cast<ConstantSDNode>(Op.getOperand(0).getOperand(0));\n    SrcEltBits.push_back(CN->getAPIntValue().zextOrTrunc(SrcEltSizeInBits));\n    SrcEltBits.append(NumSrcElts - 1, APInt(SrcEltSizeInBits, 0));\n    return CastBitData(UndefSrcElts, SrcEltBits);\n  }\n\n  // Insert constant bits from a base and sub vector sources.\n  if (Op.getOpcode() == ISD::INSERT_SUBVECTOR) {\n    // If bitcasts to larger elements we might lose track of undefs - don't\n    // allow any to be safe.\n    unsigned SrcEltSizeInBits = VT.getScalarSizeInBits();\n    bool AllowUndefs = EltSizeInBits >= SrcEltSizeInBits;\n\n    APInt UndefSrcElts, UndefSubElts;\n    SmallVector<APInt, 32> EltSrcBits, EltSubBits;\n    if (getTargetConstantBitsFromNode(Op.getOperand(1), SrcEltSizeInBits,\n                                      UndefSubElts, EltSubBits,\n                                      AllowWholeUndefs && AllowUndefs,\n                                      AllowPartialUndefs && AllowUndefs) &&\n        getTargetConstantBitsFromNode(Op.getOperand(0), SrcEltSizeInBits,\n                                      UndefSrcElts, EltSrcBits,\n                                      AllowWholeUndefs && AllowUndefs,\n                                      AllowPartialUndefs && AllowUndefs)) {\n      unsigned BaseIdx = Op.getConstantOperandVal(2);\n      UndefSrcElts.insertBits(UndefSubElts, BaseIdx);\n      for (unsigned i = 0, e = EltSubBits.size(); i != e; ++i)\n        EltSrcBits[BaseIdx + i] = EltSubBits[i];\n      return CastBitData(UndefSrcElts, EltSrcBits);\n    }\n  }\n\n  // Extract constant bits from a subvector's source.\n  if (Op.getOpcode() == ISD::EXTRACT_SUBVECTOR) {\n    // TODO - support extract_subvector through bitcasts.\n    if (EltSizeInBits != VT.getScalarSizeInBits())\n      return false;\n\n    if (getTargetConstantBitsFromNode(Op.getOperand(0), EltSizeInBits,\n                                      UndefElts, EltBits, AllowWholeUndefs,\n                                      AllowPartialUndefs)) {\n      EVT SrcVT = Op.getOperand(0).getValueType();\n      unsigned NumSrcElts = SrcVT.getVectorNumElements();\n      unsigned NumSubElts = VT.getVectorNumElements();\n      unsigned BaseIdx = Op.getConstantOperandVal(1);\n      UndefElts = UndefElts.extractBits(NumSubElts, BaseIdx);\n      if ((BaseIdx + NumSubElts) != NumSrcElts)\n        EltBits.erase(EltBits.begin() + BaseIdx + NumSubElts, EltBits.end());\n      if (BaseIdx != 0)\n        EltBits.erase(EltBits.begin(), EltBits.begin() + BaseIdx);\n      return true;\n    }\n  }\n\n  // Extract constant bits from shuffle node sources.\n  if (auto *SVN = dyn_cast<ShuffleVectorSDNode>(Op)) {\n    // TODO - support shuffle through bitcasts.\n    if (EltSizeInBits != VT.getScalarSizeInBits())\n      return false;\n\n    ArrayRef<int> Mask = SVN->getMask();\n    if ((!AllowWholeUndefs || !AllowPartialUndefs) &&\n        llvm::any_of(Mask, [](int M) { return M < 0; }))\n      return false;\n\n    APInt UndefElts0, UndefElts1;\n    SmallVector<APInt, 32> EltBits0, EltBits1;\n    if (isAnyInRange(Mask, 0, NumElts) &&\n        !getTargetConstantBitsFromNode(Op.getOperand(0), EltSizeInBits,\n                                       UndefElts0, EltBits0, AllowWholeUndefs,\n                                       AllowPartialUndefs))\n      return false;\n    if (isAnyInRange(Mask, NumElts, 2 * NumElts) &&\n        !getTargetConstantBitsFromNode(Op.getOperand(1), EltSizeInBits,\n                                       UndefElts1, EltBits1, AllowWholeUndefs,\n                                       AllowPartialUndefs))\n      return false;\n\n    UndefElts = APInt::getNullValue(NumElts);\n    for (int i = 0; i != (int)NumElts; ++i) {\n      int M = Mask[i];\n      if (M < 0) {\n        UndefElts.setBit(i);\n        EltBits.push_back(APInt::getNullValue(EltSizeInBits));\n      } else if (M < (int)NumElts) {\n        if (UndefElts0[M])\n          UndefElts.setBit(i);\n        EltBits.push_back(EltBits0[M]);\n      } else {\n        if (UndefElts1[M - NumElts])\n          UndefElts.setBit(i);\n        EltBits.push_back(EltBits1[M - NumElts]);\n      }\n    }\n    return true;\n  }\n\n  return false;\n}\n\nnamespace llvm {\nnamespace X86 {\nbool isConstantSplat(SDValue Op, APInt &SplatVal, bool AllowPartialUndefs) {\n  APInt UndefElts;\n  SmallVector<APInt, 16> EltBits;\n  if (getTargetConstantBitsFromNode(Op, Op.getScalarValueSizeInBits(),\n                                    UndefElts, EltBits, true,\n                                    AllowPartialUndefs)) {\n    int SplatIndex = -1;\n    for (int i = 0, e = EltBits.size(); i != e; ++i) {\n      if (UndefElts[i])\n        continue;\n      if (0 <= SplatIndex && EltBits[i] != EltBits[SplatIndex]) {\n        SplatIndex = -1;\n        break;\n      }\n      SplatIndex = i;\n    }\n    if (0 <= SplatIndex) {\n      SplatVal = EltBits[SplatIndex];\n      return true;\n    }\n  }\n\n  return false;\n}\n} // namespace X86\n} // namespace llvm\n\nstatic bool getTargetShuffleMaskIndices(SDValue MaskNode,\n                                        unsigned MaskEltSizeInBits,\n                                        SmallVectorImpl<uint64_t> &RawMask,\n                                        APInt &UndefElts) {\n  // Extract the raw target constant bits.\n  SmallVector<APInt, 64> EltBits;\n  if (!getTargetConstantBitsFromNode(MaskNode, MaskEltSizeInBits, UndefElts,\n                                     EltBits, /* AllowWholeUndefs */ true,\n                                     /* AllowPartialUndefs */ false))\n    return false;\n\n  // Insert the extracted elements into the mask.\n  for (const APInt &Elt : EltBits)\n    RawMask.push_back(Elt.getZExtValue());\n\n  return true;\n}\n\n/// Create a shuffle mask that matches the PACKSS/PACKUS truncation.\n/// A multi-stage pack shuffle mask is created by specifying NumStages > 1.\n/// Note: This ignores saturation, so inputs must be checked first.\nstatic void createPackShuffleMask(MVT VT, SmallVectorImpl<int> &Mask,\n                                  bool Unary, unsigned NumStages = 1) {\n  assert(Mask.empty() && \"Expected an empty shuffle mask vector\");\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned NumLanes = VT.getSizeInBits() / 128;\n  unsigned NumEltsPerLane = 128 / VT.getScalarSizeInBits();\n  unsigned Offset = Unary ? 0 : NumElts;\n  unsigned Repetitions = 1u << (NumStages - 1);\n  unsigned Increment = 1u << NumStages;\n  assert((NumEltsPerLane >> NumStages) > 0 && \"Illegal packing compaction\");\n\n  for (unsigned Lane = 0; Lane != NumLanes; ++Lane) {\n    for (unsigned Stage = 0; Stage != Repetitions; ++Stage) {\n      for (unsigned Elt = 0; Elt != NumEltsPerLane; Elt += Increment)\n        Mask.push_back(Elt + (Lane * NumEltsPerLane));\n      for (unsigned Elt = 0; Elt != NumEltsPerLane; Elt += Increment)\n        Mask.push_back(Elt + (Lane * NumEltsPerLane) + Offset);\n    }\n  }\n}\n\n// Split the demanded elts of a PACKSS/PACKUS node between its operands.\nstatic void getPackDemandedElts(EVT VT, const APInt &DemandedElts,\n                                APInt &DemandedLHS, APInt &DemandedRHS) {\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumElts = DemandedElts.getBitWidth();\n  int NumInnerElts = NumElts / 2;\n  int NumEltsPerLane = NumElts / NumLanes;\n  int NumInnerEltsPerLane = NumInnerElts / NumLanes;\n\n  DemandedLHS = APInt::getNullValue(NumInnerElts);\n  DemandedRHS = APInt::getNullValue(NumInnerElts);\n\n  // Map DemandedElts to the packed operands.\n  for (int Lane = 0; Lane != NumLanes; ++Lane) {\n    for (int Elt = 0; Elt != NumInnerEltsPerLane; ++Elt) {\n      int OuterIdx = (Lane * NumEltsPerLane) + Elt;\n      int InnerIdx = (Lane * NumInnerEltsPerLane) + Elt;\n      if (DemandedElts[OuterIdx])\n        DemandedLHS.setBit(InnerIdx);\n      if (DemandedElts[OuterIdx + NumInnerEltsPerLane])\n        DemandedRHS.setBit(InnerIdx);\n    }\n  }\n}\n\n// Split the demanded elts of a HADD/HSUB node between its operands.\nstatic void getHorizDemandedElts(EVT VT, const APInt &DemandedElts,\n                                 APInt &DemandedLHS, APInt &DemandedRHS) {\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumElts = DemandedElts.getBitWidth();\n  int NumEltsPerLane = NumElts / NumLanes;\n  int HalfEltsPerLane = NumEltsPerLane / 2;\n\n  DemandedLHS = APInt::getNullValue(NumElts);\n  DemandedRHS = APInt::getNullValue(NumElts);\n\n  // Map DemandedElts to the horizontal operands.\n  for (int Idx = 0; Idx != NumElts; ++Idx) {\n    if (!DemandedElts[Idx])\n      continue;\n    int LaneIdx = (Idx / NumEltsPerLane) * NumEltsPerLane;\n    int LocalIdx = Idx % NumEltsPerLane;\n    if (LocalIdx < HalfEltsPerLane) {\n      DemandedLHS.setBit(LaneIdx + 2 * LocalIdx + 0);\n      DemandedLHS.setBit(LaneIdx + 2 * LocalIdx + 1);\n    } else {\n      LocalIdx -= HalfEltsPerLane;\n      DemandedRHS.setBit(LaneIdx + 2 * LocalIdx + 0);\n      DemandedRHS.setBit(LaneIdx + 2 * LocalIdx + 1);\n    }\n  }\n}\n\n/// Calculates the shuffle mask corresponding to the target-specific opcode.\n/// If the mask could be calculated, returns it in \\p Mask, returns the shuffle\n/// operands in \\p Ops, and returns true.\n/// Sets \\p IsUnary to true if only one source is used. Note that this will set\n/// IsUnary for shuffles which use a single input multiple times, and in those\n/// cases it will adjust the mask to only have indices within that single input.\n/// It is an error to call this with non-empty Mask/Ops vectors.\nstatic bool getTargetShuffleMask(SDNode *N, MVT VT, bool AllowSentinelZero,\n                                 SmallVectorImpl<SDValue> &Ops,\n                                 SmallVectorImpl<int> &Mask, bool &IsUnary) {\n  unsigned NumElems = VT.getVectorNumElements();\n  unsigned MaskEltSize = VT.getScalarSizeInBits();\n  SmallVector<uint64_t, 32> RawMask;\n  APInt RawUndefs;\n  uint64_t ImmN;\n\n  assert(Mask.empty() && \"getTargetShuffleMask expects an empty Mask vector\");\n  assert(Ops.empty() && \"getTargetShuffleMask expects an empty Ops vector\");\n\n  IsUnary = false;\n  bool IsFakeUnary = false;\n  switch (N->getOpcode()) {\n  case X86ISD::BLENDI:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeBLENDMask(NumElems, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::SHUFP:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeSHUFPMask(NumElems, MaskEltSize, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::INSERTPS:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeINSERTPSMask(ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::EXTRQI:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    if (isa<ConstantSDNode>(N->getOperand(1)) &&\n        isa<ConstantSDNode>(N->getOperand(2))) {\n      int BitLen = N->getConstantOperandVal(1);\n      int BitIdx = N->getConstantOperandVal(2);\n      DecodeEXTRQIMask(NumElems, MaskEltSize, BitLen, BitIdx, Mask);\n      IsUnary = true;\n    }\n    break;\n  case X86ISD::INSERTQI:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    if (isa<ConstantSDNode>(N->getOperand(2)) &&\n        isa<ConstantSDNode>(N->getOperand(3))) {\n      int BitLen = N->getConstantOperandVal(2);\n      int BitIdx = N->getConstantOperandVal(3);\n      DecodeINSERTQIMask(NumElems, MaskEltSize, BitLen, BitIdx, Mask);\n      IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    }\n    break;\n  case X86ISD::UNPCKH:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    DecodeUNPCKHMask(NumElems, MaskEltSize, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::UNPCKL:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    DecodeUNPCKLMask(NumElems, MaskEltSize, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::MOVHLPS:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    DecodeMOVHLPSMask(NumElems, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::MOVLHPS:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    DecodeMOVLHPSMask(NumElems, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::VALIGN:\n    assert((VT.getScalarType() == MVT::i32 || VT.getScalarType() == MVT::i64) &&\n           \"Only 32-bit and 64-bit elements are supported!\");\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeVALIGNMask(NumElems, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    Ops.push_back(N->getOperand(1));\n    Ops.push_back(N->getOperand(0));\n    break;\n  case X86ISD::PALIGNR:\n    assert(VT.getScalarType() == MVT::i8 && \"Byte vector expected\");\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePALIGNRMask(NumElems, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    Ops.push_back(N->getOperand(1));\n    Ops.push_back(N->getOperand(0));\n    break;\n  case X86ISD::VSHLDQ:\n    assert(VT.getScalarType() == MVT::i8 && \"Byte vector expected\");\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePSLLDQMask(NumElems, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::VSRLDQ:\n    assert(VT.getScalarType() == MVT::i8 && \"Byte vector expected\");\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePSRLDQMask(NumElems, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::PSHUFD:\n  case X86ISD::VPERMILPI:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePSHUFMask(NumElems, MaskEltSize, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::PSHUFHW:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePSHUFHWMask(NumElems, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::PSHUFLW:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodePSHUFLWMask(NumElems, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::VZEXT_MOVL:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    DecodeZeroMoveLowMask(NumElems, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::VBROADCAST:\n    // We only decode broadcasts of same-sized vectors, peeking through to\n    // extracted subvectors is likely to cause hasOneUse issues with\n    // SimplifyDemandedBits etc.\n    if (N->getOperand(0).getValueType() == VT) {\n      DecodeVectorBroadcast(NumElems, Mask);\n      IsUnary = true;\n      break;\n    }\n    return false;\n  case X86ISD::VPERMILPV: {\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = true;\n    SDValue MaskNode = N->getOperand(1);\n    if (getTargetShuffleMaskIndices(MaskNode, MaskEltSize, RawMask,\n                                    RawUndefs)) {\n      DecodeVPERMILPMask(NumElems, MaskEltSize, RawMask, RawUndefs, Mask);\n      break;\n    }\n    return false;\n  }\n  case X86ISD::PSHUFB: {\n    assert(VT.getScalarType() == MVT::i8 && \"Byte vector expected\");\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = true;\n    SDValue MaskNode = N->getOperand(1);\n    if (getTargetShuffleMaskIndices(MaskNode, 8, RawMask, RawUndefs)) {\n      DecodePSHUFBMask(RawMask, RawUndefs, Mask);\n      break;\n    }\n    return false;\n  }\n  case X86ISD::VPERMI:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeVPERMMask(NumElems, ImmN, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::MOVSS:\n  case X86ISD::MOVSD:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    DecodeScalarMoveMask(NumElems, /* IsLoad */ false, Mask);\n    break;\n  case X86ISD::VPERM2X128:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    DecodeVPERM2X128Mask(NumElems, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::SHUF128:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    ImmN = N->getConstantOperandVal(N->getNumOperands() - 1);\n    decodeVSHUF64x2FamilyMask(NumElems, MaskEltSize, ImmN, Mask);\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    break;\n  case X86ISD::MOVSLDUP:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    DecodeMOVSLDUPMask(NumElems, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::MOVSHDUP:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    DecodeMOVSHDUPMask(NumElems, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::MOVDDUP:\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    DecodeMOVDDUPMask(NumElems, Mask);\n    IsUnary = true;\n    break;\n  case X86ISD::VPERMIL2: {\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    SDValue MaskNode = N->getOperand(2);\n    SDValue CtrlNode = N->getOperand(3);\n    if (ConstantSDNode *CtrlOp = dyn_cast<ConstantSDNode>(CtrlNode)) {\n      unsigned CtrlImm = CtrlOp->getZExtValue();\n      if (getTargetShuffleMaskIndices(MaskNode, MaskEltSize, RawMask,\n                                      RawUndefs)) {\n        DecodeVPERMIL2PMask(NumElems, MaskEltSize, CtrlImm, RawMask, RawUndefs,\n                            Mask);\n        break;\n      }\n    }\n    return false;\n  }\n  case X86ISD::VPPERM: {\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(1);\n    SDValue MaskNode = N->getOperand(2);\n    if (getTargetShuffleMaskIndices(MaskNode, 8, RawMask, RawUndefs)) {\n      DecodeVPPERMMask(RawMask, RawUndefs, Mask);\n      break;\n    }\n    return false;\n  }\n  case X86ISD::VPERMV: {\n    assert(N->getOperand(1).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = true;\n    // Unlike most shuffle nodes, VPERMV's mask operand is operand 0.\n    Ops.push_back(N->getOperand(1));\n    SDValue MaskNode = N->getOperand(0);\n    if (getTargetShuffleMaskIndices(MaskNode, MaskEltSize, RawMask,\n                                    RawUndefs)) {\n      DecodeVPERMVMask(RawMask, RawUndefs, Mask);\n      break;\n    }\n    return false;\n  }\n  case X86ISD::VPERMV3: {\n    assert(N->getOperand(0).getValueType() == VT && \"Unexpected value type\");\n    assert(N->getOperand(2).getValueType() == VT && \"Unexpected value type\");\n    IsUnary = IsFakeUnary = N->getOperand(0) == N->getOperand(2);\n    // Unlike most shuffle nodes, VPERMV3's mask operand is the middle one.\n    Ops.push_back(N->getOperand(0));\n    Ops.push_back(N->getOperand(2));\n    SDValue MaskNode = N->getOperand(1);\n    if (getTargetShuffleMaskIndices(MaskNode, MaskEltSize, RawMask,\n                                    RawUndefs)) {\n      DecodeVPERMV3Mask(RawMask, RawUndefs, Mask);\n      break;\n    }\n    return false;\n  }\n  default: llvm_unreachable(\"unknown target shuffle node\");\n  }\n\n  // Empty mask indicates the decode failed.\n  if (Mask.empty())\n    return false;\n\n  // Check if we're getting a shuffle mask with zero'd elements.\n  if (!AllowSentinelZero && isAnyZero(Mask))\n    return false;\n\n  // If we have a fake unary shuffle, the shuffle mask is spread across two\n  // inputs that are actually the same node. Re-map the mask to always point\n  // into the first input.\n  if (IsFakeUnary)\n    for (int &M : Mask)\n      if (M >= (int)Mask.size())\n        M -= Mask.size();\n\n  // If we didn't already add operands in the opcode-specific code, default to\n  // adding 1 or 2 operands starting at 0.\n  if (Ops.empty()) {\n    Ops.push_back(N->getOperand(0));\n    if (!IsUnary || IsFakeUnary)\n      Ops.push_back(N->getOperand(1));\n  }\n\n  return true;\n}\n\n/// Compute whether each element of a shuffle is zeroable.\n///\n/// A \"zeroable\" vector shuffle element is one which can be lowered to zero.\n/// Either it is an undef element in the shuffle mask, the element of the input\n/// referenced is undef, or the element of the input referenced is known to be\n/// zero. Many x86 shuffles can zero lanes cheaply and we often want to handle\n/// as many lanes with this technique as possible to simplify the remaining\n/// shuffle.\nstatic void computeZeroableShuffleElements(ArrayRef<int> Mask,\n                                           SDValue V1, SDValue V2,\n                                           APInt &KnownUndef, APInt &KnownZero) {\n  int Size = Mask.size();\n  KnownUndef = KnownZero = APInt::getNullValue(Size);\n\n  V1 = peekThroughBitcasts(V1);\n  V2 = peekThroughBitcasts(V2);\n\n  bool V1IsZero = ISD::isBuildVectorAllZeros(V1.getNode());\n  bool V2IsZero = ISD::isBuildVectorAllZeros(V2.getNode());\n\n  int VectorSizeInBits = V1.getValueSizeInBits();\n  int ScalarSizeInBits = VectorSizeInBits / Size;\n  assert(!(VectorSizeInBits % ScalarSizeInBits) && \"Illegal shuffle mask size\");\n\n  for (int i = 0; i < Size; ++i) {\n    int M = Mask[i];\n    // Handle the easy cases.\n    if (M < 0) {\n      KnownUndef.setBit(i);\n      continue;\n    }\n    if ((M >= 0 && M < Size && V1IsZero) || (M >= Size && V2IsZero)) {\n      KnownZero.setBit(i);\n      continue;\n    }\n\n    // Determine shuffle input and normalize the mask.\n    SDValue V = M < Size ? V1 : V2;\n    M %= Size;\n\n    // Currently we can only search BUILD_VECTOR for UNDEF/ZERO elements.\n    if (V.getOpcode() != ISD::BUILD_VECTOR)\n      continue;\n\n    // If the BUILD_VECTOR has fewer elements then the bitcasted portion of\n    // the (larger) source element must be UNDEF/ZERO.\n    if ((Size % V.getNumOperands()) == 0) {\n      int Scale = Size / V->getNumOperands();\n      SDValue Op = V.getOperand(M / Scale);\n      if (Op.isUndef())\n        KnownUndef.setBit(i);\n      if (X86::isZeroNode(Op))\n        KnownZero.setBit(i);\n      else if (ConstantSDNode *Cst = dyn_cast<ConstantSDNode>(Op)) {\n        APInt Val = Cst->getAPIntValue();\n        Val = Val.extractBits(ScalarSizeInBits, (M % Scale) * ScalarSizeInBits);\n        if (Val == 0)\n          KnownZero.setBit(i);\n      } else if (ConstantFPSDNode *Cst = dyn_cast<ConstantFPSDNode>(Op)) {\n        APInt Val = Cst->getValueAPF().bitcastToAPInt();\n        Val = Val.extractBits(ScalarSizeInBits, (M % Scale) * ScalarSizeInBits);\n        if (Val == 0)\n          KnownZero.setBit(i);\n      }\n      continue;\n    }\n\n    // If the BUILD_VECTOR has more elements then all the (smaller) source\n    // elements must be UNDEF or ZERO.\n    if ((V.getNumOperands() % Size) == 0) {\n      int Scale = V->getNumOperands() / Size;\n      bool AllUndef = true;\n      bool AllZero = true;\n      for (int j = 0; j < Scale; ++j) {\n        SDValue Op = V.getOperand((M * Scale) + j);\n        AllUndef &= Op.isUndef();\n        AllZero &= X86::isZeroNode(Op);\n      }\n      if (AllUndef)\n        KnownUndef.setBit(i);\n      if (AllZero)\n        KnownZero.setBit(i);\n      continue;\n    }\n  }\n}\n\n/// Decode a target shuffle mask and inputs and see if any values are\n/// known to be undef or zero from their inputs.\n/// Returns true if the target shuffle mask was decoded.\n/// FIXME: Merge this with computeZeroableShuffleElements?\nstatic bool getTargetShuffleAndZeroables(SDValue N, SmallVectorImpl<int> &Mask,\n                                         SmallVectorImpl<SDValue> &Ops,\n                                         APInt &KnownUndef, APInt &KnownZero) {\n  bool IsUnary;\n  if (!isTargetShuffle(N.getOpcode()))\n    return false;\n\n  MVT VT = N.getSimpleValueType();\n  if (!getTargetShuffleMask(N.getNode(), VT, true, Ops, Mask, IsUnary))\n    return false;\n\n  int Size = Mask.size();\n  SDValue V1 = Ops[0];\n  SDValue V2 = IsUnary ? V1 : Ops[1];\n  KnownUndef = KnownZero = APInt::getNullValue(Size);\n\n  V1 = peekThroughBitcasts(V1);\n  V2 = peekThroughBitcasts(V2);\n\n  assert((VT.getSizeInBits() % Size) == 0 &&\n         \"Illegal split of shuffle value type\");\n  unsigned EltSizeInBits = VT.getSizeInBits() / Size;\n\n  // Extract known constant input data.\n  APInt UndefSrcElts[2];\n  SmallVector<APInt, 32> SrcEltBits[2];\n  bool IsSrcConstant[2] = {\n      getTargetConstantBitsFromNode(V1, EltSizeInBits, UndefSrcElts[0],\n                                    SrcEltBits[0], true, false),\n      getTargetConstantBitsFromNode(V2, EltSizeInBits, UndefSrcElts[1],\n                                    SrcEltBits[1], true, false)};\n\n  for (int i = 0; i < Size; ++i) {\n    int M = Mask[i];\n\n    // Already decoded as SM_SentinelZero / SM_SentinelUndef.\n    if (M < 0) {\n      assert(isUndefOrZero(M) && \"Unknown shuffle sentinel value!\");\n      if (SM_SentinelUndef == M)\n        KnownUndef.setBit(i);\n      if (SM_SentinelZero == M)\n        KnownZero.setBit(i);\n      continue;\n    }\n\n    // Determine shuffle input and normalize the mask.\n    unsigned SrcIdx = M / Size;\n    SDValue V = M < Size ? V1 : V2;\n    M %= Size;\n\n    // We are referencing an UNDEF input.\n    if (V.isUndef()) {\n      KnownUndef.setBit(i);\n      continue;\n    }\n\n    // SCALAR_TO_VECTOR - only the first element is defined, and the rest UNDEF.\n    // TODO: We currently only set UNDEF for integer types - floats use the same\n    // registers as vectors and many of the scalar folded loads rely on the\n    // SCALAR_TO_VECTOR pattern.\n    if (V.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n        (Size % V.getValueType().getVectorNumElements()) == 0) {\n      int Scale = Size / V.getValueType().getVectorNumElements();\n      int Idx = M / Scale;\n      if (Idx != 0 && !VT.isFloatingPoint())\n        KnownUndef.setBit(i);\n      else if (Idx == 0 && X86::isZeroNode(V.getOperand(0)))\n        KnownZero.setBit(i);\n      continue;\n    }\n\n    // INSERT_SUBVECTOR - to widen vectors we often insert them into UNDEF\n    // base vectors.\n    if (V.getOpcode() == ISD::INSERT_SUBVECTOR) {\n      SDValue Vec = V.getOperand(0);\n      int NumVecElts = Vec.getValueType().getVectorNumElements();\n      if (Vec.isUndef() && Size == NumVecElts) {\n        int Idx = V.getConstantOperandVal(2);\n        int NumSubElts = V.getOperand(1).getValueType().getVectorNumElements();\n        if (M < Idx || (Idx + NumSubElts) <= M)\n          KnownUndef.setBit(i);\n      }\n      continue;\n    }\n\n    // Attempt to extract from the source's constant bits.\n    if (IsSrcConstant[SrcIdx]) {\n      if (UndefSrcElts[SrcIdx][M])\n        KnownUndef.setBit(i);\n      else if (SrcEltBits[SrcIdx][M] == 0)\n        KnownZero.setBit(i);\n    }\n  }\n\n  assert(VT.getVectorNumElements() == (unsigned)Size &&\n         \"Different mask size from vector size!\");\n  return true;\n}\n\n// Replace target shuffle mask elements with known undef/zero sentinels.\nstatic void resolveTargetShuffleFromZeroables(SmallVectorImpl<int> &Mask,\n                                              const APInt &KnownUndef,\n                                              const APInt &KnownZero,\n                                              bool ResolveKnownZeros= true) {\n  unsigned NumElts = Mask.size();\n  assert(KnownUndef.getBitWidth() == NumElts &&\n         KnownZero.getBitWidth() == NumElts && \"Shuffle mask size mismatch\");\n\n  for (unsigned i = 0; i != NumElts; ++i) {\n    if (KnownUndef[i])\n      Mask[i] = SM_SentinelUndef;\n    else if (ResolveKnownZeros && KnownZero[i])\n      Mask[i] = SM_SentinelZero;\n  }\n}\n\n// Extract target shuffle mask sentinel elements to known undef/zero bitmasks.\nstatic void resolveZeroablesFromTargetShuffle(const SmallVectorImpl<int> &Mask,\n                                              APInt &KnownUndef,\n                                              APInt &KnownZero) {\n  unsigned NumElts = Mask.size();\n  KnownUndef = KnownZero = APInt::getNullValue(NumElts);\n\n  for (unsigned i = 0; i != NumElts; ++i) {\n    int M = Mask[i];\n    if (SM_SentinelUndef == M)\n      KnownUndef.setBit(i);\n    if (SM_SentinelZero == M)\n      KnownZero.setBit(i);\n  }\n}\n\n// Forward declaration (for getFauxShuffleMask recursive check).\n// TODO: Use DemandedElts variant.\nstatic bool getTargetShuffleInputs(SDValue Op, SmallVectorImpl<SDValue> &Inputs,\n                                   SmallVectorImpl<int> &Mask,\n                                   const SelectionDAG &DAG, unsigned Depth,\n                                   bool ResolveKnownElts);\n\n// Attempt to decode ops that could be represented as a shuffle mask.\n// The decoded shuffle mask may contain a different number of elements to the\n// destination value type.\nstatic bool getFauxShuffleMask(SDValue N, const APInt &DemandedElts,\n                               SmallVectorImpl<int> &Mask,\n                               SmallVectorImpl<SDValue> &Ops,\n                               const SelectionDAG &DAG, unsigned Depth,\n                               bool ResolveKnownElts) {\n  Mask.clear();\n  Ops.clear();\n\n  MVT VT = N.getSimpleValueType();\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned NumSizeInBits = VT.getSizeInBits();\n  unsigned NumBitsPerElt = VT.getScalarSizeInBits();\n  if ((NumBitsPerElt % 8) != 0 || (NumSizeInBits % 8) != 0)\n    return false;\n  assert(NumElts == DemandedElts.getBitWidth() && \"Unexpected vector size\");\n  unsigned NumSizeInBytes = NumSizeInBits / 8;\n  unsigned NumBytesPerElt = NumBitsPerElt / 8;\n\n  unsigned Opcode = N.getOpcode();\n  switch (Opcode) {\n  case ISD::VECTOR_SHUFFLE: {\n    // Don't treat ISD::VECTOR_SHUFFLE as a target shuffle so decode it here.\n    ArrayRef<int> ShuffleMask = cast<ShuffleVectorSDNode>(N)->getMask();\n    if (isUndefOrInRange(ShuffleMask, 0, 2 * NumElts)) {\n      Mask.append(ShuffleMask.begin(), ShuffleMask.end());\n      Ops.push_back(N.getOperand(0));\n      Ops.push_back(N.getOperand(1));\n      return true;\n    }\n    return false;\n  }\n  case ISD::AND:\n  case X86ISD::ANDNP: {\n    // Attempt to decode as a per-byte mask.\n    APInt UndefElts;\n    SmallVector<APInt, 32> EltBits;\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n    bool IsAndN = (X86ISD::ANDNP == Opcode);\n    uint64_t ZeroMask = IsAndN ? 255 : 0;\n    if (!getTargetConstantBitsFromNode(IsAndN ? N0 : N1, 8, UndefElts, EltBits))\n      return false;\n    for (int i = 0, e = (int)EltBits.size(); i != e; ++i) {\n      if (UndefElts[i]) {\n        Mask.push_back(SM_SentinelUndef);\n        continue;\n      }\n      const APInt &ByteBits = EltBits[i];\n      if (ByteBits != 0 && ByteBits != 255)\n        return false;\n      Mask.push_back(ByteBits == ZeroMask ? SM_SentinelZero : i);\n    }\n    Ops.push_back(IsAndN ? N1 : N0);\n    return true;\n  }\n  case ISD::OR: {\n    // Handle OR(SHUFFLE,SHUFFLE) case where one source is zero and the other\n    // is a valid shuffle index.\n    SDValue N0 = peekThroughBitcasts(N.getOperand(0));\n    SDValue N1 = peekThroughBitcasts(N.getOperand(1));\n    if (!N0.getValueType().isVector() || !N1.getValueType().isVector())\n      return false;\n    SmallVector<int, 64> SrcMask0, SrcMask1;\n    SmallVector<SDValue, 2> SrcInputs0, SrcInputs1;\n    if (!getTargetShuffleInputs(N0, SrcInputs0, SrcMask0, DAG, Depth + 1,\n                                true) ||\n        !getTargetShuffleInputs(N1, SrcInputs1, SrcMask1, DAG, Depth + 1,\n                                true))\n      return false;\n\n    size_t MaskSize = std::max(SrcMask0.size(), SrcMask1.size());\n    SmallVector<int, 64> Mask0, Mask1;\n    narrowShuffleMaskElts(MaskSize / SrcMask0.size(), SrcMask0, Mask0);\n    narrowShuffleMaskElts(MaskSize / SrcMask1.size(), SrcMask1, Mask1);\n    for (int i = 0; i != (int)MaskSize; ++i) {\n      if (Mask0[i] == SM_SentinelUndef && Mask1[i] == SM_SentinelUndef)\n        Mask.push_back(SM_SentinelUndef);\n      else if (Mask0[i] == SM_SentinelZero && Mask1[i] == SM_SentinelZero)\n        Mask.push_back(SM_SentinelZero);\n      else if (Mask1[i] == SM_SentinelZero)\n        Mask.push_back(i);\n      else if (Mask0[i] == SM_SentinelZero)\n        Mask.push_back(i + MaskSize);\n      else\n        return false;\n    }\n    Ops.push_back(N0);\n    Ops.push_back(N1);\n    return true;\n  }\n  case ISD::INSERT_SUBVECTOR: {\n    SDValue Src = N.getOperand(0);\n    SDValue Sub = N.getOperand(1);\n    EVT SubVT = Sub.getValueType();\n    unsigned NumSubElts = SubVT.getVectorNumElements();\n    if (!N->isOnlyUserOf(Sub.getNode()))\n      return false;\n    uint64_t InsertIdx = N.getConstantOperandVal(2);\n    // Handle INSERT_SUBVECTOR(SRC0, EXTRACT_SUBVECTOR(SRC1)).\n    if (Sub.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n        Sub.getOperand(0).getValueType() == VT) {\n      uint64_t ExtractIdx = Sub.getConstantOperandVal(1);\n      for (int i = 0; i != (int)NumElts; ++i)\n        Mask.push_back(i);\n      for (int i = 0; i != (int)NumSubElts; ++i)\n        Mask[InsertIdx + i] = NumElts + ExtractIdx + i;\n      Ops.push_back(Src);\n      Ops.push_back(Sub.getOperand(0));\n      return true;\n    }\n    // Handle INSERT_SUBVECTOR(SRC0, SHUFFLE(SRC1)).\n    SmallVector<int, 64> SubMask;\n    SmallVector<SDValue, 2> SubInputs;\n    if (!getTargetShuffleInputs(peekThroughOneUseBitcasts(Sub), SubInputs,\n                                SubMask, DAG, Depth + 1, ResolveKnownElts))\n      return false;\n\n    // Subvector shuffle inputs must not be larger than the subvector.\n    if (llvm::any_of(SubInputs, [SubVT](SDValue SubInput) {\n          return SubVT.getFixedSizeInBits() <\n                 SubInput.getValueSizeInBits().getFixedSize();\n        }))\n      return false;\n\n    if (SubMask.size() != NumSubElts) {\n      assert(((SubMask.size() % NumSubElts) == 0 ||\n              (NumSubElts % SubMask.size()) == 0) && \"Illegal submask scale\");\n      if ((NumSubElts % SubMask.size()) == 0) {\n        int Scale = NumSubElts / SubMask.size();\n        SmallVector<int,64> ScaledSubMask;\n        narrowShuffleMaskElts(Scale, SubMask, ScaledSubMask);\n        SubMask = ScaledSubMask;\n      } else {\n        int Scale = SubMask.size() / NumSubElts;\n        NumSubElts = SubMask.size();\n        NumElts *= Scale;\n        InsertIdx *= Scale;\n      }\n    }\n    Ops.push_back(Src);\n    Ops.append(SubInputs.begin(), SubInputs.end());\n    if (ISD::isBuildVectorAllZeros(Src.getNode()))\n      Mask.append(NumElts, SM_SentinelZero);\n    else\n      for (int i = 0; i != (int)NumElts; ++i)\n        Mask.push_back(i);\n    for (int i = 0; i != (int)NumSubElts; ++i) {\n      int M = SubMask[i];\n      if (0 <= M) {\n        int InputIdx = M / NumSubElts;\n        M = (NumElts * (1 + InputIdx)) + (M % NumSubElts);\n      }\n      Mask[i + InsertIdx] = M;\n    }\n    return true;\n  }\n  case X86ISD::PINSRB:\n  case X86ISD::PINSRW:\n  case ISD::SCALAR_TO_VECTOR:\n  case ISD::INSERT_VECTOR_ELT: {\n    // Match against a insert_vector_elt/scalar_to_vector of an extract from a\n    // vector, for matching src/dst vector types.\n    SDValue Scl = N.getOperand(Opcode == ISD::SCALAR_TO_VECTOR ? 0 : 1);\n\n    unsigned DstIdx = 0;\n    if (Opcode != ISD::SCALAR_TO_VECTOR) {\n      // Check we have an in-range constant insertion index.\n      if (!isa<ConstantSDNode>(N.getOperand(2)) ||\n          N.getConstantOperandAPInt(2).uge(NumElts))\n        return false;\n      DstIdx = N.getConstantOperandVal(2);\n\n      // Attempt to recognise an INSERT*(VEC, 0, DstIdx) shuffle pattern.\n      if (X86::isZeroNode(Scl)) {\n        Ops.push_back(N.getOperand(0));\n        for (unsigned i = 0; i != NumElts; ++i)\n          Mask.push_back(i == DstIdx ? SM_SentinelZero : (int)i);\n        return true;\n      }\n    }\n\n    // Peek through trunc/aext/zext.\n    // TODO: aext shouldn't require SM_SentinelZero padding.\n    // TODO: handle shift of scalars.\n    unsigned MinBitsPerElt = Scl.getScalarValueSizeInBits();\n    while (Scl.getOpcode() == ISD::TRUNCATE ||\n           Scl.getOpcode() == ISD::ANY_EXTEND ||\n           Scl.getOpcode() == ISD::ZERO_EXTEND) {\n      Scl = Scl.getOperand(0);\n      MinBitsPerElt =\n          std::min<unsigned>(MinBitsPerElt, Scl.getScalarValueSizeInBits());\n    }\n    if ((MinBitsPerElt % 8) != 0)\n      return false;\n\n    // Attempt to find the source vector the scalar was extracted from.\n    SDValue SrcExtract;\n    if ((Scl.getOpcode() == ISD::EXTRACT_VECTOR_ELT ||\n         Scl.getOpcode() == X86ISD::PEXTRW ||\n         Scl.getOpcode() == X86ISD::PEXTRB) &&\n        Scl.getOperand(0).getValueSizeInBits() == NumSizeInBits) {\n      SrcExtract = Scl;\n    }\n    if (!SrcExtract || !isa<ConstantSDNode>(SrcExtract.getOperand(1)))\n      return false;\n\n    SDValue SrcVec = SrcExtract.getOperand(0);\n    EVT SrcVT = SrcVec.getValueType();\n    if (!SrcVT.getScalarType().isByteSized())\n      return false;\n    unsigned SrcIdx = SrcExtract.getConstantOperandVal(1);\n    unsigned SrcByte = SrcIdx * (SrcVT.getScalarSizeInBits() / 8);\n    unsigned DstByte = DstIdx * NumBytesPerElt;\n    MinBitsPerElt =\n        std::min<unsigned>(MinBitsPerElt, SrcVT.getScalarSizeInBits());\n\n    // Create 'identity' byte level shuffle mask and then add inserted bytes.\n    if (Opcode == ISD::SCALAR_TO_VECTOR) {\n      Ops.push_back(SrcVec);\n      Mask.append(NumSizeInBytes, SM_SentinelUndef);\n    } else {\n      Ops.push_back(SrcVec);\n      Ops.push_back(N.getOperand(0));\n      for (int i = 0; i != (int)NumSizeInBytes; ++i)\n        Mask.push_back(NumSizeInBytes + i);\n    }\n\n    unsigned MinBytesPerElts = MinBitsPerElt / 8;\n    MinBytesPerElts = std::min(MinBytesPerElts, NumBytesPerElt);\n    for (unsigned i = 0; i != MinBytesPerElts; ++i)\n      Mask[DstByte + i] = SrcByte + i;\n    for (unsigned i = MinBytesPerElts; i < NumBytesPerElt; ++i)\n      Mask[DstByte + i] = SM_SentinelZero;\n    return true;\n  }\n  case X86ISD::PACKSS:\n  case X86ISD::PACKUS: {\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n    assert(N0.getValueType().getVectorNumElements() == (NumElts / 2) &&\n           N1.getValueType().getVectorNumElements() == (NumElts / 2) &&\n           \"Unexpected input value type\");\n\n    APInt EltsLHS, EltsRHS;\n    getPackDemandedElts(VT, DemandedElts, EltsLHS, EltsRHS);\n\n    // If we know input saturation won't happen (or we don't care for particular\n    // lanes), we can treat this as a truncation shuffle.\n    bool Offset0 = false, Offset1 = false;\n    if (Opcode == X86ISD::PACKSS) {\n      if ((!(N0.isUndef() || EltsLHS.isNullValue()) &&\n           DAG.ComputeNumSignBits(N0, EltsLHS, Depth + 1) <= NumBitsPerElt) ||\n          (!(N1.isUndef() || EltsRHS.isNullValue()) &&\n           DAG.ComputeNumSignBits(N1, EltsRHS, Depth + 1) <= NumBitsPerElt))\n        return false;\n      // We can't easily fold ASHR into a shuffle, but if it was feeding a\n      // PACKSS then it was likely being used for sign-extension for a\n      // truncation, so just peek through and adjust the mask accordingly.\n      if (N0.getOpcode() == X86ISD::VSRAI && N->isOnlyUserOf(N0.getNode()) &&\n          N0.getConstantOperandAPInt(1) == NumBitsPerElt) {\n        Offset0 = true;\n        N0 = N0.getOperand(0);\n      }\n      if (N1.getOpcode() == X86ISD::VSRAI && N->isOnlyUserOf(N1.getNode()) &&\n          N1.getConstantOperandAPInt(1) == NumBitsPerElt) {\n        Offset1 = true;\n        N1 = N1.getOperand(0);\n      }\n    } else {\n      APInt ZeroMask = APInt::getHighBitsSet(2 * NumBitsPerElt, NumBitsPerElt);\n      if ((!(N0.isUndef() || EltsLHS.isNullValue()) &&\n           !DAG.MaskedValueIsZero(N0, ZeroMask, EltsLHS, Depth + 1)) ||\n          (!(N1.isUndef() || EltsRHS.isNullValue()) &&\n           !DAG.MaskedValueIsZero(N1, ZeroMask, EltsRHS, Depth + 1)))\n        return false;\n    }\n\n    bool IsUnary = (N0 == N1);\n\n    Ops.push_back(N0);\n    if (!IsUnary)\n      Ops.push_back(N1);\n\n    createPackShuffleMask(VT, Mask, IsUnary);\n\n    if (Offset0 || Offset1) {\n      for (int &M : Mask)\n        if ((Offset0 && isInRange(M, 0, NumElts)) ||\n            (Offset1 && isInRange(M, NumElts, 2 * NumElts)))\n          ++M;\n    }\n    return true;\n  }\n  case X86ISD::VTRUNC: {\n    SDValue Src = N.getOperand(0);\n    EVT SrcVT = Src.getValueType();\n    // Truncated source must be a simple vector.\n    if (!SrcVT.isSimple() || (SrcVT.getSizeInBits() % 128) != 0 ||\n        (SrcVT.getScalarSizeInBits() % 8) != 0)\n      return false;\n    unsigned NumSrcElts = SrcVT.getVectorNumElements();\n    unsigned NumBitsPerSrcElt = SrcVT.getScalarSizeInBits();\n    unsigned Scale = NumBitsPerSrcElt / NumBitsPerElt;\n    assert((NumBitsPerSrcElt % NumBitsPerElt) == 0 && \"Illegal truncation\");\n    for (unsigned i = 0; i != NumSrcElts; ++i)\n      Mask.push_back(i * Scale);\n    Mask.append(NumElts - NumSrcElts, SM_SentinelZero);\n    Ops.push_back(Src);\n    return true;\n  }\n  case X86ISD::VSHLI:\n  case X86ISD::VSRLI: {\n    uint64_t ShiftVal = N.getConstantOperandVal(1);\n    // Out of range bit shifts are guaranteed to be zero.\n    if (NumBitsPerElt <= ShiftVal) {\n      Mask.append(NumElts, SM_SentinelZero);\n      return true;\n    }\n\n    // We can only decode 'whole byte' bit shifts as shuffles.\n    if ((ShiftVal % 8) != 0)\n      break;\n\n    uint64_t ByteShift = ShiftVal / 8;\n    Ops.push_back(N.getOperand(0));\n\n    // Clear mask to all zeros and insert the shifted byte indices.\n    Mask.append(NumSizeInBytes, SM_SentinelZero);\n\n    if (X86ISD::VSHLI == Opcode) {\n      for (unsigned i = 0; i != NumSizeInBytes; i += NumBytesPerElt)\n        for (unsigned j = ByteShift; j != NumBytesPerElt; ++j)\n          Mask[i + j] = i + j - ByteShift;\n    } else {\n      for (unsigned i = 0; i != NumSizeInBytes; i += NumBytesPerElt)\n        for (unsigned j = ByteShift; j != NumBytesPerElt; ++j)\n          Mask[i + j - ByteShift] = i + j;\n    }\n    return true;\n  }\n  case X86ISD::VROTLI:\n  case X86ISD::VROTRI: {\n    // We can only decode 'whole byte' bit rotates as shuffles.\n    uint64_t RotateVal = N.getConstantOperandAPInt(1).urem(NumBitsPerElt);\n    if ((RotateVal % 8) != 0)\n      return false;\n    Ops.push_back(N.getOperand(0));\n    int Offset = RotateVal / 8;\n    Offset = (X86ISD::VROTLI == Opcode ? NumBytesPerElt - Offset : Offset);\n    for (int i = 0; i != (int)NumElts; ++i) {\n      int BaseIdx = i * NumBytesPerElt;\n      for (int j = 0; j != (int)NumBytesPerElt; ++j) {\n        Mask.push_back(BaseIdx + ((Offset + j) % NumBytesPerElt));\n      }\n    }\n    return true;\n  }\n  case X86ISD::VBROADCAST: {\n    SDValue Src = N.getOperand(0);\n    if (!Src.getSimpleValueType().isVector())\n      return false;\n    Ops.push_back(Src);\n    Mask.append(NumElts, 0);\n    return true;\n  }\n  case ISD::ZERO_EXTEND:\n  case ISD::ANY_EXTEND:\n  case ISD::ZERO_EXTEND_VECTOR_INREG:\n  case ISD::ANY_EXTEND_VECTOR_INREG: {\n    SDValue Src = N.getOperand(0);\n    EVT SrcVT = Src.getValueType();\n\n    // Extended source must be a simple vector.\n    if (!SrcVT.isSimple() || (SrcVT.getSizeInBits() % 128) != 0 ||\n        (SrcVT.getScalarSizeInBits() % 8) != 0)\n      return false;\n\n    bool IsAnyExtend =\n        (ISD::ANY_EXTEND == Opcode || ISD::ANY_EXTEND_VECTOR_INREG == Opcode);\n    DecodeZeroExtendMask(SrcVT.getScalarSizeInBits(), NumBitsPerElt, NumElts,\n                         IsAnyExtend, Mask);\n    Ops.push_back(Src);\n    return true;\n  }\n  }\n\n  return false;\n}\n\n/// Removes unused/repeated shuffle source inputs and adjusts the shuffle mask.\nstatic void resolveTargetShuffleInputsAndMask(SmallVectorImpl<SDValue> &Inputs,\n                                              SmallVectorImpl<int> &Mask) {\n  int MaskWidth = Mask.size();\n  SmallVector<SDValue, 16> UsedInputs;\n  for (int i = 0, e = Inputs.size(); i < e; ++i) {\n    int lo = UsedInputs.size() * MaskWidth;\n    int hi = lo + MaskWidth;\n\n    // Strip UNDEF input usage.\n    if (Inputs[i].isUndef())\n      for (int &M : Mask)\n        if ((lo <= M) && (M < hi))\n          M = SM_SentinelUndef;\n\n    // Check for unused inputs.\n    if (none_of(Mask, [lo, hi](int i) { return (lo <= i) && (i < hi); })) {\n      for (int &M : Mask)\n        if (lo <= M)\n          M -= MaskWidth;\n      continue;\n    }\n\n    // Check for repeated inputs.\n    bool IsRepeat = false;\n    for (int j = 0, ue = UsedInputs.size(); j != ue; ++j) {\n      if (UsedInputs[j] != Inputs[i])\n        continue;\n      for (int &M : Mask)\n        if (lo <= M)\n          M = (M < hi) ? ((M - lo) + (j * MaskWidth)) : (M - MaskWidth);\n      IsRepeat = true;\n      break;\n    }\n    if (IsRepeat)\n      continue;\n\n    UsedInputs.push_back(Inputs[i]);\n  }\n  Inputs = UsedInputs;\n}\n\n/// Calls getTargetShuffleAndZeroables to resolve a target shuffle mask's inputs\n/// and then sets the SM_SentinelUndef and SM_SentinelZero values.\n/// Returns true if the target shuffle mask was decoded.\nstatic bool getTargetShuffleInputs(SDValue Op, const APInt &DemandedElts,\n                                   SmallVectorImpl<SDValue> &Inputs,\n                                   SmallVectorImpl<int> &Mask,\n                                   APInt &KnownUndef, APInt &KnownZero,\n                                   const SelectionDAG &DAG, unsigned Depth,\n                                   bool ResolveKnownElts) {\n  EVT VT = Op.getValueType();\n  if (!VT.isSimple() || !VT.isVector())\n    return false;\n\n  if (getTargetShuffleAndZeroables(Op, Mask, Inputs, KnownUndef, KnownZero)) {\n    if (ResolveKnownElts)\n      resolveTargetShuffleFromZeroables(Mask, KnownUndef, KnownZero);\n    return true;\n  }\n  if (getFauxShuffleMask(Op, DemandedElts, Mask, Inputs, DAG, Depth,\n                         ResolveKnownElts)) {\n    resolveZeroablesFromTargetShuffle(Mask, KnownUndef, KnownZero);\n    return true;\n  }\n  return false;\n}\n\nstatic bool getTargetShuffleInputs(SDValue Op, SmallVectorImpl<SDValue> &Inputs,\n                                   SmallVectorImpl<int> &Mask,\n                                   const SelectionDAG &DAG, unsigned Depth = 0,\n                                   bool ResolveKnownElts = true) {\n  EVT VT = Op.getValueType();\n  if (!VT.isSimple() || !VT.isVector())\n    return false;\n\n  APInt KnownUndef, KnownZero;\n  unsigned NumElts = Op.getValueType().getVectorNumElements();\n  APInt DemandedElts = APInt::getAllOnesValue(NumElts);\n  return getTargetShuffleInputs(Op, DemandedElts, Inputs, Mask, KnownUndef,\n                                KnownZero, DAG, Depth, ResolveKnownElts);\n}\n\n/// Returns the scalar element that will make up the i'th\n/// element of the result of the vector shuffle.\nstatic SDValue getShuffleScalarElt(SDValue Op, unsigned Index,\n                                   SelectionDAG &DAG, unsigned Depth) {\n  if (Depth >= SelectionDAG::MaxRecursionDepth)\n    return SDValue(); // Limit search depth.\n\n  EVT VT = Op.getValueType();\n  unsigned Opcode = Op.getOpcode();\n  unsigned NumElems = VT.getVectorNumElements();\n\n  // Recurse into ISD::VECTOR_SHUFFLE node to find scalars.\n  if (auto *SV = dyn_cast<ShuffleVectorSDNode>(Op)) {\n    int Elt = SV->getMaskElt(Index);\n\n    if (Elt < 0)\n      return DAG.getUNDEF(VT.getVectorElementType());\n\n    SDValue Src = (Elt < (int)NumElems) ? SV->getOperand(0) : SV->getOperand(1);\n    return getShuffleScalarElt(Src, Elt % NumElems, DAG, Depth + 1);\n  }\n\n  // Recurse into target specific vector shuffles to find scalars.\n  if (isTargetShuffle(Opcode)) {\n    MVT ShufVT = VT.getSimpleVT();\n    MVT ShufSVT = ShufVT.getVectorElementType();\n    int NumElems = (int)ShufVT.getVectorNumElements();\n    SmallVector<int, 16> ShuffleMask;\n    SmallVector<SDValue, 16> ShuffleOps;\n    bool IsUnary;\n\n    if (!getTargetShuffleMask(Op.getNode(), ShufVT, true, ShuffleOps,\n                              ShuffleMask, IsUnary))\n      return SDValue();\n\n    int Elt = ShuffleMask[Index];\n    if (Elt == SM_SentinelZero)\n      return ShufSVT.isInteger() ? DAG.getConstant(0, SDLoc(Op), ShufSVT)\n                                 : DAG.getConstantFP(+0.0, SDLoc(Op), ShufSVT);\n    if (Elt == SM_SentinelUndef)\n      return DAG.getUNDEF(ShufSVT);\n\n    assert(0 <= Elt && Elt < (2 * NumElems) && \"Shuffle index out of range\");\n    SDValue Src = (Elt < NumElems) ? ShuffleOps[0] : ShuffleOps[1];\n    return getShuffleScalarElt(Src, Elt % NumElems, DAG, Depth + 1);\n  }\n\n  // Recurse into insert_subvector base/sub vector to find scalars.\n  if (Opcode == ISD::INSERT_SUBVECTOR) {\n    SDValue Vec = Op.getOperand(0);\n    SDValue Sub = Op.getOperand(1);\n    uint64_t SubIdx = Op.getConstantOperandVal(2);\n    unsigned NumSubElts = Sub.getValueType().getVectorNumElements();\n\n    if (SubIdx <= Index && Index < (SubIdx + NumSubElts))\n      return getShuffleScalarElt(Sub, Index - SubIdx, DAG, Depth + 1);\n    return getShuffleScalarElt(Vec, Index, DAG, Depth + 1);\n  }\n\n  // Recurse into concat_vectors sub vector to find scalars.\n  if (Opcode == ISD::CONCAT_VECTORS) {\n    EVT SubVT = Op.getOperand(0).getValueType();\n    unsigned NumSubElts = SubVT.getVectorNumElements();\n    uint64_t SubIdx = Index / NumSubElts;\n    uint64_t SubElt = Index % NumSubElts;\n    return getShuffleScalarElt(Op.getOperand(SubIdx), SubElt, DAG, Depth + 1);\n  }\n\n  // Recurse into extract_subvector src vector to find scalars.\n  if (Opcode == ISD::EXTRACT_SUBVECTOR) {\n    SDValue Src = Op.getOperand(0);\n    uint64_t SrcIdx = Op.getConstantOperandVal(1);\n    return getShuffleScalarElt(Src, Index + SrcIdx, DAG, Depth + 1);\n  }\n\n  // We only peek through bitcasts of the same vector width.\n  if (Opcode == ISD::BITCAST) {\n    SDValue Src = Op.getOperand(0);\n    EVT SrcVT = Src.getValueType();\n    if (SrcVT.isVector() && SrcVT.getVectorNumElements() == NumElems)\n      return getShuffleScalarElt(Src, Index, DAG, Depth + 1);\n    return SDValue();\n  }\n\n  // Actual nodes that may contain scalar elements\n\n  // For insert_vector_elt - either return the index matching scalar or recurse\n  // into the base vector.\n  if (Opcode == ISD::INSERT_VECTOR_ELT &&\n      isa<ConstantSDNode>(Op.getOperand(2))) {\n    if (Op.getConstantOperandAPInt(2) == Index)\n      return Op.getOperand(1);\n    return getShuffleScalarElt(Op.getOperand(0), Index, DAG, Depth + 1);\n  }\n\n  if (Opcode == ISD::SCALAR_TO_VECTOR)\n    return (Index == 0) ? Op.getOperand(0)\n                        : DAG.getUNDEF(VT.getVectorElementType());\n\n  if (Opcode == ISD::BUILD_VECTOR)\n    return Op.getOperand(Index);\n\n  return SDValue();\n}\n\n// Use PINSRB/PINSRW/PINSRD to create a build vector.\nstatic SDValue LowerBuildVectorAsInsert(SDValue Op, const APInt &NonZeroMask,\n                                        unsigned NumNonZero, unsigned NumZero,\n                                        SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  unsigned NumElts = VT.getVectorNumElements();\n  assert(((VT == MVT::v8i16 && Subtarget.hasSSE2()) ||\n          ((VT == MVT::v16i8 || VT == MVT::v4i32) && Subtarget.hasSSE41())) &&\n         \"Illegal vector insertion\");\n\n  SDLoc dl(Op);\n  SDValue V;\n  bool First = true;\n\n  for (unsigned i = 0; i < NumElts; ++i) {\n    bool IsNonZero = NonZeroMask[i];\n    if (!IsNonZero)\n      continue;\n\n    // If the build vector contains zeros or our first insertion is not the\n    // first index then insert into zero vector to break any register\n    // dependency else use SCALAR_TO_VECTOR.\n    if (First) {\n      First = false;\n      if (NumZero || 0 != i)\n        V = getZeroVector(VT, Subtarget, DAG, dl);\n      else {\n        assert(0 == i && \"Expected insertion into zero-index\");\n        V = DAG.getAnyExtOrTrunc(Op.getOperand(i), dl, MVT::i32);\n        V = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4i32, V);\n        V = DAG.getBitcast(VT, V);\n        continue;\n      }\n    }\n    V = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, VT, V, Op.getOperand(i),\n                    DAG.getIntPtrConstant(i, dl));\n  }\n\n  return V;\n}\n\n/// Custom lower build_vector of v16i8.\nstatic SDValue LowerBuildVectorv16i8(SDValue Op, const APInt &NonZeroMask,\n                                     unsigned NumNonZero, unsigned NumZero,\n                                     SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  if (NumNonZero > 8 && !Subtarget.hasSSE41())\n    return SDValue();\n\n  // SSE4.1 - use PINSRB to insert each byte directly.\n  if (Subtarget.hasSSE41())\n    return LowerBuildVectorAsInsert(Op, NonZeroMask, NumNonZero, NumZero, DAG,\n                                    Subtarget);\n\n  SDLoc dl(Op);\n  SDValue V;\n\n  // Pre-SSE4.1 - merge byte pairs and insert with PINSRW.\n  for (unsigned i = 0; i < 16; i += 2) {\n    bool ThisIsNonZero = NonZeroMask[i];\n    bool NextIsNonZero = NonZeroMask[i + 1];\n    if (!ThisIsNonZero && !NextIsNonZero)\n      continue;\n\n    // FIXME: Investigate combining the first 4 bytes as a i32 instead.\n    SDValue Elt;\n    if (ThisIsNonZero) {\n      if (NumZero || NextIsNonZero)\n        Elt = DAG.getZExtOrTrunc(Op.getOperand(i), dl, MVT::i32);\n      else\n        Elt = DAG.getAnyExtOrTrunc(Op.getOperand(i), dl, MVT::i32);\n    }\n\n    if (NextIsNonZero) {\n      SDValue NextElt = Op.getOperand(i + 1);\n      if (i == 0 && NumZero)\n        NextElt = DAG.getZExtOrTrunc(NextElt, dl, MVT::i32);\n      else\n        NextElt = DAG.getAnyExtOrTrunc(NextElt, dl, MVT::i32);\n      NextElt = DAG.getNode(ISD::SHL, dl, MVT::i32, NextElt,\n                            DAG.getConstant(8, dl, MVT::i8));\n      if (ThisIsNonZero)\n        Elt = DAG.getNode(ISD::OR, dl, MVT::i32, NextElt, Elt);\n      else\n        Elt = NextElt;\n    }\n\n    // If our first insertion is not the first index or zeros are needed, then\n    // insert into zero vector. Otherwise, use SCALAR_TO_VECTOR (leaves high\n    // elements undefined).\n    if (!V) {\n      if (i != 0 || NumZero)\n        V = getZeroVector(MVT::v8i16, Subtarget, DAG, dl);\n      else {\n        V = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4i32, Elt);\n        V = DAG.getBitcast(MVT::v8i16, V);\n        continue;\n      }\n    }\n    Elt = DAG.getNode(ISD::TRUNCATE, dl, MVT::i16, Elt);\n    V = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, MVT::v8i16, V, Elt,\n                    DAG.getIntPtrConstant(i / 2, dl));\n  }\n\n  return DAG.getBitcast(MVT::v16i8, V);\n}\n\n/// Custom lower build_vector of v8i16.\nstatic SDValue LowerBuildVectorv8i16(SDValue Op, const APInt &NonZeroMask,\n                                     unsigned NumNonZero, unsigned NumZero,\n                                     SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  if (NumNonZero > 4 && !Subtarget.hasSSE41())\n    return SDValue();\n\n  // Use PINSRW to insert each byte directly.\n  return LowerBuildVectorAsInsert(Op, NonZeroMask, NumNonZero, NumZero, DAG,\n                                  Subtarget);\n}\n\n/// Custom lower build_vector of v4i32 or v4f32.\nstatic SDValue LowerBuildVectorv4x32(SDValue Op, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  // If this is a splat of a pair of elements, use MOVDDUP (unless the target\n  // has XOP; in that case defer lowering to potentially use VPERMIL2PS).\n  // Because we're creating a less complicated build vector here, we may enable\n  // further folding of the MOVDDUP via shuffle transforms.\n  if (Subtarget.hasSSE3() && !Subtarget.hasXOP() &&\n      Op.getOperand(0) == Op.getOperand(2) &&\n      Op.getOperand(1) == Op.getOperand(3) &&\n      Op.getOperand(0) != Op.getOperand(1)) {\n    SDLoc DL(Op);\n    MVT VT = Op.getSimpleValueType();\n    MVT EltVT = VT.getVectorElementType();\n    // Create a new build vector with the first 2 elements followed by undef\n    // padding, bitcast to v2f64, duplicate, and bitcast back.\n    SDValue Ops[4] = { Op.getOperand(0), Op.getOperand(1),\n                       DAG.getUNDEF(EltVT), DAG.getUNDEF(EltVT) };\n    SDValue NewBV = DAG.getBitcast(MVT::v2f64, DAG.getBuildVector(VT, DL, Ops));\n    SDValue Dup = DAG.getNode(X86ISD::MOVDDUP, DL, MVT::v2f64, NewBV);\n    return DAG.getBitcast(VT, Dup);\n  }\n\n  // Find all zeroable elements.\n  std::bitset<4> Zeroable, Undefs;\n  for (int i = 0; i < 4; ++i) {\n    SDValue Elt = Op.getOperand(i);\n    Undefs[i] = Elt.isUndef();\n    Zeroable[i] = (Elt.isUndef() || X86::isZeroNode(Elt));\n  }\n  assert(Zeroable.size() - Zeroable.count() > 1 &&\n         \"We expect at least two non-zero elements!\");\n\n  // We only know how to deal with build_vector nodes where elements are either\n  // zeroable or extract_vector_elt with constant index.\n  SDValue FirstNonZero;\n  unsigned FirstNonZeroIdx;\n  for (unsigned i = 0; i < 4; ++i) {\n    if (Zeroable[i])\n      continue;\n    SDValue Elt = Op.getOperand(i);\n    if (Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        !isa<ConstantSDNode>(Elt.getOperand(1)))\n      return SDValue();\n    // Make sure that this node is extracting from a 128-bit vector.\n    MVT VT = Elt.getOperand(0).getSimpleValueType();\n    if (!VT.is128BitVector())\n      return SDValue();\n    if (!FirstNonZero.getNode()) {\n      FirstNonZero = Elt;\n      FirstNonZeroIdx = i;\n    }\n  }\n\n  assert(FirstNonZero.getNode() && \"Unexpected build vector of all zeros!\");\n  SDValue V1 = FirstNonZero.getOperand(0);\n  MVT VT = V1.getSimpleValueType();\n\n  // See if this build_vector can be lowered as a blend with zero.\n  SDValue Elt;\n  unsigned EltMaskIdx, EltIdx;\n  int Mask[4];\n  for (EltIdx = 0; EltIdx < 4; ++EltIdx) {\n    if (Zeroable[EltIdx]) {\n      // The zero vector will be on the right hand side.\n      Mask[EltIdx] = EltIdx+4;\n      continue;\n    }\n\n    Elt = Op->getOperand(EltIdx);\n    // By construction, Elt is a EXTRACT_VECTOR_ELT with constant index.\n    EltMaskIdx = Elt.getConstantOperandVal(1);\n    if (Elt.getOperand(0) != V1 || EltMaskIdx != EltIdx)\n      break;\n    Mask[EltIdx] = EltIdx;\n  }\n\n  if (EltIdx == 4) {\n    // Let the shuffle legalizer deal with blend operations.\n    SDValue VZeroOrUndef = (Zeroable == Undefs)\n                               ? DAG.getUNDEF(VT)\n                               : getZeroVector(VT, Subtarget, DAG, SDLoc(Op));\n    if (V1.getSimpleValueType() != VT)\n      V1 = DAG.getBitcast(VT, V1);\n    return DAG.getVectorShuffle(VT, SDLoc(V1), V1, VZeroOrUndef, Mask);\n  }\n\n  // See if we can lower this build_vector to a INSERTPS.\n  if (!Subtarget.hasSSE41())\n    return SDValue();\n\n  SDValue V2 = Elt.getOperand(0);\n  if (Elt == FirstNonZero && EltIdx == FirstNonZeroIdx)\n    V1 = SDValue();\n\n  bool CanFold = true;\n  for (unsigned i = EltIdx + 1; i < 4 && CanFold; ++i) {\n    if (Zeroable[i])\n      continue;\n\n    SDValue Current = Op->getOperand(i);\n    SDValue SrcVector = Current->getOperand(0);\n    if (!V1.getNode())\n      V1 = SrcVector;\n    CanFold = (SrcVector == V1) && (Current.getConstantOperandAPInt(1) == i);\n  }\n\n  if (!CanFold)\n    return SDValue();\n\n  assert(V1.getNode() && \"Expected at least two non-zero elements!\");\n  if (V1.getSimpleValueType() != MVT::v4f32)\n    V1 = DAG.getBitcast(MVT::v4f32, V1);\n  if (V2.getSimpleValueType() != MVT::v4f32)\n    V2 = DAG.getBitcast(MVT::v4f32, V2);\n\n  // Ok, we can emit an INSERTPS instruction.\n  unsigned ZMask = Zeroable.to_ulong();\n\n  unsigned InsertPSMask = EltMaskIdx << 6 | EltIdx << 4 | ZMask;\n  assert((InsertPSMask & ~0xFFu) == 0 && \"Invalid mask!\");\n  SDLoc DL(Op);\n  SDValue Result = DAG.getNode(X86ISD::INSERTPS, DL, MVT::v4f32, V1, V2,\n                               DAG.getIntPtrConstant(InsertPSMask, DL, true));\n  return DAG.getBitcast(VT, Result);\n}\n\n/// Return a vector logical shift node.\nstatic SDValue getVShift(bool isLeft, EVT VT, SDValue SrcOp, unsigned NumBits,\n                         SelectionDAG &DAG, const TargetLowering &TLI,\n                         const SDLoc &dl) {\n  assert(VT.is128BitVector() && \"Unknown type for VShift\");\n  MVT ShVT = MVT::v16i8;\n  unsigned Opc = isLeft ? X86ISD::VSHLDQ : X86ISD::VSRLDQ;\n  SrcOp = DAG.getBitcast(ShVT, SrcOp);\n  assert(NumBits % 8 == 0 && \"Only support byte sized shifts\");\n  SDValue ShiftVal = DAG.getTargetConstant(NumBits / 8, dl, MVT::i8);\n  return DAG.getBitcast(VT, DAG.getNode(Opc, dl, ShVT, SrcOp, ShiftVal));\n}\n\nstatic SDValue LowerAsSplatVectorLoad(SDValue SrcOp, MVT VT, const SDLoc &dl,\n                                      SelectionDAG &DAG) {\n\n  // Check if the scalar load can be widened into a vector load. And if\n  // the address is \"base + cst\" see if the cst can be \"absorbed\" into\n  // the shuffle mask.\n  if (LoadSDNode *LD = dyn_cast<LoadSDNode>(SrcOp)) {\n    SDValue Ptr = LD->getBasePtr();\n    if (!ISD::isNormalLoad(LD) || !LD->isSimple())\n      return SDValue();\n    EVT PVT = LD->getValueType(0);\n    if (PVT != MVT::i32 && PVT != MVT::f32)\n      return SDValue();\n\n    int FI = -1;\n    int64_t Offset = 0;\n    if (FrameIndexSDNode *FINode = dyn_cast<FrameIndexSDNode>(Ptr)) {\n      FI = FINode->getIndex();\n      Offset = 0;\n    } else if (DAG.isBaseWithConstantOffset(Ptr) &&\n               isa<FrameIndexSDNode>(Ptr.getOperand(0))) {\n      FI = cast<FrameIndexSDNode>(Ptr.getOperand(0))->getIndex();\n      Offset = Ptr.getConstantOperandVal(1);\n      Ptr = Ptr.getOperand(0);\n    } else {\n      return SDValue();\n    }\n\n    // FIXME: 256-bit vector instructions don't require a strict alignment,\n    // improve this code to support it better.\n    Align RequiredAlign(VT.getSizeInBits() / 8);\n    SDValue Chain = LD->getChain();\n    // Make sure the stack object alignment is at least 16 or 32.\n    MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();\n    MaybeAlign InferredAlign = DAG.InferPtrAlign(Ptr);\n    if (!InferredAlign || *InferredAlign < RequiredAlign) {\n      if (MFI.isFixedObjectIndex(FI)) {\n        // Can't change the alignment. FIXME: It's possible to compute\n        // the exact stack offset and reference FI + adjust offset instead.\n        // If someone *really* cares about this. That's the way to implement it.\n        return SDValue();\n      } else {\n        MFI.setObjectAlignment(FI, RequiredAlign);\n      }\n    }\n\n    // (Offset % 16 or 32) must be multiple of 4. Then address is then\n    // Ptr + (Offset & ~15).\n    if (Offset < 0)\n      return SDValue();\n    if ((Offset % RequiredAlign.value()) & 3)\n      return SDValue();\n    int64_t StartOffset = Offset & ~int64_t(RequiredAlign.value() - 1);\n    if (StartOffset) {\n      SDLoc DL(Ptr);\n      Ptr = DAG.getNode(ISD::ADD, DL, Ptr.getValueType(), Ptr,\n                        DAG.getConstant(StartOffset, DL, Ptr.getValueType()));\n    }\n\n    int EltNo = (Offset - StartOffset) >> 2;\n    unsigned NumElems = VT.getVectorNumElements();\n\n    EVT NVT = EVT::getVectorVT(*DAG.getContext(), PVT, NumElems);\n    SDValue V1 = DAG.getLoad(NVT, dl, Chain, Ptr,\n                             LD->getPointerInfo().getWithOffset(StartOffset));\n\n    SmallVector<int, 8> Mask(NumElems, EltNo);\n\n    return DAG.getVectorShuffle(NVT, dl, V1, DAG.getUNDEF(NVT), Mask);\n  }\n\n  return SDValue();\n}\n\n// Recurse to find a LoadSDNode source and the accumulated ByteOffest.\nstatic bool findEltLoadSrc(SDValue Elt, LoadSDNode *&Ld, int64_t &ByteOffset) {\n  if (ISD::isNON_EXTLoad(Elt.getNode())) {\n    auto *BaseLd = cast<LoadSDNode>(Elt);\n    if (!BaseLd->isSimple())\n      return false;\n    Ld = BaseLd;\n    ByteOffset = 0;\n    return true;\n  }\n\n  switch (Elt.getOpcode()) {\n  case ISD::BITCAST:\n  case ISD::TRUNCATE:\n  case ISD::SCALAR_TO_VECTOR:\n    return findEltLoadSrc(Elt.getOperand(0), Ld, ByteOffset);\n  case ISD::SRL:\n    if (auto *IdxC = dyn_cast<ConstantSDNode>(Elt.getOperand(1))) {\n      uint64_t Idx = IdxC->getZExtValue();\n      if ((Idx % 8) == 0 && findEltLoadSrc(Elt.getOperand(0), Ld, ByteOffset)) {\n        ByteOffset += Idx / 8;\n        return true;\n      }\n    }\n    break;\n  case ISD::EXTRACT_VECTOR_ELT:\n    if (auto *IdxC = dyn_cast<ConstantSDNode>(Elt.getOperand(1))) {\n      SDValue Src = Elt.getOperand(0);\n      unsigned SrcSizeInBits = Src.getScalarValueSizeInBits();\n      unsigned DstSizeInBits = Elt.getScalarValueSizeInBits();\n      if (DstSizeInBits == SrcSizeInBits && (SrcSizeInBits % 8) == 0 &&\n          findEltLoadSrc(Src, Ld, ByteOffset)) {\n        uint64_t Idx = IdxC->getZExtValue();\n        ByteOffset += Idx * (SrcSizeInBits / 8);\n        return true;\n      }\n    }\n    break;\n  }\n\n  return false;\n}\n\n/// Given the initializing elements 'Elts' of a vector of type 'VT', see if the\n/// elements can be replaced by a single large load which has the same value as\n/// a build_vector or insert_subvector whose loaded operands are 'Elts'.\n///\n/// Example: <load i32 *a, load i32 *a+4, zero, undef> -> zextload a\nstatic SDValue EltsFromConsecutiveLoads(EVT VT, ArrayRef<SDValue> Elts,\n                                        const SDLoc &DL, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget,\n                                        bool isAfterLegalize) {\n  if ((VT.getScalarSizeInBits() % 8) != 0)\n    return SDValue();\n\n  unsigned NumElems = Elts.size();\n\n  int LastLoadedElt = -1;\n  APInt LoadMask = APInt::getNullValue(NumElems);\n  APInt ZeroMask = APInt::getNullValue(NumElems);\n  APInt UndefMask = APInt::getNullValue(NumElems);\n\n  SmallVector<LoadSDNode*, 8> Loads(NumElems, nullptr);\n  SmallVector<int64_t, 8> ByteOffsets(NumElems, 0);\n\n  // For each element in the initializer, see if we've found a load, zero or an\n  // undef.\n  for (unsigned i = 0; i < NumElems; ++i) {\n    SDValue Elt = peekThroughBitcasts(Elts[i]);\n    if (!Elt.getNode())\n      return SDValue();\n    if (Elt.isUndef()) {\n      UndefMask.setBit(i);\n      continue;\n    }\n    if (X86::isZeroNode(Elt) || ISD::isBuildVectorAllZeros(Elt.getNode())) {\n      ZeroMask.setBit(i);\n      continue;\n    }\n\n    // Each loaded element must be the correct fractional portion of the\n    // requested vector load.\n    unsigned EltSizeInBits = Elt.getValueSizeInBits();\n    if ((NumElems * EltSizeInBits) != VT.getSizeInBits())\n      return SDValue();\n\n    if (!findEltLoadSrc(Elt, Loads[i], ByteOffsets[i]) || ByteOffsets[i] < 0)\n      return SDValue();\n    unsigned LoadSizeInBits = Loads[i]->getValueSizeInBits(0);\n    if (((ByteOffsets[i] * 8) + EltSizeInBits) > LoadSizeInBits)\n      return SDValue();\n\n    LoadMask.setBit(i);\n    LastLoadedElt = i;\n  }\n  assert((ZeroMask.countPopulation() + UndefMask.countPopulation() +\n          LoadMask.countPopulation()) == NumElems &&\n         \"Incomplete element masks\");\n\n  // Handle Special Cases - all undef or undef/zero.\n  if (UndefMask.countPopulation() == NumElems)\n    return DAG.getUNDEF(VT);\n  if ((ZeroMask.countPopulation() + UndefMask.countPopulation()) == NumElems)\n    return VT.isInteger() ? DAG.getConstant(0, DL, VT)\n                          : DAG.getConstantFP(0.0, DL, VT);\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  int FirstLoadedElt = LoadMask.countTrailingZeros();\n  SDValue EltBase = peekThroughBitcasts(Elts[FirstLoadedElt]);\n  EVT EltBaseVT = EltBase.getValueType();\n  assert(EltBaseVT.getSizeInBits() == EltBaseVT.getStoreSizeInBits() &&\n         \"Register/Memory size mismatch\");\n  LoadSDNode *LDBase = Loads[FirstLoadedElt];\n  assert(LDBase && \"Did not find base load for merging consecutive loads\");\n  unsigned BaseSizeInBits = EltBaseVT.getStoreSizeInBits();\n  unsigned BaseSizeInBytes = BaseSizeInBits / 8;\n  int NumLoadedElts = (1 + LastLoadedElt - FirstLoadedElt);\n  int LoadSizeInBits = NumLoadedElts * BaseSizeInBits;\n  assert((BaseSizeInBits % 8) == 0 && \"Sub-byte element loads detected\");\n\n  // TODO: Support offsetting the base load.\n  if (ByteOffsets[FirstLoadedElt] != 0)\n    return SDValue();\n\n  // Check to see if the element's load is consecutive to the base load\n  // or offset from a previous (already checked) load.\n  auto CheckConsecutiveLoad = [&](LoadSDNode *Base, int EltIdx) {\n    LoadSDNode *Ld = Loads[EltIdx];\n    int64_t ByteOffset = ByteOffsets[EltIdx];\n    if (ByteOffset && (ByteOffset % BaseSizeInBytes) == 0) {\n      int64_t BaseIdx = EltIdx - (ByteOffset / BaseSizeInBytes);\n      return (0 <= BaseIdx && BaseIdx < (int)NumElems && LoadMask[BaseIdx] &&\n              Loads[BaseIdx] == Ld && ByteOffsets[BaseIdx] == 0);\n    }\n    return DAG.areNonVolatileConsecutiveLoads(Ld, Base, BaseSizeInBytes,\n                                              EltIdx - FirstLoadedElt);\n  };\n\n  // Consecutive loads can contain UNDEFS but not ZERO elements.\n  // Consecutive loads with UNDEFs and ZEROs elements require a\n  // an additional shuffle stage to clear the ZERO elements.\n  bool IsConsecutiveLoad = true;\n  bool IsConsecutiveLoadWithZeros = true;\n  for (int i = FirstLoadedElt + 1; i <= LastLoadedElt; ++i) {\n    if (LoadMask[i]) {\n      if (!CheckConsecutiveLoad(LDBase, i)) {\n        IsConsecutiveLoad = false;\n        IsConsecutiveLoadWithZeros = false;\n        break;\n      }\n    } else if (ZeroMask[i]) {\n      IsConsecutiveLoad = false;\n    }\n  }\n\n  auto CreateLoad = [&DAG, &DL, &Loads](EVT VT, LoadSDNode *LDBase) {\n    auto MMOFlags = LDBase->getMemOperand()->getFlags();\n    assert(LDBase->isSimple() &&\n           \"Cannot merge volatile or atomic loads.\");\n    SDValue NewLd =\n        DAG.getLoad(VT, DL, LDBase->getChain(), LDBase->getBasePtr(),\n                    LDBase->getPointerInfo(), LDBase->getOriginalAlign(),\n                    MMOFlags);\n    for (auto *LD : Loads)\n      if (LD)\n        DAG.makeEquivalentMemoryOrdering(LD, NewLd);\n    return NewLd;\n  };\n\n  // Check if the base load is entirely dereferenceable.\n  bool IsDereferenceable = LDBase->getPointerInfo().isDereferenceable(\n      VT.getSizeInBits() / 8, *DAG.getContext(), DAG.getDataLayout());\n\n  // LOAD - all consecutive load/undefs (must start/end with a load or be\n  // entirely dereferenceable). If we have found an entire vector of loads and\n  // undefs, then return a large load of the entire vector width starting at the\n  // base pointer. If the vector contains zeros, then attempt to shuffle those\n  // elements.\n  if (FirstLoadedElt == 0 &&\n      (NumLoadedElts == (int)NumElems || IsDereferenceable) &&\n      (IsConsecutiveLoad || IsConsecutiveLoadWithZeros)) {\n    if (isAfterLegalize && !TLI.isOperationLegal(ISD::LOAD, VT))\n      return SDValue();\n\n    // Don't create 256-bit non-temporal aligned loads without AVX2 as these\n    // will lower to regular temporal loads and use the cache.\n    if (LDBase->isNonTemporal() && LDBase->getAlignment() >= 32 &&\n        VT.is256BitVector() && !Subtarget.hasInt256())\n      return SDValue();\n\n    if (NumElems == 1)\n      return DAG.getBitcast(VT, Elts[FirstLoadedElt]);\n\n    if (!ZeroMask)\n      return CreateLoad(VT, LDBase);\n\n    // IsConsecutiveLoadWithZeros - we need to create a shuffle of the loaded\n    // vector and a zero vector to clear out the zero elements.\n    if (!isAfterLegalize && VT.isVector()) {\n      unsigned NumMaskElts = VT.getVectorNumElements();\n      if ((NumMaskElts % NumElems) == 0) {\n        unsigned Scale = NumMaskElts / NumElems;\n        SmallVector<int, 4> ClearMask(NumMaskElts, -1);\n        for (unsigned i = 0; i < NumElems; ++i) {\n          if (UndefMask[i])\n            continue;\n          int Offset = ZeroMask[i] ? NumMaskElts : 0;\n          for (unsigned j = 0; j != Scale; ++j)\n            ClearMask[(i * Scale) + j] = (i * Scale) + j + Offset;\n        }\n        SDValue V = CreateLoad(VT, LDBase);\n        SDValue Z = VT.isInteger() ? DAG.getConstant(0, DL, VT)\n                                   : DAG.getConstantFP(0.0, DL, VT);\n        return DAG.getVectorShuffle(VT, DL, V, Z, ClearMask);\n      }\n    }\n  }\n\n  // If the upper half of a ymm/zmm load is undef then just load the lower half.\n  if (VT.is256BitVector() || VT.is512BitVector()) {\n    unsigned HalfNumElems = NumElems / 2;\n    if (UndefMask.extractBits(HalfNumElems, HalfNumElems).isAllOnesValue()) {\n      EVT HalfVT =\n          EVT::getVectorVT(*DAG.getContext(), VT.getScalarType(), HalfNumElems);\n      SDValue HalfLD =\n          EltsFromConsecutiveLoads(HalfVT, Elts.drop_back(HalfNumElems), DL,\n                                   DAG, Subtarget, isAfterLegalize);\n      if (HalfLD)\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, DAG.getUNDEF(VT),\n                           HalfLD, DAG.getIntPtrConstant(0, DL));\n    }\n  }\n\n  // VZEXT_LOAD - consecutive 32/64-bit load/undefs followed by zeros/undefs.\n  if (IsConsecutiveLoad && FirstLoadedElt == 0 &&\n      (LoadSizeInBits == 32 || LoadSizeInBits == 64) &&\n      ((VT.is128BitVector() || VT.is256BitVector() || VT.is512BitVector()))) {\n    MVT VecSVT = VT.isFloatingPoint() ? MVT::getFloatingPointVT(LoadSizeInBits)\n                                      : MVT::getIntegerVT(LoadSizeInBits);\n    MVT VecVT = MVT::getVectorVT(VecSVT, VT.getSizeInBits() / LoadSizeInBits);\n    // Allow v4f32 on SSE1 only targets.\n    // FIXME: Add more isel patterns so we can just use VT directly.\n    if (!Subtarget.hasSSE2() && VT == MVT::v4f32)\n      VecVT = MVT::v4f32;\n    if (TLI.isTypeLegal(VecVT)) {\n      SDVTList Tys = DAG.getVTList(VecVT, MVT::Other);\n      SDValue Ops[] = { LDBase->getChain(), LDBase->getBasePtr() };\n      SDValue ResNode = DAG.getMemIntrinsicNode(\n          X86ISD::VZEXT_LOAD, DL, Tys, Ops, VecSVT, LDBase->getPointerInfo(),\n          LDBase->getOriginalAlign(), MachineMemOperand::MOLoad);\n      for (auto *LD : Loads)\n        if (LD)\n          DAG.makeEquivalentMemoryOrdering(LD, ResNode);\n      return DAG.getBitcast(VT, ResNode);\n    }\n  }\n\n  // BROADCAST - match the smallest possible repetition pattern, load that\n  // scalar/subvector element and then broadcast to the entire vector.\n  if (ZeroMask.isNullValue() && isPowerOf2_32(NumElems) && Subtarget.hasAVX() &&\n      (VT.is128BitVector() || VT.is256BitVector() || VT.is512BitVector())) {\n    for (unsigned SubElems = 1; SubElems < NumElems; SubElems *= 2) {\n      unsigned RepeatSize = SubElems * BaseSizeInBits;\n      unsigned ScalarSize = std::min(RepeatSize, 64u);\n      if (!Subtarget.hasAVX2() && ScalarSize < 32)\n        continue;\n\n      // Don't attempt a 1:N subvector broadcast - it should be caught by\n      // combineConcatVectorOps, else will cause infinite loops.\n      if (RepeatSize > ScalarSize && SubElems == 1)\n        continue;\n\n      bool Match = true;\n      SmallVector<SDValue, 8> RepeatedLoads(SubElems, DAG.getUNDEF(EltBaseVT));\n      for (unsigned i = 0; i != NumElems && Match; ++i) {\n        if (!LoadMask[i])\n          continue;\n        SDValue Elt = peekThroughBitcasts(Elts[i]);\n        if (RepeatedLoads[i % SubElems].isUndef())\n          RepeatedLoads[i % SubElems] = Elt;\n        else\n          Match &= (RepeatedLoads[i % SubElems] == Elt);\n      }\n\n      // We must have loads at both ends of the repetition.\n      Match &= !RepeatedLoads.front().isUndef();\n      Match &= !RepeatedLoads.back().isUndef();\n      if (!Match)\n        continue;\n\n      EVT RepeatVT =\n          VT.isInteger() && (RepeatSize != 64 || TLI.isTypeLegal(MVT::i64))\n              ? EVT::getIntegerVT(*DAG.getContext(), ScalarSize)\n              : EVT::getFloatingPointVT(ScalarSize);\n      if (RepeatSize > ScalarSize)\n        RepeatVT = EVT::getVectorVT(*DAG.getContext(), RepeatVT,\n                                    RepeatSize / ScalarSize);\n      EVT BroadcastVT =\n          EVT::getVectorVT(*DAG.getContext(), RepeatVT.getScalarType(),\n                           VT.getSizeInBits() / ScalarSize);\n      if (TLI.isTypeLegal(BroadcastVT)) {\n        if (SDValue RepeatLoad = EltsFromConsecutiveLoads(\n                RepeatVT, RepeatedLoads, DL, DAG, Subtarget, isAfterLegalize)) {\n          SDValue Broadcast = RepeatLoad;\n          if (RepeatSize > ScalarSize) {\n            while (Broadcast.getValueSizeInBits() < VT.getSizeInBits())\n              Broadcast = concatSubVectors(Broadcast, Broadcast, DAG, DL);\n          } else {\n            Broadcast =\n                DAG.getNode(X86ISD::VBROADCAST, DL, BroadcastVT, RepeatLoad);\n          }\n          return DAG.getBitcast(VT, Broadcast);\n        }\n      }\n    }\n  }\n\n  return SDValue();\n}\n\n// Combine a vector ops (shuffles etc.) that is equal to build_vector load1,\n// load2, load3, load4, <0, 1, 2, 3> into a vector load if the load addresses\n// are consecutive, non-overlapping, and in the right order.\nstatic SDValue combineToConsecutiveLoads(EVT VT, SDValue Op, const SDLoc &DL,\n                                         SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget,\n                                         bool isAfterLegalize) {\n  SmallVector<SDValue, 64> Elts;\n  for (unsigned i = 0, e = VT.getVectorNumElements(); i != e; ++i) {\n    if (SDValue Elt = getShuffleScalarElt(Op, i, DAG, 0)) {\n      Elts.push_back(Elt);\n      continue;\n    }\n    return SDValue();\n  }\n  assert(Elts.size() == VT.getVectorNumElements());\n  return EltsFromConsecutiveLoads(VT, Elts, DL, DAG, Subtarget,\n                                  isAfterLegalize);\n}\n\nstatic Constant *getConstantVector(MVT VT, const APInt &SplatValue,\n                                   unsigned SplatBitSize, LLVMContext &C) {\n  unsigned ScalarSize = VT.getScalarSizeInBits();\n  unsigned NumElm = SplatBitSize / ScalarSize;\n\n  SmallVector<Constant *, 32> ConstantVec;\n  for (unsigned i = 0; i < NumElm; i++) {\n    APInt Val = SplatValue.extractBits(ScalarSize, ScalarSize * i);\n    Constant *Const;\n    if (VT.isFloatingPoint()) {\n      if (ScalarSize == 32) {\n        Const = ConstantFP::get(C, APFloat(APFloat::IEEEsingle(), Val));\n      } else {\n        assert(ScalarSize == 64 && \"Unsupported floating point scalar size\");\n        Const = ConstantFP::get(C, APFloat(APFloat::IEEEdouble(), Val));\n      }\n    } else\n      Const = Constant::getIntegerValue(Type::getIntNTy(C, ScalarSize), Val);\n    ConstantVec.push_back(Const);\n  }\n  return ConstantVector::get(ArrayRef<Constant *>(ConstantVec));\n}\n\nstatic bool isFoldableUseOfShuffle(SDNode *N) {\n  for (auto *U : N->uses()) {\n    unsigned Opc = U->getOpcode();\n    // VPERMV/VPERMV3 shuffles can never fold their index operands.\n    if (Opc == X86ISD::VPERMV && U->getOperand(0).getNode() == N)\n      return false;\n    if (Opc == X86ISD::VPERMV3 && U->getOperand(1).getNode() == N)\n      return false;\n    if (isTargetShuffle(Opc))\n      return true;\n    if (Opc == ISD::BITCAST) // Ignore bitcasts\n      return isFoldableUseOfShuffle(U);\n    if (N->hasOneUse())\n      return true;\n  }\n  return false;\n}\n\n/// Attempt to use the vbroadcast instruction to generate a splat value\n/// from a splat BUILD_VECTOR which uses:\n///  a. A single scalar load, or a constant.\n///  b. Repeated pattern of constants (e.g. <0,1,0,1> or <0,1,2,3,0,1,2,3>).\n///\n/// The VBROADCAST node is returned when a pattern is found,\n/// or SDValue() otherwise.\nstatic SDValue lowerBuildVectorAsBroadcast(BuildVectorSDNode *BVOp,\n                                           const X86Subtarget &Subtarget,\n                                           SelectionDAG &DAG) {\n  // VBROADCAST requires AVX.\n  // TODO: Splats could be generated for non-AVX CPUs using SSE\n  // instructions, but there's less potential gain for only 128-bit vectors.\n  if (!Subtarget.hasAVX())\n    return SDValue();\n\n  MVT VT = BVOp->getSimpleValueType(0);\n  unsigned NumElts = VT.getVectorNumElements();\n  SDLoc dl(BVOp);\n\n  assert((VT.is128BitVector() || VT.is256BitVector() || VT.is512BitVector()) &&\n         \"Unsupported vector type for broadcast.\");\n\n  // See if the build vector is a repeating sequence of scalars (inc. splat).\n  SDValue Ld;\n  BitVector UndefElements;\n  SmallVector<SDValue, 16> Sequence;\n  if (BVOp->getRepeatedSequence(Sequence, &UndefElements)) {\n    assert((NumElts % Sequence.size()) == 0 && \"Sequence doesn't fit.\");\n    if (Sequence.size() == 1)\n      Ld = Sequence[0];\n  }\n\n  // Attempt to use VBROADCASTM\n  // From this pattern:\n  // a. t0 = (zext_i64 (bitcast_i8 v2i1 X))\n  // b. t1 = (build_vector t0 t0)\n  //\n  // Create (VBROADCASTM v2i1 X)\n  if (!Sequence.empty() && Subtarget.hasCDI()) {\n    // If not a splat, are the upper sequence values zeroable?\n    unsigned SeqLen = Sequence.size();\n    bool UpperZeroOrUndef =\n        SeqLen == 1 ||\n        llvm::all_of(makeArrayRef(Sequence).drop_front(), [](SDValue V) {\n          return !V || V.isUndef() || isNullConstant(V);\n        });\n    SDValue Op0 = Sequence[0];\n    if (UpperZeroOrUndef && ((Op0.getOpcode() == ISD::BITCAST) ||\n                             (Op0.getOpcode() == ISD::ZERO_EXTEND &&\n                              Op0.getOperand(0).getOpcode() == ISD::BITCAST))) {\n      SDValue BOperand = Op0.getOpcode() == ISD::BITCAST\n                             ? Op0.getOperand(0)\n                             : Op0.getOperand(0).getOperand(0);\n      MVT MaskVT = BOperand.getSimpleValueType();\n      MVT EltType = MVT::getIntegerVT(VT.getScalarSizeInBits() * SeqLen);\n      if ((EltType == MVT::i64 && MaskVT == MVT::v8i1) ||  // for broadcastmb2q\n          (EltType == MVT::i32 && MaskVT == MVT::v16i1)) { // for broadcastmw2d\n        MVT BcstVT = MVT::getVectorVT(EltType, NumElts / SeqLen);\n        if (!VT.is512BitVector() && !Subtarget.hasVLX()) {\n          unsigned Scale = 512 / VT.getSizeInBits();\n          BcstVT = MVT::getVectorVT(EltType, Scale * (NumElts / SeqLen));\n        }\n        SDValue Bcst = DAG.getNode(X86ISD::VBROADCASTM, dl, BcstVT, BOperand);\n        if (BcstVT.getSizeInBits() != VT.getSizeInBits())\n          Bcst = extractSubVector(Bcst, 0, DAG, dl, VT.getSizeInBits());\n        return DAG.getBitcast(VT, Bcst);\n      }\n    }\n  }\n\n  unsigned NumUndefElts = UndefElements.count();\n  if (!Ld || (NumElts - NumUndefElts) <= 1) {\n    APInt SplatValue, Undef;\n    unsigned SplatBitSize;\n    bool HasUndef;\n    // Check if this is a repeated constant pattern suitable for broadcasting.\n    if (BVOp->isConstantSplat(SplatValue, Undef, SplatBitSize, HasUndef) &&\n        SplatBitSize > VT.getScalarSizeInBits() &&\n        SplatBitSize < VT.getSizeInBits()) {\n      // Avoid replacing with broadcast when it's a use of a shuffle\n      // instruction to preserve the present custom lowering of shuffles.\n      if (isFoldableUseOfShuffle(BVOp))\n        return SDValue();\n      // replace BUILD_VECTOR with broadcast of the repeated constants.\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      LLVMContext *Ctx = DAG.getContext();\n      MVT PVT = TLI.getPointerTy(DAG.getDataLayout());\n      if (Subtarget.hasAVX()) {\n        if (SplatBitSize == 32 || SplatBitSize == 64 ||\n            (SplatBitSize < 32 && Subtarget.hasAVX2())) {\n          // Splatted value can fit in one INTEGER constant in constant pool.\n          // Load the constant and broadcast it.\n          MVT CVT = MVT::getIntegerVT(SplatBitSize);\n          Type *ScalarTy = Type::getIntNTy(*Ctx, SplatBitSize);\n          Constant *C = Constant::getIntegerValue(ScalarTy, SplatValue);\n          SDValue CP = DAG.getConstantPool(C, PVT);\n          unsigned Repeat = VT.getSizeInBits() / SplatBitSize;\n\n          Align Alignment = cast<ConstantPoolSDNode>(CP)->getAlign();\n          SDVTList Tys =\n              DAG.getVTList(MVT::getVectorVT(CVT, Repeat), MVT::Other);\n          SDValue Ops[] = {DAG.getEntryNode(), CP};\n          MachinePointerInfo MPI =\n              MachinePointerInfo::getConstantPool(DAG.getMachineFunction());\n          SDValue Brdcst = DAG.getMemIntrinsicNode(\n              X86ISD::VBROADCAST_LOAD, dl, Tys, Ops, CVT, MPI, Alignment,\n              MachineMemOperand::MOLoad);\n          return DAG.getBitcast(VT, Brdcst);\n        }\n        if (SplatBitSize > 64) {\n          // Load the vector of constants and broadcast it.\n          Constant *VecC = getConstantVector(VT, SplatValue, SplatBitSize,\n                                             *Ctx);\n          SDValue VCP = DAG.getConstantPool(VecC, PVT);\n          unsigned NumElm = SplatBitSize / VT.getScalarSizeInBits();\n          MVT VVT = MVT::getVectorVT(VT.getScalarType(), NumElm);\n          Align Alignment = cast<ConstantPoolSDNode>(VCP)->getAlign();\n          SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n          SDValue Ops[] = {DAG.getEntryNode(), VCP};\n          MachinePointerInfo MPI =\n              MachinePointerInfo::getConstantPool(DAG.getMachineFunction());\n          return DAG.getMemIntrinsicNode(\n              X86ISD::SUBV_BROADCAST_LOAD, dl, Tys, Ops, VVT, MPI, Alignment,\n              MachineMemOperand::MOLoad);\n        }\n      }\n    }\n\n    // If we are moving a scalar into a vector (Ld must be set and all elements\n    // but 1 are undef) and that operation is not obviously supported by\n    // vmovd/vmovq/vmovss/vmovsd, then keep trying to form a broadcast.\n    // That's better than general shuffling and may eliminate a load to GPR and\n    // move from scalar to vector register.\n    if (!Ld || NumElts - NumUndefElts != 1)\n      return SDValue();\n    unsigned ScalarSize = Ld.getValueSizeInBits();\n    if (!(UndefElements[0] || (ScalarSize != 32 && ScalarSize != 64)))\n      return SDValue();\n  }\n\n  bool ConstSplatVal =\n      (Ld.getOpcode() == ISD::Constant || Ld.getOpcode() == ISD::ConstantFP);\n  bool IsLoad = ISD::isNormalLoad(Ld.getNode());\n\n  // TODO: Handle broadcasts of non-constant sequences.\n\n  // Make sure that all of the users of a non-constant load are from the\n  // BUILD_VECTOR node.\n  // FIXME: Is the use count needed for non-constant, non-load case?\n  if (!ConstSplatVal && !IsLoad && !BVOp->isOnlyUserOf(Ld.getNode()))\n    return SDValue();\n\n  unsigned ScalarSize = Ld.getValueSizeInBits();\n  bool IsGE256 = (VT.getSizeInBits() >= 256);\n\n  // When optimizing for size, generate up to 5 extra bytes for a broadcast\n  // instruction to save 8 or more bytes of constant pool data.\n  // TODO: If multiple splats are generated to load the same constant,\n  // it may be detrimental to overall size. There needs to be a way to detect\n  // that condition to know if this is truly a size win.\n  bool OptForSize = DAG.shouldOptForSize();\n\n  // Handle broadcasting a single constant scalar from the constant pool\n  // into a vector.\n  // On Sandybridge (no AVX2), it is still better to load a constant vector\n  // from the constant pool and not to broadcast it from a scalar.\n  // But override that restriction when optimizing for size.\n  // TODO: Check if splatting is recommended for other AVX-capable CPUs.\n  if (ConstSplatVal && (Subtarget.hasAVX2() || OptForSize)) {\n    EVT CVT = Ld.getValueType();\n    assert(!CVT.isVector() && \"Must not broadcast a vector type\");\n\n    // Splat f32, i32, v4f64, v4i64 in all cases with AVX2.\n    // For size optimization, also splat v2f64 and v2i64, and for size opt\n    // with AVX2, also splat i8 and i16.\n    // With pattern matching, the VBROADCAST node may become a VMOVDDUP.\n    if (ScalarSize == 32 || (IsGE256 && ScalarSize == 64) ||\n        (OptForSize && (ScalarSize == 64 || Subtarget.hasAVX2()))) {\n      const Constant *C = nullptr;\n      if (ConstantSDNode *CI = dyn_cast<ConstantSDNode>(Ld))\n        C = CI->getConstantIntValue();\n      else if (ConstantFPSDNode *CF = dyn_cast<ConstantFPSDNode>(Ld))\n        C = CF->getConstantFPValue();\n\n      assert(C && \"Invalid constant type\");\n\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      SDValue CP =\n          DAG.getConstantPool(C, TLI.getPointerTy(DAG.getDataLayout()));\n      Align Alignment = cast<ConstantPoolSDNode>(CP)->getAlign();\n\n      SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n      SDValue Ops[] = {DAG.getEntryNode(), CP};\n      MachinePointerInfo MPI =\n          MachinePointerInfo::getConstantPool(DAG.getMachineFunction());\n      return DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, dl, Tys, Ops, CVT,\n                                     MPI, Alignment, MachineMemOperand::MOLoad);\n    }\n  }\n\n  // Handle AVX2 in-register broadcasts.\n  if (!IsLoad && Subtarget.hasInt256() &&\n      (ScalarSize == 32 || (IsGE256 && ScalarSize == 64)))\n    return DAG.getNode(X86ISD::VBROADCAST, dl, VT, Ld);\n\n  // The scalar source must be a normal load.\n  if (!IsLoad)\n    return SDValue();\n\n  // Make sure the non-chain result is only used by this build vector.\n  if (!Ld->hasNUsesOfValue(NumElts - NumUndefElts, 0))\n    return SDValue();\n\n  if (ScalarSize == 32 || (IsGE256 && ScalarSize == 64) ||\n      (Subtarget.hasVLX() && ScalarSize == 64)) {\n    auto *LN = cast<LoadSDNode>(Ld);\n    SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n    SDValue Ops[] = {LN->getChain(), LN->getBasePtr()};\n    SDValue BCast =\n        DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, dl, Tys, Ops,\n                                LN->getMemoryVT(), LN->getMemOperand());\n    DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BCast.getValue(1));\n    return BCast;\n  }\n\n  // The integer check is needed for the 64-bit into 128-bit so it doesn't match\n  // double since there is no vbroadcastsd xmm\n  if (Subtarget.hasInt256() && Ld.getValueType().isInteger() &&\n      (ScalarSize == 8 || ScalarSize == 16 || ScalarSize == 64)) {\n    auto *LN = cast<LoadSDNode>(Ld);\n    SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n    SDValue Ops[] = {LN->getChain(), LN->getBasePtr()};\n    SDValue BCast =\n        DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, dl, Tys, Ops,\n                                LN->getMemoryVT(), LN->getMemOperand());\n    DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BCast.getValue(1));\n    return BCast;\n  }\n\n  // Unsupported broadcast.\n  return SDValue();\n}\n\n/// For an EXTRACT_VECTOR_ELT with a constant index return the real\n/// underlying vector and index.\n///\n/// Modifies \\p ExtractedFromVec to the real vector and returns the real\n/// index.\nstatic int getUnderlyingExtractedFromVec(SDValue &ExtractedFromVec,\n                                         SDValue ExtIdx) {\n  int Idx = cast<ConstantSDNode>(ExtIdx)->getZExtValue();\n  if (!isa<ShuffleVectorSDNode>(ExtractedFromVec))\n    return Idx;\n\n  // For 256-bit vectors, LowerEXTRACT_VECTOR_ELT_SSE4 may have already\n  // lowered this:\n  //   (extract_vector_elt (v8f32 %1), Constant<6>)\n  // to:\n  //   (extract_vector_elt (vector_shuffle<2,u,u,u>\n  //                           (extract_subvector (v8f32 %0), Constant<4>),\n  //                           undef)\n  //                       Constant<0>)\n  // In this case the vector is the extract_subvector expression and the index\n  // is 2, as specified by the shuffle.\n  ShuffleVectorSDNode *SVOp = cast<ShuffleVectorSDNode>(ExtractedFromVec);\n  SDValue ShuffleVec = SVOp->getOperand(0);\n  MVT ShuffleVecVT = ShuffleVec.getSimpleValueType();\n  assert(ShuffleVecVT.getVectorElementType() ==\n         ExtractedFromVec.getSimpleValueType().getVectorElementType());\n\n  int ShuffleIdx = SVOp->getMaskElt(Idx);\n  if (isUndefOrInRange(ShuffleIdx, 0, ShuffleVecVT.getVectorNumElements())) {\n    ExtractedFromVec = ShuffleVec;\n    return ShuffleIdx;\n  }\n  return Idx;\n}\n\nstatic SDValue buildFromShuffleMostly(SDValue Op, SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n\n  // Skip if insert_vec_elt is not supported.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!TLI.isOperationLegalOrCustom(ISD::INSERT_VECTOR_ELT, VT))\n    return SDValue();\n\n  SDLoc DL(Op);\n  unsigned NumElems = Op.getNumOperands();\n\n  SDValue VecIn1;\n  SDValue VecIn2;\n  SmallVector<unsigned, 4> InsertIndices;\n  SmallVector<int, 8> Mask(NumElems, -1);\n\n  for (unsigned i = 0; i != NumElems; ++i) {\n    unsigned Opc = Op.getOperand(i).getOpcode();\n\n    if (Opc == ISD::UNDEF)\n      continue;\n\n    if (Opc != ISD::EXTRACT_VECTOR_ELT) {\n      // Quit if more than 1 elements need inserting.\n      if (InsertIndices.size() > 1)\n        return SDValue();\n\n      InsertIndices.push_back(i);\n      continue;\n    }\n\n    SDValue ExtractedFromVec = Op.getOperand(i).getOperand(0);\n    SDValue ExtIdx = Op.getOperand(i).getOperand(1);\n\n    // Quit if non-constant index.\n    if (!isa<ConstantSDNode>(ExtIdx))\n      return SDValue();\n    int Idx = getUnderlyingExtractedFromVec(ExtractedFromVec, ExtIdx);\n\n    // Quit if extracted from vector of different type.\n    if (ExtractedFromVec.getValueType() != VT)\n      return SDValue();\n\n    if (!VecIn1.getNode())\n      VecIn1 = ExtractedFromVec;\n    else if (VecIn1 != ExtractedFromVec) {\n      if (!VecIn2.getNode())\n        VecIn2 = ExtractedFromVec;\n      else if (VecIn2 != ExtractedFromVec)\n        // Quit if more than 2 vectors to shuffle\n        return SDValue();\n    }\n\n    if (ExtractedFromVec == VecIn1)\n      Mask[i] = Idx;\n    else if (ExtractedFromVec == VecIn2)\n      Mask[i] = Idx + NumElems;\n  }\n\n  if (!VecIn1.getNode())\n    return SDValue();\n\n  VecIn2 = VecIn2.getNode() ? VecIn2 : DAG.getUNDEF(VT);\n  SDValue NV = DAG.getVectorShuffle(VT, DL, VecIn1, VecIn2, Mask);\n\n  for (unsigned Idx : InsertIndices)\n    NV = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, VT, NV, Op.getOperand(Idx),\n                     DAG.getIntPtrConstant(Idx, DL));\n\n  return NV;\n}\n\n// Lower BUILD_VECTOR operation for v8i1 and v16i1 types.\nstatic SDValue LowerBUILD_VECTORvXi1(SDValue Op, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n\n  MVT VT = Op.getSimpleValueType();\n  assert((VT.getVectorElementType() == MVT::i1) &&\n         \"Unexpected type in LowerBUILD_VECTORvXi1!\");\n\n  SDLoc dl(Op);\n  if (ISD::isBuildVectorAllZeros(Op.getNode()) ||\n      ISD::isBuildVectorAllOnes(Op.getNode()))\n    return Op;\n\n  uint64_t Immediate = 0;\n  SmallVector<unsigned, 16> NonConstIdx;\n  bool IsSplat = true;\n  bool HasConstElts = false;\n  int SplatIdx = -1;\n  for (unsigned idx = 0, e = Op.getNumOperands(); idx < e; ++idx) {\n    SDValue In = Op.getOperand(idx);\n    if (In.isUndef())\n      continue;\n    if (auto *InC = dyn_cast<ConstantSDNode>(In)) {\n      Immediate |= (InC->getZExtValue() & 0x1) << idx;\n      HasConstElts = true;\n    } else {\n      NonConstIdx.push_back(idx);\n    }\n    if (SplatIdx < 0)\n      SplatIdx = idx;\n    else if (In != Op.getOperand(SplatIdx))\n      IsSplat = false;\n  }\n\n  // for splat use \" (select i1 splat_elt, all-ones, all-zeroes)\"\n  if (IsSplat) {\n    // The build_vector allows the scalar element to be larger than the vector\n    // element type. We need to mask it to use as a condition unless we know\n    // the upper bits are zero.\n    // FIXME: Use computeKnownBits instead of checking specific opcode?\n    SDValue Cond = Op.getOperand(SplatIdx);\n    assert(Cond.getValueType() == MVT::i8 && \"Unexpected VT!\");\n    if (Cond.getOpcode() != ISD::SETCC)\n      Cond = DAG.getNode(ISD::AND, dl, MVT::i8, Cond,\n                         DAG.getConstant(1, dl, MVT::i8));\n\n    // Perform the select in the scalar domain so we can use cmov.\n    if (VT == MVT::v64i1 && !Subtarget.is64Bit()) {\n      SDValue Select = DAG.getSelect(dl, MVT::i32, Cond,\n                                     DAG.getAllOnesConstant(dl, MVT::i32),\n                                     DAG.getConstant(0, dl, MVT::i32));\n      Select = DAG.getBitcast(MVT::v32i1, Select);\n      return DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v64i1, Select, Select);\n    } else {\n      MVT ImmVT = MVT::getIntegerVT(std::max((unsigned)VT.getSizeInBits(), 8U));\n      SDValue Select = DAG.getSelect(dl, ImmVT, Cond,\n                                     DAG.getAllOnesConstant(dl, ImmVT),\n                                     DAG.getConstant(0, dl, ImmVT));\n      MVT VecVT = VT.getSizeInBits() >= 8 ? VT : MVT::v8i1;\n      Select = DAG.getBitcast(VecVT, Select);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, Select,\n                         DAG.getIntPtrConstant(0, dl));\n    }\n  }\n\n  // insert elements one by one\n  SDValue DstVec;\n  if (HasConstElts) {\n    if (VT == MVT::v64i1 && !Subtarget.is64Bit()) {\n      SDValue ImmL = DAG.getConstant(Lo_32(Immediate), dl, MVT::i32);\n      SDValue ImmH = DAG.getConstant(Hi_32(Immediate), dl, MVT::i32);\n      ImmL = DAG.getBitcast(MVT::v32i1, ImmL);\n      ImmH = DAG.getBitcast(MVT::v32i1, ImmH);\n      DstVec = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v64i1, ImmL, ImmH);\n    } else {\n      MVT ImmVT = MVT::getIntegerVT(std::max((unsigned)VT.getSizeInBits(), 8U));\n      SDValue Imm = DAG.getConstant(Immediate, dl, ImmVT);\n      MVT VecVT = VT.getSizeInBits() >= 8 ? VT : MVT::v8i1;\n      DstVec = DAG.getBitcast(VecVT, Imm);\n      DstVec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, DstVec,\n                           DAG.getIntPtrConstant(0, dl));\n    }\n  } else\n    DstVec = DAG.getUNDEF(VT);\n\n  for (unsigned i = 0, e = NonConstIdx.size(); i != e; ++i) {\n    unsigned InsertIdx = NonConstIdx[i];\n    DstVec = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, VT, DstVec,\n                         Op.getOperand(InsertIdx),\n                         DAG.getIntPtrConstant(InsertIdx, dl));\n  }\n  return DstVec;\n}\n\n/// This is a helper function of LowerToHorizontalOp().\n/// This function checks that the build_vector \\p N in input implements a\n/// 128-bit partial horizontal operation on a 256-bit vector, but that operation\n/// may not match the layout of an x86 256-bit horizontal instruction.\n/// In other words, if this returns true, then some extraction/insertion will\n/// be required to produce a valid horizontal instruction.\n///\n/// Parameter \\p Opcode defines the kind of horizontal operation to match.\n/// For example, if \\p Opcode is equal to ISD::ADD, then this function\n/// checks if \\p N implements a horizontal arithmetic add; if instead \\p Opcode\n/// is equal to ISD::SUB, then this function checks if this is a horizontal\n/// arithmetic sub.\n///\n/// This function only analyzes elements of \\p N whose indices are\n/// in range [BaseIdx, LastIdx).\n///\n/// TODO: This function was originally used to match both real and fake partial\n/// horizontal operations, but the index-matching logic is incorrect for that.\n/// See the corrected implementation in isHopBuildVector(). Can we reduce this\n/// code because it is only used for partial h-op matching now?\nstatic bool isHorizontalBinOpPart(const BuildVectorSDNode *N, unsigned Opcode,\n                                  SelectionDAG &DAG,\n                                  unsigned BaseIdx, unsigned LastIdx,\n                                  SDValue &V0, SDValue &V1) {\n  EVT VT = N->getValueType(0);\n  assert(VT.is256BitVector() && \"Only use for matching partial 256-bit h-ops\");\n  assert(BaseIdx * 2 <= LastIdx && \"Invalid Indices in input!\");\n  assert(VT.isVector() && VT.getVectorNumElements() >= LastIdx &&\n         \"Invalid Vector in input!\");\n\n  bool IsCommutable = (Opcode == ISD::ADD || Opcode == ISD::FADD);\n  bool CanFold = true;\n  unsigned ExpectedVExtractIdx = BaseIdx;\n  unsigned NumElts = LastIdx - BaseIdx;\n  V0 = DAG.getUNDEF(VT);\n  V1 = DAG.getUNDEF(VT);\n\n  // Check if N implements a horizontal binop.\n  for (unsigned i = 0, e = NumElts; i != e && CanFold; ++i) {\n    SDValue Op = N->getOperand(i + BaseIdx);\n\n    // Skip UNDEFs.\n    if (Op->isUndef()) {\n      // Update the expected vector extract index.\n      if (i * 2 == NumElts)\n        ExpectedVExtractIdx = BaseIdx;\n      ExpectedVExtractIdx += 2;\n      continue;\n    }\n\n    CanFold = Op->getOpcode() == Opcode && Op->hasOneUse();\n\n    if (!CanFold)\n      break;\n\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    // Try to match the following pattern:\n    // (BINOP (extract_vector_elt A, I), (extract_vector_elt A, I+1))\n    CanFold = (Op0.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n        Op1.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n        Op0.getOperand(0) == Op1.getOperand(0) &&\n        isa<ConstantSDNode>(Op0.getOperand(1)) &&\n        isa<ConstantSDNode>(Op1.getOperand(1)));\n    if (!CanFold)\n      break;\n\n    unsigned I0 = Op0.getConstantOperandVal(1);\n    unsigned I1 = Op1.getConstantOperandVal(1);\n\n    if (i * 2 < NumElts) {\n      if (V0.isUndef()) {\n        V0 = Op0.getOperand(0);\n        if (V0.getValueType() != VT)\n          return false;\n      }\n    } else {\n      if (V1.isUndef()) {\n        V1 = Op0.getOperand(0);\n        if (V1.getValueType() != VT)\n          return false;\n      }\n      if (i * 2 == NumElts)\n        ExpectedVExtractIdx = BaseIdx;\n    }\n\n    SDValue Expected = (i * 2 < NumElts) ? V0 : V1;\n    if (I0 == ExpectedVExtractIdx)\n      CanFold = I1 == I0 + 1 && Op0.getOperand(0) == Expected;\n    else if (IsCommutable && I1 == ExpectedVExtractIdx) {\n      // Try to match the following dag sequence:\n      // (BINOP (extract_vector_elt A, I+1), (extract_vector_elt A, I))\n      CanFold = I0 == I1 + 1 && Op1.getOperand(0) == Expected;\n    } else\n      CanFold = false;\n\n    ExpectedVExtractIdx += 2;\n  }\n\n  return CanFold;\n}\n\n/// Emit a sequence of two 128-bit horizontal add/sub followed by\n/// a concat_vector.\n///\n/// This is a helper function of LowerToHorizontalOp().\n/// This function expects two 256-bit vectors called V0 and V1.\n/// At first, each vector is split into two separate 128-bit vectors.\n/// Then, the resulting 128-bit vectors are used to implement two\n/// horizontal binary operations.\n///\n/// The kind of horizontal binary operation is defined by \\p X86Opcode.\n///\n/// \\p Mode specifies how the 128-bit parts of V0 and V1 are passed in input to\n/// the two new horizontal binop.\n/// When Mode is set, the first horizontal binop dag node would take as input\n/// the lower 128-bit of V0 and the upper 128-bit of V0. The second\n/// horizontal binop dag node would take as input the lower 128-bit of V1\n/// and the upper 128-bit of V1.\n///   Example:\n///     HADD V0_LO, V0_HI\n///     HADD V1_LO, V1_HI\n///\n/// Otherwise, the first horizontal binop dag node takes as input the lower\n/// 128-bit of V0 and the lower 128-bit of V1, and the second horizontal binop\n/// dag node takes the upper 128-bit of V0 and the upper 128-bit of V1.\n///   Example:\n///     HADD V0_LO, V1_LO\n///     HADD V0_HI, V1_HI\n///\n/// If \\p isUndefLO is set, then the algorithm propagates UNDEF to the lower\n/// 128-bits of the result. If \\p isUndefHI is set, then UNDEF is propagated to\n/// the upper 128-bits of the result.\nstatic SDValue ExpandHorizontalBinOp(const SDValue &V0, const SDValue &V1,\n                                     const SDLoc &DL, SelectionDAG &DAG,\n                                     unsigned X86Opcode, bool Mode,\n                                     bool isUndefLO, bool isUndefHI) {\n  MVT VT = V0.getSimpleValueType();\n  assert(VT.is256BitVector() && VT == V1.getSimpleValueType() &&\n         \"Invalid nodes in input!\");\n\n  unsigned NumElts = VT.getVectorNumElements();\n  SDValue V0_LO = extract128BitVector(V0, 0, DAG, DL);\n  SDValue V0_HI = extract128BitVector(V0, NumElts/2, DAG, DL);\n  SDValue V1_LO = extract128BitVector(V1, 0, DAG, DL);\n  SDValue V1_HI = extract128BitVector(V1, NumElts/2, DAG, DL);\n  MVT NewVT = V0_LO.getSimpleValueType();\n\n  SDValue LO = DAG.getUNDEF(NewVT);\n  SDValue HI = DAG.getUNDEF(NewVT);\n\n  if (Mode) {\n    // Don't emit a horizontal binop if the result is expected to be UNDEF.\n    if (!isUndefLO && !V0->isUndef())\n      LO = DAG.getNode(X86Opcode, DL, NewVT, V0_LO, V0_HI);\n    if (!isUndefHI && !V1->isUndef())\n      HI = DAG.getNode(X86Opcode, DL, NewVT, V1_LO, V1_HI);\n  } else {\n    // Don't emit a horizontal binop if the result is expected to be UNDEF.\n    if (!isUndefLO && (!V0_LO->isUndef() || !V1_LO->isUndef()))\n      LO = DAG.getNode(X86Opcode, DL, NewVT, V0_LO, V1_LO);\n\n    if (!isUndefHI && (!V0_HI->isUndef() || !V1_HI->isUndef()))\n      HI = DAG.getNode(X86Opcode, DL, NewVT, V0_HI, V1_HI);\n  }\n\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, LO, HI);\n}\n\n/// Returns true iff \\p BV builds a vector with the result equivalent to\n/// the result of ADDSUB/SUBADD operation.\n/// If true is returned then the operands of ADDSUB = Opnd0 +- Opnd1\n/// (SUBADD = Opnd0 -+ Opnd1) operation are written to the parameters\n/// \\p Opnd0 and \\p Opnd1.\nstatic bool isAddSubOrSubAdd(const BuildVectorSDNode *BV,\n                             const X86Subtarget &Subtarget, SelectionDAG &DAG,\n                             SDValue &Opnd0, SDValue &Opnd1,\n                             unsigned &NumExtracts,\n                             bool &IsSubAdd) {\n\n  MVT VT = BV->getSimpleValueType(0);\n  if (!Subtarget.hasSSE3() || !VT.isFloatingPoint())\n    return false;\n\n  unsigned NumElts = VT.getVectorNumElements();\n  SDValue InVec0 = DAG.getUNDEF(VT);\n  SDValue InVec1 = DAG.getUNDEF(VT);\n\n  NumExtracts = 0;\n\n  // Odd-numbered elements in the input build vector are obtained from\n  // adding/subtracting two integer/float elements.\n  // Even-numbered elements in the input build vector are obtained from\n  // subtracting/adding two integer/float elements.\n  unsigned Opc[2] = {0, 0};\n  for (unsigned i = 0, e = NumElts; i != e; ++i) {\n    SDValue Op = BV->getOperand(i);\n\n    // Skip 'undef' values.\n    unsigned Opcode = Op.getOpcode();\n    if (Opcode == ISD::UNDEF)\n      continue;\n\n    // Early exit if we found an unexpected opcode.\n    if (Opcode != ISD::FADD && Opcode != ISD::FSUB)\n      return false;\n\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    // Try to match the following pattern:\n    // (BINOP (extract_vector_elt A, i), (extract_vector_elt B, i))\n    // Early exit if we cannot match that sequence.\n    if (Op0.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        Op1.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        !isa<ConstantSDNode>(Op0.getOperand(1)) ||\n        Op0.getOperand(1) != Op1.getOperand(1))\n      return false;\n\n    unsigned I0 = Op0.getConstantOperandVal(1);\n    if (I0 != i)\n      return false;\n\n    // We found a valid add/sub node, make sure its the same opcode as previous\n    // elements for this parity.\n    if (Opc[i % 2] != 0 && Opc[i % 2] != Opcode)\n      return false;\n    Opc[i % 2] = Opcode;\n\n    // Update InVec0 and InVec1.\n    if (InVec0.isUndef()) {\n      InVec0 = Op0.getOperand(0);\n      if (InVec0.getSimpleValueType() != VT)\n        return false;\n    }\n    if (InVec1.isUndef()) {\n      InVec1 = Op1.getOperand(0);\n      if (InVec1.getSimpleValueType() != VT)\n        return false;\n    }\n\n    // Make sure that operands in input to each add/sub node always\n    // come from a same pair of vectors.\n    if (InVec0 != Op0.getOperand(0)) {\n      if (Opcode == ISD::FSUB)\n        return false;\n\n      // FADD is commutable. Try to commute the operands\n      // and then test again.\n      std::swap(Op0, Op1);\n      if (InVec0 != Op0.getOperand(0))\n        return false;\n    }\n\n    if (InVec1 != Op1.getOperand(0))\n      return false;\n\n    // Increment the number of extractions done.\n    ++NumExtracts;\n  }\n\n  // Ensure we have found an opcode for both parities and that they are\n  // different. Don't try to fold this build_vector into an ADDSUB/SUBADD if the\n  // inputs are undef.\n  if (!Opc[0] || !Opc[1] || Opc[0] == Opc[1] ||\n      InVec0.isUndef() || InVec1.isUndef())\n    return false;\n\n  IsSubAdd = Opc[0] == ISD::FADD;\n\n  Opnd0 = InVec0;\n  Opnd1 = InVec1;\n  return true;\n}\n\n/// Returns true if is possible to fold MUL and an idiom that has already been\n/// recognized as ADDSUB/SUBADD(\\p Opnd0, \\p Opnd1) into\n/// FMADDSUB/FMSUBADD(x, y, \\p Opnd1). If (and only if) true is returned, the\n/// operands of FMADDSUB/FMSUBADD are written to parameters \\p Opnd0, \\p Opnd1, \\p Opnd2.\n///\n/// Prior to calling this function it should be known that there is some\n/// SDNode that potentially can be replaced with an X86ISD::ADDSUB operation\n/// using \\p Opnd0 and \\p Opnd1 as operands. Also, this method is called\n/// before replacement of such SDNode with ADDSUB operation. Thus the number\n/// of \\p Opnd0 uses is expected to be equal to 2.\n/// For example, this function may be called for the following IR:\n///    %AB = fmul fast <2 x double> %A, %B\n///    %Sub = fsub fast <2 x double> %AB, %C\n///    %Add = fadd fast <2 x double> %AB, %C\n///    %Addsub = shufflevector <2 x double> %Sub, <2 x double> %Add,\n///                            <2 x i32> <i32 0, i32 3>\n/// There is a def for %Addsub here, which potentially can be replaced by\n/// X86ISD::ADDSUB operation:\n///    %Addsub = X86ISD::ADDSUB %AB, %C\n/// and such ADDSUB can further be replaced with FMADDSUB:\n///    %Addsub = FMADDSUB %A, %B, %C.\n///\n/// The main reason why this method is called before the replacement of the\n/// recognized ADDSUB idiom with ADDSUB operation is that such replacement\n/// is illegal sometimes. E.g. 512-bit ADDSUB is not available, while 512-bit\n/// FMADDSUB is.\nstatic bool isFMAddSubOrFMSubAdd(const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG,\n                                 SDValue &Opnd0, SDValue &Opnd1, SDValue &Opnd2,\n                                 unsigned ExpectedUses) {\n  if (Opnd0.getOpcode() != ISD::FMUL ||\n      !Opnd0->hasNUsesOfValue(ExpectedUses, 0) || !Subtarget.hasAnyFMA())\n    return false;\n\n  // FIXME: These checks must match the similar ones in\n  // DAGCombiner::visitFADDForFMACombine. It would be good to have one\n  // function that would answer if it is Ok to fuse MUL + ADD to FMADD\n  // or MUL + ADDSUB to FMADDSUB.\n  const TargetOptions &Options = DAG.getTarget().Options;\n  bool AllowFusion =\n      (Options.AllowFPOpFusion == FPOpFusion::Fast || Options.UnsafeFPMath);\n  if (!AllowFusion)\n    return false;\n\n  Opnd2 = Opnd1;\n  Opnd1 = Opnd0.getOperand(1);\n  Opnd0 = Opnd0.getOperand(0);\n\n  return true;\n}\n\n/// Try to fold a build_vector that performs an 'addsub' or 'fmaddsub' or\n/// 'fsubadd' operation accordingly to X86ISD::ADDSUB or X86ISD::FMADDSUB or\n/// X86ISD::FMSUBADD node.\nstatic SDValue lowerToAddSubOrFMAddSub(const BuildVectorSDNode *BV,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  SDValue Opnd0, Opnd1;\n  unsigned NumExtracts;\n  bool IsSubAdd;\n  if (!isAddSubOrSubAdd(BV, Subtarget, DAG, Opnd0, Opnd1, NumExtracts,\n                        IsSubAdd))\n    return SDValue();\n\n  MVT VT = BV->getSimpleValueType(0);\n  SDLoc DL(BV);\n\n  // Try to generate X86ISD::FMADDSUB node here.\n  SDValue Opnd2;\n  if (isFMAddSubOrFMSubAdd(Subtarget, DAG, Opnd0, Opnd1, Opnd2, NumExtracts)) {\n    unsigned Opc = IsSubAdd ? X86ISD::FMSUBADD : X86ISD::FMADDSUB;\n    return DAG.getNode(Opc, DL, VT, Opnd0, Opnd1, Opnd2);\n  }\n\n  // We only support ADDSUB.\n  if (IsSubAdd)\n    return SDValue();\n\n  // Do not generate X86ISD::ADDSUB node for 512-bit types even though\n  // the ADDSUB idiom has been successfully recognized. There are no known\n  // X86 targets with 512-bit ADDSUB instructions!\n  // 512-bit ADDSUB idiom recognition was needed only as part of FMADDSUB idiom\n  // recognition.\n  if (VT.is512BitVector())\n    return SDValue();\n\n  return DAG.getNode(X86ISD::ADDSUB, DL, VT, Opnd0, Opnd1);\n}\n\nstatic bool isHopBuildVector(const BuildVectorSDNode *BV, SelectionDAG &DAG,\n                             unsigned &HOpcode, SDValue &V0, SDValue &V1) {\n  // Initialize outputs to known values.\n  MVT VT = BV->getSimpleValueType(0);\n  HOpcode = ISD::DELETED_NODE;\n  V0 = DAG.getUNDEF(VT);\n  V1 = DAG.getUNDEF(VT);\n\n  // x86 256-bit horizontal ops are defined in a non-obvious way. Each 128-bit\n  // half of the result is calculated independently from the 128-bit halves of\n  // the inputs, so that makes the index-checking logic below more complicated.\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned GenericOpcode = ISD::DELETED_NODE;\n  unsigned Num128BitChunks = VT.is256BitVector() ? 2 : 1;\n  unsigned NumEltsIn128Bits = NumElts / Num128BitChunks;\n  unsigned NumEltsIn64Bits = NumEltsIn128Bits / 2;\n  for (unsigned i = 0; i != Num128BitChunks; ++i) {\n    for (unsigned j = 0; j != NumEltsIn128Bits; ++j) {\n      // Ignore undef elements.\n      SDValue Op = BV->getOperand(i * NumEltsIn128Bits + j);\n      if (Op.isUndef())\n        continue;\n\n      // If there's an opcode mismatch, we're done.\n      if (HOpcode != ISD::DELETED_NODE && Op.getOpcode() != GenericOpcode)\n        return false;\n\n      // Initialize horizontal opcode.\n      if (HOpcode == ISD::DELETED_NODE) {\n        GenericOpcode = Op.getOpcode();\n        switch (GenericOpcode) {\n        case ISD::ADD: HOpcode = X86ISD::HADD; break;\n        case ISD::SUB: HOpcode = X86ISD::HSUB; break;\n        case ISD::FADD: HOpcode = X86ISD::FHADD; break;\n        case ISD::FSUB: HOpcode = X86ISD::FHSUB; break;\n        default: return false;\n        }\n      }\n\n      SDValue Op0 = Op.getOperand(0);\n      SDValue Op1 = Op.getOperand(1);\n      if (Op0.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n          Op1.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n          Op0.getOperand(0) != Op1.getOperand(0) ||\n          !isa<ConstantSDNode>(Op0.getOperand(1)) ||\n          !isa<ConstantSDNode>(Op1.getOperand(1)) || !Op.hasOneUse())\n        return false;\n\n      // The source vector is chosen based on which 64-bit half of the\n      // destination vector is being calculated.\n      if (j < NumEltsIn64Bits) {\n        if (V0.isUndef())\n          V0 = Op0.getOperand(0);\n      } else {\n        if (V1.isUndef())\n          V1 = Op0.getOperand(0);\n      }\n\n      SDValue SourceVec = (j < NumEltsIn64Bits) ? V0 : V1;\n      if (SourceVec != Op0.getOperand(0))\n        return false;\n\n      // op (extract_vector_elt A, I), (extract_vector_elt A, I+1)\n      unsigned ExtIndex0 = Op0.getConstantOperandVal(1);\n      unsigned ExtIndex1 = Op1.getConstantOperandVal(1);\n      unsigned ExpectedIndex = i * NumEltsIn128Bits +\n                               (j % NumEltsIn64Bits) * 2;\n      if (ExpectedIndex == ExtIndex0 && ExtIndex1 == ExtIndex0 + 1)\n        continue;\n\n      // If this is not a commutative op, this does not match.\n      if (GenericOpcode != ISD::ADD && GenericOpcode != ISD::FADD)\n        return false;\n\n      // Addition is commutative, so try swapping the extract indexes.\n      // op (extract_vector_elt A, I+1), (extract_vector_elt A, I)\n      if (ExpectedIndex == ExtIndex1 && ExtIndex0 == ExtIndex1 + 1)\n        continue;\n\n      // Extract indexes do not match horizontal requirement.\n      return false;\n    }\n  }\n  // We matched. Opcode and operands are returned by reference as arguments.\n  return true;\n}\n\nstatic SDValue getHopForBuildVector(const BuildVectorSDNode *BV,\n                                    SelectionDAG &DAG, unsigned HOpcode,\n                                    SDValue V0, SDValue V1) {\n  // If either input vector is not the same size as the build vector,\n  // extract/insert the low bits to the correct size.\n  // This is free (examples: zmm --> xmm, xmm --> ymm).\n  MVT VT = BV->getSimpleValueType(0);\n  unsigned Width = VT.getSizeInBits();\n  if (V0.getValueSizeInBits() > Width)\n    V0 = extractSubVector(V0, 0, DAG, SDLoc(BV), Width);\n  else if (V0.getValueSizeInBits() < Width)\n    V0 = insertSubVector(DAG.getUNDEF(VT), V0, 0, DAG, SDLoc(BV), Width);\n\n  if (V1.getValueSizeInBits() > Width)\n    V1 = extractSubVector(V1, 0, DAG, SDLoc(BV), Width);\n  else if (V1.getValueSizeInBits() < Width)\n    V1 = insertSubVector(DAG.getUNDEF(VT), V1, 0, DAG, SDLoc(BV), Width);\n\n  unsigned NumElts = VT.getVectorNumElements();\n  APInt DemandedElts = APInt::getAllOnesValue(NumElts);\n  for (unsigned i = 0; i != NumElts; ++i)\n    if (BV->getOperand(i).isUndef())\n      DemandedElts.clearBit(i);\n\n  // If we don't need the upper xmm, then perform as a xmm hop.\n  unsigned HalfNumElts = NumElts / 2;\n  if (VT.is256BitVector() && DemandedElts.lshr(HalfNumElts) == 0) {\n    MVT HalfVT = VT.getHalfNumVectorElementsVT();\n    V0 = extractSubVector(V0, 0, DAG, SDLoc(BV), 128);\n    V1 = extractSubVector(V1, 0, DAG, SDLoc(BV), 128);\n    SDValue Half = DAG.getNode(HOpcode, SDLoc(BV), HalfVT, V0, V1);\n    return insertSubVector(DAG.getUNDEF(VT), Half, 0, DAG, SDLoc(BV), 256);\n  }\n\n  return DAG.getNode(HOpcode, SDLoc(BV), VT, V0, V1);\n}\n\n/// Lower BUILD_VECTOR to a horizontal add/sub operation if possible.\nstatic SDValue LowerToHorizontalOp(const BuildVectorSDNode *BV,\n                                   const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  // We need at least 2 non-undef elements to make this worthwhile by default.\n  unsigned NumNonUndefs =\n      count_if(BV->op_values(), [](SDValue V) { return !V.isUndef(); });\n  if (NumNonUndefs < 2)\n    return SDValue();\n\n  // There are 4 sets of horizontal math operations distinguished by type:\n  // int/FP at 128-bit/256-bit. Each type was introduced with a different\n  // subtarget feature. Try to match those \"native\" patterns first.\n  MVT VT = BV->getSimpleValueType(0);\n  if (((VT == MVT::v4f32 || VT == MVT::v2f64) && Subtarget.hasSSE3()) ||\n      ((VT == MVT::v8i16 || VT == MVT::v4i32) && Subtarget.hasSSSE3()) ||\n      ((VT == MVT::v8f32 || VT == MVT::v4f64) && Subtarget.hasAVX()) ||\n      ((VT == MVT::v16i16 || VT == MVT::v8i32) && Subtarget.hasAVX2())) {\n    unsigned HOpcode;\n    SDValue V0, V1;\n    if (isHopBuildVector(BV, DAG, HOpcode, V0, V1))\n      return getHopForBuildVector(BV, DAG, HOpcode, V0, V1);\n  }\n\n  // Try harder to match 256-bit ops by using extract/concat.\n  if (!Subtarget.hasAVX() || !VT.is256BitVector())\n    return SDValue();\n\n  // Count the number of UNDEF operands in the build_vector in input.\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned Half = NumElts / 2;\n  unsigned NumUndefsLO = 0;\n  unsigned NumUndefsHI = 0;\n  for (unsigned i = 0, e = Half; i != e; ++i)\n    if (BV->getOperand(i)->isUndef())\n      NumUndefsLO++;\n\n  for (unsigned i = Half, e = NumElts; i != e; ++i)\n    if (BV->getOperand(i)->isUndef())\n      NumUndefsHI++;\n\n  SDLoc DL(BV);\n  SDValue InVec0, InVec1;\n  if (VT == MVT::v8i32 || VT == MVT::v16i16) {\n    SDValue InVec2, InVec3;\n    unsigned X86Opcode;\n    bool CanFold = true;\n\n    if (isHorizontalBinOpPart(BV, ISD::ADD, DAG, 0, Half, InVec0, InVec1) &&\n        isHorizontalBinOpPart(BV, ISD::ADD, DAG, Half, NumElts, InVec2,\n                              InVec3) &&\n        ((InVec0.isUndef() || InVec2.isUndef()) || InVec0 == InVec2) &&\n        ((InVec1.isUndef() || InVec3.isUndef()) || InVec1 == InVec3))\n      X86Opcode = X86ISD::HADD;\n    else if (isHorizontalBinOpPart(BV, ISD::SUB, DAG, 0, Half, InVec0,\n                                   InVec1) &&\n             isHorizontalBinOpPart(BV, ISD::SUB, DAG, Half, NumElts, InVec2,\n                                   InVec3) &&\n             ((InVec0.isUndef() || InVec2.isUndef()) || InVec0 == InVec2) &&\n             ((InVec1.isUndef() || InVec3.isUndef()) || InVec1 == InVec3))\n      X86Opcode = X86ISD::HSUB;\n    else\n      CanFold = false;\n\n    if (CanFold) {\n      // Do not try to expand this build_vector into a pair of horizontal\n      // add/sub if we can emit a pair of scalar add/sub.\n      if (NumUndefsLO + 1 == Half || NumUndefsHI + 1 == Half)\n        return SDValue();\n\n      // Convert this build_vector into a pair of horizontal binops followed by\n      // a concat vector. We must adjust the outputs from the partial horizontal\n      // matching calls above to account for undefined vector halves.\n      SDValue V0 = InVec0.isUndef() ? InVec2 : InVec0;\n      SDValue V1 = InVec1.isUndef() ? InVec3 : InVec1;\n      assert((!V0.isUndef() || !V1.isUndef()) && \"Horizontal-op of undefs?\");\n      bool isUndefLO = NumUndefsLO == Half;\n      bool isUndefHI = NumUndefsHI == Half;\n      return ExpandHorizontalBinOp(V0, V1, DL, DAG, X86Opcode, false, isUndefLO,\n                                   isUndefHI);\n    }\n  }\n\n  if (VT == MVT::v8f32 || VT == MVT::v4f64 || VT == MVT::v8i32 ||\n      VT == MVT::v16i16) {\n    unsigned X86Opcode;\n    if (isHorizontalBinOpPart(BV, ISD::ADD, DAG, 0, NumElts, InVec0, InVec1))\n      X86Opcode = X86ISD::HADD;\n    else if (isHorizontalBinOpPart(BV, ISD::SUB, DAG, 0, NumElts, InVec0,\n                                   InVec1))\n      X86Opcode = X86ISD::HSUB;\n    else if (isHorizontalBinOpPart(BV, ISD::FADD, DAG, 0, NumElts, InVec0,\n                                   InVec1))\n      X86Opcode = X86ISD::FHADD;\n    else if (isHorizontalBinOpPart(BV, ISD::FSUB, DAG, 0, NumElts, InVec0,\n                                   InVec1))\n      X86Opcode = X86ISD::FHSUB;\n    else\n      return SDValue();\n\n    // Don't try to expand this build_vector into a pair of horizontal add/sub\n    // if we can simply emit a pair of scalar add/sub.\n    if (NumUndefsLO + 1 == Half || NumUndefsHI + 1 == Half)\n      return SDValue();\n\n    // Convert this build_vector into two horizontal add/sub followed by\n    // a concat vector.\n    bool isUndefLO = NumUndefsLO == Half;\n    bool isUndefHI = NumUndefsHI == Half;\n    return ExpandHorizontalBinOp(InVec0, InVec1, DL, DAG, X86Opcode, true,\n                                 isUndefLO, isUndefHI);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue LowerShift(SDValue Op, const X86Subtarget &Subtarget,\n                          SelectionDAG &DAG);\n\n/// If a BUILD_VECTOR's source elements all apply the same bit operation and\n/// one of their operands is constant, lower to a pair of BUILD_VECTOR and\n/// just apply the bit to the vectors.\n/// NOTE: Its not in our interest to start make a general purpose vectorizer\n/// from this, but enough scalar bit operations are created from the later\n/// legalization + scalarization stages to need basic support.\nstatic SDValue lowerBuildVectorToBitOp(BuildVectorSDNode *Op,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  SDLoc DL(Op);\n  MVT VT = Op->getSimpleValueType(0);\n  unsigned NumElems = VT.getVectorNumElements();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // Check that all elements have the same opcode.\n  // TODO: Should we allow UNDEFS and if so how many?\n  unsigned Opcode = Op->getOperand(0).getOpcode();\n  for (unsigned i = 1; i < NumElems; ++i)\n    if (Opcode != Op->getOperand(i).getOpcode())\n      return SDValue();\n\n  // TODO: We may be able to add support for other Ops (ADD/SUB + shifts).\n  bool IsShift = false;\n  switch (Opcode) {\n  default:\n    return SDValue();\n  case ISD::SHL:\n  case ISD::SRL:\n  case ISD::SRA:\n    IsShift = true;\n    break;\n  case ISD::AND:\n  case ISD::XOR:\n  case ISD::OR:\n    // Don't do this if the buildvector is a splat - we'd replace one\n    // constant with an entire vector.\n    if (Op->getSplatValue())\n      return SDValue();\n    if (!TLI.isOperationLegalOrPromote(Opcode, VT))\n      return SDValue();\n    break;\n  }\n\n  SmallVector<SDValue, 4> LHSElts, RHSElts;\n  for (SDValue Elt : Op->ops()) {\n    SDValue LHS = Elt.getOperand(0);\n    SDValue RHS = Elt.getOperand(1);\n\n    // We expect the canonicalized RHS operand to be the constant.\n    if (!isa<ConstantSDNode>(RHS))\n      return SDValue();\n\n    // Extend shift amounts.\n    if (RHS.getValueSizeInBits() != VT.getScalarSizeInBits()) {\n      if (!IsShift)\n        return SDValue();\n      RHS = DAG.getZExtOrTrunc(RHS, DL, VT.getScalarType());\n    }\n\n    LHSElts.push_back(LHS);\n    RHSElts.push_back(RHS);\n  }\n\n  // Limit to shifts by uniform immediates.\n  // TODO: Only accept vXi8/vXi64 special cases?\n  // TODO: Permit non-uniform XOP/AVX2/MULLO cases?\n  if (IsShift && any_of(RHSElts, [&](SDValue V) { return RHSElts[0] != V; }))\n    return SDValue();\n\n  SDValue LHS = DAG.getBuildVector(VT, DL, LHSElts);\n  SDValue RHS = DAG.getBuildVector(VT, DL, RHSElts);\n  SDValue Res = DAG.getNode(Opcode, DL, VT, LHS, RHS);\n\n  if (!IsShift)\n    return Res;\n\n  // Immediately lower the shift to ensure the constant build vector doesn't\n  // get converted to a constant pool before the shift is lowered.\n  return LowerShift(Res, Subtarget, DAG);\n}\n\n/// Create a vector constant without a load. SSE/AVX provide the bare minimum\n/// functionality to do this, so it's all zeros, all ones, or some derivation\n/// that is cheap to calculate.\nstatic SDValue materializeVectorConstant(SDValue Op, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  // Vectors containing all zeros can be matched by pxor and xorps.\n  if (ISD::isBuildVectorAllZeros(Op.getNode()))\n    return Op;\n\n  // Vectors containing all ones can be matched by pcmpeqd on 128-bit width\n  // vectors or broken into v4i32 operations on 256-bit vectors. AVX2 can use\n  // vpcmpeqd on 256-bit vectors.\n  if (Subtarget.hasSSE2() && ISD::isBuildVectorAllOnes(Op.getNode())) {\n    if (VT == MVT::v4i32 || VT == MVT::v8i32 || VT == MVT::v16i32)\n      return Op;\n\n    return getOnesVector(VT, DAG, DL);\n  }\n\n  return SDValue();\n}\n\n/// Look for opportunities to create a VPERMV/VPERMILPV/PSHUFB variable permute\n/// from a vector of source values and a vector of extraction indices.\n/// The vectors might be manipulated to match the type of the permute op.\nstatic SDValue createVariablePermute(MVT VT, SDValue SrcVec, SDValue IndicesVec,\n                                     SDLoc &DL, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  MVT ShuffleVT = VT;\n  EVT IndicesVT = EVT(VT).changeVectorElementTypeToInteger();\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned SizeInBits = VT.getSizeInBits();\n\n  // Adjust IndicesVec to match VT size.\n  assert(IndicesVec.getValueType().getVectorNumElements() >= NumElts &&\n         \"Illegal variable permute mask size\");\n  if (IndicesVec.getValueType().getVectorNumElements() > NumElts)\n    IndicesVec = extractSubVector(IndicesVec, 0, DAG, SDLoc(IndicesVec),\n                                  NumElts * VT.getScalarSizeInBits());\n  IndicesVec = DAG.getZExtOrTrunc(IndicesVec, SDLoc(IndicesVec), IndicesVT);\n\n  // Handle SrcVec that don't match VT type.\n  if (SrcVec.getValueSizeInBits() != SizeInBits) {\n    if ((SrcVec.getValueSizeInBits() % SizeInBits) == 0) {\n      // Handle larger SrcVec by treating it as a larger permute.\n      unsigned Scale = SrcVec.getValueSizeInBits() / SizeInBits;\n      VT = MVT::getVectorVT(VT.getScalarType(), Scale * NumElts);\n      IndicesVT = EVT(VT).changeVectorElementTypeToInteger();\n      IndicesVec = widenSubVector(IndicesVT.getSimpleVT(), IndicesVec, false,\n                                  Subtarget, DAG, SDLoc(IndicesVec));\n      SDValue NewSrcVec =\n          createVariablePermute(VT, SrcVec, IndicesVec, DL, DAG, Subtarget);\n      if (NewSrcVec)\n        return extractSubVector(NewSrcVec, 0, DAG, DL, SizeInBits);\n      return SDValue();\n    } else if (SrcVec.getValueSizeInBits() < SizeInBits) {\n      // Widen smaller SrcVec to match VT.\n      SrcVec = widenSubVector(VT, SrcVec, false, Subtarget, DAG, SDLoc(SrcVec));\n    } else\n      return SDValue();\n  }\n\n  auto ScaleIndices = [&DAG](SDValue Idx, uint64_t Scale) {\n    assert(isPowerOf2_64(Scale) && \"Illegal variable permute shuffle scale\");\n    EVT SrcVT = Idx.getValueType();\n    unsigned NumDstBits = SrcVT.getScalarSizeInBits() / Scale;\n    uint64_t IndexScale = 0;\n    uint64_t IndexOffset = 0;\n\n    // If we're scaling a smaller permute op, then we need to repeat the\n    // indices, scaling and offsetting them as well.\n    // e.g. v4i32 -> v16i8 (Scale = 4)\n    // IndexScale = v4i32 Splat(4 << 24 | 4 << 16 | 4 << 8 | 4)\n    // IndexOffset = v4i32 Splat(3 << 24 | 2 << 16 | 1 << 8 | 0)\n    for (uint64_t i = 0; i != Scale; ++i) {\n      IndexScale |= Scale << (i * NumDstBits);\n      IndexOffset |= i << (i * NumDstBits);\n    }\n\n    Idx = DAG.getNode(ISD::MUL, SDLoc(Idx), SrcVT, Idx,\n                      DAG.getConstant(IndexScale, SDLoc(Idx), SrcVT));\n    Idx = DAG.getNode(ISD::ADD, SDLoc(Idx), SrcVT, Idx,\n                      DAG.getConstant(IndexOffset, SDLoc(Idx), SrcVT));\n    return Idx;\n  };\n\n  unsigned Opcode = 0;\n  switch (VT.SimpleTy) {\n  default:\n    break;\n  case MVT::v16i8:\n    if (Subtarget.hasSSSE3())\n      Opcode = X86ISD::PSHUFB;\n    break;\n  case MVT::v8i16:\n    if (Subtarget.hasVLX() && Subtarget.hasBWI())\n      Opcode = X86ISD::VPERMV;\n    else if (Subtarget.hasSSSE3()) {\n      Opcode = X86ISD::PSHUFB;\n      ShuffleVT = MVT::v16i8;\n    }\n    break;\n  case MVT::v4f32:\n  case MVT::v4i32:\n    if (Subtarget.hasAVX()) {\n      Opcode = X86ISD::VPERMILPV;\n      ShuffleVT = MVT::v4f32;\n    } else if (Subtarget.hasSSSE3()) {\n      Opcode = X86ISD::PSHUFB;\n      ShuffleVT = MVT::v16i8;\n    }\n    break;\n  case MVT::v2f64:\n  case MVT::v2i64:\n    if (Subtarget.hasAVX()) {\n      // VPERMILPD selects using bit#1 of the index vector, so scale IndicesVec.\n      IndicesVec = DAG.getNode(ISD::ADD, DL, IndicesVT, IndicesVec, IndicesVec);\n      Opcode = X86ISD::VPERMILPV;\n      ShuffleVT = MVT::v2f64;\n    } else if (Subtarget.hasSSE41()) {\n      // SSE41 can compare v2i64 - select between indices 0 and 1.\n      return DAG.getSelectCC(\n          DL, IndicesVec,\n          getZeroVector(IndicesVT.getSimpleVT(), Subtarget, DAG, DL),\n          DAG.getVectorShuffle(VT, DL, SrcVec, SrcVec, {0, 0}),\n          DAG.getVectorShuffle(VT, DL, SrcVec, SrcVec, {1, 1}),\n          ISD::CondCode::SETEQ);\n    }\n    break;\n  case MVT::v32i8:\n    if (Subtarget.hasVLX() && Subtarget.hasVBMI())\n      Opcode = X86ISD::VPERMV;\n    else if (Subtarget.hasXOP()) {\n      SDValue LoSrc = extract128BitVector(SrcVec, 0, DAG, DL);\n      SDValue HiSrc = extract128BitVector(SrcVec, 16, DAG, DL);\n      SDValue LoIdx = extract128BitVector(IndicesVec, 0, DAG, DL);\n      SDValue HiIdx = extract128BitVector(IndicesVec, 16, DAG, DL);\n      return DAG.getNode(\n          ISD::CONCAT_VECTORS, DL, VT,\n          DAG.getNode(X86ISD::VPPERM, DL, MVT::v16i8, LoSrc, HiSrc, LoIdx),\n          DAG.getNode(X86ISD::VPPERM, DL, MVT::v16i8, LoSrc, HiSrc, HiIdx));\n    } else if (Subtarget.hasAVX()) {\n      SDValue Lo = extract128BitVector(SrcVec, 0, DAG, DL);\n      SDValue Hi = extract128BitVector(SrcVec, 16, DAG, DL);\n      SDValue LoLo = DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Lo, Lo);\n      SDValue HiHi = DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Hi, Hi);\n      auto PSHUFBBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                              ArrayRef<SDValue> Ops) {\n        // Permute Lo and Hi and then select based on index range.\n        // This works as SHUFB uses bits[3:0] to permute elements and we don't\n        // care about the bit[7] as its just an index vector.\n        SDValue Idx = Ops[2];\n        EVT VT = Idx.getValueType();\n        return DAG.getSelectCC(DL, Idx, DAG.getConstant(15, DL, VT),\n                               DAG.getNode(X86ISD::PSHUFB, DL, VT, Ops[1], Idx),\n                               DAG.getNode(X86ISD::PSHUFB, DL, VT, Ops[0], Idx),\n                               ISD::CondCode::SETGT);\n      };\n      SDValue Ops[] = {LoLo, HiHi, IndicesVec};\n      return SplitOpsAndApply(DAG, Subtarget, DL, MVT::v32i8, Ops,\n                              PSHUFBBuilder);\n    }\n    break;\n  case MVT::v16i16:\n    if (Subtarget.hasVLX() && Subtarget.hasBWI())\n      Opcode = X86ISD::VPERMV;\n    else if (Subtarget.hasAVX()) {\n      // Scale to v32i8 and perform as v32i8.\n      IndicesVec = ScaleIndices(IndicesVec, 2);\n      return DAG.getBitcast(\n          VT, createVariablePermute(\n                  MVT::v32i8, DAG.getBitcast(MVT::v32i8, SrcVec),\n                  DAG.getBitcast(MVT::v32i8, IndicesVec), DL, DAG, Subtarget));\n    }\n    break;\n  case MVT::v8f32:\n  case MVT::v8i32:\n    if (Subtarget.hasAVX2())\n      Opcode = X86ISD::VPERMV;\n    else if (Subtarget.hasAVX()) {\n      SrcVec = DAG.getBitcast(MVT::v8f32, SrcVec);\n      SDValue LoLo = DAG.getVectorShuffle(MVT::v8f32, DL, SrcVec, SrcVec,\n                                          {0, 1, 2, 3, 0, 1, 2, 3});\n      SDValue HiHi = DAG.getVectorShuffle(MVT::v8f32, DL, SrcVec, SrcVec,\n                                          {4, 5, 6, 7, 4, 5, 6, 7});\n      if (Subtarget.hasXOP())\n        return DAG.getBitcast(\n            VT, DAG.getNode(X86ISD::VPERMIL2, DL, MVT::v8f32, LoLo, HiHi,\n                            IndicesVec, DAG.getTargetConstant(0, DL, MVT::i8)));\n      // Permute Lo and Hi and then select based on index range.\n      // This works as VPERMILPS only uses index bits[0:1] to permute elements.\n      SDValue Res = DAG.getSelectCC(\n          DL, IndicesVec, DAG.getConstant(3, DL, MVT::v8i32),\n          DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v8f32, HiHi, IndicesVec),\n          DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v8f32, LoLo, IndicesVec),\n          ISD::CondCode::SETGT);\n      return DAG.getBitcast(VT, Res);\n    }\n    break;\n  case MVT::v4i64:\n  case MVT::v4f64:\n    if (Subtarget.hasAVX512()) {\n      if (!Subtarget.hasVLX()) {\n        MVT WidenSrcVT = MVT::getVectorVT(VT.getScalarType(), 8);\n        SrcVec = widenSubVector(WidenSrcVT, SrcVec, false, Subtarget, DAG,\n                                SDLoc(SrcVec));\n        IndicesVec = widenSubVector(MVT::v8i64, IndicesVec, false, Subtarget,\n                                    DAG, SDLoc(IndicesVec));\n        SDValue Res = createVariablePermute(WidenSrcVT, SrcVec, IndicesVec, DL,\n                                            DAG, Subtarget);\n        return extract256BitVector(Res, 0, DAG, DL);\n      }\n      Opcode = X86ISD::VPERMV;\n    } else if (Subtarget.hasAVX()) {\n      SrcVec = DAG.getBitcast(MVT::v4f64, SrcVec);\n      SDValue LoLo =\n          DAG.getVectorShuffle(MVT::v4f64, DL, SrcVec, SrcVec, {0, 1, 0, 1});\n      SDValue HiHi =\n          DAG.getVectorShuffle(MVT::v4f64, DL, SrcVec, SrcVec, {2, 3, 2, 3});\n      // VPERMIL2PD selects with bit#1 of the index vector, so scale IndicesVec.\n      IndicesVec = DAG.getNode(ISD::ADD, DL, IndicesVT, IndicesVec, IndicesVec);\n      if (Subtarget.hasXOP())\n        return DAG.getBitcast(\n            VT, DAG.getNode(X86ISD::VPERMIL2, DL, MVT::v4f64, LoLo, HiHi,\n                            IndicesVec, DAG.getTargetConstant(0, DL, MVT::i8)));\n      // Permute Lo and Hi and then select based on index range.\n      // This works as VPERMILPD only uses index bit[1] to permute elements.\n      SDValue Res = DAG.getSelectCC(\n          DL, IndicesVec, DAG.getConstant(2, DL, MVT::v4i64),\n          DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v4f64, HiHi, IndicesVec),\n          DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v4f64, LoLo, IndicesVec),\n          ISD::CondCode::SETGT);\n      return DAG.getBitcast(VT, Res);\n    }\n    break;\n  case MVT::v64i8:\n    if (Subtarget.hasVBMI())\n      Opcode = X86ISD::VPERMV;\n    break;\n  case MVT::v32i16:\n    if (Subtarget.hasBWI())\n      Opcode = X86ISD::VPERMV;\n    break;\n  case MVT::v16f32:\n  case MVT::v16i32:\n  case MVT::v8f64:\n  case MVT::v8i64:\n    if (Subtarget.hasAVX512())\n      Opcode = X86ISD::VPERMV;\n    break;\n  }\n  if (!Opcode)\n    return SDValue();\n\n  assert((VT.getSizeInBits() == ShuffleVT.getSizeInBits()) &&\n         (VT.getScalarSizeInBits() % ShuffleVT.getScalarSizeInBits()) == 0 &&\n         \"Illegal variable permute shuffle type\");\n\n  uint64_t Scale = VT.getScalarSizeInBits() / ShuffleVT.getScalarSizeInBits();\n  if (Scale > 1)\n    IndicesVec = ScaleIndices(IndicesVec, Scale);\n\n  EVT ShuffleIdxVT = EVT(ShuffleVT).changeVectorElementTypeToInteger();\n  IndicesVec = DAG.getBitcast(ShuffleIdxVT, IndicesVec);\n\n  SrcVec = DAG.getBitcast(ShuffleVT, SrcVec);\n  SDValue Res = Opcode == X86ISD::VPERMV\n                    ? DAG.getNode(Opcode, DL, ShuffleVT, IndicesVec, SrcVec)\n                    : DAG.getNode(Opcode, DL, ShuffleVT, SrcVec, IndicesVec);\n  return DAG.getBitcast(VT, Res);\n}\n\n// Tries to lower a BUILD_VECTOR composed of extract-extract chains that can be\n// reasoned to be a permutation of a vector by indices in a non-constant vector.\n// (build_vector (extract_elt V, (extract_elt I, 0)),\n//               (extract_elt V, (extract_elt I, 1)),\n//                    ...\n// ->\n// (vpermv I, V)\n//\n// TODO: Handle undefs\n// TODO: Utilize pshufb and zero mask blending to support more efficient\n// construction of vectors with constant-0 elements.\nstatic SDValue\nLowerBUILD_VECTORAsVariablePermute(SDValue V, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  SDValue SrcVec, IndicesVec;\n  // Check for a match of the permute source vector and permute index elements.\n  // This is done by checking that the i-th build_vector operand is of the form:\n  // (extract_elt SrcVec, (extract_elt IndicesVec, i)).\n  for (unsigned Idx = 0, E = V.getNumOperands(); Idx != E; ++Idx) {\n    SDValue Op = V.getOperand(Idx);\n    if (Op.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return SDValue();\n\n    // If this is the first extract encountered in V, set the source vector,\n    // otherwise verify the extract is from the previously defined source\n    // vector.\n    if (!SrcVec)\n      SrcVec = Op.getOperand(0);\n    else if (SrcVec != Op.getOperand(0))\n      return SDValue();\n    SDValue ExtractedIndex = Op->getOperand(1);\n    // Peek through extends.\n    if (ExtractedIndex.getOpcode() == ISD::ZERO_EXTEND ||\n        ExtractedIndex.getOpcode() == ISD::SIGN_EXTEND)\n      ExtractedIndex = ExtractedIndex.getOperand(0);\n    if (ExtractedIndex.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return SDValue();\n\n    // If this is the first extract from the index vector candidate, set the\n    // indices vector, otherwise verify the extract is from the previously\n    // defined indices vector.\n    if (!IndicesVec)\n      IndicesVec = ExtractedIndex.getOperand(0);\n    else if (IndicesVec != ExtractedIndex.getOperand(0))\n      return SDValue();\n\n    auto *PermIdx = dyn_cast<ConstantSDNode>(ExtractedIndex.getOperand(1));\n    if (!PermIdx || PermIdx->getAPIntValue() != Idx)\n      return SDValue();\n  }\n\n  SDLoc DL(V);\n  MVT VT = V.getSimpleValueType();\n  return createVariablePermute(VT, SrcVec, IndicesVec, DL, DAG, Subtarget);\n}\n\nSDValue\nX86TargetLowering::LowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG) const {\n  SDLoc dl(Op);\n\n  MVT VT = Op.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  unsigned NumElems = Op.getNumOperands();\n\n  // Generate vectors for predicate vectors.\n  if (VT.getVectorElementType() == MVT::i1 && Subtarget.hasAVX512())\n    return LowerBUILD_VECTORvXi1(Op, DAG, Subtarget);\n\n  if (SDValue VectorConstant = materializeVectorConstant(Op, DAG, Subtarget))\n    return VectorConstant;\n\n  unsigned EVTBits = EltVT.getSizeInBits();\n  APInt UndefMask = APInt::getNullValue(NumElems);\n  APInt ZeroMask = APInt::getNullValue(NumElems);\n  APInt NonZeroMask = APInt::getNullValue(NumElems);\n  bool IsAllConstants = true;\n  SmallSet<SDValue, 8> Values;\n  unsigned NumConstants = NumElems;\n  for (unsigned i = 0; i < NumElems; ++i) {\n    SDValue Elt = Op.getOperand(i);\n    if (Elt.isUndef()) {\n      UndefMask.setBit(i);\n      continue;\n    }\n    Values.insert(Elt);\n    if (!isa<ConstantSDNode>(Elt) && !isa<ConstantFPSDNode>(Elt)) {\n      IsAllConstants = false;\n      NumConstants--;\n    }\n    if (X86::isZeroNode(Elt)) {\n      ZeroMask.setBit(i);\n    } else {\n      NonZeroMask.setBit(i);\n    }\n  }\n\n  // All undef vector. Return an UNDEF. All zero vectors were handled above.\n  if (NonZeroMask == 0) {\n    assert(UndefMask.isAllOnesValue() && \"Fully undef mask expected\");\n    return DAG.getUNDEF(VT);\n  }\n\n  BuildVectorSDNode *BV = cast<BuildVectorSDNode>(Op.getNode());\n\n  // If the upper elts of a ymm/zmm are undef/zero then we might be better off\n  // lowering to a smaller build vector and padding with undef/zero.\n  if ((VT.is256BitVector() || VT.is512BitVector()) &&\n      !isFoldableUseOfShuffle(BV)) {\n    unsigned UpperElems = NumElems / 2;\n    APInt UndefOrZeroMask = UndefMask | ZeroMask;\n    unsigned NumUpperUndefsOrZeros = UndefOrZeroMask.countLeadingOnes();\n    if (NumUpperUndefsOrZeros >= UpperElems) {\n      if (VT.is512BitVector() &&\n          NumUpperUndefsOrZeros >= (NumElems - (NumElems / 4)))\n        UpperElems = NumElems - (NumElems / 4);\n      bool UndefUpper = UndefMask.countLeadingOnes() >= UpperElems;\n      MVT LowerVT = MVT::getVectorVT(EltVT, NumElems - UpperElems);\n      SDValue NewBV =\n          DAG.getBuildVector(LowerVT, dl, Op->ops().drop_back(UpperElems));\n      return widenSubVector(VT, NewBV, !UndefUpper, Subtarget, DAG, dl);\n    }\n  }\n\n  if (SDValue AddSub = lowerToAddSubOrFMAddSub(BV, Subtarget, DAG))\n    return AddSub;\n  if (SDValue HorizontalOp = LowerToHorizontalOp(BV, Subtarget, DAG))\n    return HorizontalOp;\n  if (SDValue Broadcast = lowerBuildVectorAsBroadcast(BV, Subtarget, DAG))\n    return Broadcast;\n  if (SDValue BitOp = lowerBuildVectorToBitOp(BV, Subtarget, DAG))\n    return BitOp;\n\n  unsigned NumZero = ZeroMask.countPopulation();\n  unsigned NumNonZero = NonZeroMask.countPopulation();\n\n  // If we are inserting one variable into a vector of non-zero constants, try\n  // to avoid loading each constant element as a scalar. Load the constants as a\n  // vector and then insert the variable scalar element. If insertion is not\n  // supported, fall back to a shuffle to get the scalar blended with the\n  // constants. Insertion into a zero vector is handled as a special-case\n  // somewhere below here.\n  if (NumConstants == NumElems - 1 && NumNonZero != 1 &&\n      (isOperationLegalOrCustom(ISD::INSERT_VECTOR_ELT, VT) ||\n       isOperationLegalOrCustom(ISD::VECTOR_SHUFFLE, VT))) {\n    // Create an all-constant vector. The variable element in the old\n    // build vector is replaced by undef in the constant vector. Save the\n    // variable scalar element and its index for use in the insertelement.\n    LLVMContext &Context = *DAG.getContext();\n    Type *EltType = Op.getValueType().getScalarType().getTypeForEVT(Context);\n    SmallVector<Constant *, 16> ConstVecOps(NumElems, UndefValue::get(EltType));\n    SDValue VarElt;\n    SDValue InsIndex;\n    for (unsigned i = 0; i != NumElems; ++i) {\n      SDValue Elt = Op.getOperand(i);\n      if (auto *C = dyn_cast<ConstantSDNode>(Elt))\n        ConstVecOps[i] = ConstantInt::get(Context, C->getAPIntValue());\n      else if (auto *C = dyn_cast<ConstantFPSDNode>(Elt))\n        ConstVecOps[i] = ConstantFP::get(Context, C->getValueAPF());\n      else if (!Elt.isUndef()) {\n        assert(!VarElt.getNode() && !InsIndex.getNode() &&\n               \"Expected one variable element in this vector\");\n        VarElt = Elt;\n        InsIndex = DAG.getVectorIdxConstant(i, dl);\n      }\n    }\n    Constant *CV = ConstantVector::get(ConstVecOps);\n    SDValue DAGConstVec = DAG.getConstantPool(CV, VT);\n\n    // The constants we just created may not be legal (eg, floating point). We\n    // must lower the vector right here because we can not guarantee that we'll\n    // legalize it before loading it. This is also why we could not just create\n    // a new build vector here. If the build vector contains illegal constants,\n    // it could get split back up into a series of insert elements.\n    // TODO: Improve this by using shorter loads with broadcast/VZEXT_LOAD.\n    SDValue LegalDAGConstVec = LowerConstantPool(DAGConstVec, DAG);\n    MachineFunction &MF = DAG.getMachineFunction();\n    MachinePointerInfo MPI = MachinePointerInfo::getConstantPool(MF);\n    SDValue Ld = DAG.getLoad(VT, dl, DAG.getEntryNode(), LegalDAGConstVec, MPI);\n    unsigned InsertC = cast<ConstantSDNode>(InsIndex)->getZExtValue();\n    unsigned NumEltsInLow128Bits = 128 / VT.getScalarSizeInBits();\n    if (InsertC < NumEltsInLow128Bits)\n      return DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, VT, Ld, VarElt, InsIndex);\n\n    // There's no good way to insert into the high elements of a >128-bit\n    // vector, so use shuffles to avoid an extract/insert sequence.\n    assert(VT.getSizeInBits() > 128 && \"Invalid insertion index?\");\n    assert(Subtarget.hasAVX() && \"Must have AVX with >16-byte vector\");\n    SmallVector<int, 8> ShuffleMask;\n    unsigned NumElts = VT.getVectorNumElements();\n    for (unsigned i = 0; i != NumElts; ++i)\n      ShuffleMask.push_back(i == InsertC ? NumElts : i);\n    SDValue S2V = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, VarElt);\n    return DAG.getVectorShuffle(VT, dl, Ld, S2V, ShuffleMask);\n  }\n\n  // Special case for single non-zero, non-undef, element.\n  if (NumNonZero == 1) {\n    unsigned Idx = NonZeroMask.countTrailingZeros();\n    SDValue Item = Op.getOperand(Idx);\n\n    // If we have a constant or non-constant insertion into the low element of\n    // a vector, we can do this with SCALAR_TO_VECTOR + shuffle of zero into\n    // the rest of the elements.  This will be matched as movd/movq/movss/movsd\n    // depending on what the source datatype is.\n    if (Idx == 0) {\n      if (NumZero == 0)\n        return DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Item);\n\n      if (EltVT == MVT::i32 || EltVT == MVT::f32 || EltVT == MVT::f64 ||\n          (EltVT == MVT::i64 && Subtarget.is64Bit())) {\n        assert((VT.is128BitVector() || VT.is256BitVector() ||\n                VT.is512BitVector()) &&\n               \"Expected an SSE value type!\");\n        Item = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Item);\n        // Turn it into a MOVL (i.e. movss, movsd, or movd) to a zero vector.\n        return getShuffleVectorZeroOrUndef(Item, 0, true, Subtarget, DAG);\n      }\n\n      // We can't directly insert an i8 or i16 into a vector, so zero extend\n      // it to i32 first.\n      if (EltVT == MVT::i16 || EltVT == MVT::i8) {\n        Item = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, Item);\n        MVT ShufVT = MVT::getVectorVT(MVT::i32, VT.getSizeInBits()/32);\n        Item = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, ShufVT, Item);\n        Item = getShuffleVectorZeroOrUndef(Item, 0, true, Subtarget, DAG);\n        return DAG.getBitcast(VT, Item);\n      }\n    }\n\n    // Is it a vector logical left shift?\n    if (NumElems == 2 && Idx == 1 &&\n        X86::isZeroNode(Op.getOperand(0)) &&\n        !X86::isZeroNode(Op.getOperand(1))) {\n      unsigned NumBits = VT.getSizeInBits();\n      return getVShift(true, VT,\n                       DAG.getNode(ISD::SCALAR_TO_VECTOR, dl,\n                                   VT, Op.getOperand(1)),\n                       NumBits/2, DAG, *this, dl);\n    }\n\n    if (IsAllConstants) // Otherwise, it's better to do a constpool load.\n      return SDValue();\n\n    // Otherwise, if this is a vector with i32 or f32 elements, and the element\n    // is a non-constant being inserted into an element other than the low one,\n    // we can't use a constant pool load.  Instead, use SCALAR_TO_VECTOR (aka\n    // movd/movss) to move this into the low element, then shuffle it into\n    // place.\n    if (EVTBits == 32) {\n      Item = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Item);\n      return getShuffleVectorZeroOrUndef(Item, Idx, NumZero > 0, Subtarget, DAG);\n    }\n  }\n\n  // Splat is obviously ok. Let legalizer expand it to a shuffle.\n  if (Values.size() == 1) {\n    if (EVTBits == 32) {\n      // Instead of a shuffle like this:\n      // shuffle (scalar_to_vector (load (ptr + 4))), undef, <0, 0, 0, 0>\n      // Check if it's possible to issue this instead.\n      // shuffle (vload ptr)), undef, <1, 1, 1, 1>\n      unsigned Idx = NonZeroMask.countTrailingZeros();\n      SDValue Item = Op.getOperand(Idx);\n      if (Op.getNode()->isOnlyUserOf(Item.getNode()))\n        return LowerAsSplatVectorLoad(Item, VT, dl, DAG);\n    }\n    return SDValue();\n  }\n\n  // A vector full of immediates; various special cases are already\n  // handled, so this is best done with a single constant-pool load.\n  if (IsAllConstants)\n    return SDValue();\n\n  if (SDValue V = LowerBUILD_VECTORAsVariablePermute(Op, DAG, Subtarget))\n      return V;\n\n  // See if we can use a vector load to get all of the elements.\n  {\n    SmallVector<SDValue, 64> Ops(Op->op_begin(), Op->op_begin() + NumElems);\n    if (SDValue LD =\n            EltsFromConsecutiveLoads(VT, Ops, dl, DAG, Subtarget, false))\n      return LD;\n  }\n\n  // If this is a splat of pairs of 32-bit elements, we can use a narrower\n  // build_vector and broadcast it.\n  // TODO: We could probably generalize this more.\n  if (Subtarget.hasAVX2() && EVTBits == 32 && Values.size() == 2) {\n    SDValue Ops[4] = { Op.getOperand(0), Op.getOperand(1),\n                       DAG.getUNDEF(EltVT), DAG.getUNDEF(EltVT) };\n    auto CanSplat = [](SDValue Op, unsigned NumElems, ArrayRef<SDValue> Ops) {\n      // Make sure all the even/odd operands match.\n      for (unsigned i = 2; i != NumElems; ++i)\n        if (Ops[i % 2] != Op.getOperand(i))\n          return false;\n      return true;\n    };\n    if (CanSplat(Op, NumElems, Ops)) {\n      MVT WideEltVT = VT.isFloatingPoint() ? MVT::f64 : MVT::i64;\n      MVT NarrowVT = MVT::getVectorVT(EltVT, 4);\n      // Create a new build vector and cast to v2i64/v2f64.\n      SDValue NewBV = DAG.getBitcast(MVT::getVectorVT(WideEltVT, 2),\n                                     DAG.getBuildVector(NarrowVT, dl, Ops));\n      // Broadcast from v2i64/v2f64 and cast to final VT.\n      MVT BcastVT = MVT::getVectorVT(WideEltVT, NumElems/2);\n      return DAG.getBitcast(VT, DAG.getNode(X86ISD::VBROADCAST, dl, BcastVT,\n                                            NewBV));\n    }\n  }\n\n  // For AVX-length vectors, build the individual 128-bit pieces and use\n  // shuffles to put them in place.\n  if (VT.getSizeInBits() > 128) {\n    MVT HVT = MVT::getVectorVT(EltVT, NumElems/2);\n\n    // Build both the lower and upper subvector.\n    SDValue Lower =\n        DAG.getBuildVector(HVT, dl, Op->ops().slice(0, NumElems / 2));\n    SDValue Upper = DAG.getBuildVector(\n        HVT, dl, Op->ops().slice(NumElems / 2, NumElems /2));\n\n    // Recreate the wider vector with the lower and upper part.\n    return concatSubVectors(Lower, Upper, DAG, dl);\n  }\n\n  // Let legalizer expand 2-wide build_vectors.\n  if (EVTBits == 64) {\n    if (NumNonZero == 1) {\n      // One half is zero or undef.\n      unsigned Idx = NonZeroMask.countTrailingZeros();\n      SDValue V2 = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT,\n                               Op.getOperand(Idx));\n      return getShuffleVectorZeroOrUndef(V2, Idx, true, Subtarget, DAG);\n    }\n    return SDValue();\n  }\n\n  // If element VT is < 32 bits, convert it to inserts into a zero vector.\n  if (EVTBits == 8 && NumElems == 16)\n    if (SDValue V = LowerBuildVectorv16i8(Op, NonZeroMask, NumNonZero, NumZero,\n                                          DAG, Subtarget))\n      return V;\n\n  if (EVTBits == 16 && NumElems == 8)\n    if (SDValue V = LowerBuildVectorv8i16(Op, NonZeroMask, NumNonZero, NumZero,\n                                          DAG, Subtarget))\n      return V;\n\n  // If element VT is == 32 bits and has 4 elems, try to generate an INSERTPS\n  if (EVTBits == 32 && NumElems == 4)\n    if (SDValue V = LowerBuildVectorv4x32(Op, DAG, Subtarget))\n      return V;\n\n  // If element VT is == 32 bits, turn it into a number of shuffles.\n  if (NumElems == 4 && NumZero > 0) {\n    SmallVector<SDValue, 8> Ops(NumElems);\n    for (unsigned i = 0; i < 4; ++i) {\n      bool isZero = !NonZeroMask[i];\n      if (isZero)\n        Ops[i] = getZeroVector(VT, Subtarget, DAG, dl);\n      else\n        Ops[i] = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Op.getOperand(i));\n    }\n\n    for (unsigned i = 0; i < 2; ++i) {\n      switch (NonZeroMask.extractBitsAsZExtValue(2, i * 2)) {\n        default: llvm_unreachable(\"Unexpected NonZero count\");\n        case 0:\n          Ops[i] = Ops[i*2];  // Must be a zero vector.\n          break;\n        case 1:\n          Ops[i] = getMOVL(DAG, dl, VT, Ops[i*2+1], Ops[i*2]);\n          break;\n        case 2:\n          Ops[i] = getMOVL(DAG, dl, VT, Ops[i*2], Ops[i*2+1]);\n          break;\n        case 3:\n          Ops[i] = getUnpackl(DAG, dl, VT, Ops[i*2], Ops[i*2+1]);\n          break;\n      }\n    }\n\n    bool Reverse1 = NonZeroMask.extractBitsAsZExtValue(2, 0) == 2;\n    bool Reverse2 = NonZeroMask.extractBitsAsZExtValue(2, 2) == 2;\n    int MaskVec[] = {\n      Reverse1 ? 1 : 0,\n      Reverse1 ? 0 : 1,\n      static_cast<int>(Reverse2 ? NumElems+1 : NumElems),\n      static_cast<int>(Reverse2 ? NumElems   : NumElems+1)\n    };\n    return DAG.getVectorShuffle(VT, dl, Ops[0], Ops[1], MaskVec);\n  }\n\n  assert(Values.size() > 1 && \"Expected non-undef and non-splat vector\");\n\n  // Check for a build vector from mostly shuffle plus few inserting.\n  if (SDValue Sh = buildFromShuffleMostly(Op, DAG))\n    return Sh;\n\n  // For SSE 4.1, use insertps to put the high elements into the low element.\n  if (Subtarget.hasSSE41()) {\n    SDValue Result;\n    if (!Op.getOperand(0).isUndef())\n      Result = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Op.getOperand(0));\n    else\n      Result = DAG.getUNDEF(VT);\n\n    for (unsigned i = 1; i < NumElems; ++i) {\n      if (Op.getOperand(i).isUndef()) continue;\n      Result = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, VT, Result,\n                           Op.getOperand(i), DAG.getIntPtrConstant(i, dl));\n    }\n    return Result;\n  }\n\n  // Otherwise, expand into a number of unpckl*, start by extending each of\n  // our (non-undef) elements to the full vector width with the element in the\n  // bottom slot of the vector (which generates no code for SSE).\n  SmallVector<SDValue, 8> Ops(NumElems);\n  for (unsigned i = 0; i < NumElems; ++i) {\n    if (!Op.getOperand(i).isUndef())\n      Ops[i] = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, Op.getOperand(i));\n    else\n      Ops[i] = DAG.getUNDEF(VT);\n  }\n\n  // Next, we iteratively mix elements, e.g. for v4f32:\n  //   Step 1: unpcklps 0, 1 ==> X: <?, ?, 1, 0>\n  //         : unpcklps 2, 3 ==> Y: <?, ?, 3, 2>\n  //   Step 2: unpcklpd X, Y ==>    <3, 2, 1, 0>\n  for (unsigned Scale = 1; Scale < NumElems; Scale *= 2) {\n    // Generate scaled UNPCKL shuffle mask.\n    SmallVector<int, 16> Mask;\n    for(unsigned i = 0; i != Scale; ++i)\n      Mask.push_back(i);\n    for (unsigned i = 0; i != Scale; ++i)\n      Mask.push_back(NumElems+i);\n    Mask.append(NumElems - Mask.size(), SM_SentinelUndef);\n\n    for (unsigned i = 0, e = NumElems / (2 * Scale); i != e; ++i)\n      Ops[i] = DAG.getVectorShuffle(VT, dl, Ops[2*i], Ops[(2*i)+1], Mask);\n  }\n  return Ops[0];\n}\n\n// 256-bit AVX can use the vinsertf128 instruction\n// to create 256-bit vectors from two other 128-bit ones.\n// TODO: Detect subvector broadcast here instead of DAG combine?\nstatic SDValue LowerAVXCONCAT_VECTORS(SDValue Op, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  SDLoc dl(Op);\n  MVT ResVT = Op.getSimpleValueType();\n\n  assert((ResVT.is256BitVector() ||\n          ResVT.is512BitVector()) && \"Value type must be 256-/512-bit wide\");\n\n  unsigned NumOperands = Op.getNumOperands();\n  unsigned NumZero = 0;\n  unsigned NumNonZero = 0;\n  unsigned NonZeros = 0;\n  for (unsigned i = 0; i != NumOperands; ++i) {\n    SDValue SubVec = Op.getOperand(i);\n    if (SubVec.isUndef())\n      continue;\n    if (ISD::isBuildVectorAllZeros(SubVec.getNode()))\n      ++NumZero;\n    else {\n      assert(i < sizeof(NonZeros) * CHAR_BIT); // Ensure the shift is in range.\n      NonZeros |= 1 << i;\n      ++NumNonZero;\n    }\n  }\n\n  // If we have more than 2 non-zeros, build each half separately.\n  if (NumNonZero > 2) {\n    MVT HalfVT = ResVT.getHalfNumVectorElementsVT();\n    ArrayRef<SDUse> Ops = Op->ops();\n    SDValue Lo = DAG.getNode(ISD::CONCAT_VECTORS, dl, HalfVT,\n                             Ops.slice(0, NumOperands/2));\n    SDValue Hi = DAG.getNode(ISD::CONCAT_VECTORS, dl, HalfVT,\n                             Ops.slice(NumOperands/2));\n    return DAG.getNode(ISD::CONCAT_VECTORS, dl, ResVT, Lo, Hi);\n  }\n\n  // Otherwise, build it up through insert_subvectors.\n  SDValue Vec = NumZero ? getZeroVector(ResVT, Subtarget, DAG, dl)\n                        : DAG.getUNDEF(ResVT);\n\n  MVT SubVT = Op.getOperand(0).getSimpleValueType();\n  unsigned NumSubElems = SubVT.getVectorNumElements();\n  for (unsigned i = 0; i != NumOperands; ++i) {\n    if ((NonZeros & (1 << i)) == 0)\n      continue;\n\n    Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ResVT, Vec,\n                      Op.getOperand(i),\n                      DAG.getIntPtrConstant(i * NumSubElems, dl));\n  }\n\n  return Vec;\n}\n\n// Returns true if the given node is a type promotion (by concatenating i1\n// zeros) of the result of a node that already zeros all upper bits of\n// k-register.\n// TODO: Merge this with LowerAVXCONCAT_VECTORS?\nstatic SDValue LowerCONCAT_VECTORSvXi1(SDValue Op,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG & DAG) {\n  SDLoc dl(Op);\n  MVT ResVT = Op.getSimpleValueType();\n  unsigned NumOperands = Op.getNumOperands();\n\n  assert(NumOperands > 1 && isPowerOf2_32(NumOperands) &&\n         \"Unexpected number of operands in CONCAT_VECTORS\");\n\n  uint64_t Zeros = 0;\n  uint64_t NonZeros = 0;\n  for (unsigned i = 0; i != NumOperands; ++i) {\n    SDValue SubVec = Op.getOperand(i);\n    if (SubVec.isUndef())\n      continue;\n    assert(i < sizeof(NonZeros) * CHAR_BIT); // Ensure the shift is in range.\n    if (ISD::isBuildVectorAllZeros(SubVec.getNode()))\n      Zeros |= (uint64_t)1 << i;\n    else\n      NonZeros |= (uint64_t)1 << i;\n  }\n\n  unsigned NumElems = ResVT.getVectorNumElements();\n\n  // If we are inserting non-zero vector and there are zeros in LSBs and undef\n  // in the MSBs we need to emit a KSHIFTL. The generic lowering to\n  // insert_subvector will give us two kshifts.\n  if (isPowerOf2_64(NonZeros) && Zeros != 0 && NonZeros > Zeros &&\n      Log2_64(NonZeros) != NumOperands - 1) {\n    MVT ShiftVT = ResVT;\n    if ((!Subtarget.hasDQI() && NumElems == 8) || NumElems < 8)\n      ShiftVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n    unsigned Idx = Log2_64(NonZeros);\n    SDValue SubVec = Op.getOperand(Idx);\n    unsigned SubVecNumElts = SubVec.getSimpleValueType().getVectorNumElements();\n    SubVec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ShiftVT,\n                         DAG.getUNDEF(ShiftVT), SubVec,\n                         DAG.getIntPtrConstant(0, dl));\n    Op = DAG.getNode(X86ISD::KSHIFTL, dl, ShiftVT, SubVec,\n                     DAG.getTargetConstant(Idx * SubVecNumElts, dl, MVT::i8));\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, ResVT, Op,\n                       DAG.getIntPtrConstant(0, dl));\n  }\n\n  // If there are zero or one non-zeros we can handle this very simply.\n  if (NonZeros == 0 || isPowerOf2_64(NonZeros)) {\n    SDValue Vec = Zeros ? DAG.getConstant(0, dl, ResVT) : DAG.getUNDEF(ResVT);\n    if (!NonZeros)\n      return Vec;\n    unsigned Idx = Log2_64(NonZeros);\n    SDValue SubVec = Op.getOperand(Idx);\n    unsigned SubVecNumElts = SubVec.getSimpleValueType().getVectorNumElements();\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ResVT, Vec, SubVec,\n                       DAG.getIntPtrConstant(Idx * SubVecNumElts, dl));\n  }\n\n  if (NumOperands > 2) {\n    MVT HalfVT = ResVT.getHalfNumVectorElementsVT();\n    ArrayRef<SDUse> Ops = Op->ops();\n    SDValue Lo = DAG.getNode(ISD::CONCAT_VECTORS, dl, HalfVT,\n                             Ops.slice(0, NumOperands/2));\n    SDValue Hi = DAG.getNode(ISD::CONCAT_VECTORS, dl, HalfVT,\n                             Ops.slice(NumOperands/2));\n    return DAG.getNode(ISD::CONCAT_VECTORS, dl, ResVT, Lo, Hi);\n  }\n\n  assert(countPopulation(NonZeros) == 2 && \"Simple cases not handled?\");\n\n  if (ResVT.getVectorNumElements() >= 16)\n    return Op; // The operation is legal with KUNPCK\n\n  SDValue Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ResVT,\n                            DAG.getUNDEF(ResVT), Op.getOperand(0),\n                            DAG.getIntPtrConstant(0, dl));\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, ResVT, Vec, Op.getOperand(1),\n                     DAG.getIntPtrConstant(NumElems/2, dl));\n}\n\nstatic SDValue LowerCONCAT_VECTORS(SDValue Op,\n                                   const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  if (VT.getVectorElementType() == MVT::i1)\n    return LowerCONCAT_VECTORSvXi1(Op, Subtarget, DAG);\n\n  assert((VT.is256BitVector() && Op.getNumOperands() == 2) ||\n         (VT.is512BitVector() && (Op.getNumOperands() == 2 ||\n          Op.getNumOperands() == 4)));\n\n  // AVX can use the vinsertf128 instruction to create 256-bit vectors\n  // from two other 128-bit ones.\n\n  // 512-bit vector may contain 2 256-bit vectors or 4 128-bit vectors\n  return LowerAVXCONCAT_VECTORS(Op, DAG, Subtarget);\n}\n\n//===----------------------------------------------------------------------===//\n// Vector shuffle lowering\n//\n// This is an experimental code path for lowering vector shuffles on x86. It is\n// designed to handle arbitrary vector shuffles and blends, gracefully\n// degrading performance as necessary. It works hard to recognize idiomatic\n// shuffles and lower them to optimal instruction patterns without leaving\n// a framework that allows reasonably efficient handling of all vector shuffle\n// patterns.\n//===----------------------------------------------------------------------===//\n\n/// Tiny helper function to identify a no-op mask.\n///\n/// This is a somewhat boring predicate function. It checks whether the mask\n/// array input, which is assumed to be a single-input shuffle mask of the kind\n/// used by the X86 shuffle instructions (not a fully general\n/// ShuffleVectorSDNode mask) requires any shuffles to occur. Both undef and an\n/// in-place shuffle are 'no-op's.\nstatic bool isNoopShuffleMask(ArrayRef<int> Mask) {\n  for (int i = 0, Size = Mask.size(); i < Size; ++i) {\n    assert(Mask[i] >= -1 && \"Out of bound mask element!\");\n    if (Mask[i] >= 0 && Mask[i] != i)\n      return false;\n  }\n  return true;\n}\n\n/// Test whether there are elements crossing LaneSizeInBits lanes in this\n/// shuffle mask.\n///\n/// X86 divides up its shuffles into in-lane and cross-lane shuffle operations\n/// and we routinely test for these.\nstatic bool isLaneCrossingShuffleMask(unsigned LaneSizeInBits,\n                                      unsigned ScalarSizeInBits,\n                                      ArrayRef<int> Mask) {\n  assert(LaneSizeInBits && ScalarSizeInBits &&\n         (LaneSizeInBits % ScalarSizeInBits) == 0 &&\n         \"Illegal shuffle lane size\");\n  int LaneSize = LaneSizeInBits / ScalarSizeInBits;\n  int Size = Mask.size();\n  for (int i = 0; i < Size; ++i)\n    if (Mask[i] >= 0 && (Mask[i] % Size) / LaneSize != i / LaneSize)\n      return true;\n  return false;\n}\n\n/// Test whether there are elements crossing 128-bit lanes in this\n/// shuffle mask.\nstatic bool is128BitLaneCrossingShuffleMask(MVT VT, ArrayRef<int> Mask) {\n  return isLaneCrossingShuffleMask(128, VT.getScalarSizeInBits(), Mask);\n}\n\n/// Test whether elements in each LaneSizeInBits lane in this shuffle mask come\n/// from multiple lanes - this is different to isLaneCrossingShuffleMask to\n/// better support 'repeated mask + lane permute' style shuffles.\nstatic bool isMultiLaneShuffleMask(unsigned LaneSizeInBits,\n                                   unsigned ScalarSizeInBits,\n                                   ArrayRef<int> Mask) {\n  assert(LaneSizeInBits && ScalarSizeInBits &&\n         (LaneSizeInBits % ScalarSizeInBits) == 0 &&\n         \"Illegal shuffle lane size\");\n  int NumElts = Mask.size();\n  int NumEltsPerLane = LaneSizeInBits / ScalarSizeInBits;\n  int NumLanes = NumElts / NumEltsPerLane;\n  if (NumLanes > 1) {\n    for (int i = 0; i != NumLanes; ++i) {\n      int SrcLane = -1;\n      for (int j = 0; j != NumEltsPerLane; ++j) {\n        int M = Mask[(i * NumEltsPerLane) + j];\n        if (M < 0)\n          continue;\n        int Lane = (M % NumElts) / NumEltsPerLane;\n        if (SrcLane >= 0 && SrcLane != Lane)\n          return true;\n        SrcLane = Lane;\n      }\n    }\n  }\n  return false;\n}\n\n/// Test whether a shuffle mask is equivalent within each sub-lane.\n///\n/// This checks a shuffle mask to see if it is performing the same\n/// lane-relative shuffle in each sub-lane. This trivially implies\n/// that it is also not lane-crossing. It may however involve a blend from the\n/// same lane of a second vector.\n///\n/// The specific repeated shuffle mask is populated in \\p RepeatedMask, as it is\n/// non-trivial to compute in the face of undef lanes. The representation is\n/// suitable for use with existing 128-bit shuffles as entries from the second\n/// vector have been remapped to [LaneSize, 2*LaneSize).\nstatic bool isRepeatedShuffleMask(unsigned LaneSizeInBits, MVT VT,\n                                  ArrayRef<int> Mask,\n                                  SmallVectorImpl<int> &RepeatedMask) {\n  auto LaneSize = LaneSizeInBits / VT.getScalarSizeInBits();\n  RepeatedMask.assign(LaneSize, -1);\n  int Size = Mask.size();\n  for (int i = 0; i < Size; ++i) {\n    assert(Mask[i] == SM_SentinelUndef || Mask[i] >= 0);\n    if (Mask[i] < 0)\n      continue;\n    if ((Mask[i] % Size) / LaneSize != i / LaneSize)\n      // This entry crosses lanes, so there is no way to model this shuffle.\n      return false;\n\n    // Ok, handle the in-lane shuffles by detecting if and when they repeat.\n    // Adjust second vector indices to start at LaneSize instead of Size.\n    int LocalM = Mask[i] < Size ? Mask[i] % LaneSize\n                                : Mask[i] % LaneSize + LaneSize;\n    if (RepeatedMask[i % LaneSize] < 0)\n      // This is the first non-undef entry in this slot of a 128-bit lane.\n      RepeatedMask[i % LaneSize] = LocalM;\n    else if (RepeatedMask[i % LaneSize] != LocalM)\n      // Found a mismatch with the repeated mask.\n      return false;\n  }\n  return true;\n}\n\n/// Test whether a shuffle mask is equivalent within each 128-bit lane.\nstatic bool\nis128BitLaneRepeatedShuffleMask(MVT VT, ArrayRef<int> Mask,\n                                SmallVectorImpl<int> &RepeatedMask) {\n  return isRepeatedShuffleMask(128, VT, Mask, RepeatedMask);\n}\n\nstatic bool\nis128BitLaneRepeatedShuffleMask(MVT VT, ArrayRef<int> Mask) {\n  SmallVector<int, 32> RepeatedMask;\n  return isRepeatedShuffleMask(128, VT, Mask, RepeatedMask);\n}\n\n/// Test whether a shuffle mask is equivalent within each 256-bit lane.\nstatic bool\nis256BitLaneRepeatedShuffleMask(MVT VT, ArrayRef<int> Mask,\n                                SmallVectorImpl<int> &RepeatedMask) {\n  return isRepeatedShuffleMask(256, VT, Mask, RepeatedMask);\n}\n\n/// Test whether a target shuffle mask is equivalent within each sub-lane.\n/// Unlike isRepeatedShuffleMask we must respect SM_SentinelZero.\nstatic bool isRepeatedTargetShuffleMask(unsigned LaneSizeInBits,\n                                        unsigned EltSizeInBits,\n                                        ArrayRef<int> Mask,\n                                        SmallVectorImpl<int> &RepeatedMask) {\n  int LaneSize = LaneSizeInBits / EltSizeInBits;\n  RepeatedMask.assign(LaneSize, SM_SentinelUndef);\n  int Size = Mask.size();\n  for (int i = 0; i < Size; ++i) {\n    assert(isUndefOrZero(Mask[i]) || (Mask[i] >= 0));\n    if (Mask[i] == SM_SentinelUndef)\n      continue;\n    if (Mask[i] == SM_SentinelZero) {\n      if (!isUndefOrZero(RepeatedMask[i % LaneSize]))\n        return false;\n      RepeatedMask[i % LaneSize] = SM_SentinelZero;\n      continue;\n    }\n    if ((Mask[i] % Size) / LaneSize != i / LaneSize)\n      // This entry crosses lanes, so there is no way to model this shuffle.\n      return false;\n\n    // Ok, handle the in-lane shuffles by detecting if and when they repeat.\n    // Adjust second vector indices to start at LaneSize instead of Size.\n    int LocalM =\n        Mask[i] < Size ? Mask[i] % LaneSize : Mask[i] % LaneSize + LaneSize;\n    if (RepeatedMask[i % LaneSize] == SM_SentinelUndef)\n      // This is the first non-undef entry in this slot of a 128-bit lane.\n      RepeatedMask[i % LaneSize] = LocalM;\n    else if (RepeatedMask[i % LaneSize] != LocalM)\n      // Found a mismatch with the repeated mask.\n      return false;\n  }\n  return true;\n}\n\n/// Test whether a target shuffle mask is equivalent within each sub-lane.\n/// Unlike isRepeatedShuffleMask we must respect SM_SentinelZero.\nstatic bool isRepeatedTargetShuffleMask(unsigned LaneSizeInBits, MVT VT,\n                                        ArrayRef<int> Mask,\n                                        SmallVectorImpl<int> &RepeatedMask) {\n  return isRepeatedTargetShuffleMask(LaneSizeInBits, VT.getScalarSizeInBits(),\n                                     Mask, RepeatedMask);\n}\n\n/// Checks whether the vector elements referenced by two shuffle masks are\n/// equivalent.\nstatic bool IsElementEquivalent(int MaskSize, SDValue Op, SDValue ExpectedOp,\n                                int Idx, int ExpectedIdx) {\n  assert(0 <= Idx && Idx < MaskSize && 0 <= ExpectedIdx &&\n         ExpectedIdx < MaskSize && \"Out of range element index\");\n  if (!Op || !ExpectedOp || Op.getOpcode() != ExpectedOp.getOpcode())\n    return false;\n\n  switch (Op.getOpcode()) {\n  case ISD::BUILD_VECTOR:\n    // If the values are build vectors, we can look through them to find\n    // equivalent inputs that make the shuffles equivalent.\n    // TODO: Handle MaskSize != Op.getNumOperands()?\n    if (MaskSize == (int)Op.getNumOperands() &&\n        MaskSize == (int)ExpectedOp.getNumOperands())\n      return Op.getOperand(Idx) == ExpectedOp.getOperand(ExpectedIdx);\n    break;\n  case X86ISD::VBROADCAST:\n  case X86ISD::VBROADCAST_LOAD:\n    // TODO: Handle MaskSize != Op.getValueType().getVectorNumElements()?\n    return (Op == ExpectedOp &&\n            (int)Op.getValueType().getVectorNumElements() == MaskSize);\n  case X86ISD::HADD:\n  case X86ISD::HSUB:\n  case X86ISD::FHADD:\n  case X86ISD::FHSUB:\n  case X86ISD::PACKSS:\n  case X86ISD::PACKUS:\n    // HOP(X,X) can refer to the elt from the lower/upper half of a lane.\n    // TODO: Handle MaskSize != NumElts?\n    // TODO: Handle HOP(X,Y) vs HOP(Y,X) equivalence cases.\n    if (Op == ExpectedOp && Op.getOperand(0) == Op.getOperand(1)) {\n      MVT VT = Op.getSimpleValueType();\n      int NumElts = VT.getVectorNumElements();\n      if (MaskSize == NumElts) {\n        int NumLanes = VT.getSizeInBits() / 128;\n        int NumEltsPerLane = NumElts / NumLanes;\n        int NumHalfEltsPerLane = NumEltsPerLane / 2;\n        bool SameLane =\n            (Idx / NumEltsPerLane) == (ExpectedIdx / NumEltsPerLane);\n        bool SameElt =\n            (Idx % NumHalfEltsPerLane) == (ExpectedIdx % NumHalfEltsPerLane);\n        return SameLane && SameElt;\n      }\n    }\n    break;\n  }\n\n  return false;\n}\n\n/// Checks whether a shuffle mask is equivalent to an explicit list of\n/// arguments.\n///\n/// This is a fast way to test a shuffle mask against a fixed pattern:\n///\n///   if (isShuffleEquivalent(Mask, 3, 2, {1, 0})) { ... }\n///\n/// It returns true if the mask is exactly as wide as the argument list, and\n/// each element of the mask is either -1 (signifying undef) or the value given\n/// in the argument.\nstatic bool isShuffleEquivalent(ArrayRef<int> Mask, ArrayRef<int> ExpectedMask,\n                                SDValue V1 = SDValue(),\n                                SDValue V2 = SDValue()) {\n  int Size = Mask.size();\n  if (Size != (int)ExpectedMask.size())\n    return false;\n\n  for (int i = 0; i < Size; ++i) {\n    assert(Mask[i] >= -1 && \"Out of bound mask element!\");\n    int MaskIdx = Mask[i];\n    int ExpectedIdx = ExpectedMask[i];\n    if (0 <= MaskIdx && MaskIdx != ExpectedIdx) {\n      SDValue MaskV = MaskIdx < Size ? V1 : V2;\n      SDValue ExpectedV = ExpectedIdx < Size ? V1 : V2;\n      MaskIdx = MaskIdx < Size ? MaskIdx : (MaskIdx - Size);\n      ExpectedIdx = ExpectedIdx < Size ? ExpectedIdx : (ExpectedIdx - Size);\n      if (!IsElementEquivalent(Size, MaskV, ExpectedV, MaskIdx, ExpectedIdx))\n        return false;\n    }\n  }\n  return true;\n}\n\n/// Checks whether a target shuffle mask is equivalent to an explicit pattern.\n///\n/// The masks must be exactly the same width.\n///\n/// If an element in Mask matches SM_SentinelUndef (-1) then the corresponding\n/// value in ExpectedMask is always accepted. Otherwise the indices must match.\n///\n/// SM_SentinelZero is accepted as a valid negative index but must match in\n/// both.\nstatic bool isTargetShuffleEquivalent(MVT VT, ArrayRef<int> Mask,\n                                      ArrayRef<int> ExpectedMask,\n                                      SDValue V1 = SDValue(),\n                                      SDValue V2 = SDValue()) {\n  int Size = Mask.size();\n  if (Size != (int)ExpectedMask.size())\n    return false;\n  assert(isUndefOrZeroOrInRange(ExpectedMask, 0, 2 * Size) &&\n         \"Illegal target shuffle mask\");\n\n  // Check for out-of-range target shuffle mask indices.\n  if (!isUndefOrZeroOrInRange(Mask, 0, 2 * Size))\n    return false;\n\n  // Don't use V1/V2 if they're not the same size as the shuffle mask type.\n  if (V1 && V1.getValueSizeInBits() != VT.getSizeInBits())\n    V1 = SDValue();\n  if (V2 && V2.getValueSizeInBits() != VT.getSizeInBits())\n    V2 = SDValue();\n\n  for (int i = 0; i < Size; ++i) {\n    int MaskIdx = Mask[i];\n    int ExpectedIdx = ExpectedMask[i];\n    if (MaskIdx == SM_SentinelUndef || MaskIdx == ExpectedIdx)\n      continue;\n    if (0 <= MaskIdx && 0 <= ExpectedIdx) {\n      SDValue MaskV = MaskIdx < Size ? V1 : V2;\n      SDValue ExpectedV = ExpectedIdx < Size ? V1 : V2;\n      MaskIdx = MaskIdx < Size ? MaskIdx : (MaskIdx - Size);\n      ExpectedIdx = ExpectedIdx < Size ? ExpectedIdx : (ExpectedIdx - Size);\n      if (IsElementEquivalent(Size, MaskV, ExpectedV, MaskIdx, ExpectedIdx))\n        continue;\n    }\n    // TODO - handle SM_Sentinel equivalences.\n    return false;\n  }\n  return true;\n}\n\n// Attempt to create a shuffle mask from a VSELECT condition mask.\nstatic bool createShuffleMaskFromVSELECT(SmallVectorImpl<int> &Mask,\n                                         SDValue Cond) {\n  EVT CondVT = Cond.getValueType();\n  unsigned EltSizeInBits = CondVT.getScalarSizeInBits();\n  unsigned NumElts = CondVT.getVectorNumElements();\n\n  APInt UndefElts;\n  SmallVector<APInt, 32> EltBits;\n  if (!getTargetConstantBitsFromNode(Cond, EltSizeInBits, UndefElts, EltBits,\n                                     true, false))\n    return false;\n\n  Mask.resize(NumElts, SM_SentinelUndef);\n\n  for (int i = 0; i != (int)NumElts; ++i) {\n    Mask[i] = i;\n    // Arbitrarily choose from the 2nd operand if the select condition element\n    // is undef.\n    // TODO: Can we do better by matching patterns such as even/odd?\n    if (UndefElts[i] || EltBits[i].isNullValue())\n      Mask[i] += NumElts;\n  }\n\n  return true;\n}\n\n// Check if the shuffle mask is suitable for the AVX vpunpcklwd or vpunpckhwd\n// instructions.\nstatic bool isUnpackWdShuffleMask(ArrayRef<int> Mask, MVT VT) {\n  if (VT != MVT::v8i32 && VT != MVT::v8f32)\n    return false;\n\n  SmallVector<int, 8> Unpcklwd;\n  createUnpackShuffleMask(MVT::v8i16, Unpcklwd, /* Lo = */ true,\n                          /* Unary = */ false);\n  SmallVector<int, 8> Unpckhwd;\n  createUnpackShuffleMask(MVT::v8i16, Unpckhwd, /* Lo = */ false,\n                          /* Unary = */ false);\n  bool IsUnpackwdMask = (isTargetShuffleEquivalent(VT, Mask, Unpcklwd) ||\n                         isTargetShuffleEquivalent(VT, Mask, Unpckhwd));\n  return IsUnpackwdMask;\n}\n\nstatic bool is128BitUnpackShuffleMask(ArrayRef<int> Mask) {\n  // Create 128-bit vector type based on mask size.\n  MVT EltVT = MVT::getIntegerVT(128 / Mask.size());\n  MVT VT = MVT::getVectorVT(EltVT, Mask.size());\n\n  // We can't assume a canonical shuffle mask, so try the commuted version too.\n  SmallVector<int, 4> CommutedMask(Mask.begin(), Mask.end());\n  ShuffleVectorSDNode::commuteMask(CommutedMask);\n\n  // Match any of unary/binary or low/high.\n  for (unsigned i = 0; i != 4; ++i) {\n    SmallVector<int, 16> UnpackMask;\n    createUnpackShuffleMask(VT, UnpackMask, (i >> 1) % 2, i % 2);\n    if (isTargetShuffleEquivalent(VT, Mask, UnpackMask) ||\n        isTargetShuffleEquivalent(VT, CommutedMask, UnpackMask))\n      return true;\n  }\n  return false;\n}\n\n/// Return true if a shuffle mask chooses elements identically in its top and\n/// bottom halves. For example, any splat mask has the same top and bottom\n/// halves. If an element is undefined in only one half of the mask, the halves\n/// are not considered identical.\nstatic bool hasIdenticalHalvesShuffleMask(ArrayRef<int> Mask) {\n  assert(Mask.size() % 2 == 0 && \"Expecting even number of elements in mask\");\n  unsigned HalfSize = Mask.size() / 2;\n  for (unsigned i = 0; i != HalfSize; ++i) {\n    if (Mask[i] != Mask[i + HalfSize])\n      return false;\n  }\n  return true;\n}\n\n/// Get a 4-lane 8-bit shuffle immediate for a mask.\n///\n/// This helper function produces an 8-bit shuffle immediate corresponding to\n/// the ubiquitous shuffle encoding scheme used in x86 instructions for\n/// shuffling 4 lanes. It can be used with most of the PSHUF instructions for\n/// example.\n///\n/// NB: We rely heavily on \"undef\" masks preserving the input lane.\nstatic unsigned getV4X86ShuffleImm(ArrayRef<int> Mask) {\n  assert(Mask.size() == 4 && \"Only 4-lane shuffle masks\");\n  assert(Mask[0] >= -1 && Mask[0] < 4 && \"Out of bound mask element!\");\n  assert(Mask[1] >= -1 && Mask[1] < 4 && \"Out of bound mask element!\");\n  assert(Mask[2] >= -1 && Mask[2] < 4 && \"Out of bound mask element!\");\n  assert(Mask[3] >= -1 && Mask[3] < 4 && \"Out of bound mask element!\");\n\n  // If the mask only uses one non-undef element, then fully 'splat' it to\n  // improve later broadcast matching.\n  int FirstIndex = find_if(Mask, [](int M) { return M >= 0; }) - Mask.begin();\n  assert(0 <= FirstIndex && FirstIndex < 4 && \"All undef shuffle mask\");\n\n  int FirstElt = Mask[FirstIndex];\n  if (all_of(Mask, [FirstElt](int M) { return M < 0 || M == FirstElt; }))\n    return (FirstElt << 6) | (FirstElt << 4) | (FirstElt << 2) | FirstElt;\n\n  unsigned Imm = 0;\n  Imm |= (Mask[0] < 0 ? 0 : Mask[0]) << 0;\n  Imm |= (Mask[1] < 0 ? 1 : Mask[1]) << 2;\n  Imm |= (Mask[2] < 0 ? 2 : Mask[2]) << 4;\n  Imm |= (Mask[3] < 0 ? 3 : Mask[3]) << 6;\n  return Imm;\n}\n\nstatic SDValue getV4X86ShuffleImm8ForMask(ArrayRef<int> Mask, const SDLoc &DL,\n                                          SelectionDAG &DAG) {\n  return DAG.getTargetConstant(getV4X86ShuffleImm(Mask), DL, MVT::i8);\n}\n\n// The Shuffle result is as follow:\n// 0*a[0]0*a[1]...0*a[n] , n >=0 where a[] elements in a ascending order.\n// Each Zeroable's element correspond to a particular Mask's element.\n// As described in computeZeroableShuffleElements function.\n//\n// The function looks for a sub-mask that the nonzero elements are in\n// increasing order. If such sub-mask exist. The function returns true.\nstatic bool isNonZeroElementsInOrder(const APInt &Zeroable,\n                                     ArrayRef<int> Mask, const EVT &VectorType,\n                                     bool &IsZeroSideLeft) {\n  int NextElement = -1;\n  // Check if the Mask's nonzero elements are in increasing order.\n  for (int i = 0, e = Mask.size(); i < e; i++) {\n    // Checks if the mask's zeros elements are built from only zeros.\n    assert(Mask[i] >= -1 && \"Out of bound mask element!\");\n    if (Mask[i] < 0)\n      return false;\n    if (Zeroable[i])\n      continue;\n    // Find the lowest non zero element\n    if (NextElement < 0) {\n      NextElement = Mask[i] != 0 ? VectorType.getVectorNumElements() : 0;\n      IsZeroSideLeft = NextElement != 0;\n    }\n    // Exit if the mask's non zero elements are not in increasing order.\n    if (NextElement != Mask[i])\n      return false;\n    NextElement++;\n  }\n  return true;\n}\n\n/// Try to lower a shuffle with a single PSHUFB of V1 or V2.\nstatic SDValue lowerShuffleWithPSHUFB(const SDLoc &DL, MVT VT,\n                                      ArrayRef<int> Mask, SDValue V1,\n                                      SDValue V2, const APInt &Zeroable,\n                                      const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  int Size = Mask.size();\n  int LaneSize = 128 / VT.getScalarSizeInBits();\n  const int NumBytes = VT.getSizeInBits() / 8;\n  const int NumEltBytes = VT.getScalarSizeInBits() / 8;\n\n  assert((Subtarget.hasSSSE3() && VT.is128BitVector()) ||\n         (Subtarget.hasAVX2() && VT.is256BitVector()) ||\n         (Subtarget.hasBWI() && VT.is512BitVector()));\n\n  SmallVector<SDValue, 64> PSHUFBMask(NumBytes);\n  // Sign bit set in i8 mask means zero element.\n  SDValue ZeroMask = DAG.getConstant(0x80, DL, MVT::i8);\n\n  SDValue V;\n  for (int i = 0; i < NumBytes; ++i) {\n    int M = Mask[i / NumEltBytes];\n    if (M < 0) {\n      PSHUFBMask[i] = DAG.getUNDEF(MVT::i8);\n      continue;\n    }\n    if (Zeroable[i / NumEltBytes]) {\n      PSHUFBMask[i] = ZeroMask;\n      continue;\n    }\n\n    // We can only use a single input of V1 or V2.\n    SDValue SrcV = (M >= Size ? V2 : V1);\n    if (V && V != SrcV)\n      return SDValue();\n    V = SrcV;\n    M %= Size;\n\n    // PSHUFB can't cross lanes, ensure this doesn't happen.\n    if ((M / LaneSize) != ((i / NumEltBytes) / LaneSize))\n      return SDValue();\n\n    M = M % LaneSize;\n    M = M * NumEltBytes + (i % NumEltBytes);\n    PSHUFBMask[i] = DAG.getConstant(M, DL, MVT::i8);\n  }\n  assert(V && \"Failed to find a source input\");\n\n  MVT I8VT = MVT::getVectorVT(MVT::i8, NumBytes);\n  return DAG.getBitcast(\n      VT, DAG.getNode(X86ISD::PSHUFB, DL, I8VT, DAG.getBitcast(I8VT, V),\n                      DAG.getBuildVector(I8VT, DL, PSHUFBMask)));\n}\n\nstatic SDValue getMaskNode(SDValue Mask, MVT MaskVT,\n                           const X86Subtarget &Subtarget, SelectionDAG &DAG,\n                           const SDLoc &dl);\n\n// X86 has dedicated shuffle that can be lowered to VEXPAND\nstatic SDValue lowerShuffleToEXPAND(const SDLoc &DL, MVT VT,\n                                    const APInt &Zeroable,\n                                    ArrayRef<int> Mask, SDValue &V1,\n                                    SDValue &V2, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  bool IsLeftZeroSide = true;\n  if (!isNonZeroElementsInOrder(Zeroable, Mask, V1.getValueType(),\n                                IsLeftZeroSide))\n    return SDValue();\n  unsigned VEXPANDMask = (~Zeroable).getZExtValue();\n  MVT IntegerType =\n      MVT::getIntegerVT(std::max((int)VT.getVectorNumElements(), 8));\n  SDValue MaskNode = DAG.getConstant(VEXPANDMask, DL, IntegerType);\n  unsigned NumElts = VT.getVectorNumElements();\n  assert((NumElts == 4 || NumElts == 8 || NumElts == 16) &&\n         \"Unexpected number of vector elements\");\n  SDValue VMask = getMaskNode(MaskNode, MVT::getVectorVT(MVT::i1, NumElts),\n                              Subtarget, DAG, DL);\n  SDValue ZeroVector = getZeroVector(VT, Subtarget, DAG, DL);\n  SDValue ExpandedVector = IsLeftZeroSide ? V2 : V1;\n  return DAG.getNode(X86ISD::EXPAND, DL, VT, ExpandedVector, ZeroVector, VMask);\n}\n\nstatic bool matchShuffleWithUNPCK(MVT VT, SDValue &V1, SDValue &V2,\n                                  unsigned &UnpackOpcode, bool IsUnary,\n                                  ArrayRef<int> TargetMask, const SDLoc &DL,\n                                  SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  int NumElts = VT.getVectorNumElements();\n\n  bool Undef1 = true, Undef2 = true, Zero1 = true, Zero2 = true;\n  for (int i = 0; i != NumElts; i += 2) {\n    int M1 = TargetMask[i + 0];\n    int M2 = TargetMask[i + 1];\n    Undef1 &= (SM_SentinelUndef == M1);\n    Undef2 &= (SM_SentinelUndef == M2);\n    Zero1 &= isUndefOrZero(M1);\n    Zero2 &= isUndefOrZero(M2);\n  }\n  assert(!((Undef1 || Zero1) && (Undef2 || Zero2)) &&\n         \"Zeroable shuffle detected\");\n\n  // Attempt to match the target mask against the unpack lo/hi mask patterns.\n  SmallVector<int, 64> Unpckl, Unpckh;\n  createUnpackShuffleMask(VT, Unpckl, /* Lo = */ true, IsUnary);\n  if (isTargetShuffleEquivalent(VT, TargetMask, Unpckl, V1,\n                                (IsUnary ? V1 : V2))) {\n    UnpackOpcode = X86ISD::UNPCKL;\n    V2 = (Undef2 ? DAG.getUNDEF(VT) : (IsUnary ? V1 : V2));\n    V1 = (Undef1 ? DAG.getUNDEF(VT) : V1);\n    return true;\n  }\n\n  createUnpackShuffleMask(VT, Unpckh, /* Lo = */ false, IsUnary);\n  if (isTargetShuffleEquivalent(VT, TargetMask, Unpckh, V1,\n                                (IsUnary ? V1 : V2))) {\n    UnpackOpcode = X86ISD::UNPCKH;\n    V2 = (Undef2 ? DAG.getUNDEF(VT) : (IsUnary ? V1 : V2));\n    V1 = (Undef1 ? DAG.getUNDEF(VT) : V1);\n    return true;\n  }\n\n  // If an unary shuffle, attempt to match as an unpack lo/hi with zero.\n  if (IsUnary && (Zero1 || Zero2)) {\n    // Don't bother if we can blend instead.\n    if ((Subtarget.hasSSE41() || VT == MVT::v2i64 || VT == MVT::v2f64) &&\n        isSequentialOrUndefOrZeroInRange(TargetMask, 0, NumElts, 0))\n      return false;\n\n    bool MatchLo = true, MatchHi = true;\n    for (int i = 0; (i != NumElts) && (MatchLo || MatchHi); ++i) {\n      int M = TargetMask[i];\n\n      // Ignore if the input is known to be zero or the index is undef.\n      if ((((i & 1) == 0) && Zero1) || (((i & 1) == 1) && Zero2) ||\n          (M == SM_SentinelUndef))\n        continue;\n\n      MatchLo &= (M == Unpckl[i]);\n      MatchHi &= (M == Unpckh[i]);\n    }\n\n    if (MatchLo || MatchHi) {\n      UnpackOpcode = MatchLo ? X86ISD::UNPCKL : X86ISD::UNPCKH;\n      V2 = Zero2 ? getZeroVector(VT, Subtarget, DAG, DL) : V1;\n      V1 = Zero1 ? getZeroVector(VT, Subtarget, DAG, DL) : V1;\n      return true;\n    }\n  }\n\n  // If a binary shuffle, commute and try again.\n  if (!IsUnary) {\n    ShuffleVectorSDNode::commuteMask(Unpckl);\n    if (isTargetShuffleEquivalent(VT, TargetMask, Unpckl)) {\n      UnpackOpcode = X86ISD::UNPCKL;\n      std::swap(V1, V2);\n      return true;\n    }\n\n    ShuffleVectorSDNode::commuteMask(Unpckh);\n    if (isTargetShuffleEquivalent(VT, TargetMask, Unpckh)) {\n      UnpackOpcode = X86ISD::UNPCKH;\n      std::swap(V1, V2);\n      return true;\n    }\n  }\n\n  return false;\n}\n\n// X86 has dedicated unpack instructions that can handle specific blend\n// operations: UNPCKH and UNPCKL.\nstatic SDValue lowerShuffleWithUNPCK(const SDLoc &DL, MVT VT,\n                                     ArrayRef<int> Mask, SDValue V1, SDValue V2,\n                                     SelectionDAG &DAG) {\n  SmallVector<int, 8> Unpckl;\n  createUnpackShuffleMask(VT, Unpckl, /* Lo = */ true, /* Unary = */ false);\n  if (isShuffleEquivalent(Mask, Unpckl, V1, V2))\n    return DAG.getNode(X86ISD::UNPCKL, DL, VT, V1, V2);\n\n  SmallVector<int, 8> Unpckh;\n  createUnpackShuffleMask(VT, Unpckh, /* Lo = */ false, /* Unary = */ false);\n  if (isShuffleEquivalent(Mask, Unpckh, V1, V2))\n    return DAG.getNode(X86ISD::UNPCKH, DL, VT, V1, V2);\n\n  // Commute and try again.\n  ShuffleVectorSDNode::commuteMask(Unpckl);\n  if (isShuffleEquivalent(Mask, Unpckl, V1, V2))\n    return DAG.getNode(X86ISD::UNPCKL, DL, VT, V2, V1);\n\n  ShuffleVectorSDNode::commuteMask(Unpckh);\n  if (isShuffleEquivalent(Mask, Unpckh, V1, V2))\n    return DAG.getNode(X86ISD::UNPCKH, DL, VT, V2, V1);\n\n  return SDValue();\n}\n\n/// Check if the mask can be mapped to a preliminary shuffle (vperm 64-bit)\n/// followed by unpack 256-bit.\nstatic SDValue lowerShuffleWithUNPCK256(const SDLoc &DL, MVT VT,\n                                        ArrayRef<int> Mask, SDValue V1,\n                                        SDValue V2, SelectionDAG &DAG) {\n  SmallVector<int, 32> Unpckl, Unpckh;\n  createSplat2ShuffleMask(VT, Unpckl, /* Lo */ true);\n  createSplat2ShuffleMask(VT, Unpckh, /* Lo */ false);\n\n  unsigned UnpackOpcode;\n  if (isShuffleEquivalent(Mask, Unpckl, V1, V2))\n    UnpackOpcode = X86ISD::UNPCKL;\n  else if (isShuffleEquivalent(Mask, Unpckh, V1, V2))\n    UnpackOpcode = X86ISD::UNPCKH;\n  else\n    return SDValue();\n\n  // This is a \"natural\" unpack operation (rather than the 128-bit sectored\n  // operation implemented by AVX). We need to rearrange 64-bit chunks of the\n  // input in order to use the x86 instruction.\n  V1 = DAG.getVectorShuffle(MVT::v4f64, DL, DAG.getBitcast(MVT::v4f64, V1),\n                            DAG.getUNDEF(MVT::v4f64), {0, 2, 1, 3});\n  V1 = DAG.getBitcast(VT, V1);\n  return DAG.getNode(UnpackOpcode, DL, VT, V1, V1);\n}\n\n// Check if the mask can be mapped to a TRUNCATE or VTRUNC, truncating the\n// source into the lower elements and zeroing the upper elements.\nstatic bool matchShuffleAsVTRUNC(MVT &SrcVT, MVT &DstVT, MVT VT,\n                                 ArrayRef<int> Mask, const APInt &Zeroable,\n                                 const X86Subtarget &Subtarget) {\n  if (!VT.is512BitVector() && !Subtarget.hasVLX())\n    return false;\n\n  unsigned NumElts = Mask.size();\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  unsigned MaxScale = 64 / EltSizeInBits;\n\n  for (unsigned Scale = 2; Scale <= MaxScale; Scale += Scale) {\n    unsigned SrcEltBits = EltSizeInBits * Scale;\n    if (SrcEltBits < 32 && !Subtarget.hasBWI())\n      continue;\n    unsigned NumSrcElts = NumElts / Scale;\n    if (!isSequentialOrUndefInRange(Mask, 0, NumSrcElts, 0, Scale))\n      continue;\n    unsigned UpperElts = NumElts - NumSrcElts;\n    if (!Zeroable.extractBits(UpperElts, NumSrcElts).isAllOnesValue())\n      continue;\n    SrcVT = MVT::getIntegerVT(EltSizeInBits * Scale);\n    SrcVT = MVT::getVectorVT(SrcVT, NumSrcElts);\n    DstVT = MVT::getIntegerVT(EltSizeInBits);\n    if ((NumSrcElts * EltSizeInBits) >= 128) {\n      // ISD::TRUNCATE\n      DstVT = MVT::getVectorVT(DstVT, NumSrcElts);\n    } else {\n      // X86ISD::VTRUNC\n      DstVT = MVT::getVectorVT(DstVT, 128 / EltSizeInBits);\n    }\n    return true;\n  }\n\n  return false;\n}\n\n// Helper to create TRUNCATE/VTRUNC nodes, optionally with zero/undef upper\n// element padding to the final DstVT.\nstatic SDValue getAVX512TruncNode(const SDLoc &DL, MVT DstVT, SDValue Src,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG, bool ZeroUppers) {\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT DstSVT = DstVT.getScalarType();\n  unsigned NumDstElts = DstVT.getVectorNumElements();\n  unsigned NumSrcElts = SrcVT.getVectorNumElements();\n  unsigned DstEltSizeInBits = DstVT.getScalarSizeInBits();\n\n  if (!DAG.getTargetLoweringInfo().isTypeLegal(SrcVT))\n    return SDValue();\n\n  // Perform a direct ISD::TRUNCATE if possible.\n  if (NumSrcElts == NumDstElts)\n    return DAG.getNode(ISD::TRUNCATE, DL, DstVT, Src);\n\n  if (NumSrcElts > NumDstElts) {\n    MVT TruncVT = MVT::getVectorVT(DstSVT, NumSrcElts);\n    SDValue Trunc = DAG.getNode(ISD::TRUNCATE, DL, TruncVT, Src);\n    return extractSubVector(Trunc, 0, DAG, DL, DstVT.getSizeInBits());\n  }\n\n  if ((NumSrcElts * DstEltSizeInBits) >= 128) {\n    MVT TruncVT = MVT::getVectorVT(DstSVT, NumSrcElts);\n    SDValue Trunc = DAG.getNode(ISD::TRUNCATE, DL, TruncVT, Src);\n    return widenSubVector(Trunc, ZeroUppers, Subtarget, DAG, DL,\n                          DstVT.getSizeInBits());\n  }\n\n  // Non-VLX targets must truncate from a 512-bit type, so we need to\n  // widen, truncate and then possibly extract the original subvector.\n  if (!Subtarget.hasVLX() && !SrcVT.is512BitVector()) {\n    SDValue NewSrc = widenSubVector(Src, ZeroUppers, Subtarget, DAG, DL, 512);\n    return getAVX512TruncNode(DL, DstVT, NewSrc, Subtarget, DAG, ZeroUppers);\n  }\n\n  // Fallback to a X86ISD::VTRUNC, padding if necessary.\n  MVT TruncVT = MVT::getVectorVT(DstSVT, 128 / DstEltSizeInBits);\n  SDValue Trunc = DAG.getNode(X86ISD::VTRUNC, DL, TruncVT, Src);\n  if (DstVT != TruncVT)\n    Trunc = widenSubVector(Trunc, ZeroUppers, Subtarget, DAG, DL,\n                           DstVT.getSizeInBits());\n  return Trunc;\n}\n\n// Try to lower trunc+vector_shuffle to a vpmovdb or a vpmovdw instruction.\n//\n// An example is the following:\n//\n// t0: ch = EntryToken\n//           t2: v4i64,ch = CopyFromReg t0, Register:v4i64 %0\n//         t25: v4i32 = truncate t2\n//       t41: v8i16 = bitcast t25\n//       t21: v8i16 = BUILD_VECTOR undef:i16, undef:i16, undef:i16, undef:i16,\n//       Constant:i16<0>, Constant:i16<0>, Constant:i16<0>, Constant:i16<0>\n//     t51: v8i16 = vector_shuffle<0,2,4,6,12,13,14,15> t41, t21\n//   t18: v2i64 = bitcast t51\n//\n// One can just use a single vpmovdw instruction, without avx512vl we need to\n// use the zmm variant and extract the lower subvector, padding with zeroes.\n// TODO: Merge with lowerShuffleAsVTRUNC.\nstatic SDValue lowerShuffleWithVPMOV(const SDLoc &DL, MVT VT, SDValue V1,\n                                     SDValue V2, ArrayRef<int> Mask,\n                                     const APInt &Zeroable,\n                                     const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  assert((VT == MVT::v16i8 || VT == MVT::v8i16) && \"Unexpected VTRUNC type\");\n  if (!Subtarget.hasAVX512())\n    return SDValue();\n\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  unsigned MaxScale = 64 / EltSizeInBits;\n  for (unsigned Scale = 2; Scale <= MaxScale; Scale += Scale) {\n    unsigned NumSrcElts = NumElts / Scale;\n    unsigned UpperElts = NumElts - NumSrcElts;\n    if (!isSequentialOrUndefInRange(Mask, 0, NumSrcElts, 0, Scale) ||\n        !Zeroable.extractBits(UpperElts, NumSrcElts).isAllOnesValue())\n      continue;\n\n    SDValue Src = V1;\n    if (!Src.hasOneUse())\n      return SDValue();\n\n    Src = peekThroughOneUseBitcasts(Src);\n    if (Src.getOpcode() != ISD::TRUNCATE ||\n        Src.getScalarValueSizeInBits() != (EltSizeInBits * Scale))\n      return SDValue();\n    Src = Src.getOperand(0);\n\n    // VPMOVWB is only available with avx512bw.\n    MVT SrcVT = Src.getSimpleValueType();\n    if (SrcVT.getVectorElementType() == MVT::i16 && VT == MVT::v16i8 &&\n        !Subtarget.hasBWI())\n      return SDValue();\n\n    bool UndefUppers = isUndefInRange(Mask, NumSrcElts, UpperElts);\n    return getAVX512TruncNode(DL, VT, Src, Subtarget, DAG, !UndefUppers);\n  }\n\n  return SDValue();\n}\n\n// Attempt to match binary shuffle patterns as a truncate.\nstatic SDValue lowerShuffleAsVTRUNC(const SDLoc &DL, MVT VT, SDValue V1,\n                                    SDValue V2, ArrayRef<int> Mask,\n                                    const APInt &Zeroable,\n                                    const X86Subtarget &Subtarget,\n                                    SelectionDAG &DAG) {\n  assert((VT.is128BitVector() || VT.is256BitVector()) &&\n         \"Unexpected VTRUNC type\");\n  if (!Subtarget.hasAVX512())\n    return SDValue();\n\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  unsigned MaxScale = 64 / EltSizeInBits;\n  for (unsigned Scale = 2; Scale <= MaxScale; Scale += Scale) {\n    // TODO: Support non-BWI VPMOVWB truncations?\n    unsigned SrcEltBits = EltSizeInBits * Scale;\n    if (SrcEltBits < 32 && !Subtarget.hasBWI())\n      continue;\n\n    // Match shuffle <0,Scale,2*Scale,..,undef_or_zero,undef_or_zero,...>\n    // Bail if the V2 elements are undef.\n    unsigned NumHalfSrcElts = NumElts / Scale;\n    unsigned NumSrcElts = 2 * NumHalfSrcElts;\n    if (!isSequentialOrUndefInRange(Mask, 0, NumSrcElts, 0, Scale) ||\n        isUndefInRange(Mask, NumHalfSrcElts, NumHalfSrcElts))\n      continue;\n\n    // The elements beyond the truncation must be undef/zero.\n    unsigned UpperElts = NumElts - NumSrcElts;\n    if (UpperElts > 0 &&\n        !Zeroable.extractBits(UpperElts, NumSrcElts).isAllOnesValue())\n      continue;\n    bool UndefUppers =\n        UpperElts > 0 && isUndefInRange(Mask, NumSrcElts, UpperElts);\n\n    // As we're using both sources then we need to concat them together\n    // and truncate from the double-sized src.\n    MVT ConcatVT = MVT::getVectorVT(VT.getScalarType(), NumElts * 2);\n    SDValue Src = DAG.getNode(ISD::CONCAT_VECTORS, DL, ConcatVT, V1, V2);\n\n    MVT SrcSVT = MVT::getIntegerVT(SrcEltBits);\n    MVT SrcVT = MVT::getVectorVT(SrcSVT, NumSrcElts);\n    Src = DAG.getBitcast(SrcVT, Src);\n    return getAVX512TruncNode(DL, VT, Src, Subtarget, DAG, !UndefUppers);\n  }\n\n  return SDValue();\n}\n\n/// Check whether a compaction lowering can be done by dropping even\n/// elements and compute how many times even elements must be dropped.\n///\n/// This handles shuffles which take every Nth element where N is a power of\n/// two. Example shuffle masks:\n///\n///  N = 1:  0,  2,  4,  6,  8, 10, 12, 14,  0,  2,  4,  6,  8, 10, 12, 14\n///  N = 1:  0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30\n///  N = 2:  0,  4,  8, 12,  0,  4,  8, 12,  0,  4,  8, 12,  0,  4,  8, 12\n///  N = 2:  0,  4,  8, 12, 16, 20, 24, 28,  0,  4,  8, 12, 16, 20, 24, 28\n///  N = 3:  0,  8,  0,  8,  0,  8,  0,  8,  0,  8,  0,  8,  0,  8,  0,  8\n///  N = 3:  0,  8, 16, 24,  0,  8, 16, 24,  0,  8, 16, 24,  0,  8, 16, 24\n///\n/// Any of these lanes can of course be undef.\n///\n/// This routine only supports N <= 3.\n/// FIXME: Evaluate whether either AVX or AVX-512 have any opportunities here\n/// for larger N.\n///\n/// \\returns N above, or the number of times even elements must be dropped if\n/// there is such a number. Otherwise returns zero.\nstatic int canLowerByDroppingEvenElements(ArrayRef<int> Mask,\n                                          bool IsSingleInput) {\n  // The modulus for the shuffle vector entries is based on whether this is\n  // a single input or not.\n  int ShuffleModulus = Mask.size() * (IsSingleInput ? 1 : 2);\n  assert(isPowerOf2_32((uint32_t)ShuffleModulus) &&\n         \"We should only be called with masks with a power-of-2 size!\");\n\n  uint64_t ModMask = (uint64_t)ShuffleModulus - 1;\n\n  // We track whether the input is viable for all power-of-2 strides 2^1, 2^2,\n  // and 2^3 simultaneously. This is because we may have ambiguity with\n  // partially undef inputs.\n  bool ViableForN[3] = {true, true, true};\n\n  for (int i = 0, e = Mask.size(); i < e; ++i) {\n    // Ignore undef lanes, we'll optimistically collapse them to the pattern we\n    // want.\n    if (Mask[i] < 0)\n      continue;\n\n    bool IsAnyViable = false;\n    for (unsigned j = 0; j != array_lengthof(ViableForN); ++j)\n      if (ViableForN[j]) {\n        uint64_t N = j + 1;\n\n        // The shuffle mask must be equal to (i * 2^N) % M.\n        if ((uint64_t)Mask[i] == (((uint64_t)i << N) & ModMask))\n          IsAnyViable = true;\n        else\n          ViableForN[j] = false;\n      }\n    // Early exit if we exhaust the possible powers of two.\n    if (!IsAnyViable)\n      break;\n  }\n\n  for (unsigned j = 0; j != array_lengthof(ViableForN); ++j)\n    if (ViableForN[j])\n      return j + 1;\n\n  // Return 0 as there is no viable power of two.\n  return 0;\n}\n\n// X86 has dedicated pack instructions that can handle specific truncation\n// operations: PACKSS and PACKUS.\n// Checks for compaction shuffle masks if MaxStages > 1.\n// TODO: Add support for matching multiple PACKSS/PACKUS stages.\nstatic bool matchShuffleWithPACK(MVT VT, MVT &SrcVT, SDValue &V1, SDValue &V2,\n                                 unsigned &PackOpcode, ArrayRef<int> TargetMask,\n                                 SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget,\n                                 unsigned MaxStages = 1) {\n  unsigned NumElts = VT.getVectorNumElements();\n  unsigned BitSize = VT.getScalarSizeInBits();\n  assert(0 < MaxStages && MaxStages <= 3 && (BitSize << MaxStages) <= 64 &&\n         \"Illegal maximum compaction\");\n\n  auto MatchPACK = [&](SDValue N1, SDValue N2, MVT PackVT) {\n    unsigned NumSrcBits = PackVT.getScalarSizeInBits();\n    unsigned NumPackedBits = NumSrcBits - BitSize;\n    SDValue VV1 = DAG.getBitcast(PackVT, N1);\n    SDValue VV2 = DAG.getBitcast(PackVT, N2);\n    if (Subtarget.hasSSE41() || BitSize == 8) {\n      APInt ZeroMask = APInt::getHighBitsSet(NumSrcBits, NumPackedBits);\n      if ((N1.isUndef() || DAG.MaskedValueIsZero(VV1, ZeroMask)) &&\n          (N2.isUndef() || DAG.MaskedValueIsZero(VV2, ZeroMask))) {\n        V1 = VV1;\n        V2 = VV2;\n        SrcVT = PackVT;\n        PackOpcode = X86ISD::PACKUS;\n        return true;\n      }\n    }\n    if ((N1.isUndef() || DAG.ComputeNumSignBits(VV1) > NumPackedBits) &&\n        (N2.isUndef() || DAG.ComputeNumSignBits(VV2) > NumPackedBits)) {\n      V1 = VV1;\n      V2 = VV2;\n      SrcVT = PackVT;\n      PackOpcode = X86ISD::PACKSS;\n      return true;\n    }\n    return false;\n  };\n\n  // Attempt to match against wider and wider compaction patterns.\n  for (unsigned NumStages = 1; NumStages <= MaxStages; ++NumStages) {\n    MVT PackSVT = MVT::getIntegerVT(BitSize << NumStages);\n    MVT PackVT = MVT::getVectorVT(PackSVT, NumElts >> NumStages);\n\n    // Try binary shuffle.\n    SmallVector<int, 32> BinaryMask;\n    createPackShuffleMask(VT, BinaryMask, false, NumStages);\n    if (isTargetShuffleEquivalent(VT, TargetMask, BinaryMask, V1, V2))\n      if (MatchPACK(V1, V2, PackVT))\n        return true;\n\n    // Try unary shuffle.\n    SmallVector<int, 32> UnaryMask;\n    createPackShuffleMask(VT, UnaryMask, true, NumStages);\n    if (isTargetShuffleEquivalent(VT, TargetMask, UnaryMask, V1))\n      if (MatchPACK(V1, V1, PackVT))\n        return true;\n  }\n\n  return false;\n}\n\nstatic SDValue lowerShuffleWithPACK(const SDLoc &DL, MVT VT, ArrayRef<int> Mask,\n                                    SDValue V1, SDValue V2, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  MVT PackVT;\n  unsigned PackOpcode;\n  unsigned SizeBits = VT.getSizeInBits();\n  unsigned EltBits = VT.getScalarSizeInBits();\n  unsigned MaxStages = Log2_32(64 / EltBits);\n  if (!matchShuffleWithPACK(VT, PackVT, V1, V2, PackOpcode, Mask, DAG,\n                            Subtarget, MaxStages))\n    return SDValue();\n\n  unsigned CurrentEltBits = PackVT.getScalarSizeInBits();\n  unsigned NumStages = Log2_32(CurrentEltBits / EltBits);\n\n  // Don't lower multi-stage packs on AVX512, truncation is better.\n  if (NumStages != 1 && SizeBits == 128 && Subtarget.hasVLX())\n    return SDValue();\n\n  // Pack to the largest type possible:\n  // vXi64/vXi32 -> PACK*SDW and vXi16 -> PACK*SWB.\n  unsigned MaxPackBits = 16;\n  if (CurrentEltBits > 16 &&\n      (PackOpcode == X86ISD::PACKSS || Subtarget.hasSSE41()))\n    MaxPackBits = 32;\n\n  // Repeatedly pack down to the target size.\n  SDValue Res;\n  for (unsigned i = 0; i != NumStages; ++i) {\n    unsigned SrcEltBits = std::min(MaxPackBits, CurrentEltBits);\n    unsigned NumSrcElts = SizeBits / SrcEltBits;\n    MVT SrcSVT = MVT::getIntegerVT(SrcEltBits);\n    MVT DstSVT = MVT::getIntegerVT(SrcEltBits / 2);\n    MVT SrcVT = MVT::getVectorVT(SrcSVT, NumSrcElts);\n    MVT DstVT = MVT::getVectorVT(DstSVT, NumSrcElts * 2);\n    Res = DAG.getNode(PackOpcode, DL, DstVT, DAG.getBitcast(SrcVT, V1),\n                      DAG.getBitcast(SrcVT, V2));\n    V1 = V2 = Res;\n    CurrentEltBits /= 2;\n  }\n  assert(Res && Res.getValueType() == VT &&\n         \"Failed to lower compaction shuffle\");\n  return Res;\n}\n\n/// Try to emit a bitmask instruction for a shuffle.\n///\n/// This handles cases where we can model a blend exactly as a bitmask due to\n/// one of the inputs being zeroable.\nstatic SDValue lowerShuffleAsBitMask(const SDLoc &DL, MVT VT, SDValue V1,\n                                     SDValue V2, ArrayRef<int> Mask,\n                                     const APInt &Zeroable,\n                                     const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  MVT MaskVT = VT;\n  MVT EltVT = VT.getVectorElementType();\n  SDValue Zero, AllOnes;\n  // Use f64 if i64 isn't legal.\n  if (EltVT == MVT::i64 && !Subtarget.is64Bit()) {\n    EltVT = MVT::f64;\n    MaskVT = MVT::getVectorVT(EltVT, Mask.size());\n  }\n\n  MVT LogicVT = VT;\n  if (EltVT == MVT::f32 || EltVT == MVT::f64) {\n    Zero = DAG.getConstantFP(0.0, DL, EltVT);\n    APFloat AllOnesValue = APFloat::getAllOnesValue(\n        SelectionDAG::EVTToAPFloatSemantics(EltVT), EltVT.getSizeInBits());\n    AllOnes = DAG.getConstantFP(AllOnesValue, DL, EltVT);\n    LogicVT =\n        MVT::getVectorVT(EltVT == MVT::f64 ? MVT::i64 : MVT::i32, Mask.size());\n  } else {\n    Zero = DAG.getConstant(0, DL, EltVT);\n    AllOnes = DAG.getAllOnesConstant(DL, EltVT);\n  }\n\n  SmallVector<SDValue, 16> VMaskOps(Mask.size(), Zero);\n  SDValue V;\n  for (int i = 0, Size = Mask.size(); i < Size; ++i) {\n    if (Zeroable[i])\n      continue;\n    if (Mask[i] % Size != i)\n      return SDValue(); // Not a blend.\n    if (!V)\n      V = Mask[i] < Size ? V1 : V2;\n    else if (V != (Mask[i] < Size ? V1 : V2))\n      return SDValue(); // Can only let one input through the mask.\n\n    VMaskOps[i] = AllOnes;\n  }\n  if (!V)\n    return SDValue(); // No non-zeroable elements!\n\n  SDValue VMask = DAG.getBuildVector(MaskVT, DL, VMaskOps);\n  VMask = DAG.getBitcast(LogicVT, VMask);\n  V = DAG.getBitcast(LogicVT, V);\n  SDValue And = DAG.getNode(ISD::AND, DL, LogicVT, V, VMask);\n  return DAG.getBitcast(VT, And);\n}\n\n/// Try to emit a blend instruction for a shuffle using bit math.\n///\n/// This is used as a fallback approach when first class blend instructions are\n/// unavailable. Currently it is only suitable for integer vectors, but could\n/// be generalized for floating point vectors if desirable.\nstatic SDValue lowerShuffleAsBitBlend(const SDLoc &DL, MVT VT, SDValue V1,\n                                      SDValue V2, ArrayRef<int> Mask,\n                                      SelectionDAG &DAG) {\n  assert(VT.isInteger() && \"Only supports integer vector types!\");\n  MVT EltVT = VT.getVectorElementType();\n  SDValue Zero = DAG.getConstant(0, DL, EltVT);\n  SDValue AllOnes = DAG.getAllOnesConstant(DL, EltVT);\n  SmallVector<SDValue, 16> MaskOps;\n  for (int i = 0, Size = Mask.size(); i < Size; ++i) {\n    if (Mask[i] >= 0 && Mask[i] != i && Mask[i] != i + Size)\n      return SDValue(); // Shuffled input!\n    MaskOps.push_back(Mask[i] < Size ? AllOnes : Zero);\n  }\n\n  SDValue V1Mask = DAG.getBuildVector(VT, DL, MaskOps);\n  V1 = DAG.getNode(ISD::AND, DL, VT, V1, V1Mask);\n  V2 = DAG.getNode(X86ISD::ANDNP, DL, VT, V1Mask, V2);\n  return DAG.getNode(ISD::OR, DL, VT, V1, V2);\n}\n\nstatic SDValue getVectorMaskingNode(SDValue Op, SDValue Mask,\n                                    SDValue PreservedSrc,\n                                    const X86Subtarget &Subtarget,\n                                    SelectionDAG &DAG);\n\nstatic bool matchShuffleAsBlend(SDValue V1, SDValue V2,\n                                MutableArrayRef<int> Mask,\n                                const APInt &Zeroable, bool &ForceV1Zero,\n                                bool &ForceV2Zero, uint64_t &BlendMask) {\n  bool V1IsZeroOrUndef =\n      V1.isUndef() || ISD::isBuildVectorAllZeros(V1.getNode());\n  bool V2IsZeroOrUndef =\n      V2.isUndef() || ISD::isBuildVectorAllZeros(V2.getNode());\n\n  BlendMask = 0;\n  ForceV1Zero = false, ForceV2Zero = false;\n  assert(Mask.size() <= 64 && \"Shuffle mask too big for blend mask\");\n\n  // Attempt to generate the binary blend mask. If an input is zero then\n  // we can use any lane.\n  for (int i = 0, Size = Mask.size(); i < Size; ++i) {\n    int M = Mask[i];\n    if (M == SM_SentinelUndef)\n      continue;\n    if (M == i)\n      continue;\n    if (M == i + Size) {\n      BlendMask |= 1ull << i;\n      continue;\n    }\n    if (Zeroable[i]) {\n      if (V1IsZeroOrUndef) {\n        ForceV1Zero = true;\n        Mask[i] = i;\n        continue;\n      }\n      if (V2IsZeroOrUndef) {\n        ForceV2Zero = true;\n        BlendMask |= 1ull << i;\n        Mask[i] = i + Size;\n        continue;\n      }\n    }\n    return false;\n  }\n  return true;\n}\n\nstatic uint64_t scaleVectorShuffleBlendMask(uint64_t BlendMask, int Size,\n                                            int Scale) {\n  uint64_t ScaledMask = 0;\n  for (int i = 0; i != Size; ++i)\n    if (BlendMask & (1ull << i))\n      ScaledMask |= ((1ull << Scale) - 1) << (i * Scale);\n  return ScaledMask;\n}\n\n/// Try to emit a blend instruction for a shuffle.\n///\n/// This doesn't do any checks for the availability of instructions for blending\n/// these values. It relies on the availability of the X86ISD::BLENDI pattern to\n/// be matched in the backend with the type given. What it does check for is\n/// that the shuffle mask is a blend, or convertible into a blend with zero.\nstatic SDValue lowerShuffleAsBlend(const SDLoc &DL, MVT VT, SDValue V1,\n                                   SDValue V2, ArrayRef<int> Original,\n                                   const APInt &Zeroable,\n                                   const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  uint64_t BlendMask = 0;\n  bool ForceV1Zero = false, ForceV2Zero = false;\n  SmallVector<int, 64> Mask(Original.begin(), Original.end());\n  if (!matchShuffleAsBlend(V1, V2, Mask, Zeroable, ForceV1Zero, ForceV2Zero,\n                           BlendMask))\n    return SDValue();\n\n  // Create a REAL zero vector - ISD::isBuildVectorAllZeros allows UNDEFs.\n  if (ForceV1Zero)\n    V1 = getZeroVector(VT, Subtarget, DAG, DL);\n  if (ForceV2Zero)\n    V2 = getZeroVector(VT, Subtarget, DAG, DL);\n\n  switch (VT.SimpleTy) {\n  case MVT::v4i64:\n  case MVT::v8i32:\n    assert(Subtarget.hasAVX2() && \"256-bit integer blends require AVX2!\");\n    LLVM_FALLTHROUGH;\n  case MVT::v4f64:\n  case MVT::v8f32:\n    assert(Subtarget.hasAVX() && \"256-bit float blends require AVX!\");\n    LLVM_FALLTHROUGH;\n  case MVT::v2f64:\n  case MVT::v2i64:\n  case MVT::v4f32:\n  case MVT::v4i32:\n  case MVT::v8i16:\n    assert(Subtarget.hasSSE41() && \"128-bit blends require SSE41!\");\n    return DAG.getNode(X86ISD::BLENDI, DL, VT, V1, V2,\n                       DAG.getTargetConstant(BlendMask, DL, MVT::i8));\n  case MVT::v16i16: {\n    assert(Subtarget.hasAVX2() && \"v16i16 blends require AVX2!\");\n    SmallVector<int, 8> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MVT::v16i16, Mask, RepeatedMask)) {\n      // We can lower these with PBLENDW which is mirrored across 128-bit lanes.\n      assert(RepeatedMask.size() == 8 && \"Repeated mask size doesn't match!\");\n      BlendMask = 0;\n      for (int i = 0; i < 8; ++i)\n        if (RepeatedMask[i] >= 8)\n          BlendMask |= 1ull << i;\n      return DAG.getNode(X86ISD::BLENDI, DL, MVT::v16i16, V1, V2,\n                         DAG.getTargetConstant(BlendMask, DL, MVT::i8));\n    }\n    // Use PBLENDW for lower/upper lanes and then blend lanes.\n    // TODO - we should allow 2 PBLENDW here and leave shuffle combine to\n    // merge to VSELECT where useful.\n    uint64_t LoMask = BlendMask & 0xFF;\n    uint64_t HiMask = (BlendMask >> 8) & 0xFF;\n    if (LoMask == 0 || LoMask == 255 || HiMask == 0 || HiMask == 255) {\n      SDValue Lo = DAG.getNode(X86ISD::BLENDI, DL, MVT::v16i16, V1, V2,\n                               DAG.getTargetConstant(LoMask, DL, MVT::i8));\n      SDValue Hi = DAG.getNode(X86ISD::BLENDI, DL, MVT::v16i16, V1, V2,\n                               DAG.getTargetConstant(HiMask, DL, MVT::i8));\n      return DAG.getVectorShuffle(\n          MVT::v16i16, DL, Lo, Hi,\n          {0, 1, 2, 3, 4, 5, 6, 7, 24, 25, 26, 27, 28, 29, 30, 31});\n    }\n    LLVM_FALLTHROUGH;\n  }\n  case MVT::v32i8:\n    assert(Subtarget.hasAVX2() && \"256-bit byte-blends require AVX2!\");\n    LLVM_FALLTHROUGH;\n  case MVT::v16i8: {\n    assert(Subtarget.hasSSE41() && \"128-bit byte-blends require SSE41!\");\n\n    // Attempt to lower to a bitmask if we can. VPAND is faster than VPBLENDVB.\n    if (SDValue Masked = lowerShuffleAsBitMask(DL, VT, V1, V2, Mask, Zeroable,\n                                               Subtarget, DAG))\n      return Masked;\n\n    if (Subtarget.hasBWI() && Subtarget.hasVLX()) {\n      MVT IntegerType =\n          MVT::getIntegerVT(std::max((int)VT.getVectorNumElements(), 8));\n      SDValue MaskNode = DAG.getConstant(BlendMask, DL, IntegerType);\n      return getVectorMaskingNode(V2, MaskNode, V1, Subtarget, DAG);\n    }\n\n    // If we have VPTERNLOG, we can use that as a bit blend.\n    if (Subtarget.hasVLX())\n      if (SDValue BitBlend =\n              lowerShuffleAsBitBlend(DL, VT, V1, V2, Mask, DAG))\n        return BitBlend;\n\n    // Scale the blend by the number of bytes per element.\n    int Scale = VT.getScalarSizeInBits() / 8;\n\n    // This form of blend is always done on bytes. Compute the byte vector\n    // type.\n    MVT BlendVT = MVT::getVectorVT(MVT::i8, VT.getSizeInBits() / 8);\n\n    // x86 allows load folding with blendvb from the 2nd source operand. But\n    // we are still using LLVM select here (see comment below), so that's V1.\n    // If V2 can be load-folded and V1 cannot be load-folded, then commute to\n    // allow that load-folding possibility.\n    if (!ISD::isNormalLoad(V1.getNode()) && ISD::isNormalLoad(V2.getNode())) {\n      ShuffleVectorSDNode::commuteMask(Mask);\n      std::swap(V1, V2);\n    }\n\n    // Compute the VSELECT mask. Note that VSELECT is really confusing in the\n    // mix of LLVM's code generator and the x86 backend. We tell the code\n    // generator that boolean values in the elements of an x86 vector register\n    // are -1 for true and 0 for false. We then use the LLVM semantics of 'true'\n    // mapping a select to operand #1, and 'false' mapping to operand #2. The\n    // reality in x86 is that vector masks (pre-AVX-512) use only the high bit\n    // of the element (the remaining are ignored) and 0 in that high bit would\n    // mean operand #1 while 1 in the high bit would mean operand #2. So while\n    // the LLVM model for boolean values in vector elements gets the relevant\n    // bit set, it is set backwards and over constrained relative to x86's\n    // actual model.\n    SmallVector<SDValue, 32> VSELECTMask;\n    for (int i = 0, Size = Mask.size(); i < Size; ++i)\n      for (int j = 0; j < Scale; ++j)\n        VSELECTMask.push_back(\n            Mask[i] < 0 ? DAG.getUNDEF(MVT::i8)\n                        : DAG.getConstant(Mask[i] < Size ? -1 : 0, DL,\n                                          MVT::i8));\n\n    V1 = DAG.getBitcast(BlendVT, V1);\n    V2 = DAG.getBitcast(BlendVT, V2);\n    return DAG.getBitcast(\n        VT,\n        DAG.getSelect(DL, BlendVT, DAG.getBuildVector(BlendVT, DL, VSELECTMask),\n                      V1, V2));\n  }\n  case MVT::v16f32:\n  case MVT::v8f64:\n  case MVT::v8i64:\n  case MVT::v16i32:\n  case MVT::v32i16:\n  case MVT::v64i8: {\n    // Attempt to lower to a bitmask if we can. Only if not optimizing for size.\n    bool OptForSize = DAG.shouldOptForSize();\n    if (!OptForSize) {\n      if (SDValue Masked = lowerShuffleAsBitMask(DL, VT, V1, V2, Mask, Zeroable,\n                                                 Subtarget, DAG))\n        return Masked;\n    }\n\n    // Otherwise load an immediate into a GPR, cast to k-register, and use a\n    // masked move.\n    MVT IntegerType =\n        MVT::getIntegerVT(std::max((int)VT.getVectorNumElements(), 8));\n    SDValue MaskNode = DAG.getConstant(BlendMask, DL, IntegerType);\n    return getVectorMaskingNode(V2, MaskNode, V1, Subtarget, DAG);\n  }\n  default:\n    llvm_unreachable(\"Not a supported integer vector type!\");\n  }\n}\n\n/// Try to lower as a blend of elements from two inputs followed by\n/// a single-input permutation.\n///\n/// This matches the pattern where we can blend elements from two inputs and\n/// then reduce the shuffle to a single-input permutation.\nstatic SDValue lowerShuffleAsBlendAndPermute(const SDLoc &DL, MVT VT,\n                                             SDValue V1, SDValue V2,\n                                             ArrayRef<int> Mask,\n                                             SelectionDAG &DAG,\n                                             bool ImmBlends = false) {\n  // We build up the blend mask while checking whether a blend is a viable way\n  // to reduce the shuffle.\n  SmallVector<int, 32> BlendMask(Mask.size(), -1);\n  SmallVector<int, 32> PermuteMask(Mask.size(), -1);\n\n  for (int i = 0, Size = Mask.size(); i < Size; ++i) {\n    if (Mask[i] < 0)\n      continue;\n\n    assert(Mask[i] < Size * 2 && \"Shuffle input is out of bounds.\");\n\n    if (BlendMask[Mask[i] % Size] < 0)\n      BlendMask[Mask[i] % Size] = Mask[i];\n    else if (BlendMask[Mask[i] % Size] != Mask[i])\n      return SDValue(); // Can't blend in the needed input!\n\n    PermuteMask[i] = Mask[i] % Size;\n  }\n\n  // If only immediate blends, then bail if the blend mask can't be widened to\n  // i16.\n  unsigned EltSize = VT.getScalarSizeInBits();\n  if (ImmBlends && EltSize == 8 && !canWidenShuffleElements(BlendMask))\n    return SDValue();\n\n  SDValue V = DAG.getVectorShuffle(VT, DL, V1, V2, BlendMask);\n  return DAG.getVectorShuffle(VT, DL, V, DAG.getUNDEF(VT), PermuteMask);\n}\n\n/// Try to lower as an unpack of elements from two inputs followed by\n/// a single-input permutation.\n///\n/// This matches the pattern where we can unpack elements from two inputs and\n/// then reduce the shuffle to a single-input (wider) permutation.\nstatic SDValue lowerShuffleAsUNPCKAndPermute(const SDLoc &DL, MVT VT,\n                                             SDValue V1, SDValue V2,\n                                             ArrayRef<int> Mask,\n                                             SelectionDAG &DAG) {\n  int NumElts = Mask.size();\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumLaneElts = NumElts / NumLanes;\n  int NumHalfLaneElts = NumLaneElts / 2;\n\n  bool MatchLo = true, MatchHi = true;\n  SDValue Ops[2] = {DAG.getUNDEF(VT), DAG.getUNDEF(VT)};\n\n  // Determine UNPCKL/UNPCKH type and operand order.\n  for (int Lane = 0; Lane != NumElts; Lane += NumLaneElts) {\n    for (int Elt = 0; Elt != NumLaneElts; ++Elt) {\n      int M = Mask[Lane + Elt];\n      if (M < 0)\n        continue;\n\n      SDValue &Op = Ops[Elt & 1];\n      if (M < NumElts && (Op.isUndef() || Op == V1))\n        Op = V1;\n      else if (NumElts <= M && (Op.isUndef() || Op == V2))\n        Op = V2;\n      else\n        return SDValue();\n\n      int Lo = Lane, Mid = Lane + NumHalfLaneElts, Hi = Lane + NumLaneElts;\n      MatchLo &= isUndefOrInRange(M, Lo, Mid) ||\n                 isUndefOrInRange(M, NumElts + Lo, NumElts + Mid);\n      MatchHi &= isUndefOrInRange(M, Mid, Hi) ||\n                 isUndefOrInRange(M, NumElts + Mid, NumElts + Hi);\n      if (!MatchLo && !MatchHi)\n        return SDValue();\n    }\n  }\n  assert((MatchLo ^ MatchHi) && \"Failed to match UNPCKLO/UNPCKHI\");\n\n  // Now check that each pair of elts come from the same unpack pair\n  // and set the permute mask based on each pair.\n  // TODO - Investigate cases where we permute individual elements.\n  SmallVector<int, 32> PermuteMask(NumElts, -1);\n  for (int Lane = 0; Lane != NumElts; Lane += NumLaneElts) {\n    for (int Elt = 0; Elt != NumLaneElts; Elt += 2) {\n      int M0 = Mask[Lane + Elt + 0];\n      int M1 = Mask[Lane + Elt + 1];\n      if (0 <= M0 && 0 <= M1 &&\n          (M0 % NumHalfLaneElts) != (M1 % NumHalfLaneElts))\n        return SDValue();\n      if (0 <= M0)\n        PermuteMask[Lane + Elt + 0] = Lane + (2 * (M0 % NumHalfLaneElts));\n      if (0 <= M1)\n        PermuteMask[Lane + Elt + 1] = Lane + (2 * (M1 % NumHalfLaneElts)) + 1;\n    }\n  }\n\n  unsigned UnpckOp = MatchLo ? X86ISD::UNPCKL : X86ISD::UNPCKH;\n  SDValue Unpck = DAG.getNode(UnpckOp, DL, VT, Ops);\n  return DAG.getVectorShuffle(VT, DL, Unpck, DAG.getUNDEF(VT), PermuteMask);\n}\n\n/// Helper to form a PALIGNR-based rotate+permute, merging 2 inputs and then\n/// permuting the elements of the result in place.\nstatic SDValue lowerShuffleAsByteRotateAndPermute(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  if ((VT.is128BitVector() && !Subtarget.hasSSSE3()) ||\n      (VT.is256BitVector() && !Subtarget.hasAVX2()) ||\n      (VT.is512BitVector() && !Subtarget.hasBWI()))\n    return SDValue();\n\n  // We don't currently support lane crossing permutes.\n  if (is128BitLaneCrossingShuffleMask(VT, Mask))\n    return SDValue();\n\n  int Scale = VT.getScalarSizeInBits() / 8;\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumElts = VT.getVectorNumElements();\n  int NumEltsPerLane = NumElts / NumLanes;\n\n  // Determine range of mask elts.\n  bool Blend1 = true;\n  bool Blend2 = true;\n  std::pair<int, int> Range1 = std::make_pair(INT_MAX, INT_MIN);\n  std::pair<int, int> Range2 = std::make_pair(INT_MAX, INT_MIN);\n  for (int Lane = 0; Lane != NumElts; Lane += NumEltsPerLane) {\n    for (int Elt = 0; Elt != NumEltsPerLane; ++Elt) {\n      int M = Mask[Lane + Elt];\n      if (M < 0)\n        continue;\n      if (M < NumElts) {\n        Blend1 &= (M == (Lane + Elt));\n        assert(Lane <= M && M < (Lane + NumEltsPerLane) && \"Out of range mask\");\n        M = M % NumEltsPerLane;\n        Range1.first = std::min(Range1.first, M);\n        Range1.second = std::max(Range1.second, M);\n      } else {\n        M -= NumElts;\n        Blend2 &= (M == (Lane + Elt));\n        assert(Lane <= M && M < (Lane + NumEltsPerLane) && \"Out of range mask\");\n        M = M % NumEltsPerLane;\n        Range2.first = std::min(Range2.first, M);\n        Range2.second = std::max(Range2.second, M);\n      }\n    }\n  }\n\n  // Bail if we don't need both elements.\n  // TODO - it might be worth doing this for unary shuffles if the permute\n  // can be widened.\n  if (!(0 <= Range1.first && Range1.second < NumEltsPerLane) ||\n      !(0 <= Range2.first && Range2.second < NumEltsPerLane))\n    return SDValue();\n\n  if (VT.getSizeInBits() > 128 && (Blend1 || Blend2))\n    return SDValue();\n\n  // Rotate the 2 ops so we can access both ranges, then permute the result.\n  auto RotateAndPermute = [&](SDValue Lo, SDValue Hi, int RotAmt, int Ofs) {\n    MVT ByteVT = MVT::getVectorVT(MVT::i8, VT.getSizeInBits() / 8);\n    SDValue Rotate = DAG.getBitcast(\n        VT, DAG.getNode(X86ISD::PALIGNR, DL, ByteVT, DAG.getBitcast(ByteVT, Hi),\n                        DAG.getBitcast(ByteVT, Lo),\n                        DAG.getTargetConstant(Scale * RotAmt, DL, MVT::i8)));\n    SmallVector<int, 64> PermMask(NumElts, SM_SentinelUndef);\n    for (int Lane = 0; Lane != NumElts; Lane += NumEltsPerLane) {\n      for (int Elt = 0; Elt != NumEltsPerLane; ++Elt) {\n        int M = Mask[Lane + Elt];\n        if (M < 0)\n          continue;\n        if (M < NumElts)\n          PermMask[Lane + Elt] = Lane + ((M + Ofs - RotAmt) % NumEltsPerLane);\n        else\n          PermMask[Lane + Elt] = Lane + ((M - Ofs - RotAmt) % NumEltsPerLane);\n      }\n    }\n    return DAG.getVectorShuffle(VT, DL, Rotate, DAG.getUNDEF(VT), PermMask);\n  };\n\n  // Check if the ranges are small enough to rotate from either direction.\n  if (Range2.second < Range1.first)\n    return RotateAndPermute(V1, V2, Range1.first, 0);\n  if (Range1.second < Range2.first)\n    return RotateAndPermute(V2, V1, Range2.first, NumElts);\n  return SDValue();\n}\n\n/// Generic routine to decompose a shuffle and blend into independent\n/// blends and permutes.\n///\n/// This matches the extremely common pattern for handling combined\n/// shuffle+blend operations on newer X86 ISAs where we have very fast blend\n/// operations. It will try to pick the best arrangement of shuffles and\n/// blends. For vXi8/vXi16 shuffles we may use unpack instead of blend.\nstatic SDValue lowerShuffleAsDecomposedShuffleMerge(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  int NumElts = Mask.size();\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumEltsPerLane = NumElts / NumLanes;\n\n  // Shuffle the input elements into the desired positions in V1 and V2 and\n  // unpack/blend them together.\n  bool IsAlternating = true;\n  SmallVector<int, 32> V1Mask(NumElts, -1);\n  SmallVector<int, 32> V2Mask(NumElts, -1);\n  SmallVector<int, 32> FinalMask(NumElts, -1);\n  for (int i = 0; i < NumElts; ++i) {\n    int M = Mask[i];\n    if (M >= 0 && M < NumElts) {\n      V1Mask[i] = M;\n      FinalMask[i] = i;\n      IsAlternating &= (i & 1) == 0;\n    } else if (M >= NumElts) {\n      V2Mask[i] = M - NumElts;\n      FinalMask[i] = i + NumElts;\n      IsAlternating &= (i & 1) == 1;\n    }\n  }\n\n  // Try to lower with the simpler initial blend/unpack/rotate strategies unless\n  // one of the input shuffles would be a no-op. We prefer to shuffle inputs as\n  // the shuffle may be able to fold with a load or other benefit. However, when\n  // we'll have to do 2x as many shuffles in order to achieve this, a 2-input\n  // pre-shuffle first is a better strategy.\n  if (!isNoopShuffleMask(V1Mask) && !isNoopShuffleMask(V2Mask)) {\n    // Only prefer immediate blends to unpack/rotate.\n    if (SDValue BlendPerm = lowerShuffleAsBlendAndPermute(DL, VT, V1, V2, Mask,\n                                                          DAG, true))\n      return BlendPerm;\n    if (SDValue UnpackPerm = lowerShuffleAsUNPCKAndPermute(DL, VT, V1, V2, Mask,\n                                                           DAG))\n      return UnpackPerm;\n    if (SDValue RotatePerm = lowerShuffleAsByteRotateAndPermute(\n            DL, VT, V1, V2, Mask, Subtarget, DAG))\n      return RotatePerm;\n    // Unpack/rotate failed - try again with variable blends.\n    if (SDValue BlendPerm = lowerShuffleAsBlendAndPermute(DL, VT, V1, V2, Mask,\n                                                          DAG))\n      return BlendPerm;\n  }\n\n  // If the final mask is an alternating blend of vXi8/vXi16, convert to an\n  // UNPCKL(SHUFFLE, SHUFFLE) pattern.\n  // TODO: It doesn't have to be alternating - but each lane mustn't have more\n  // than half the elements coming from each source.\n  if (IsAlternating && VT.getScalarSizeInBits() < 32) {\n    V1Mask.assign(NumElts, -1);\n    V2Mask.assign(NumElts, -1);\n    FinalMask.assign(NumElts, -1);\n    for (int i = 0; i != NumElts; i += NumEltsPerLane)\n      for (int j = 0; j != NumEltsPerLane; ++j) {\n        int M = Mask[i + j];\n        if (M >= 0 && M < NumElts) {\n          V1Mask[i + (j / 2)] = M;\n          FinalMask[i + j] = i + (j / 2);\n        } else if (M >= NumElts) {\n          V2Mask[i + (j / 2)] = M - NumElts;\n          FinalMask[i + j] = i + (j / 2) + NumElts;\n        }\n      }\n  }\n\n  V1 = DAG.getVectorShuffle(VT, DL, V1, DAG.getUNDEF(VT), V1Mask);\n  V2 = DAG.getVectorShuffle(VT, DL, V2, DAG.getUNDEF(VT), V2Mask);\n  return DAG.getVectorShuffle(VT, DL, V1, V2, FinalMask);\n}\n\n/// Try to lower a vector shuffle as a bit rotation.\n///\n/// Look for a repeated rotation pattern in each sub group.\n/// Returns a ISD::ROTL element rotation amount or -1 if failed.\nstatic int matchShuffleAsBitRotate(ArrayRef<int> Mask, int NumSubElts) {\n  int NumElts = Mask.size();\n  assert((NumElts % NumSubElts) == 0 && \"Illegal shuffle mask\");\n\n  int RotateAmt = -1;\n  for (int i = 0; i != NumElts; i += NumSubElts) {\n    for (int j = 0; j != NumSubElts; ++j) {\n      int M = Mask[i + j];\n      if (M < 0)\n        continue;\n      if (!isInRange(M, i, i + NumSubElts))\n        return -1;\n      int Offset = (NumSubElts - (M - (i + j))) % NumSubElts;\n      if (0 <= RotateAmt && Offset != RotateAmt)\n        return -1;\n      RotateAmt = Offset;\n    }\n  }\n  return RotateAmt;\n}\n\nstatic int matchShuffleAsBitRotate(MVT &RotateVT, int EltSizeInBits,\n                                   const X86Subtarget &Subtarget,\n                                   ArrayRef<int> Mask) {\n  assert(!isNoopShuffleMask(Mask) && \"We shouldn't lower no-op shuffles!\");\n  assert(EltSizeInBits < 64 && \"Can't rotate 64-bit integers\");\n\n  // AVX512 only has vXi32/vXi64 rotates, so limit the rotation sub group size.\n  int MinSubElts = Subtarget.hasAVX512() ? std::max(32 / EltSizeInBits, 2) : 2;\n  int MaxSubElts = 64 / EltSizeInBits;\n  for (int NumSubElts = MinSubElts; NumSubElts <= MaxSubElts; NumSubElts *= 2) {\n    int RotateAmt = matchShuffleAsBitRotate(Mask, NumSubElts);\n    if (RotateAmt < 0)\n      continue;\n\n    int NumElts = Mask.size();\n    MVT RotateSVT = MVT::getIntegerVT(EltSizeInBits * NumSubElts);\n    RotateVT = MVT::getVectorVT(RotateSVT, NumElts / NumSubElts);\n    return RotateAmt * EltSizeInBits;\n  }\n\n  return -1;\n}\n\n/// Lower shuffle using X86ISD::VROTLI rotations.\nstatic SDValue lowerShuffleAsBitRotate(const SDLoc &DL, MVT VT, SDValue V1,\n                                       ArrayRef<int> Mask,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  // Only XOP + AVX512 targets have bit rotation instructions.\n  // If we at least have SSSE3 (PSHUFB) then we shouldn't attempt to use this.\n  bool IsLegal =\n      (VT.is128BitVector() && Subtarget.hasXOP()) || Subtarget.hasAVX512();\n  if (!IsLegal && Subtarget.hasSSE3())\n    return SDValue();\n\n  MVT RotateVT;\n  int RotateAmt = matchShuffleAsBitRotate(RotateVT, VT.getScalarSizeInBits(),\n                                          Subtarget, Mask);\n  if (RotateAmt < 0)\n    return SDValue();\n\n  // For pre-SSSE3 targets, if we are shuffling vXi8 elts then ISD::ROTL,\n  // expanded to OR(SRL,SHL), will be more efficient, but if they can\n  // widen to vXi16 or more then existing lowering should will be better.\n  if (!IsLegal) {\n    if ((RotateAmt % 16) == 0)\n      return SDValue();\n    // TODO: Use getTargetVShiftByConstNode.\n    unsigned ShlAmt = RotateAmt;\n    unsigned SrlAmt = RotateVT.getScalarSizeInBits() - RotateAmt;\n    V1 = DAG.getBitcast(RotateVT, V1);\n    SDValue SHL = DAG.getNode(X86ISD::VSHLI, DL, RotateVT, V1,\n                              DAG.getTargetConstant(ShlAmt, DL, MVT::i8));\n    SDValue SRL = DAG.getNode(X86ISD::VSRLI, DL, RotateVT, V1,\n                              DAG.getTargetConstant(SrlAmt, DL, MVT::i8));\n    SDValue Rot = DAG.getNode(ISD::OR, DL, RotateVT, SHL, SRL);\n    return DAG.getBitcast(VT, Rot);\n  }\n\n  SDValue Rot =\n      DAG.getNode(X86ISD::VROTLI, DL, RotateVT, DAG.getBitcast(RotateVT, V1),\n                  DAG.getTargetConstant(RotateAmt, DL, MVT::i8));\n  return DAG.getBitcast(VT, Rot);\n}\n\n/// Try to match a vector shuffle as an element rotation.\n///\n/// This is used for support PALIGNR for SSSE3 or VALIGND/Q for AVX512.\nstatic int matchShuffleAsElementRotate(SDValue &V1, SDValue &V2,\n                                       ArrayRef<int> Mask) {\n  int NumElts = Mask.size();\n\n  // We need to detect various ways of spelling a rotation:\n  //   [11, 12, 13, 14, 15,  0,  1,  2]\n  //   [-1, 12, 13, 14, -1, -1,  1, -1]\n  //   [-1, -1, -1, -1, -1, -1,  1,  2]\n  //   [ 3,  4,  5,  6,  7,  8,  9, 10]\n  //   [-1,  4,  5,  6, -1, -1,  9, -1]\n  //   [-1,  4,  5,  6, -1, -1, -1, -1]\n  int Rotation = 0;\n  SDValue Lo, Hi;\n  for (int i = 0; i < NumElts; ++i) {\n    int M = Mask[i];\n    assert((M == SM_SentinelUndef || (0 <= M && M < (2*NumElts))) &&\n           \"Unexpected mask index.\");\n    if (M < 0)\n      continue;\n\n    // Determine where a rotated vector would have started.\n    int StartIdx = i - (M % NumElts);\n    if (StartIdx == 0)\n      // The identity rotation isn't interesting, stop.\n      return -1;\n\n    // If we found the tail of a vector the rotation must be the missing\n    // front. If we found the head of a vector, it must be how much of the\n    // head.\n    int CandidateRotation = StartIdx < 0 ? -StartIdx : NumElts - StartIdx;\n\n    if (Rotation == 0)\n      Rotation = CandidateRotation;\n    else if (Rotation != CandidateRotation)\n      // The rotations don't match, so we can't match this mask.\n      return -1;\n\n    // Compute which value this mask is pointing at.\n    SDValue MaskV = M < NumElts ? V1 : V2;\n\n    // Compute which of the two target values this index should be assigned\n    // to. This reflects whether the high elements are remaining or the low\n    // elements are remaining.\n    SDValue &TargetV = StartIdx < 0 ? Hi : Lo;\n\n    // Either set up this value if we've not encountered it before, or check\n    // that it remains consistent.\n    if (!TargetV)\n      TargetV = MaskV;\n    else if (TargetV != MaskV)\n      // This may be a rotation, but it pulls from the inputs in some\n      // unsupported interleaving.\n      return -1;\n  }\n\n  // Check that we successfully analyzed the mask, and normalize the results.\n  assert(Rotation != 0 && \"Failed to locate a viable rotation!\");\n  assert((Lo || Hi) && \"Failed to find a rotated input vector!\");\n  if (!Lo)\n    Lo = Hi;\n  else if (!Hi)\n    Hi = Lo;\n\n  V1 = Lo;\n  V2 = Hi;\n\n  return Rotation;\n}\n\n/// Try to lower a vector shuffle as a byte rotation.\n///\n/// SSSE3 has a generic PALIGNR instruction in x86 that will do an arbitrary\n/// byte-rotation of the concatenation of two vectors; pre-SSSE3 can use\n/// a PSRLDQ/PSLLDQ/POR pattern to get a similar effect. This routine will\n/// try to generically lower a vector shuffle through such an pattern. It\n/// does not check for the profitability of lowering either as PALIGNR or\n/// PSRLDQ/PSLLDQ/POR, only whether the mask is valid to lower in that form.\n/// This matches shuffle vectors that look like:\n///\n///   v8i16 [11, 12, 13, 14, 15, 0, 1, 2]\n///\n/// Essentially it concatenates V1 and V2, shifts right by some number of\n/// elements, and takes the low elements as the result. Note that while this is\n/// specified as a *right shift* because x86 is little-endian, it is a *left\n/// rotate* of the vector lanes.\nstatic int matchShuffleAsByteRotate(MVT VT, SDValue &V1, SDValue &V2,\n                                    ArrayRef<int> Mask) {\n  // Don't accept any shuffles with zero elements.\n  if (isAnyZero(Mask))\n    return -1;\n\n  // PALIGNR works on 128-bit lanes.\n  SmallVector<int, 16> RepeatedMask;\n  if (!is128BitLaneRepeatedShuffleMask(VT, Mask, RepeatedMask))\n    return -1;\n\n  int Rotation = matchShuffleAsElementRotate(V1, V2, RepeatedMask);\n  if (Rotation <= 0)\n    return -1;\n\n  // PALIGNR rotates bytes, so we need to scale the\n  // rotation based on how many bytes are in the vector lane.\n  int NumElts = RepeatedMask.size();\n  int Scale = 16 / NumElts;\n  return Rotation * Scale;\n}\n\nstatic SDValue lowerShuffleAsByteRotate(const SDLoc &DL, MVT VT, SDValue V1,\n                                        SDValue V2, ArrayRef<int> Mask,\n                                        const X86Subtarget &Subtarget,\n                                        SelectionDAG &DAG) {\n  assert(!isNoopShuffleMask(Mask) && \"We shouldn't lower no-op shuffles!\");\n\n  SDValue Lo = V1, Hi = V2;\n  int ByteRotation = matchShuffleAsByteRotate(VT, Lo, Hi, Mask);\n  if (ByteRotation <= 0)\n    return SDValue();\n\n  // Cast the inputs to i8 vector of correct length to match PALIGNR or\n  // PSLLDQ/PSRLDQ.\n  MVT ByteVT = MVT::getVectorVT(MVT::i8, VT.getSizeInBits() / 8);\n  Lo = DAG.getBitcast(ByteVT, Lo);\n  Hi = DAG.getBitcast(ByteVT, Hi);\n\n  // SSSE3 targets can use the palignr instruction.\n  if (Subtarget.hasSSSE3()) {\n    assert((!VT.is512BitVector() || Subtarget.hasBWI()) &&\n           \"512-bit PALIGNR requires BWI instructions\");\n    return DAG.getBitcast(\n        VT, DAG.getNode(X86ISD::PALIGNR, DL, ByteVT, Lo, Hi,\n                        DAG.getTargetConstant(ByteRotation, DL, MVT::i8)));\n  }\n\n  assert(VT.is128BitVector() &&\n         \"Rotate-based lowering only supports 128-bit lowering!\");\n  assert(Mask.size() <= 16 &&\n         \"Can shuffle at most 16 bytes in a 128-bit vector!\");\n  assert(ByteVT == MVT::v16i8 &&\n         \"SSE2 rotate lowering only needed for v16i8!\");\n\n  // Default SSE2 implementation\n  int LoByteShift = 16 - ByteRotation;\n  int HiByteShift = ByteRotation;\n\n  SDValue LoShift =\n      DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, Lo,\n                  DAG.getTargetConstant(LoByteShift, DL, MVT::i8));\n  SDValue HiShift =\n      DAG.getNode(X86ISD::VSRLDQ, DL, MVT::v16i8, Hi,\n                  DAG.getTargetConstant(HiByteShift, DL, MVT::i8));\n  return DAG.getBitcast(VT,\n                        DAG.getNode(ISD::OR, DL, MVT::v16i8, LoShift, HiShift));\n}\n\n/// Try to lower a vector shuffle as a dword/qword rotation.\n///\n/// AVX512 has a VALIGND/VALIGNQ instructions that will do an arbitrary\n/// rotation of the concatenation of two vectors; This routine will\n/// try to generically lower a vector shuffle through such an pattern.\n///\n/// Essentially it concatenates V1 and V2, shifts right by some number of\n/// elements, and takes the low elements as the result. Note that while this is\n/// specified as a *right shift* because x86 is little-endian, it is a *left\n/// rotate* of the vector lanes.\nstatic SDValue lowerShuffleAsVALIGN(const SDLoc &DL, MVT VT, SDValue V1,\n                                    SDValue V2, ArrayRef<int> Mask,\n                                    const X86Subtarget &Subtarget,\n                                    SelectionDAG &DAG) {\n  assert((VT.getScalarType() == MVT::i32 || VT.getScalarType() == MVT::i64) &&\n         \"Only 32-bit and 64-bit elements are supported!\");\n\n  // 128/256-bit vectors are only supported with VLX.\n  assert((Subtarget.hasVLX() || (!VT.is128BitVector() && !VT.is256BitVector()))\n         && \"VLX required for 128/256-bit vectors\");\n\n  SDValue Lo = V1, Hi = V2;\n  int Rotation = matchShuffleAsElementRotate(Lo, Hi, Mask);\n  if (Rotation <= 0)\n    return SDValue();\n\n  return DAG.getNode(X86ISD::VALIGN, DL, VT, Lo, Hi,\n                     DAG.getTargetConstant(Rotation, DL, MVT::i8));\n}\n\n/// Try to lower a vector shuffle as a byte shift sequence.\nstatic SDValue lowerShuffleAsByteShiftMask(const SDLoc &DL, MVT VT, SDValue V1,\n                                           SDValue V2, ArrayRef<int> Mask,\n                                           const APInt &Zeroable,\n                                           const X86Subtarget &Subtarget,\n                                           SelectionDAG &DAG) {\n  assert(!isNoopShuffleMask(Mask) && \"We shouldn't lower no-op shuffles!\");\n  assert(VT.is128BitVector() && \"Only 128-bit vectors supported\");\n\n  // We need a shuffle that has zeros at one/both ends and a sequential\n  // shuffle from one source within.\n  unsigned ZeroLo = Zeroable.countTrailingOnes();\n  unsigned ZeroHi = Zeroable.countLeadingOnes();\n  if (!ZeroLo && !ZeroHi)\n    return SDValue();\n\n  unsigned NumElts = Mask.size();\n  unsigned Len = NumElts - (ZeroLo + ZeroHi);\n  if (!isSequentialOrUndefInRange(Mask, ZeroLo, Len, Mask[ZeroLo]))\n    return SDValue();\n\n  unsigned Scale = VT.getScalarSizeInBits() / 8;\n  ArrayRef<int> StubMask = Mask.slice(ZeroLo, Len);\n  if (!isUndefOrInRange(StubMask, 0, NumElts) &&\n      !isUndefOrInRange(StubMask, NumElts, 2 * NumElts))\n    return SDValue();\n\n  SDValue Res = Mask[ZeroLo] < (int)NumElts ? V1 : V2;\n  Res = DAG.getBitcast(MVT::v16i8, Res);\n\n  // Use VSHLDQ/VSRLDQ ops to zero the ends of a vector and leave an\n  // inner sequential set of elements, possibly offset:\n  // 01234567 --> zzzzzz01 --> 1zzzzzzz\n  // 01234567 --> 4567zzzz --> zzzzz456\n  // 01234567 --> z0123456 --> 3456zzzz --> zz3456zz\n  if (ZeroLo == 0) {\n    unsigned Shift = (NumElts - 1) - (Mask[ZeroLo + Len - 1] % NumElts);\n    Res = DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * Shift, DL, MVT::i8));\n    Res = DAG.getNode(X86ISD::VSRLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * ZeroHi, DL, MVT::i8));\n  } else if (ZeroHi == 0) {\n    unsigned Shift = Mask[ZeroLo] % NumElts;\n    Res = DAG.getNode(X86ISD::VSRLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * Shift, DL, MVT::i8));\n    Res = DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * ZeroLo, DL, MVT::i8));\n  } else if (!Subtarget.hasSSSE3()) {\n    // If we don't have PSHUFB then its worth avoiding an AND constant mask\n    // by performing 3 byte shifts. Shuffle combining can kick in above that.\n    // TODO: There may be some cases where VSH{LR}DQ+PAND is still better.\n    unsigned Shift = (NumElts - 1) - (Mask[ZeroLo + Len - 1] % NumElts);\n    Res = DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * Shift, DL, MVT::i8));\n    Shift += Mask[ZeroLo] % NumElts;\n    Res = DAG.getNode(X86ISD::VSRLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * Shift, DL, MVT::i8));\n    Res = DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, Res,\n                      DAG.getTargetConstant(Scale * ZeroLo, DL, MVT::i8));\n  } else\n    return SDValue();\n\n  return DAG.getBitcast(VT, Res);\n}\n\n/// Try to lower a vector shuffle as a bit shift (shifts in zeros).\n///\n/// Attempts to match a shuffle mask against the PSLL(W/D/Q/DQ) and\n/// PSRL(W/D/Q/DQ) SSE2 and AVX2 logical bit-shift instructions. The function\n/// matches elements from one of the input vectors shuffled to the left or\n/// right with zeroable elements 'shifted in'. It handles both the strictly\n/// bit-wise element shifts and the byte shift across an entire 128-bit double\n/// quad word lane.\n///\n/// PSHL : (little-endian) left bit shift.\n/// [ zz, 0, zz,  2 ]\n/// [ -1, 4, zz, -1 ]\n/// PSRL : (little-endian) right bit shift.\n/// [  1, zz,  3, zz]\n/// [ -1, -1,  7, zz]\n/// PSLLDQ : (little-endian) left byte shift\n/// [ zz,  0,  1,  2,  3,  4,  5,  6]\n/// [ zz, zz, -1, -1,  2,  3,  4, -1]\n/// [ zz, zz, zz, zz, zz, zz, -1,  1]\n/// PSRLDQ : (little-endian) right byte shift\n/// [  5, 6,  7, zz, zz, zz, zz, zz]\n/// [ -1, 5,  6,  7, zz, zz, zz, zz]\n/// [  1, 2, -1, -1, -1, -1, zz, zz]\nstatic int matchShuffleAsShift(MVT &ShiftVT, unsigned &Opcode,\n                               unsigned ScalarSizeInBits, ArrayRef<int> Mask,\n                               int MaskOffset, const APInt &Zeroable,\n                               const X86Subtarget &Subtarget) {\n  int Size = Mask.size();\n  unsigned SizeInBits = Size * ScalarSizeInBits;\n\n  auto CheckZeros = [&](int Shift, int Scale, bool Left) {\n    for (int i = 0; i < Size; i += Scale)\n      for (int j = 0; j < Shift; ++j)\n        if (!Zeroable[i + j + (Left ? 0 : (Scale - Shift))])\n          return false;\n\n    return true;\n  };\n\n  auto MatchShift = [&](int Shift, int Scale, bool Left) {\n    for (int i = 0; i != Size; i += Scale) {\n      unsigned Pos = Left ? i + Shift : i;\n      unsigned Low = Left ? i : i + Shift;\n      unsigned Len = Scale - Shift;\n      if (!isSequentialOrUndefInRange(Mask, Pos, Len, Low + MaskOffset))\n        return -1;\n    }\n\n    int ShiftEltBits = ScalarSizeInBits * Scale;\n    bool ByteShift = ShiftEltBits > 64;\n    Opcode = Left ? (ByteShift ? X86ISD::VSHLDQ : X86ISD::VSHLI)\n                  : (ByteShift ? X86ISD::VSRLDQ : X86ISD::VSRLI);\n    int ShiftAmt = Shift * ScalarSizeInBits / (ByteShift ? 8 : 1);\n\n    // Normalize the scale for byte shifts to still produce an i64 element\n    // type.\n    Scale = ByteShift ? Scale / 2 : Scale;\n\n    // We need to round trip through the appropriate type for the shift.\n    MVT ShiftSVT = MVT::getIntegerVT(ScalarSizeInBits * Scale);\n    ShiftVT = ByteShift ? MVT::getVectorVT(MVT::i8, SizeInBits / 8)\n                        : MVT::getVectorVT(ShiftSVT, Size / Scale);\n    return (int)ShiftAmt;\n  };\n\n  // SSE/AVX supports logical shifts up to 64-bit integers - so we can just\n  // keep doubling the size of the integer elements up to that. We can\n  // then shift the elements of the integer vector by whole multiples of\n  // their width within the elements of the larger integer vector. Test each\n  // multiple to see if we can find a match with the moved element indices\n  // and that the shifted in elements are all zeroable.\n  unsigned MaxWidth = ((SizeInBits == 512) && !Subtarget.hasBWI() ? 64 : 128);\n  for (int Scale = 2; Scale * ScalarSizeInBits <= MaxWidth; Scale *= 2)\n    for (int Shift = 1; Shift != Scale; ++Shift)\n      for (bool Left : {true, false})\n        if (CheckZeros(Shift, Scale, Left)) {\n          int ShiftAmt = MatchShift(Shift, Scale, Left);\n          if (0 < ShiftAmt)\n            return ShiftAmt;\n        }\n\n  // no match\n  return -1;\n}\n\nstatic SDValue lowerShuffleAsShift(const SDLoc &DL, MVT VT, SDValue V1,\n                                   SDValue V2, ArrayRef<int> Mask,\n                                   const APInt &Zeroable,\n                                   const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  int Size = Mask.size();\n  assert(Size == (int)VT.getVectorNumElements() && \"Unexpected mask size\");\n\n  MVT ShiftVT;\n  SDValue V = V1;\n  unsigned Opcode;\n\n  // Try to match shuffle against V1 shift.\n  int ShiftAmt = matchShuffleAsShift(ShiftVT, Opcode, VT.getScalarSizeInBits(),\n                                     Mask, 0, Zeroable, Subtarget);\n\n  // If V1 failed, try to match shuffle against V2 shift.\n  if (ShiftAmt < 0) {\n    ShiftAmt = matchShuffleAsShift(ShiftVT, Opcode, VT.getScalarSizeInBits(),\n                                   Mask, Size, Zeroable, Subtarget);\n    V = V2;\n  }\n\n  if (ShiftAmt < 0)\n    return SDValue();\n\n  assert(DAG.getTargetLoweringInfo().isTypeLegal(ShiftVT) &&\n         \"Illegal integer vector type\");\n  V = DAG.getBitcast(ShiftVT, V);\n  V = DAG.getNode(Opcode, DL, ShiftVT, V,\n                  DAG.getTargetConstant(ShiftAmt, DL, MVT::i8));\n  return DAG.getBitcast(VT, V);\n}\n\n// EXTRQ: Extract Len elements from lower half of source, starting at Idx.\n// Remainder of lower half result is zero and upper half is all undef.\nstatic bool matchShuffleAsEXTRQ(MVT VT, SDValue &V1, SDValue &V2,\n                                ArrayRef<int> Mask, uint64_t &BitLen,\n                                uint64_t &BitIdx, const APInt &Zeroable) {\n  int Size = Mask.size();\n  int HalfSize = Size / 2;\n  assert(Size == (int)VT.getVectorNumElements() && \"Unexpected mask size\");\n  assert(!Zeroable.isAllOnesValue() && \"Fully zeroable shuffle mask\");\n\n  // Upper half must be undefined.\n  if (!isUndefUpperHalf(Mask))\n    return false;\n\n  // Determine the extraction length from the part of the\n  // lower half that isn't zeroable.\n  int Len = HalfSize;\n  for (; Len > 0; --Len)\n    if (!Zeroable[Len - 1])\n      break;\n  assert(Len > 0 && \"Zeroable shuffle mask\");\n\n  // Attempt to match first Len sequential elements from the lower half.\n  SDValue Src;\n  int Idx = -1;\n  for (int i = 0; i != Len; ++i) {\n    int M = Mask[i];\n    if (M == SM_SentinelUndef)\n      continue;\n    SDValue &V = (M < Size ? V1 : V2);\n    M = M % Size;\n\n    // The extracted elements must start at a valid index and all mask\n    // elements must be in the lower half.\n    if (i > M || M >= HalfSize)\n      return false;\n\n    if (Idx < 0 || (Src == V && Idx == (M - i))) {\n      Src = V;\n      Idx = M - i;\n      continue;\n    }\n    return false;\n  }\n\n  if (!Src || Idx < 0)\n    return false;\n\n  assert((Idx + Len) <= HalfSize && \"Illegal extraction mask\");\n  BitLen = (Len * VT.getScalarSizeInBits()) & 0x3f;\n  BitIdx = (Idx * VT.getScalarSizeInBits()) & 0x3f;\n  V1 = Src;\n  return true;\n}\n\n// INSERTQ: Extract lowest Len elements from lower half of second source and\n// insert over first source, starting at Idx.\n// { A[0], .., A[Idx-1], B[0], .., B[Len-1], A[Idx+Len], .., UNDEF, ... }\nstatic bool matchShuffleAsINSERTQ(MVT VT, SDValue &V1, SDValue &V2,\n                                  ArrayRef<int> Mask, uint64_t &BitLen,\n                                  uint64_t &BitIdx) {\n  int Size = Mask.size();\n  int HalfSize = Size / 2;\n  assert(Size == (int)VT.getVectorNumElements() && \"Unexpected mask size\");\n\n  // Upper half must be undefined.\n  if (!isUndefUpperHalf(Mask))\n    return false;\n\n  for (int Idx = 0; Idx != HalfSize; ++Idx) {\n    SDValue Base;\n\n    // Attempt to match first source from mask before insertion point.\n    if (isUndefInRange(Mask, 0, Idx)) {\n      /* EMPTY */\n    } else if (isSequentialOrUndefInRange(Mask, 0, Idx, 0)) {\n      Base = V1;\n    } else if (isSequentialOrUndefInRange(Mask, 0, Idx, Size)) {\n      Base = V2;\n    } else {\n      continue;\n    }\n\n    // Extend the extraction length looking to match both the insertion of\n    // the second source and the remaining elements of the first.\n    for (int Hi = Idx + 1; Hi <= HalfSize; ++Hi) {\n      SDValue Insert;\n      int Len = Hi - Idx;\n\n      // Match insertion.\n      if (isSequentialOrUndefInRange(Mask, Idx, Len, 0)) {\n        Insert = V1;\n      } else if (isSequentialOrUndefInRange(Mask, Idx, Len, Size)) {\n        Insert = V2;\n      } else {\n        continue;\n      }\n\n      // Match the remaining elements of the lower half.\n      if (isUndefInRange(Mask, Hi, HalfSize - Hi)) {\n        /* EMPTY */\n      } else if ((!Base || (Base == V1)) &&\n                 isSequentialOrUndefInRange(Mask, Hi, HalfSize - Hi, Hi)) {\n        Base = V1;\n      } else if ((!Base || (Base == V2)) &&\n                 isSequentialOrUndefInRange(Mask, Hi, HalfSize - Hi,\n                                            Size + Hi)) {\n        Base = V2;\n      } else {\n        continue;\n      }\n\n      BitLen = (Len * VT.getScalarSizeInBits()) & 0x3f;\n      BitIdx = (Idx * VT.getScalarSizeInBits()) & 0x3f;\n      V1 = Base;\n      V2 = Insert;\n      return true;\n    }\n  }\n\n  return false;\n}\n\n/// Try to lower a vector shuffle using SSE4a EXTRQ/INSERTQ.\nstatic SDValue lowerShuffleWithSSE4A(const SDLoc &DL, MVT VT, SDValue V1,\n                                     SDValue V2, ArrayRef<int> Mask,\n                                     const APInt &Zeroable, SelectionDAG &DAG) {\n  uint64_t BitLen, BitIdx;\n  if (matchShuffleAsEXTRQ(VT, V1, V2, Mask, BitLen, BitIdx, Zeroable))\n    return DAG.getNode(X86ISD::EXTRQI, DL, VT, V1,\n                       DAG.getTargetConstant(BitLen, DL, MVT::i8),\n                       DAG.getTargetConstant(BitIdx, DL, MVT::i8));\n\n  if (matchShuffleAsINSERTQ(VT, V1, V2, Mask, BitLen, BitIdx))\n    return DAG.getNode(X86ISD::INSERTQI, DL, VT, V1 ? V1 : DAG.getUNDEF(VT),\n                       V2 ? V2 : DAG.getUNDEF(VT),\n                       DAG.getTargetConstant(BitLen, DL, MVT::i8),\n                       DAG.getTargetConstant(BitIdx, DL, MVT::i8));\n\n  return SDValue();\n}\n\n/// Lower a vector shuffle as a zero or any extension.\n///\n/// Given a specific number of elements, element bit width, and extension\n/// stride, produce either a zero or any extension based on the available\n/// features of the subtarget. The extended elements are consecutive and\n/// begin and can start from an offsetted element index in the input; to\n/// avoid excess shuffling the offset must either being in the bottom lane\n/// or at the start of a higher lane. All extended elements must be from\n/// the same lane.\nstatic SDValue lowerShuffleAsSpecificZeroOrAnyExtend(\n    const SDLoc &DL, MVT VT, int Scale, int Offset, bool AnyExt, SDValue InputV,\n    ArrayRef<int> Mask, const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  assert(Scale > 1 && \"Need a scale to extend.\");\n  int EltBits = VT.getScalarSizeInBits();\n  int NumElements = VT.getVectorNumElements();\n  int NumEltsPerLane = 128 / EltBits;\n  int OffsetLane = Offset / NumEltsPerLane;\n  assert((EltBits == 8 || EltBits == 16 || EltBits == 32) &&\n         \"Only 8, 16, and 32 bit elements can be extended.\");\n  assert(Scale * EltBits <= 64 && \"Cannot zero extend past 64 bits.\");\n  assert(0 <= Offset && \"Extension offset must be positive.\");\n  assert((Offset < NumEltsPerLane || Offset % NumEltsPerLane == 0) &&\n         \"Extension offset must be in the first lane or start an upper lane.\");\n\n  // Check that an index is in same lane as the base offset.\n  auto SafeOffset = [&](int Idx) {\n    return OffsetLane == (Idx / NumEltsPerLane);\n  };\n\n  // Shift along an input so that the offset base moves to the first element.\n  auto ShuffleOffset = [&](SDValue V) {\n    if (!Offset)\n      return V;\n\n    SmallVector<int, 8> ShMask((unsigned)NumElements, -1);\n    for (int i = 0; i * Scale < NumElements; ++i) {\n      int SrcIdx = i + Offset;\n      ShMask[i] = SafeOffset(SrcIdx) ? SrcIdx : -1;\n    }\n    return DAG.getVectorShuffle(VT, DL, V, DAG.getUNDEF(VT), ShMask);\n  };\n\n  // Found a valid a/zext mask! Try various lowering strategies based on the\n  // input type and available ISA extensions.\n  if (Subtarget.hasSSE41()) {\n    // Not worth offsetting 128-bit vectors if scale == 2, a pattern using\n    // PUNPCK will catch this in a later shuffle match.\n    if (Offset && Scale == 2 && VT.is128BitVector())\n      return SDValue();\n    MVT ExtVT = MVT::getVectorVT(MVT::getIntegerVT(EltBits * Scale),\n                                 NumElements / Scale);\n    InputV = ShuffleOffset(InputV);\n    InputV = getEXTEND_VECTOR_INREG(AnyExt ? ISD::ANY_EXTEND : ISD::ZERO_EXTEND,\n                                    DL, ExtVT, InputV, DAG);\n    return DAG.getBitcast(VT, InputV);\n  }\n\n  assert(VT.is128BitVector() && \"Only 128-bit vectors can be extended.\");\n\n  // For any extends we can cheat for larger element sizes and use shuffle\n  // instructions that can fold with a load and/or copy.\n  if (AnyExt && EltBits == 32) {\n    int PSHUFDMask[4] = {Offset, -1, SafeOffset(Offset + 1) ? Offset + 1 : -1,\n                         -1};\n    return DAG.getBitcast(\n        VT, DAG.getNode(X86ISD::PSHUFD, DL, MVT::v4i32,\n                        DAG.getBitcast(MVT::v4i32, InputV),\n                        getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG)));\n  }\n  if (AnyExt && EltBits == 16 && Scale > 2) {\n    int PSHUFDMask[4] = {Offset / 2, -1,\n                         SafeOffset(Offset + 1) ? (Offset + 1) / 2 : -1, -1};\n    InputV = DAG.getNode(X86ISD::PSHUFD, DL, MVT::v4i32,\n                         DAG.getBitcast(MVT::v4i32, InputV),\n                         getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG));\n    int PSHUFWMask[4] = {1, -1, -1, -1};\n    unsigned OddEvenOp = (Offset & 1) ? X86ISD::PSHUFLW : X86ISD::PSHUFHW;\n    return DAG.getBitcast(\n        VT, DAG.getNode(OddEvenOp, DL, MVT::v8i16,\n                        DAG.getBitcast(MVT::v8i16, InputV),\n                        getV4X86ShuffleImm8ForMask(PSHUFWMask, DL, DAG)));\n  }\n\n  // The SSE4A EXTRQ instruction can efficiently extend the first 2 lanes\n  // to 64-bits.\n  if ((Scale * EltBits) == 64 && EltBits < 32 && Subtarget.hasSSE4A()) {\n    assert(NumElements == (int)Mask.size() && \"Unexpected shuffle mask size!\");\n    assert(VT.is128BitVector() && \"Unexpected vector width!\");\n\n    int LoIdx = Offset * EltBits;\n    SDValue Lo = DAG.getBitcast(\n        MVT::v2i64, DAG.getNode(X86ISD::EXTRQI, DL, VT, InputV,\n                                DAG.getTargetConstant(EltBits, DL, MVT::i8),\n                                DAG.getTargetConstant(LoIdx, DL, MVT::i8)));\n\n    if (isUndefUpperHalf(Mask) || !SafeOffset(Offset + 1))\n      return DAG.getBitcast(VT, Lo);\n\n    int HiIdx = (Offset + 1) * EltBits;\n    SDValue Hi = DAG.getBitcast(\n        MVT::v2i64, DAG.getNode(X86ISD::EXTRQI, DL, VT, InputV,\n                                DAG.getTargetConstant(EltBits, DL, MVT::i8),\n                                DAG.getTargetConstant(HiIdx, DL, MVT::i8)));\n    return DAG.getBitcast(VT,\n                          DAG.getNode(X86ISD::UNPCKL, DL, MVT::v2i64, Lo, Hi));\n  }\n\n  // If this would require more than 2 unpack instructions to expand, use\n  // pshufb when available. We can only use more than 2 unpack instructions\n  // when zero extending i8 elements which also makes it easier to use pshufb.\n  if (Scale > 4 && EltBits == 8 && Subtarget.hasSSSE3()) {\n    assert(NumElements == 16 && \"Unexpected byte vector width!\");\n    SDValue PSHUFBMask[16];\n    for (int i = 0; i < 16; ++i) {\n      int Idx = Offset + (i / Scale);\n      if ((i % Scale == 0 && SafeOffset(Idx))) {\n        PSHUFBMask[i] = DAG.getConstant(Idx, DL, MVT::i8);\n        continue;\n      }\n      PSHUFBMask[i] =\n          AnyExt ? DAG.getUNDEF(MVT::i8) : DAG.getConstant(0x80, DL, MVT::i8);\n    }\n    InputV = DAG.getBitcast(MVT::v16i8, InputV);\n    return DAG.getBitcast(\n        VT, DAG.getNode(X86ISD::PSHUFB, DL, MVT::v16i8, InputV,\n                        DAG.getBuildVector(MVT::v16i8, DL, PSHUFBMask)));\n  }\n\n  // If we are extending from an offset, ensure we start on a boundary that\n  // we can unpack from.\n  int AlignToUnpack = Offset % (NumElements / Scale);\n  if (AlignToUnpack) {\n    SmallVector<int, 8> ShMask((unsigned)NumElements, -1);\n    for (int i = AlignToUnpack; i < NumElements; ++i)\n      ShMask[i - AlignToUnpack] = i;\n    InputV = DAG.getVectorShuffle(VT, DL, InputV, DAG.getUNDEF(VT), ShMask);\n    Offset -= AlignToUnpack;\n  }\n\n  // Otherwise emit a sequence of unpacks.\n  do {\n    unsigned UnpackLoHi = X86ISD::UNPCKL;\n    if (Offset >= (NumElements / 2)) {\n      UnpackLoHi = X86ISD::UNPCKH;\n      Offset -= (NumElements / 2);\n    }\n\n    MVT InputVT = MVT::getVectorVT(MVT::getIntegerVT(EltBits), NumElements);\n    SDValue Ext = AnyExt ? DAG.getUNDEF(InputVT)\n                         : getZeroVector(InputVT, Subtarget, DAG, DL);\n    InputV = DAG.getBitcast(InputVT, InputV);\n    InputV = DAG.getNode(UnpackLoHi, DL, InputVT, InputV, Ext);\n    Scale /= 2;\n    EltBits *= 2;\n    NumElements /= 2;\n  } while (Scale > 1);\n  return DAG.getBitcast(VT, InputV);\n}\n\n/// Try to lower a vector shuffle as a zero extension on any microarch.\n///\n/// This routine will try to do everything in its power to cleverly lower\n/// a shuffle which happens to match the pattern of a zero extend. It doesn't\n/// check for the profitability of this lowering,  it tries to aggressively\n/// match this pattern. It will use all of the micro-architectural details it\n/// can to emit an efficient lowering. It handles both blends with all-zero\n/// inputs to explicitly zero-extend and undef-lanes (sometimes undef due to\n/// masking out later).\n///\n/// The reason we have dedicated lowering for zext-style shuffles is that they\n/// are both incredibly common and often quite performance sensitive.\nstatic SDValue lowerShuffleAsZeroOrAnyExtend(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const APInt &Zeroable, const X86Subtarget &Subtarget,\n    SelectionDAG &DAG) {\n  int Bits = VT.getSizeInBits();\n  int NumLanes = Bits / 128;\n  int NumElements = VT.getVectorNumElements();\n  int NumEltsPerLane = NumElements / NumLanes;\n  assert(VT.getScalarSizeInBits() <= 32 &&\n         \"Exceeds 32-bit integer zero extension limit\");\n  assert((int)Mask.size() == NumElements && \"Unexpected shuffle mask size\");\n\n  // Define a helper function to check a particular ext-scale and lower to it if\n  // valid.\n  auto Lower = [&](int Scale) -> SDValue {\n    SDValue InputV;\n    bool AnyExt = true;\n    int Offset = 0;\n    int Matches = 0;\n    for (int i = 0; i < NumElements; ++i) {\n      int M = Mask[i];\n      if (M < 0)\n        continue; // Valid anywhere but doesn't tell us anything.\n      if (i % Scale != 0) {\n        // Each of the extended elements need to be zeroable.\n        if (!Zeroable[i])\n          return SDValue();\n\n        // We no longer are in the anyext case.\n        AnyExt = false;\n        continue;\n      }\n\n      // Each of the base elements needs to be consecutive indices into the\n      // same input vector.\n      SDValue V = M < NumElements ? V1 : V2;\n      M = M % NumElements;\n      if (!InputV) {\n        InputV = V;\n        Offset = M - (i / Scale);\n      } else if (InputV != V)\n        return SDValue(); // Flip-flopping inputs.\n\n      // Offset must start in the lowest 128-bit lane or at the start of an\n      // upper lane.\n      // FIXME: Is it ever worth allowing a negative base offset?\n      if (!((0 <= Offset && Offset < NumEltsPerLane) ||\n            (Offset % NumEltsPerLane) == 0))\n        return SDValue();\n\n      // If we are offsetting, all referenced entries must come from the same\n      // lane.\n      if (Offset && (Offset / NumEltsPerLane) != (M / NumEltsPerLane))\n        return SDValue();\n\n      if ((M % NumElements) != (Offset + (i / Scale)))\n        return SDValue(); // Non-consecutive strided elements.\n      Matches++;\n    }\n\n    // If we fail to find an input, we have a zero-shuffle which should always\n    // have already been handled.\n    // FIXME: Maybe handle this here in case during blending we end up with one?\n    if (!InputV)\n      return SDValue();\n\n    // If we are offsetting, don't extend if we only match a single input, we\n    // can always do better by using a basic PSHUF or PUNPCK.\n    if (Offset != 0 && Matches < 2)\n      return SDValue();\n\n    return lowerShuffleAsSpecificZeroOrAnyExtend(DL, VT, Scale, Offset, AnyExt,\n                                                 InputV, Mask, Subtarget, DAG);\n  };\n\n  // The widest scale possible for extending is to a 64-bit integer.\n  assert(Bits % 64 == 0 &&\n         \"The number of bits in a vector must be divisible by 64 on x86!\");\n  int NumExtElements = Bits / 64;\n\n  // Each iteration, try extending the elements half as much, but into twice as\n  // many elements.\n  for (; NumExtElements < NumElements; NumExtElements *= 2) {\n    assert(NumElements % NumExtElements == 0 &&\n           \"The input vector size must be divisible by the extended size.\");\n    if (SDValue V = Lower(NumElements / NumExtElements))\n      return V;\n  }\n\n  // General extends failed, but 128-bit vectors may be able to use MOVQ.\n  if (Bits != 128)\n    return SDValue();\n\n  // Returns one of the source operands if the shuffle can be reduced to a\n  // MOVQ, copying the lower 64-bits and zero-extending to the upper 64-bits.\n  auto CanZExtLowHalf = [&]() {\n    for (int i = NumElements / 2; i != NumElements; ++i)\n      if (!Zeroable[i])\n        return SDValue();\n    if (isSequentialOrUndefInRange(Mask, 0, NumElements / 2, 0))\n      return V1;\n    if (isSequentialOrUndefInRange(Mask, 0, NumElements / 2, NumElements))\n      return V2;\n    return SDValue();\n  };\n\n  if (SDValue V = CanZExtLowHalf()) {\n    V = DAG.getBitcast(MVT::v2i64, V);\n    V = DAG.getNode(X86ISD::VZEXT_MOVL, DL, MVT::v2i64, V);\n    return DAG.getBitcast(VT, V);\n  }\n\n  // No viable ext lowering found.\n  return SDValue();\n}\n\n/// Try to get a scalar value for a specific element of a vector.\n///\n/// Looks through BUILD_VECTOR and SCALAR_TO_VECTOR nodes to find a scalar.\nstatic SDValue getScalarValueForVectorElement(SDValue V, int Idx,\n                                              SelectionDAG &DAG) {\n  MVT VT = V.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  V = peekThroughBitcasts(V);\n\n  // If the bitcasts shift the element size, we can't extract an equivalent\n  // element from it.\n  MVT NewVT = V.getSimpleValueType();\n  if (!NewVT.isVector() || NewVT.getScalarSizeInBits() != VT.getScalarSizeInBits())\n    return SDValue();\n\n  if (V.getOpcode() == ISD::BUILD_VECTOR ||\n      (Idx == 0 && V.getOpcode() == ISD::SCALAR_TO_VECTOR)) {\n    // Ensure the scalar operand is the same size as the destination.\n    // FIXME: Add support for scalar truncation where possible.\n    SDValue S = V.getOperand(Idx);\n    if (EltVT.getSizeInBits() == S.getSimpleValueType().getSizeInBits())\n      return DAG.getBitcast(EltVT, S);\n  }\n\n  return SDValue();\n}\n\n/// Helper to test for a load that can be folded with x86 shuffles.\n///\n/// This is particularly important because the set of instructions varies\n/// significantly based on whether the operand is a load or not.\nstatic bool isShuffleFoldableLoad(SDValue V) {\n  V = peekThroughBitcasts(V);\n  return ISD::isNON_EXTLoad(V.getNode());\n}\n\n/// Try to lower insertion of a single element into a zero vector.\n///\n/// This is a common pattern that we have especially efficient patterns to lower\n/// across all subtarget feature sets.\nstatic SDValue lowerShuffleAsElementInsertion(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const APInt &Zeroable, const X86Subtarget &Subtarget,\n    SelectionDAG &DAG) {\n  MVT ExtVT = VT;\n  MVT EltVT = VT.getVectorElementType();\n\n  int V2Index =\n      find_if(Mask, [&Mask](int M) { return M >= (int)Mask.size(); }) -\n      Mask.begin();\n  bool IsV1Zeroable = true;\n  for (int i = 0, Size = Mask.size(); i < Size; ++i)\n    if (i != V2Index && !Zeroable[i]) {\n      IsV1Zeroable = false;\n      break;\n    }\n\n  // Check for a single input from a SCALAR_TO_VECTOR node.\n  // FIXME: All of this should be canonicalized into INSERT_VECTOR_ELT and\n  // all the smarts here sunk into that routine. However, the current\n  // lowering of BUILD_VECTOR makes that nearly impossible until the old\n  // vector shuffle lowering is dead.\n  SDValue V2S = getScalarValueForVectorElement(V2, Mask[V2Index] - Mask.size(),\n                                               DAG);\n  if (V2S && DAG.getTargetLoweringInfo().isTypeLegal(V2S.getValueType())) {\n    // We need to zext the scalar if it is smaller than an i32.\n    V2S = DAG.getBitcast(EltVT, V2S);\n    if (EltVT == MVT::i8 || EltVT == MVT::i16) {\n      // Using zext to expand a narrow element won't work for non-zero\n      // insertions.\n      if (!IsV1Zeroable)\n        return SDValue();\n\n      // Zero-extend directly to i32.\n      ExtVT = MVT::getVectorVT(MVT::i32, ExtVT.getSizeInBits() / 32);\n      V2S = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i32, V2S);\n    }\n    V2 = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, ExtVT, V2S);\n  } else if (Mask[V2Index] != (int)Mask.size() || EltVT == MVT::i8 ||\n             EltVT == MVT::i16) {\n    // Either not inserting from the low element of the input or the input\n    // element size is too small to use VZEXT_MOVL to clear the high bits.\n    return SDValue();\n  }\n\n  if (!IsV1Zeroable) {\n    // If V1 can't be treated as a zero vector we have fewer options to lower\n    // this. We can't support integer vectors or non-zero targets cheaply, and\n    // the V1 elements can't be permuted in any way.\n    assert(VT == ExtVT && \"Cannot change extended type when non-zeroable!\");\n    if (!VT.isFloatingPoint() || V2Index != 0)\n      return SDValue();\n    SmallVector<int, 8> V1Mask(Mask.begin(), Mask.end());\n    V1Mask[V2Index] = -1;\n    if (!isNoopShuffleMask(V1Mask))\n      return SDValue();\n    if (!VT.is128BitVector())\n      return SDValue();\n\n    // Otherwise, use MOVSD or MOVSS.\n    assert((EltVT == MVT::f32 || EltVT == MVT::f64) &&\n           \"Only two types of floating point element types to handle!\");\n    return DAG.getNode(EltVT == MVT::f32 ? X86ISD::MOVSS : X86ISD::MOVSD, DL,\n                       ExtVT, V1, V2);\n  }\n\n  // This lowering only works for the low element with floating point vectors.\n  if (VT.isFloatingPoint() && V2Index != 0)\n    return SDValue();\n\n  V2 = DAG.getNode(X86ISD::VZEXT_MOVL, DL, ExtVT, V2);\n  if (ExtVT != VT)\n    V2 = DAG.getBitcast(VT, V2);\n\n  if (V2Index != 0) {\n    // If we have 4 or fewer lanes we can cheaply shuffle the element into\n    // the desired position. Otherwise it is more efficient to do a vector\n    // shift left. We know that we can do a vector shift left because all\n    // the inputs are zero.\n    if (VT.isFloatingPoint() || VT.getVectorNumElements() <= 4) {\n      SmallVector<int, 4> V2Shuffle(Mask.size(), 1);\n      V2Shuffle[V2Index] = 0;\n      V2 = DAG.getVectorShuffle(VT, DL, V2, DAG.getUNDEF(VT), V2Shuffle);\n    } else {\n      V2 = DAG.getBitcast(MVT::v16i8, V2);\n      V2 = DAG.getNode(X86ISD::VSHLDQ, DL, MVT::v16i8, V2,\n                       DAG.getTargetConstant(\n                           V2Index * EltVT.getSizeInBits() / 8, DL, MVT::i8));\n      V2 = DAG.getBitcast(VT, V2);\n    }\n  }\n  return V2;\n}\n\n/// Try to lower broadcast of a single - truncated - integer element,\n/// coming from a scalar_to_vector/build_vector node \\p V0 with larger elements.\n///\n/// This assumes we have AVX2.\nstatic SDValue lowerShuffleAsTruncBroadcast(const SDLoc &DL, MVT VT, SDValue V0,\n                                            int BroadcastIdx,\n                                            const X86Subtarget &Subtarget,\n                                            SelectionDAG &DAG) {\n  assert(Subtarget.hasAVX2() &&\n         \"We can only lower integer broadcasts with AVX2!\");\n\n  MVT EltVT = VT.getVectorElementType();\n  MVT V0VT = V0.getSimpleValueType();\n\n  assert(VT.isInteger() && \"Unexpected non-integer trunc broadcast!\");\n  assert(V0VT.isVector() && \"Unexpected non-vector vector-sized value!\");\n\n  MVT V0EltVT = V0VT.getVectorElementType();\n  if (!V0EltVT.isInteger())\n    return SDValue();\n\n  const unsigned EltSize = EltVT.getSizeInBits();\n  const unsigned V0EltSize = V0EltVT.getSizeInBits();\n\n  // This is only a truncation if the original element type is larger.\n  if (V0EltSize <= EltSize)\n    return SDValue();\n\n  assert(((V0EltSize % EltSize) == 0) &&\n         \"Scalar type sizes must all be powers of 2 on x86!\");\n\n  const unsigned V0Opc = V0.getOpcode();\n  const unsigned Scale = V0EltSize / EltSize;\n  const unsigned V0BroadcastIdx = BroadcastIdx / Scale;\n\n  if ((V0Opc != ISD::SCALAR_TO_VECTOR || V0BroadcastIdx != 0) &&\n      V0Opc != ISD::BUILD_VECTOR)\n    return SDValue();\n\n  SDValue Scalar = V0.getOperand(V0BroadcastIdx);\n\n  // If we're extracting non-least-significant bits, shift so we can truncate.\n  // Hopefully, we can fold away the trunc/srl/load into the broadcast.\n  // Even if we can't (and !isShuffleFoldableLoad(Scalar)), prefer\n  // vpbroadcast+vmovd+shr to vpshufb(m)+vmovd.\n  if (const int OffsetIdx = BroadcastIdx % Scale)\n    Scalar = DAG.getNode(ISD::SRL, DL, Scalar.getValueType(), Scalar,\n                         DAG.getConstant(OffsetIdx * EltSize, DL, MVT::i8));\n\n  return DAG.getNode(X86ISD::VBROADCAST, DL, VT,\n                     DAG.getNode(ISD::TRUNCATE, DL, EltVT, Scalar));\n}\n\n/// Test whether this can be lowered with a single SHUFPS instruction.\n///\n/// This is used to disable more specialized lowerings when the shufps lowering\n/// will happen to be efficient.\nstatic bool isSingleSHUFPSMask(ArrayRef<int> Mask) {\n  // This routine only handles 128-bit shufps.\n  assert(Mask.size() == 4 && \"Unsupported mask size!\");\n  assert(Mask[0] >= -1 && Mask[0] < 8 && \"Out of bound mask element!\");\n  assert(Mask[1] >= -1 && Mask[1] < 8 && \"Out of bound mask element!\");\n  assert(Mask[2] >= -1 && Mask[2] < 8 && \"Out of bound mask element!\");\n  assert(Mask[3] >= -1 && Mask[3] < 8 && \"Out of bound mask element!\");\n\n  // To lower with a single SHUFPS we need to have the low half and high half\n  // each requiring a single input.\n  if (Mask[0] >= 0 && Mask[1] >= 0 && (Mask[0] < 4) != (Mask[1] < 4))\n    return false;\n  if (Mask[2] >= 0 && Mask[3] >= 0 && (Mask[2] < 4) != (Mask[3] < 4))\n    return false;\n\n  return true;\n}\n\n/// If we are extracting two 128-bit halves of a vector and shuffling the\n/// result, match that to a 256-bit AVX2 vperm* instruction to avoid a\n/// multi-shuffle lowering.\nstatic SDValue lowerShuffleOfExtractsAsVperm(const SDLoc &DL, SDValue N0,\n                                             SDValue N1, ArrayRef<int> Mask,\n                                             SelectionDAG &DAG) {\n  MVT VT = N0.getSimpleValueType();\n  assert((VT.is128BitVector() &&\n          (VT.getScalarSizeInBits() == 32 || VT.getScalarSizeInBits() == 64)) &&\n         \"VPERM* family of shuffles requires 32-bit or 64-bit elements\");\n\n  // Check that both sources are extracts of the same source vector.\n  if (!N0.hasOneUse() || !N1.hasOneUse() ||\n      N0.getOpcode() != ISD::EXTRACT_SUBVECTOR ||\n      N1.getOpcode() != ISD::EXTRACT_SUBVECTOR ||\n      N0.getOperand(0) != N1.getOperand(0))\n    return SDValue();\n\n  SDValue WideVec = N0.getOperand(0);\n  MVT WideVT = WideVec.getSimpleValueType();\n  if (!WideVT.is256BitVector())\n    return SDValue();\n\n  // Match extracts of each half of the wide source vector. Commute the shuffle\n  // if the extract of the low half is N1.\n  unsigned NumElts = VT.getVectorNumElements();\n  SmallVector<int, 4> NewMask(Mask.begin(), Mask.end());\n  const APInt &ExtIndex0 = N0.getConstantOperandAPInt(1);\n  const APInt &ExtIndex1 = N1.getConstantOperandAPInt(1);\n  if (ExtIndex1 == 0 && ExtIndex0 == NumElts)\n    ShuffleVectorSDNode::commuteMask(NewMask);\n  else if (ExtIndex0 != 0 || ExtIndex1 != NumElts)\n    return SDValue();\n\n  // Final bailout: if the mask is simple, we are better off using an extract\n  // and a simple narrow shuffle. Prefer extract+unpack(h/l)ps to vpermps\n  // because that avoids a constant load from memory.\n  if (NumElts == 4 &&\n      (isSingleSHUFPSMask(NewMask) || is128BitUnpackShuffleMask(NewMask)))\n    return SDValue();\n\n  // Extend the shuffle mask with undef elements.\n  NewMask.append(NumElts, -1);\n\n  // shuf (extract X, 0), (extract X, 4), M --> extract (shuf X, undef, M'), 0\n  SDValue Shuf = DAG.getVectorShuffle(WideVT, DL, WideVec, DAG.getUNDEF(WideVT),\n                                      NewMask);\n  // This is free: ymm -> xmm.\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Shuf,\n                     DAG.getIntPtrConstant(0, DL));\n}\n\n/// Try to lower broadcast of a single element.\n///\n/// For convenience, this code also bundles all of the subtarget feature set\n/// filtering. While a little annoying to re-dispatch on type here, there isn't\n/// a convenient way to factor it out.\nstatic SDValue lowerShuffleAsBroadcast(const SDLoc &DL, MVT VT, SDValue V1,\n                                       SDValue V2, ArrayRef<int> Mask,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  if (!((Subtarget.hasSSE3() && VT == MVT::v2f64) ||\n        (Subtarget.hasAVX() && VT.isFloatingPoint()) ||\n        (Subtarget.hasAVX2() && VT.isInteger())))\n    return SDValue();\n\n  // With MOVDDUP (v2f64) we can broadcast from a register or a load, otherwise\n  // we can only broadcast from a register with AVX2.\n  unsigned NumEltBits = VT.getScalarSizeInBits();\n  unsigned Opcode = (VT == MVT::v2f64 && !Subtarget.hasAVX2())\n                        ? X86ISD::MOVDDUP\n                        : X86ISD::VBROADCAST;\n  bool BroadcastFromReg = (Opcode == X86ISD::MOVDDUP) || Subtarget.hasAVX2();\n\n  // Check that the mask is a broadcast.\n  int BroadcastIdx = getSplatIndex(Mask);\n  if (BroadcastIdx < 0)\n    return SDValue();\n  assert(BroadcastIdx < (int)Mask.size() && \"We only expect to be called with \"\n                                            \"a sorted mask where the broadcast \"\n                                            \"comes from V1.\");\n\n  // Go up the chain of (vector) values to find a scalar load that we can\n  // combine with the broadcast.\n  // TODO: Combine this logic with findEltLoadSrc() used by\n  //       EltsFromConsecutiveLoads().\n  int BitOffset = BroadcastIdx * NumEltBits;\n  SDValue V = V1;\n  for (;;) {\n    switch (V.getOpcode()) {\n    case ISD::BITCAST: {\n      V = V.getOperand(0);\n      continue;\n    }\n    case ISD::CONCAT_VECTORS: {\n      int OpBitWidth = V.getOperand(0).getValueSizeInBits();\n      int OpIdx = BitOffset / OpBitWidth;\n      V = V.getOperand(OpIdx);\n      BitOffset %= OpBitWidth;\n      continue;\n    }\n    case ISD::EXTRACT_SUBVECTOR: {\n      // The extraction index adds to the existing offset.\n      unsigned EltBitWidth = V.getScalarValueSizeInBits();\n      unsigned Idx = V.getConstantOperandVal(1);\n      unsigned BeginOffset = Idx * EltBitWidth;\n      BitOffset += BeginOffset;\n      V = V.getOperand(0);\n      continue;\n    }\n    case ISD::INSERT_SUBVECTOR: {\n      SDValue VOuter = V.getOperand(0), VInner = V.getOperand(1);\n      int EltBitWidth = VOuter.getScalarValueSizeInBits();\n      int Idx = (int)V.getConstantOperandVal(2);\n      int NumSubElts = (int)VInner.getSimpleValueType().getVectorNumElements();\n      int BeginOffset = Idx * EltBitWidth;\n      int EndOffset = BeginOffset + NumSubElts * EltBitWidth;\n      if (BeginOffset <= BitOffset && BitOffset < EndOffset) {\n        BitOffset -= BeginOffset;\n        V = VInner;\n      } else {\n        V = VOuter;\n      }\n      continue;\n    }\n    }\n    break;\n  }\n  assert((BitOffset % NumEltBits) == 0 && \"Illegal bit-offset\");\n  BroadcastIdx = BitOffset / NumEltBits;\n\n  // Do we need to bitcast the source to retrieve the original broadcast index?\n  bool BitCastSrc = V.getScalarValueSizeInBits() != NumEltBits;\n\n  // Check if this is a broadcast of a scalar. We special case lowering\n  // for scalars so that we can more effectively fold with loads.\n  // If the original value has a larger element type than the shuffle, the\n  // broadcast element is in essence truncated. Make that explicit to ease\n  // folding.\n  if (BitCastSrc && VT.isInteger())\n    if (SDValue TruncBroadcast = lowerShuffleAsTruncBroadcast(\n            DL, VT, V, BroadcastIdx, Subtarget, DAG))\n      return TruncBroadcast;\n\n  // Also check the simpler case, where we can directly reuse the scalar.\n  if (!BitCastSrc &&\n      ((V.getOpcode() == ISD::BUILD_VECTOR && V.hasOneUse()) ||\n       (V.getOpcode() == ISD::SCALAR_TO_VECTOR && BroadcastIdx == 0))) {\n    V = V.getOperand(BroadcastIdx);\n\n    // If we can't broadcast from a register, check that the input is a load.\n    if (!BroadcastFromReg && !isShuffleFoldableLoad(V))\n      return SDValue();\n  } else if (ISD::isNormalLoad(V.getNode()) &&\n             cast<LoadSDNode>(V)->isSimple()) {\n    // We do not check for one-use of the vector load because a broadcast load\n    // is expected to be a win for code size, register pressure, and possibly\n    // uops even if the original vector load is not eliminated.\n\n    // Reduce the vector load and shuffle to a broadcasted scalar load.\n    LoadSDNode *Ld = cast<LoadSDNode>(V);\n    SDValue BaseAddr = Ld->getOperand(1);\n    MVT SVT = VT.getScalarType();\n    unsigned Offset = BroadcastIdx * SVT.getStoreSize();\n    assert((int)(Offset * 8) == BitOffset && \"Unexpected bit-offset\");\n    SDValue NewAddr =\n        DAG.getMemBasePlusOffset(BaseAddr, TypeSize::Fixed(Offset), DL);\n\n    // Directly form VBROADCAST_LOAD if we're using VBROADCAST opcode rather\n    // than MOVDDUP.\n    // FIXME: Should we add VBROADCAST_LOAD isel patterns for pre-AVX?\n    if (Opcode == X86ISD::VBROADCAST) {\n      SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n      SDValue Ops[] = {Ld->getChain(), NewAddr};\n      V = DAG.getMemIntrinsicNode(\n          X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, SVT,\n          DAG.getMachineFunction().getMachineMemOperand(\n              Ld->getMemOperand(), Offset, SVT.getStoreSize()));\n      DAG.makeEquivalentMemoryOrdering(Ld, V);\n      return DAG.getBitcast(VT, V);\n    }\n    assert(SVT == MVT::f64 && \"Unexpected VT!\");\n    V = DAG.getLoad(SVT, DL, Ld->getChain(), NewAddr,\n                    DAG.getMachineFunction().getMachineMemOperand(\n                        Ld->getMemOperand(), Offset, SVT.getStoreSize()));\n    DAG.makeEquivalentMemoryOrdering(Ld, V);\n  } else if (!BroadcastFromReg) {\n    // We can't broadcast from a vector register.\n    return SDValue();\n  } else if (BitOffset != 0) {\n    // We can only broadcast from the zero-element of a vector register,\n    // but it can be advantageous to broadcast from the zero-element of a\n    // subvector.\n    if (!VT.is256BitVector() && !VT.is512BitVector())\n      return SDValue();\n\n    // VPERMQ/VPERMPD can perform the cross-lane shuffle directly.\n    if (VT == MVT::v4f64 || VT == MVT::v4i64)\n      return SDValue();\n\n    // Only broadcast the zero-element of a 128-bit subvector.\n    if ((BitOffset % 128) != 0)\n      return SDValue();\n\n    assert((BitOffset % V.getScalarValueSizeInBits()) == 0 &&\n           \"Unexpected bit-offset\");\n    assert((V.getValueSizeInBits() == 256 || V.getValueSizeInBits() == 512) &&\n           \"Unexpected vector size\");\n    unsigned ExtractIdx = BitOffset / V.getScalarValueSizeInBits();\n    V = extract128BitVector(V, ExtractIdx, DAG, DL);\n  }\n\n  if (Opcode == X86ISD::MOVDDUP && !V.getValueType().isVector())\n    V = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v2f64,\n                    DAG.getBitcast(MVT::f64, V));\n\n  // If this is a scalar, do the broadcast on this type and bitcast.\n  if (!V.getValueType().isVector()) {\n    assert(V.getScalarValueSizeInBits() == NumEltBits &&\n           \"Unexpected scalar size\");\n    MVT BroadcastVT = MVT::getVectorVT(V.getSimpleValueType(),\n                                       VT.getVectorNumElements());\n    return DAG.getBitcast(VT, DAG.getNode(Opcode, DL, BroadcastVT, V));\n  }\n\n  // We only support broadcasting from 128-bit vectors to minimize the\n  // number of patterns we need to deal with in isel. So extract down to\n  // 128-bits, removing as many bitcasts as possible.\n  if (V.getValueSizeInBits() > 128)\n    V = extract128BitVector(peekThroughBitcasts(V), 0, DAG, DL);\n\n  // Otherwise cast V to a vector with the same element type as VT, but\n  // possibly narrower than VT. Then perform the broadcast.\n  unsigned NumSrcElts = V.getValueSizeInBits() / NumEltBits;\n  MVT CastVT = MVT::getVectorVT(VT.getVectorElementType(), NumSrcElts);\n  return DAG.getNode(Opcode, DL, VT, DAG.getBitcast(CastVT, V));\n}\n\n// Check for whether we can use INSERTPS to perform the shuffle. We only use\n// INSERTPS when the V1 elements are already in the correct locations\n// because otherwise we can just always use two SHUFPS instructions which\n// are much smaller to encode than a SHUFPS and an INSERTPS. We can also\n// perform INSERTPS if a single V1 element is out of place and all V2\n// elements are zeroable.\nstatic bool matchShuffleAsInsertPS(SDValue &V1, SDValue &V2,\n                                   unsigned &InsertPSMask,\n                                   const APInt &Zeroable,\n                                   ArrayRef<int> Mask, SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType().is128BitVector() && \"Bad operand type!\");\n  assert(V2.getSimpleValueType().is128BitVector() && \"Bad operand type!\");\n  assert(Mask.size() == 4 && \"Unexpected mask size for v4 shuffle!\");\n\n  // Attempt to match INSERTPS with one element from VA or VB being\n  // inserted into VA (or undef). If successful, V1, V2 and InsertPSMask\n  // are updated.\n  auto matchAsInsertPS = [&](SDValue VA, SDValue VB,\n                             ArrayRef<int> CandidateMask) {\n    unsigned ZMask = 0;\n    int VADstIndex = -1;\n    int VBDstIndex = -1;\n    bool VAUsedInPlace = false;\n\n    for (int i = 0; i < 4; ++i) {\n      // Synthesize a zero mask from the zeroable elements (includes undefs).\n      if (Zeroable[i]) {\n        ZMask |= 1 << i;\n        continue;\n      }\n\n      // Flag if we use any VA inputs in place.\n      if (i == CandidateMask[i]) {\n        VAUsedInPlace = true;\n        continue;\n      }\n\n      // We can only insert a single non-zeroable element.\n      if (VADstIndex >= 0 || VBDstIndex >= 0)\n        return false;\n\n      if (CandidateMask[i] < 4) {\n        // VA input out of place for insertion.\n        VADstIndex = i;\n      } else {\n        // VB input for insertion.\n        VBDstIndex = i;\n      }\n    }\n\n    // Don't bother if we have no (non-zeroable) element for insertion.\n    if (VADstIndex < 0 && VBDstIndex < 0)\n      return false;\n\n    // Determine element insertion src/dst indices. The src index is from the\n    // start of the inserted vector, not the start of the concatenated vector.\n    unsigned VBSrcIndex = 0;\n    if (VADstIndex >= 0) {\n      // If we have a VA input out of place, we use VA as the V2 element\n      // insertion and don't use the original V2 at all.\n      VBSrcIndex = CandidateMask[VADstIndex];\n      VBDstIndex = VADstIndex;\n      VB = VA;\n    } else {\n      VBSrcIndex = CandidateMask[VBDstIndex] - 4;\n    }\n\n    // If no V1 inputs are used in place, then the result is created only from\n    // the zero mask and the V2 insertion - so remove V1 dependency.\n    if (!VAUsedInPlace)\n      VA = DAG.getUNDEF(MVT::v4f32);\n\n    // Update V1, V2 and InsertPSMask accordingly.\n    V1 = VA;\n    V2 = VB;\n\n    // Insert the V2 element into the desired position.\n    InsertPSMask = VBSrcIndex << 6 | VBDstIndex << 4 | ZMask;\n    assert((InsertPSMask & ~0xFFu) == 0 && \"Invalid mask!\");\n    return true;\n  };\n\n  if (matchAsInsertPS(V1, V2, Mask))\n    return true;\n\n  // Commute and try again.\n  SmallVector<int, 4> CommutedMask(Mask.begin(), Mask.end());\n  ShuffleVectorSDNode::commuteMask(CommutedMask);\n  if (matchAsInsertPS(V2, V1, CommutedMask))\n    return true;\n\n  return false;\n}\n\nstatic SDValue lowerShuffleAsInsertPS(const SDLoc &DL, SDValue V1, SDValue V2,\n                                      ArrayRef<int> Mask, const APInt &Zeroable,\n                                      SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v4f32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v4f32 && \"Bad operand type!\");\n\n  // Attempt to match the insertps pattern.\n  unsigned InsertPSMask = 0;\n  if (!matchShuffleAsInsertPS(V1, V2, InsertPSMask, Zeroable, Mask, DAG))\n    return SDValue();\n\n  // Insert the V2 element into the desired position.\n  return DAG.getNode(X86ISD::INSERTPS, DL, MVT::v4f32, V1, V2,\n                     DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n}\n\n/// Try to lower a shuffle as a permute of the inputs followed by an\n/// UNPCK instruction.\n///\n/// This specifically targets cases where we end up with alternating between\n/// the two inputs, and so can permute them into something that feeds a single\n/// UNPCK instruction. Note that this routine only targets integer vectors\n/// because for floating point vectors we have a generalized SHUFPS lowering\n/// strategy that handles everything that doesn't *exactly* match an unpack,\n/// making this clever lowering unnecessary.\nstatic SDValue lowerShuffleAsPermuteAndUnpack(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  assert(!VT.isFloatingPoint() &&\n         \"This routine only supports integer vectors.\");\n  assert(VT.is128BitVector() &&\n         \"This routine only works on 128-bit vectors.\");\n  assert(!V2.isUndef() &&\n         \"This routine should only be used when blending two inputs.\");\n  assert(Mask.size() >= 2 && \"Single element masks are invalid.\");\n\n  int Size = Mask.size();\n\n  int NumLoInputs =\n      count_if(Mask, [Size](int M) { return M >= 0 && M % Size < Size / 2; });\n  int NumHiInputs =\n      count_if(Mask, [Size](int M) { return M % Size >= Size / 2; });\n\n  bool UnpackLo = NumLoInputs >= NumHiInputs;\n\n  auto TryUnpack = [&](int ScalarSize, int Scale) {\n    SmallVector<int, 16> V1Mask((unsigned)Size, -1);\n    SmallVector<int, 16> V2Mask((unsigned)Size, -1);\n\n    for (int i = 0; i < Size; ++i) {\n      if (Mask[i] < 0)\n        continue;\n\n      // Each element of the unpack contains Scale elements from this mask.\n      int UnpackIdx = i / Scale;\n\n      // We only handle the case where V1 feeds the first slots of the unpack.\n      // We rely on canonicalization to ensure this is the case.\n      if ((UnpackIdx % 2 == 0) != (Mask[i] < Size))\n        return SDValue();\n\n      // Setup the mask for this input. The indexing is tricky as we have to\n      // handle the unpack stride.\n      SmallVectorImpl<int> &VMask = (UnpackIdx % 2 == 0) ? V1Mask : V2Mask;\n      VMask[(UnpackIdx / 2) * Scale + i % Scale + (UnpackLo ? 0 : Size / 2)] =\n          Mask[i] % Size;\n    }\n\n    // If we will have to shuffle both inputs to use the unpack, check whether\n    // we can just unpack first and shuffle the result. If so, skip this unpack.\n    if ((NumLoInputs == 0 || NumHiInputs == 0) && !isNoopShuffleMask(V1Mask) &&\n        !isNoopShuffleMask(V2Mask))\n      return SDValue();\n\n    // Shuffle the inputs into place.\n    V1 = DAG.getVectorShuffle(VT, DL, V1, DAG.getUNDEF(VT), V1Mask);\n    V2 = DAG.getVectorShuffle(VT, DL, V2, DAG.getUNDEF(VT), V2Mask);\n\n    // Cast the inputs to the type we will use to unpack them.\n    MVT UnpackVT = MVT::getVectorVT(MVT::getIntegerVT(ScalarSize), Size / Scale);\n    V1 = DAG.getBitcast(UnpackVT, V1);\n    V2 = DAG.getBitcast(UnpackVT, V2);\n\n    // Unpack the inputs and cast the result back to the desired type.\n    return DAG.getBitcast(\n        VT, DAG.getNode(UnpackLo ? X86ISD::UNPCKL : X86ISD::UNPCKH, DL,\n                        UnpackVT, V1, V2));\n  };\n\n  // We try each unpack from the largest to the smallest to try and find one\n  // that fits this mask.\n  int OrigScalarSize = VT.getScalarSizeInBits();\n  for (int ScalarSize = 64; ScalarSize >= OrigScalarSize; ScalarSize /= 2)\n    if (SDValue Unpack = TryUnpack(ScalarSize, ScalarSize / OrigScalarSize))\n      return Unpack;\n\n  // If we're shuffling with a zero vector then we're better off not doing\n  // VECTOR_SHUFFLE(UNPCK()) as we lose track of those zero elements.\n  if (ISD::isBuildVectorAllZeros(V1.getNode()) ||\n      ISD::isBuildVectorAllZeros(V2.getNode()))\n    return SDValue();\n\n  // If none of the unpack-rooted lowerings worked (or were profitable) try an\n  // initial unpack.\n  if (NumLoInputs == 0 || NumHiInputs == 0) {\n    assert((NumLoInputs > 0 || NumHiInputs > 0) &&\n           \"We have to have *some* inputs!\");\n    int HalfOffset = NumLoInputs == 0 ? Size / 2 : 0;\n\n    // FIXME: We could consider the total complexity of the permute of each\n    // possible unpacking. Or at the least we should consider how many\n    // half-crossings are created.\n    // FIXME: We could consider commuting the unpacks.\n\n    SmallVector<int, 32> PermMask((unsigned)Size, -1);\n    for (int i = 0; i < Size; ++i) {\n      if (Mask[i] < 0)\n        continue;\n\n      assert(Mask[i] % Size >= HalfOffset && \"Found input from wrong half!\");\n\n      PermMask[i] =\n          2 * ((Mask[i] % Size) - HalfOffset) + (Mask[i] < Size ? 0 : 1);\n    }\n    return DAG.getVectorShuffle(\n        VT, DL, DAG.getNode(NumLoInputs == 0 ? X86ISD::UNPCKH : X86ISD::UNPCKL,\n                            DL, VT, V1, V2),\n        DAG.getUNDEF(VT), PermMask);\n  }\n\n  return SDValue();\n}\n\n/// Handle lowering of 2-lane 64-bit floating point shuffles.\n///\n/// This is the basis function for the 2-lane 64-bit shuffles as we have full\n/// support for floating point shuffles but not integer shuffles. These\n/// instructions will incur a domain crossing penalty on some chips though so\n/// it is better to avoid lowering through this for integer vectors where\n/// possible.\nstatic SDValue lowerV2F64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v2f64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v2f64 && \"Bad operand type!\");\n  assert(Mask.size() == 2 && \"Unexpected mask size for v2 shuffle!\");\n\n  if (V2.isUndef()) {\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v2f64, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Straight shuffle of a single input vector. Simulate this by using the\n    // single input as both of the \"inputs\" to this instruction..\n    unsigned SHUFPDMask = (Mask[0] == 1) | ((Mask[1] == 1) << 1);\n\n    if (Subtarget.hasAVX()) {\n      // If we have AVX, we can use VPERMILPS which will allow folding a load\n      // into the shuffle.\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v2f64, V1,\n                         DAG.getTargetConstant(SHUFPDMask, DL, MVT::i8));\n    }\n\n    return DAG.getNode(\n        X86ISD::SHUFP, DL, MVT::v2f64,\n        Mask[0] == SM_SentinelUndef ? DAG.getUNDEF(MVT::v2f64) : V1,\n        Mask[1] == SM_SentinelUndef ? DAG.getUNDEF(MVT::v2f64) : V1,\n        DAG.getTargetConstant(SHUFPDMask, DL, MVT::i8));\n  }\n  assert(Mask[0] >= 0 && \"No undef lanes in multi-input v2 shuffles!\");\n  assert(Mask[1] >= 0 && \"No undef lanes in multi-input v2 shuffles!\");\n  assert(Mask[0] < 2 && \"We sort V1 to be the first input.\");\n  assert(Mask[1] >= 2 && \"We sort V2 to be the second input.\");\n\n  if (Subtarget.hasAVX2())\n    if (SDValue Extract = lowerShuffleOfExtractsAsVperm(DL, V1, V2, Mask, DAG))\n      return Extract;\n\n  // When loading a scalar and then shuffling it into a vector we can often do\n  // the insertion cheaply.\n  if (SDValue Insertion = lowerShuffleAsElementInsertion(\n          DL, MVT::v2f64, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return Insertion;\n  // Try inverting the insertion since for v2 masks it is easy to do and we\n  // can't reliably sort the mask one way or the other.\n  int InverseMask[2] = {Mask[0] < 0 ? -1 : (Mask[0] ^ 2),\n                        Mask[1] < 0 ? -1 : (Mask[1] ^ 2)};\n  if (SDValue Insertion = lowerShuffleAsElementInsertion(\n          DL, MVT::v2f64, V2, V1, InverseMask, Zeroable, Subtarget, DAG))\n    return Insertion;\n\n  // Try to use one of the special instruction patterns to handle two common\n  // blend patterns if a zero-blend above didn't work.\n  if (isShuffleEquivalent(Mask, {0, 3}, V1, V2) ||\n      isShuffleEquivalent(Mask, {1, 3}, V1, V2))\n    if (SDValue V1S = getScalarValueForVectorElement(V1, Mask[0], DAG))\n      // We can either use a special instruction to load over the low double or\n      // to move just the low double.\n      return DAG.getNode(\n          X86ISD::MOVSD, DL, MVT::v2f64, V2,\n          DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v2f64, V1S));\n\n  if (Subtarget.hasSSE41())\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v2f64, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v2f64, Mask, V1, V2, DAG))\n    return V;\n\n  unsigned SHUFPDMask = (Mask[0] == 1) | (((Mask[1] - 2) == 1) << 1);\n  return DAG.getNode(X86ISD::SHUFP, DL, MVT::v2f64, V1, V2,\n                     DAG.getTargetConstant(SHUFPDMask, DL, MVT::i8));\n}\n\n/// Handle lowering of 2-lane 64-bit integer shuffles.\n///\n/// Tries to lower a 2-lane 64-bit shuffle using shuffle operations provided by\n/// the integer unit to minimize domain crossing penalties. However, for blends\n/// it falls back to the floating point shuffle operation with appropriate bit\n/// casting.\nstatic SDValue lowerV2I64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v2i64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v2i64 && \"Bad operand type!\");\n  assert(Mask.size() == 2 && \"Unexpected mask size for v2 shuffle!\");\n\n  if (V2.isUndef()) {\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v2i64, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Straight shuffle of a single input vector. For everything from SSE2\n    // onward this has a single fast instruction with no scary immediates.\n    // We have to map the mask as it is actually a v4i32 shuffle instruction.\n    V1 = DAG.getBitcast(MVT::v4i32, V1);\n    int WidenedMask[4] = {Mask[0] < 0 ? -1 : (Mask[0] * 2),\n                          Mask[0] < 0 ? -1 : ((Mask[0] * 2) + 1),\n                          Mask[1] < 0 ? -1 : (Mask[1] * 2),\n                          Mask[1] < 0 ? -1 : ((Mask[1] * 2) + 1)};\n    return DAG.getBitcast(\n        MVT::v2i64,\n        DAG.getNode(X86ISD::PSHUFD, DL, MVT::v4i32, V1,\n                    getV4X86ShuffleImm8ForMask(WidenedMask, DL, DAG)));\n  }\n  assert(Mask[0] != -1 && \"No undef lanes in multi-input v2 shuffles!\");\n  assert(Mask[1] != -1 && \"No undef lanes in multi-input v2 shuffles!\");\n  assert(Mask[0] < 2 && \"We sort V1 to be the first input.\");\n  assert(Mask[1] >= 2 && \"We sort V2 to be the second input.\");\n\n  if (Subtarget.hasAVX2())\n    if (SDValue Extract = lowerShuffleOfExtractsAsVperm(DL, V1, V2, Mask, DAG))\n      return Extract;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v2i64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // When loading a scalar and then shuffling it into a vector we can often do\n  // the insertion cheaply.\n  if (SDValue Insertion = lowerShuffleAsElementInsertion(\n          DL, MVT::v2i64, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return Insertion;\n  // Try inverting the insertion since for v2 masks it is easy to do and we\n  // can't reliably sort the mask one way or the other.\n  int InverseMask[2] = {Mask[0] ^ 2, Mask[1] ^ 2};\n  if (SDValue Insertion = lowerShuffleAsElementInsertion(\n          DL, MVT::v2i64, V2, V1, InverseMask, Zeroable, Subtarget, DAG))\n    return Insertion;\n\n  // We have different paths for blend lowering, but they all must use the\n  // *exact* same predicate.\n  bool IsBlendSupported = Subtarget.hasSSE41();\n  if (IsBlendSupported)\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v2i64, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v2i64, Mask, V1, V2, DAG))\n    return V;\n\n  // Try to use byte rotation instructions.\n  // Its more profitable for pre-SSSE3 to use shuffles/unpacks.\n  if (Subtarget.hasSSSE3()) {\n    if (Subtarget.hasVLX())\n      if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v2i64, V1, V2, Mask,\n                                                Subtarget, DAG))\n        return Rotate;\n\n    if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v2i64, V1, V2, Mask,\n                                                  Subtarget, DAG))\n      return Rotate;\n  }\n\n  // If we have direct support for blends, we should lower by decomposing into\n  // a permute. That will be faster than the domain cross.\n  if (IsBlendSupported)\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v2i64, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // We implement this with SHUFPD which is pretty lame because it will likely\n  // incur 2 cycles of stall for integer vectors on Nehalem and older chips.\n  // However, all the alternatives are still more cycles and newer chips don't\n  // have this problem. It would be really nice if x86 had better shuffles here.\n  V1 = DAG.getBitcast(MVT::v2f64, V1);\n  V2 = DAG.getBitcast(MVT::v2f64, V2);\n  return DAG.getBitcast(MVT::v2i64,\n                        DAG.getVectorShuffle(MVT::v2f64, DL, V1, V2, Mask));\n}\n\n/// Lower a vector shuffle using the SHUFPS instruction.\n///\n/// This is a helper routine dedicated to lowering vector shuffles using SHUFPS.\n/// It makes no assumptions about whether this is the *best* lowering, it simply\n/// uses it.\nstatic SDValue lowerShuffleWithSHUFPS(const SDLoc &DL, MVT VT,\n                                      ArrayRef<int> Mask, SDValue V1,\n                                      SDValue V2, SelectionDAG &DAG) {\n  SDValue LowV = V1, HighV = V2;\n  SmallVector<int, 4> NewMask(Mask.begin(), Mask.end());\n  int NumV2Elements = count_if(Mask, [](int M) { return M >= 4; });\n\n  if (NumV2Elements == 1) {\n    int V2Index = find_if(Mask, [](int M) { return M >= 4; }) - Mask.begin();\n\n    // Compute the index adjacent to V2Index and in the same half by toggling\n    // the low bit.\n    int V2AdjIndex = V2Index ^ 1;\n\n    if (Mask[V2AdjIndex] < 0) {\n      // Handles all the cases where we have a single V2 element and an undef.\n      // This will only ever happen in the high lanes because we commute the\n      // vector otherwise.\n      if (V2Index < 2)\n        std::swap(LowV, HighV);\n      NewMask[V2Index] -= 4;\n    } else {\n      // Handle the case where the V2 element ends up adjacent to a V1 element.\n      // To make this work, blend them together as the first step.\n      int V1Index = V2AdjIndex;\n      int BlendMask[4] = {Mask[V2Index] - 4, 0, Mask[V1Index], 0};\n      V2 = DAG.getNode(X86ISD::SHUFP, DL, VT, V2, V1,\n                       getV4X86ShuffleImm8ForMask(BlendMask, DL, DAG));\n\n      // Now proceed to reconstruct the final blend as we have the necessary\n      // high or low half formed.\n      if (V2Index < 2) {\n        LowV = V2;\n        HighV = V1;\n      } else {\n        HighV = V2;\n      }\n      NewMask[V1Index] = 2; // We put the V1 element in V2[2].\n      NewMask[V2Index] = 0; // We shifted the V2 element into V2[0].\n    }\n  } else if (NumV2Elements == 2) {\n    if (Mask[0] < 4 && Mask[1] < 4) {\n      // Handle the easy case where we have V1 in the low lanes and V2 in the\n      // high lanes.\n      NewMask[2] -= 4;\n      NewMask[3] -= 4;\n    } else if (Mask[2] < 4 && Mask[3] < 4) {\n      // We also handle the reversed case because this utility may get called\n      // when we detect a SHUFPS pattern but can't easily commute the shuffle to\n      // arrange things in the right direction.\n      NewMask[0] -= 4;\n      NewMask[1] -= 4;\n      HighV = V1;\n      LowV = V2;\n    } else {\n      // We have a mixture of V1 and V2 in both low and high lanes. Rather than\n      // trying to place elements directly, just blend them and set up the final\n      // shuffle to place them.\n\n      // The first two blend mask elements are for V1, the second two are for\n      // V2.\n      int BlendMask[4] = {Mask[0] < 4 ? Mask[0] : Mask[1],\n                          Mask[2] < 4 ? Mask[2] : Mask[3],\n                          (Mask[0] >= 4 ? Mask[0] : Mask[1]) - 4,\n                          (Mask[2] >= 4 ? Mask[2] : Mask[3]) - 4};\n      V1 = DAG.getNode(X86ISD::SHUFP, DL, VT, V1, V2,\n                       getV4X86ShuffleImm8ForMask(BlendMask, DL, DAG));\n\n      // Now we do a normal shuffle of V1 by giving V1 as both operands to\n      // a blend.\n      LowV = HighV = V1;\n      NewMask[0] = Mask[0] < 4 ? 0 : 2;\n      NewMask[1] = Mask[0] < 4 ? 2 : 0;\n      NewMask[2] = Mask[2] < 4 ? 1 : 3;\n      NewMask[3] = Mask[2] < 4 ? 3 : 1;\n    }\n  } else if (NumV2Elements == 3) {\n    // Ideally canonicalizeShuffleMaskWithCommute should have caught this, but\n    // we can get here due to other paths (e.g repeated mask matching) that we\n    // don't want to do another round of lowerVECTOR_SHUFFLE.\n    ShuffleVectorSDNode::commuteMask(NewMask);\n    return lowerShuffleWithSHUFPS(DL, VT, NewMask, V2, V1, DAG);\n  }\n  return DAG.getNode(X86ISD::SHUFP, DL, VT, LowV, HighV,\n                     getV4X86ShuffleImm8ForMask(NewMask, DL, DAG));\n}\n\n/// Lower 4-lane 32-bit floating point shuffles.\n///\n/// Uses instructions exclusively from the floating point unit to minimize\n/// domain crossing penalties, as these are sufficient to implement all v4f32\n/// shuffles.\nstatic SDValue lowerV4F32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v4f32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v4f32 && \"Bad operand type!\");\n  assert(Mask.size() == 4 && \"Unexpected mask size for v4 shuffle!\");\n\n  int NumV2Elements = count_if(Mask, [](int M) { return M >= 4; });\n\n  if (NumV2Elements == 0) {\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v4f32, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Use even/odd duplicate instructions for masks that match their pattern.\n    if (Subtarget.hasSSE3()) {\n      if (isShuffleEquivalent(Mask, {0, 0, 2, 2}, V1, V2))\n        return DAG.getNode(X86ISD::MOVSLDUP, DL, MVT::v4f32, V1);\n      if (isShuffleEquivalent(Mask, {1, 1, 3, 3}, V1, V2))\n        return DAG.getNode(X86ISD::MOVSHDUP, DL, MVT::v4f32, V1);\n    }\n\n    if (Subtarget.hasAVX()) {\n      // If we have AVX, we can use VPERMILPS which will allow folding a load\n      // into the shuffle.\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v4f32, V1,\n                         getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n    }\n\n    // Use MOVLHPS/MOVHLPS to simulate unary shuffles. These are only valid\n    // in SSE1 because otherwise they are widened to v2f64 and never get here.\n    if (!Subtarget.hasSSE2()) {\n      if (isShuffleEquivalent(Mask, {0, 1, 0, 1}, V1, V2))\n        return DAG.getNode(X86ISD::MOVLHPS, DL, MVT::v4f32, V1, V1);\n      if (isShuffleEquivalent(Mask, {2, 3, 2, 3}, V1, V2))\n        return DAG.getNode(X86ISD::MOVHLPS, DL, MVT::v4f32, V1, V1);\n    }\n\n    // Otherwise, use a straight shuffle of a single input vector. We pass the\n    // input vector to both operands to simulate this with a SHUFPS.\n    return DAG.getNode(X86ISD::SHUFP, DL, MVT::v4f32, V1, V1,\n                       getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n  }\n\n  if (Subtarget.hasAVX2())\n    if (SDValue Extract = lowerShuffleOfExtractsAsVperm(DL, V1, V2, Mask, DAG))\n      return Extract;\n\n  // There are special ways we can lower some single-element blends. However, we\n  // have custom ways we can lower more complex single-element blends below that\n  // we defer to if both this and BLENDPS fail to match, so restrict this to\n  // when the V2 input is targeting element 0 of the mask -- that is the fast\n  // case here.\n  if (NumV2Elements == 1 && Mask[0] >= 4)\n    if (SDValue V = lowerShuffleAsElementInsertion(\n            DL, MVT::v4f32, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return V;\n\n  if (Subtarget.hasSSE41()) {\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v4f32, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n    // Use INSERTPS if we can complete the shuffle efficiently.\n    if (SDValue V = lowerShuffleAsInsertPS(DL, V1, V2, Mask, Zeroable, DAG))\n      return V;\n\n    if (!isSingleSHUFPSMask(Mask))\n      if (SDValue BlendPerm = lowerShuffleAsBlendAndPermute(DL, MVT::v4f32, V1,\n                                                            V2, Mask, DAG))\n        return BlendPerm;\n  }\n\n  // Use low/high mov instructions. These are only valid in SSE1 because\n  // otherwise they are widened to v2f64 and never get here.\n  if (!Subtarget.hasSSE2()) {\n    if (isShuffleEquivalent(Mask, {0, 1, 4, 5}, V1, V2))\n      return DAG.getNode(X86ISD::MOVLHPS, DL, MVT::v4f32, V1, V2);\n    if (isShuffleEquivalent(Mask, {2, 3, 6, 7}, V1, V2))\n      return DAG.getNode(X86ISD::MOVHLPS, DL, MVT::v4f32, V2, V1);\n  }\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v4f32, Mask, V1, V2, DAG))\n    return V;\n\n  // Otherwise fall back to a SHUFPS lowering strategy.\n  return lowerShuffleWithSHUFPS(DL, MVT::v4f32, Mask, V1, V2, DAG);\n}\n\n/// Lower 4-lane i32 vector shuffles.\n///\n/// We try to handle these with integer-domain shuffles where we can, but for\n/// blends we use the floating point domain blend instructions.\nstatic SDValue lowerV4I32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v4i32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v4i32 && \"Bad operand type!\");\n  assert(Mask.size() == 4 && \"Unexpected mask size for v4 shuffle!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(DL, MVT::v4i32, V1, V2, Mask,\n                                                   Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  int NumV2Elements = count_if(Mask, [](int M) { return M >= 4; });\n\n  if (NumV2Elements == 0) {\n    // Try to use broadcast unless the mask only has one non-undef element.\n    if (count_if(Mask, [](int M) { return M >= 0 && M < 4; }) > 1) {\n      if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v4i32, V1, V2,\n                                                      Mask, Subtarget, DAG))\n        return Broadcast;\n    }\n\n    // Straight shuffle of a single input vector. For everything from SSE2\n    // onward this has a single fast instruction with no scary immediates.\n    // We coerce the shuffle pattern to be compatible with UNPCK instructions\n    // but we aren't actually going to use the UNPCK instruction because doing\n    // so prevents folding a load into this instruction or making a copy.\n    const int UnpackLoMask[] = {0, 0, 1, 1};\n    const int UnpackHiMask[] = {2, 2, 3, 3};\n    if (isShuffleEquivalent(Mask, {0, 0, 1, 1}, V1, V2))\n      Mask = UnpackLoMask;\n    else if (isShuffleEquivalent(Mask, {2, 2, 3, 3}, V1, V2))\n      Mask = UnpackHiMask;\n\n    return DAG.getNode(X86ISD::PSHUFD, DL, MVT::v4i32, V1,\n                       getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n  }\n\n  if (Subtarget.hasAVX2())\n    if (SDValue Extract = lowerShuffleOfExtractsAsVperm(DL, V1, V2, Mask, DAG))\n      return Extract;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v4i32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // There are special ways we can lower some single-element blends.\n  if (NumV2Elements == 1)\n    if (SDValue V = lowerShuffleAsElementInsertion(\n            DL, MVT::v4i32, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return V;\n\n  // We have different paths for blend lowering, but they all must use the\n  // *exact* same predicate.\n  bool IsBlendSupported = Subtarget.hasSSE41();\n  if (IsBlendSupported)\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v4i32, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n  if (SDValue Masked = lowerShuffleAsBitMask(DL, MVT::v4i32, V1, V2, Mask,\n                                             Zeroable, Subtarget, DAG))\n    return Masked;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v4i32, Mask, V1, V2, DAG))\n    return V;\n\n  // Try to use byte rotation instructions.\n  // Its more profitable for pre-SSSE3 to use shuffles/unpacks.\n  if (Subtarget.hasSSSE3()) {\n    if (Subtarget.hasVLX())\n      if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v4i32, V1, V2, Mask,\n                                                Subtarget, DAG))\n        return Rotate;\n\n    if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v4i32, V1, V2, Mask,\n                                                  Subtarget, DAG))\n      return Rotate;\n  }\n\n  // Assume that a single SHUFPS is faster than an alternative sequence of\n  // multiple instructions (even if the CPU has a domain penalty).\n  // If some CPU is harmed by the domain switch, we can fix it in a later pass.\n  if (!isSingleSHUFPSMask(Mask)) {\n    // If we have direct support for blends, we should lower by decomposing into\n    // a permute. That will be faster than the domain cross.\n    if (IsBlendSupported)\n      return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v4i32, V1, V2, Mask,\n                                                  Subtarget, DAG);\n\n    // Try to lower by permuting the inputs into an unpack instruction.\n    if (SDValue Unpack = lowerShuffleAsPermuteAndUnpack(DL, MVT::v4i32, V1, V2,\n                                                        Mask, Subtarget, DAG))\n      return Unpack;\n  }\n\n  // We implement this with SHUFPS because it can blend from two vectors.\n  // Because we're going to eventually use SHUFPS, we use SHUFPS even to build\n  // up the inputs, bypassing domain shift penalties that we would incur if we\n  // directly used PSHUFD on Nehalem and older. For newer chips, this isn't\n  // relevant.\n  SDValue CastV1 = DAG.getBitcast(MVT::v4f32, V1);\n  SDValue CastV2 = DAG.getBitcast(MVT::v4f32, V2);\n  SDValue ShufPS = DAG.getVectorShuffle(MVT::v4f32, DL, CastV1, CastV2, Mask);\n  return DAG.getBitcast(MVT::v4i32, ShufPS);\n}\n\n/// Lowering of single-input v8i16 shuffles is the cornerstone of SSE2\n/// shuffle lowering, and the most complex part.\n///\n/// The lowering strategy is to try to form pairs of input lanes which are\n/// targeted at the same half of the final vector, and then use a dword shuffle\n/// to place them onto the right half, and finally unpack the paired lanes into\n/// their final position.\n///\n/// The exact breakdown of how to form these dword pairs and align them on the\n/// correct sides is really tricky. See the comments within the function for\n/// more of the details.\n///\n/// This code also handles repeated 128-bit lanes of v8i16 shuffles, but each\n/// lane must shuffle the *exact* same way. In fact, you must pass a v8 Mask to\n/// this routine for it to work correctly. To shuffle a 256-bit or 512-bit i16\n/// vector, form the analogous 128-bit 8-element Mask.\nstatic SDValue lowerV8I16GeneralSingleInputShuffle(\n    const SDLoc &DL, MVT VT, SDValue V, MutableArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  assert(VT.getVectorElementType() == MVT::i16 && \"Bad input type!\");\n  MVT PSHUFDVT = MVT::getVectorVT(MVT::i32, VT.getVectorNumElements() / 2);\n\n  assert(Mask.size() == 8 && \"Shuffle mask length doesn't match!\");\n  MutableArrayRef<int> LoMask = Mask.slice(0, 4);\n  MutableArrayRef<int> HiMask = Mask.slice(4, 4);\n\n  // Attempt to directly match PSHUFLW or PSHUFHW.\n  if (isUndefOrInRange(LoMask, 0, 4) &&\n      isSequentialOrUndefInRange(HiMask, 0, 4, 4)) {\n    return DAG.getNode(X86ISD::PSHUFLW, DL, VT, V,\n                       getV4X86ShuffleImm8ForMask(LoMask, DL, DAG));\n  }\n  if (isUndefOrInRange(HiMask, 4, 8) &&\n      isSequentialOrUndefInRange(LoMask, 0, 4, 0)) {\n    for (int i = 0; i != 4; ++i)\n      HiMask[i] = (HiMask[i] < 0 ? HiMask[i] : (HiMask[i] - 4));\n    return DAG.getNode(X86ISD::PSHUFHW, DL, VT, V,\n                       getV4X86ShuffleImm8ForMask(HiMask, DL, DAG));\n  }\n\n  SmallVector<int, 4> LoInputs;\n  copy_if(LoMask, std::back_inserter(LoInputs), [](int M) { return M >= 0; });\n  array_pod_sort(LoInputs.begin(), LoInputs.end());\n  LoInputs.erase(std::unique(LoInputs.begin(), LoInputs.end()), LoInputs.end());\n  SmallVector<int, 4> HiInputs;\n  copy_if(HiMask, std::back_inserter(HiInputs), [](int M) { return M >= 0; });\n  array_pod_sort(HiInputs.begin(), HiInputs.end());\n  HiInputs.erase(std::unique(HiInputs.begin(), HiInputs.end()), HiInputs.end());\n  int NumLToL = llvm::lower_bound(LoInputs, 4) - LoInputs.begin();\n  int NumHToL = LoInputs.size() - NumLToL;\n  int NumLToH = llvm::lower_bound(HiInputs, 4) - HiInputs.begin();\n  int NumHToH = HiInputs.size() - NumLToH;\n  MutableArrayRef<int> LToLInputs(LoInputs.data(), NumLToL);\n  MutableArrayRef<int> LToHInputs(HiInputs.data(), NumLToH);\n  MutableArrayRef<int> HToLInputs(LoInputs.data() + NumLToL, NumHToL);\n  MutableArrayRef<int> HToHInputs(HiInputs.data() + NumLToH, NumHToH);\n\n  // If we are shuffling values from one half - check how many different DWORD\n  // pairs we need to create. If only 1 or 2 then we can perform this as a\n  // PSHUFLW/PSHUFHW + PSHUFD instead of the PSHUFD+PSHUFLW+PSHUFHW chain below.\n  auto ShuffleDWordPairs = [&](ArrayRef<int> PSHUFHalfMask,\n                               ArrayRef<int> PSHUFDMask, unsigned ShufWOp) {\n    V = DAG.getNode(ShufWOp, DL, VT, V,\n                    getV4X86ShuffleImm8ForMask(PSHUFHalfMask, DL, DAG));\n    V = DAG.getBitcast(PSHUFDVT, V);\n    V = DAG.getNode(X86ISD::PSHUFD, DL, PSHUFDVT, V,\n                    getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG));\n    return DAG.getBitcast(VT, V);\n  };\n\n  if ((NumHToL + NumHToH) == 0 || (NumLToL + NumLToH) == 0) {\n    int PSHUFDMask[4] = { -1, -1, -1, -1 };\n    SmallVector<std::pair<int, int>, 4> DWordPairs;\n    int DOffset = ((NumHToL + NumHToH) == 0 ? 0 : 2);\n\n    // Collect the different DWORD pairs.\n    for (int DWord = 0; DWord != 4; ++DWord) {\n      int M0 = Mask[2 * DWord + 0];\n      int M1 = Mask[2 * DWord + 1];\n      M0 = (M0 >= 0 ? M0 % 4 : M0);\n      M1 = (M1 >= 0 ? M1 % 4 : M1);\n      if (M0 < 0 && M1 < 0)\n        continue;\n\n      bool Match = false;\n      for (int j = 0, e = DWordPairs.size(); j < e; ++j) {\n        auto &DWordPair = DWordPairs[j];\n        if ((M0 < 0 || isUndefOrEqual(DWordPair.first, M0)) &&\n            (M1 < 0 || isUndefOrEqual(DWordPair.second, M1))) {\n          DWordPair.first = (M0 >= 0 ? M0 : DWordPair.first);\n          DWordPair.second = (M1 >= 0 ? M1 : DWordPair.second);\n          PSHUFDMask[DWord] = DOffset + j;\n          Match = true;\n          break;\n        }\n      }\n      if (!Match) {\n        PSHUFDMask[DWord] = DOffset + DWordPairs.size();\n        DWordPairs.push_back(std::make_pair(M0, M1));\n      }\n    }\n\n    if (DWordPairs.size() <= 2) {\n      DWordPairs.resize(2, std::make_pair(-1, -1));\n      int PSHUFHalfMask[4] = {DWordPairs[0].first, DWordPairs[0].second,\n                              DWordPairs[1].first, DWordPairs[1].second};\n      if ((NumHToL + NumHToH) == 0)\n        return ShuffleDWordPairs(PSHUFHalfMask, PSHUFDMask, X86ISD::PSHUFLW);\n      if ((NumLToL + NumLToH) == 0)\n        return ShuffleDWordPairs(PSHUFHalfMask, PSHUFDMask, X86ISD::PSHUFHW);\n    }\n  }\n\n  // Simplify the 1-into-3 and 3-into-1 cases with a single pshufd. For all\n  // such inputs we can swap two of the dwords across the half mark and end up\n  // with <=2 inputs to each half in each half. Once there, we can fall through\n  // to the generic code below. For example:\n  //\n  // Input: [a, b, c, d, e, f, g, h] -PSHUFD[0,2,1,3]-> [a, b, e, f, c, d, g, h]\n  // Mask:  [0, 1, 2, 7, 4, 5, 6, 3] -----------------> [0, 1, 4, 7, 2, 3, 6, 5]\n  //\n  // However in some very rare cases we have a 1-into-3 or 3-into-1 on one half\n  // and an existing 2-into-2 on the other half. In this case we may have to\n  // pre-shuffle the 2-into-2 half to avoid turning it into a 3-into-1 or\n  // 1-into-3 which could cause us to cycle endlessly fixing each side in turn.\n  // Fortunately, we don't have to handle anything but a 2-into-2 pattern\n  // because any other situation (including a 3-into-1 or 1-into-3 in the other\n  // half than the one we target for fixing) will be fixed when we re-enter this\n  // path. We will also combine away any sequence of PSHUFD instructions that\n  // result into a single instruction. Here is an example of the tricky case:\n  //\n  // Input: [a, b, c, d, e, f, g, h] -PSHUFD[0,2,1,3]-> [a, b, e, f, c, d, g, h]\n  // Mask:  [3, 7, 1, 0, 2, 7, 3, 5] -THIS-IS-BAD!!!!-> [5, 7, 1, 0, 4, 7, 5, 3]\n  //\n  // This now has a 1-into-3 in the high half! Instead, we do two shuffles:\n  //\n  // Input: [a, b, c, d, e, f, g, h] PSHUFHW[0,2,1,3]-> [a, b, c, d, e, g, f, h]\n  // Mask:  [3, 7, 1, 0, 2, 7, 3, 5] -----------------> [3, 7, 1, 0, 2, 7, 3, 6]\n  //\n  // Input: [a, b, c, d, e, g, f, h] -PSHUFD[0,2,1,3]-> [a, b, e, g, c, d, f, h]\n  // Mask:  [3, 7, 1, 0, 2, 7, 3, 6] -----------------> [5, 7, 1, 0, 4, 7, 5, 6]\n  //\n  // The result is fine to be handled by the generic logic.\n  auto balanceSides = [&](ArrayRef<int> AToAInputs, ArrayRef<int> BToAInputs,\n                          ArrayRef<int> BToBInputs, ArrayRef<int> AToBInputs,\n                          int AOffset, int BOffset) {\n    assert((AToAInputs.size() == 3 || AToAInputs.size() == 1) &&\n           \"Must call this with A having 3 or 1 inputs from the A half.\");\n    assert((BToAInputs.size() == 1 || BToAInputs.size() == 3) &&\n           \"Must call this with B having 1 or 3 inputs from the B half.\");\n    assert(AToAInputs.size() + BToAInputs.size() == 4 &&\n           \"Must call this with either 3:1 or 1:3 inputs (summing to 4).\");\n\n    bool ThreeAInputs = AToAInputs.size() == 3;\n\n    // Compute the index of dword with only one word among the three inputs in\n    // a half by taking the sum of the half with three inputs and subtracting\n    // the sum of the actual three inputs. The difference is the remaining\n    // slot.\n    int ADWord = 0, BDWord = 0;\n    int &TripleDWord = ThreeAInputs ? ADWord : BDWord;\n    int &OneInputDWord = ThreeAInputs ? BDWord : ADWord;\n    int TripleInputOffset = ThreeAInputs ? AOffset : BOffset;\n    ArrayRef<int> TripleInputs = ThreeAInputs ? AToAInputs : BToAInputs;\n    int OneInput = ThreeAInputs ? BToAInputs[0] : AToAInputs[0];\n    int TripleInputSum = 0 + 1 + 2 + 3 + (4 * TripleInputOffset);\n    int TripleNonInputIdx =\n        TripleInputSum - std::accumulate(TripleInputs.begin(), TripleInputs.end(), 0);\n    TripleDWord = TripleNonInputIdx / 2;\n\n    // We use xor with one to compute the adjacent DWord to whichever one the\n    // OneInput is in.\n    OneInputDWord = (OneInput / 2) ^ 1;\n\n    // Check for one tricky case: We're fixing a 3<-1 or a 1<-3 shuffle for AToA\n    // and BToA inputs. If there is also such a problem with the BToB and AToB\n    // inputs, we don't try to fix it necessarily -- we'll recurse and see it in\n    // the next pass. However, if we have a 2<-2 in the BToB and AToB inputs, it\n    // is essential that we don't *create* a 3<-1 as then we might oscillate.\n    if (BToBInputs.size() == 2 && AToBInputs.size() == 2) {\n      // Compute how many inputs will be flipped by swapping these DWords. We\n      // need\n      // to balance this to ensure we don't form a 3-1 shuffle in the other\n      // half.\n      int NumFlippedAToBInputs =\n          std::count(AToBInputs.begin(), AToBInputs.end(), 2 * ADWord) +\n          std::count(AToBInputs.begin(), AToBInputs.end(), 2 * ADWord + 1);\n      int NumFlippedBToBInputs =\n          std::count(BToBInputs.begin(), BToBInputs.end(), 2 * BDWord) +\n          std::count(BToBInputs.begin(), BToBInputs.end(), 2 * BDWord + 1);\n      if ((NumFlippedAToBInputs == 1 &&\n           (NumFlippedBToBInputs == 0 || NumFlippedBToBInputs == 2)) ||\n          (NumFlippedBToBInputs == 1 &&\n           (NumFlippedAToBInputs == 0 || NumFlippedAToBInputs == 2))) {\n        // We choose whether to fix the A half or B half based on whether that\n        // half has zero flipped inputs. At zero, we may not be able to fix it\n        // with that half. We also bias towards fixing the B half because that\n        // will more commonly be the high half, and we have to bias one way.\n        auto FixFlippedInputs = [&V, &DL, &Mask, &DAG](int PinnedIdx, int DWord,\n                                                       ArrayRef<int> Inputs) {\n          int FixIdx = PinnedIdx ^ 1; // The adjacent slot to the pinned slot.\n          bool IsFixIdxInput = is_contained(Inputs, PinnedIdx ^ 1);\n          // Determine whether the free index is in the flipped dword or the\n          // unflipped dword based on where the pinned index is. We use this bit\n          // in an xor to conditionally select the adjacent dword.\n          int FixFreeIdx = 2 * (DWord ^ (PinnedIdx / 2 == DWord));\n          bool IsFixFreeIdxInput = is_contained(Inputs, FixFreeIdx);\n          if (IsFixIdxInput == IsFixFreeIdxInput)\n            FixFreeIdx += 1;\n          IsFixFreeIdxInput = is_contained(Inputs, FixFreeIdx);\n          assert(IsFixIdxInput != IsFixFreeIdxInput &&\n                 \"We need to be changing the number of flipped inputs!\");\n          int PSHUFHalfMask[] = {0, 1, 2, 3};\n          std::swap(PSHUFHalfMask[FixFreeIdx % 4], PSHUFHalfMask[FixIdx % 4]);\n          V = DAG.getNode(\n              FixIdx < 4 ? X86ISD::PSHUFLW : X86ISD::PSHUFHW, DL,\n              MVT::getVectorVT(MVT::i16, V.getValueSizeInBits() / 16), V,\n              getV4X86ShuffleImm8ForMask(PSHUFHalfMask, DL, DAG));\n\n          for (int &M : Mask)\n            if (M >= 0 && M == FixIdx)\n              M = FixFreeIdx;\n            else if (M >= 0 && M == FixFreeIdx)\n              M = FixIdx;\n        };\n        if (NumFlippedBToBInputs != 0) {\n          int BPinnedIdx =\n              BToAInputs.size() == 3 ? TripleNonInputIdx : OneInput;\n          FixFlippedInputs(BPinnedIdx, BDWord, BToBInputs);\n        } else {\n          assert(NumFlippedAToBInputs != 0 && \"Impossible given predicates!\");\n          int APinnedIdx = ThreeAInputs ? TripleNonInputIdx : OneInput;\n          FixFlippedInputs(APinnedIdx, ADWord, AToBInputs);\n        }\n      }\n    }\n\n    int PSHUFDMask[] = {0, 1, 2, 3};\n    PSHUFDMask[ADWord] = BDWord;\n    PSHUFDMask[BDWord] = ADWord;\n    V = DAG.getBitcast(\n        VT,\n        DAG.getNode(X86ISD::PSHUFD, DL, PSHUFDVT, DAG.getBitcast(PSHUFDVT, V),\n                    getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG)));\n\n    // Adjust the mask to match the new locations of A and B.\n    for (int &M : Mask)\n      if (M >= 0 && M/2 == ADWord)\n        M = 2 * BDWord + M % 2;\n      else if (M >= 0 && M/2 == BDWord)\n        M = 2 * ADWord + M % 2;\n\n    // Recurse back into this routine to re-compute state now that this isn't\n    // a 3 and 1 problem.\n    return lowerV8I16GeneralSingleInputShuffle(DL, VT, V, Mask, Subtarget, DAG);\n  };\n  if ((NumLToL == 3 && NumHToL == 1) || (NumLToL == 1 && NumHToL == 3))\n    return balanceSides(LToLInputs, HToLInputs, HToHInputs, LToHInputs, 0, 4);\n  if ((NumHToH == 3 && NumLToH == 1) || (NumHToH == 1 && NumLToH == 3))\n    return balanceSides(HToHInputs, LToHInputs, LToLInputs, HToLInputs, 4, 0);\n\n  // At this point there are at most two inputs to the low and high halves from\n  // each half. That means the inputs can always be grouped into dwords and\n  // those dwords can then be moved to the correct half with a dword shuffle.\n  // We use at most one low and one high word shuffle to collect these paired\n  // inputs into dwords, and finally a dword shuffle to place them.\n  int PSHUFLMask[4] = {-1, -1, -1, -1};\n  int PSHUFHMask[4] = {-1, -1, -1, -1};\n  int PSHUFDMask[4] = {-1, -1, -1, -1};\n\n  // First fix the masks for all the inputs that are staying in their\n  // original halves. This will then dictate the targets of the cross-half\n  // shuffles.\n  auto fixInPlaceInputs =\n      [&PSHUFDMask](ArrayRef<int> InPlaceInputs, ArrayRef<int> IncomingInputs,\n                    MutableArrayRef<int> SourceHalfMask,\n                    MutableArrayRef<int> HalfMask, int HalfOffset) {\n    if (InPlaceInputs.empty())\n      return;\n    if (InPlaceInputs.size() == 1) {\n      SourceHalfMask[InPlaceInputs[0] - HalfOffset] =\n          InPlaceInputs[0] - HalfOffset;\n      PSHUFDMask[InPlaceInputs[0] / 2] = InPlaceInputs[0] / 2;\n      return;\n    }\n    if (IncomingInputs.empty()) {\n      // Just fix all of the in place inputs.\n      for (int Input : InPlaceInputs) {\n        SourceHalfMask[Input - HalfOffset] = Input - HalfOffset;\n        PSHUFDMask[Input / 2] = Input / 2;\n      }\n      return;\n    }\n\n    assert(InPlaceInputs.size() == 2 && \"Cannot handle 3 or 4 inputs!\");\n    SourceHalfMask[InPlaceInputs[0] - HalfOffset] =\n        InPlaceInputs[0] - HalfOffset;\n    // Put the second input next to the first so that they are packed into\n    // a dword. We find the adjacent index by toggling the low bit.\n    int AdjIndex = InPlaceInputs[0] ^ 1;\n    SourceHalfMask[AdjIndex - HalfOffset] = InPlaceInputs[1] - HalfOffset;\n    std::replace(HalfMask.begin(), HalfMask.end(), InPlaceInputs[1], AdjIndex);\n    PSHUFDMask[AdjIndex / 2] = AdjIndex / 2;\n  };\n  fixInPlaceInputs(LToLInputs, HToLInputs, PSHUFLMask, LoMask, 0);\n  fixInPlaceInputs(HToHInputs, LToHInputs, PSHUFHMask, HiMask, 4);\n\n  // Now gather the cross-half inputs and place them into a free dword of\n  // their target half.\n  // FIXME: This operation could almost certainly be simplified dramatically to\n  // look more like the 3-1 fixing operation.\n  auto moveInputsToRightHalf = [&PSHUFDMask](\n      MutableArrayRef<int> IncomingInputs, ArrayRef<int> ExistingInputs,\n      MutableArrayRef<int> SourceHalfMask, MutableArrayRef<int> HalfMask,\n      MutableArrayRef<int> FinalSourceHalfMask, int SourceOffset,\n      int DestOffset) {\n    auto isWordClobbered = [](ArrayRef<int> SourceHalfMask, int Word) {\n      return SourceHalfMask[Word] >= 0 && SourceHalfMask[Word] != Word;\n    };\n    auto isDWordClobbered = [&isWordClobbered](ArrayRef<int> SourceHalfMask,\n                                               int Word) {\n      int LowWord = Word & ~1;\n      int HighWord = Word | 1;\n      return isWordClobbered(SourceHalfMask, LowWord) ||\n             isWordClobbered(SourceHalfMask, HighWord);\n    };\n\n    if (IncomingInputs.empty())\n      return;\n\n    if (ExistingInputs.empty()) {\n      // Map any dwords with inputs from them into the right half.\n      for (int Input : IncomingInputs) {\n        // If the source half mask maps over the inputs, turn those into\n        // swaps and use the swapped lane.\n        if (isWordClobbered(SourceHalfMask, Input - SourceOffset)) {\n          if (SourceHalfMask[SourceHalfMask[Input - SourceOffset]] < 0) {\n            SourceHalfMask[SourceHalfMask[Input - SourceOffset]] =\n                Input - SourceOffset;\n            // We have to swap the uses in our half mask in one sweep.\n            for (int &M : HalfMask)\n              if (M == SourceHalfMask[Input - SourceOffset] + SourceOffset)\n                M = Input;\n              else if (M == Input)\n                M = SourceHalfMask[Input - SourceOffset] + SourceOffset;\n          } else {\n            assert(SourceHalfMask[SourceHalfMask[Input - SourceOffset]] ==\n                       Input - SourceOffset &&\n                   \"Previous placement doesn't match!\");\n          }\n          // Note that this correctly re-maps both when we do a swap and when\n          // we observe the other side of the swap above. We rely on that to\n          // avoid swapping the members of the input list directly.\n          Input = SourceHalfMask[Input - SourceOffset] + SourceOffset;\n        }\n\n        // Map the input's dword into the correct half.\n        if (PSHUFDMask[(Input - SourceOffset + DestOffset) / 2] < 0)\n          PSHUFDMask[(Input - SourceOffset + DestOffset) / 2] = Input / 2;\n        else\n          assert(PSHUFDMask[(Input - SourceOffset + DestOffset) / 2] ==\n                     Input / 2 &&\n                 \"Previous placement doesn't match!\");\n      }\n\n      // And just directly shift any other-half mask elements to be same-half\n      // as we will have mirrored the dword containing the element into the\n      // same position within that half.\n      for (int &M : HalfMask)\n        if (M >= SourceOffset && M < SourceOffset + 4) {\n          M = M - SourceOffset + DestOffset;\n          assert(M >= 0 && \"This should never wrap below zero!\");\n        }\n      return;\n    }\n\n    // Ensure we have the input in a viable dword of its current half. This\n    // is particularly tricky because the original position may be clobbered\n    // by inputs being moved and *staying* in that half.\n    if (IncomingInputs.size() == 1) {\n      if (isWordClobbered(SourceHalfMask, IncomingInputs[0] - SourceOffset)) {\n        int InputFixed = find(SourceHalfMask, -1) - std::begin(SourceHalfMask) +\n                         SourceOffset;\n        SourceHalfMask[InputFixed - SourceOffset] =\n            IncomingInputs[0] - SourceOffset;\n        std::replace(HalfMask.begin(), HalfMask.end(), IncomingInputs[0],\n                     InputFixed);\n        IncomingInputs[0] = InputFixed;\n      }\n    } else if (IncomingInputs.size() == 2) {\n      if (IncomingInputs[0] / 2 != IncomingInputs[1] / 2 ||\n          isDWordClobbered(SourceHalfMask, IncomingInputs[0] - SourceOffset)) {\n        // We have two non-adjacent or clobbered inputs we need to extract from\n        // the source half. To do this, we need to map them into some adjacent\n        // dword slot in the source mask.\n        int InputsFixed[2] = {IncomingInputs[0] - SourceOffset,\n                              IncomingInputs[1] - SourceOffset};\n\n        // If there is a free slot in the source half mask adjacent to one of\n        // the inputs, place the other input in it. We use (Index XOR 1) to\n        // compute an adjacent index.\n        if (!isWordClobbered(SourceHalfMask, InputsFixed[0]) &&\n            SourceHalfMask[InputsFixed[0] ^ 1] < 0) {\n          SourceHalfMask[InputsFixed[0]] = InputsFixed[0];\n          SourceHalfMask[InputsFixed[0] ^ 1] = InputsFixed[1];\n          InputsFixed[1] = InputsFixed[0] ^ 1;\n        } else if (!isWordClobbered(SourceHalfMask, InputsFixed[1]) &&\n                   SourceHalfMask[InputsFixed[1] ^ 1] < 0) {\n          SourceHalfMask[InputsFixed[1]] = InputsFixed[1];\n          SourceHalfMask[InputsFixed[1] ^ 1] = InputsFixed[0];\n          InputsFixed[0] = InputsFixed[1] ^ 1;\n        } else if (SourceHalfMask[2 * ((InputsFixed[0] / 2) ^ 1)] < 0 &&\n                   SourceHalfMask[2 * ((InputsFixed[0] / 2) ^ 1) + 1] < 0) {\n          // The two inputs are in the same DWord but it is clobbered and the\n          // adjacent DWord isn't used at all. Move both inputs to the free\n          // slot.\n          SourceHalfMask[2 * ((InputsFixed[0] / 2) ^ 1)] = InputsFixed[0];\n          SourceHalfMask[2 * ((InputsFixed[0] / 2) ^ 1) + 1] = InputsFixed[1];\n          InputsFixed[0] = 2 * ((InputsFixed[0] / 2) ^ 1);\n          InputsFixed[1] = 2 * ((InputsFixed[0] / 2) ^ 1) + 1;\n        } else {\n          // The only way we hit this point is if there is no clobbering\n          // (because there are no off-half inputs to this half) and there is no\n          // free slot adjacent to one of the inputs. In this case, we have to\n          // swap an input with a non-input.\n          for (int i = 0; i < 4; ++i)\n            assert((SourceHalfMask[i] < 0 || SourceHalfMask[i] == i) &&\n                   \"We can't handle any clobbers here!\");\n          assert(InputsFixed[1] != (InputsFixed[0] ^ 1) &&\n                 \"Cannot have adjacent inputs here!\");\n\n          SourceHalfMask[InputsFixed[0] ^ 1] = InputsFixed[1];\n          SourceHalfMask[InputsFixed[1]] = InputsFixed[0] ^ 1;\n\n          // We also have to update the final source mask in this case because\n          // it may need to undo the above swap.\n          for (int &M : FinalSourceHalfMask)\n            if (M == (InputsFixed[0] ^ 1) + SourceOffset)\n              M = InputsFixed[1] + SourceOffset;\n            else if (M == InputsFixed[1] + SourceOffset)\n              M = (InputsFixed[0] ^ 1) + SourceOffset;\n\n          InputsFixed[1] = InputsFixed[0] ^ 1;\n        }\n\n        // Point everything at the fixed inputs.\n        for (int &M : HalfMask)\n          if (M == IncomingInputs[0])\n            M = InputsFixed[0] + SourceOffset;\n          else if (M == IncomingInputs[1])\n            M = InputsFixed[1] + SourceOffset;\n\n        IncomingInputs[0] = InputsFixed[0] + SourceOffset;\n        IncomingInputs[1] = InputsFixed[1] + SourceOffset;\n      }\n    } else {\n      llvm_unreachable(\"Unhandled input size!\");\n    }\n\n    // Now hoist the DWord down to the right half.\n    int FreeDWord = (PSHUFDMask[DestOffset / 2] < 0 ? 0 : 1) + DestOffset / 2;\n    assert(PSHUFDMask[FreeDWord] < 0 && \"DWord not free\");\n    PSHUFDMask[FreeDWord] = IncomingInputs[0] / 2;\n    for (int &M : HalfMask)\n      for (int Input : IncomingInputs)\n        if (M == Input)\n          M = FreeDWord * 2 + Input % 2;\n  };\n  moveInputsToRightHalf(HToLInputs, LToLInputs, PSHUFHMask, LoMask, HiMask,\n                        /*SourceOffset*/ 4, /*DestOffset*/ 0);\n  moveInputsToRightHalf(LToHInputs, HToHInputs, PSHUFLMask, HiMask, LoMask,\n                        /*SourceOffset*/ 0, /*DestOffset*/ 4);\n\n  // Now enact all the shuffles we've computed to move the inputs into their\n  // target half.\n  if (!isNoopShuffleMask(PSHUFLMask))\n    V = DAG.getNode(X86ISD::PSHUFLW, DL, VT, V,\n                    getV4X86ShuffleImm8ForMask(PSHUFLMask, DL, DAG));\n  if (!isNoopShuffleMask(PSHUFHMask))\n    V = DAG.getNode(X86ISD::PSHUFHW, DL, VT, V,\n                    getV4X86ShuffleImm8ForMask(PSHUFHMask, DL, DAG));\n  if (!isNoopShuffleMask(PSHUFDMask))\n    V = DAG.getBitcast(\n        VT,\n        DAG.getNode(X86ISD::PSHUFD, DL, PSHUFDVT, DAG.getBitcast(PSHUFDVT, V),\n                    getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG)));\n\n  // At this point, each half should contain all its inputs, and we can then\n  // just shuffle them into their final position.\n  assert(count_if(LoMask, [](int M) { return M >= 4; }) == 0 &&\n         \"Failed to lift all the high half inputs to the low mask!\");\n  assert(count_if(HiMask, [](int M) { return M >= 0 && M < 4; }) == 0 &&\n         \"Failed to lift all the low half inputs to the high mask!\");\n\n  // Do a half shuffle for the low mask.\n  if (!isNoopShuffleMask(LoMask))\n    V = DAG.getNode(X86ISD::PSHUFLW, DL, VT, V,\n                    getV4X86ShuffleImm8ForMask(LoMask, DL, DAG));\n\n  // Do a half shuffle with the high mask after shifting its values down.\n  for (int &M : HiMask)\n    if (M >= 0)\n      M -= 4;\n  if (!isNoopShuffleMask(HiMask))\n    V = DAG.getNode(X86ISD::PSHUFHW, DL, VT, V,\n                    getV4X86ShuffleImm8ForMask(HiMask, DL, DAG));\n\n  return V;\n}\n\n/// Helper to form a PSHUFB-based shuffle+blend, opportunistically avoiding the\n/// blend if only one input is used.\nstatic SDValue lowerShuffleAsBlendOfPSHUFBs(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const APInt &Zeroable, SelectionDAG &DAG, bool &V1InUse, bool &V2InUse) {\n  assert(!is128BitLaneCrossingShuffleMask(VT, Mask) &&\n         \"Lane crossing shuffle masks not supported\");\n\n  int NumBytes = VT.getSizeInBits() / 8;\n  int Size = Mask.size();\n  int Scale = NumBytes / Size;\n\n  SmallVector<SDValue, 64> V1Mask(NumBytes, DAG.getUNDEF(MVT::i8));\n  SmallVector<SDValue, 64> V2Mask(NumBytes, DAG.getUNDEF(MVT::i8));\n  V1InUse = false;\n  V2InUse = false;\n\n  for (int i = 0; i < NumBytes; ++i) {\n    int M = Mask[i / Scale];\n    if (M < 0)\n      continue;\n\n    const int ZeroMask = 0x80;\n    int V1Idx = M < Size ? M * Scale + i % Scale : ZeroMask;\n    int V2Idx = M < Size ? ZeroMask : (M - Size) * Scale + i % Scale;\n    if (Zeroable[i / Scale])\n      V1Idx = V2Idx = ZeroMask;\n\n    V1Mask[i] = DAG.getConstant(V1Idx, DL, MVT::i8);\n    V2Mask[i] = DAG.getConstant(V2Idx, DL, MVT::i8);\n    V1InUse |= (ZeroMask != V1Idx);\n    V2InUse |= (ZeroMask != V2Idx);\n  }\n\n  MVT ShufVT = MVT::getVectorVT(MVT::i8, NumBytes);\n  if (V1InUse)\n    V1 = DAG.getNode(X86ISD::PSHUFB, DL, ShufVT, DAG.getBitcast(ShufVT, V1),\n                     DAG.getBuildVector(ShufVT, DL, V1Mask));\n  if (V2InUse)\n    V2 = DAG.getNode(X86ISD::PSHUFB, DL, ShufVT, DAG.getBitcast(ShufVT, V2),\n                     DAG.getBuildVector(ShufVT, DL, V2Mask));\n\n  // If we need shuffled inputs from both, blend the two.\n  SDValue V;\n  if (V1InUse && V2InUse)\n    V = DAG.getNode(ISD::OR, DL, ShufVT, V1, V2);\n  else\n    V = V1InUse ? V1 : V2;\n\n  // Cast the result back to the correct type.\n  return DAG.getBitcast(VT, V);\n}\n\n/// Generic lowering of 8-lane i16 shuffles.\n///\n/// This handles both single-input shuffles and combined shuffle/blends with\n/// two inputs. The single input shuffles are immediately delegated to\n/// a dedicated lowering routine.\n///\n/// The blends are lowered in one of three fundamental ways. If there are few\n/// enough inputs, it delegates to a basic UNPCK-based strategy. If the shuffle\n/// of the input is significantly cheaper when lowered as an interleaving of\n/// the two inputs, try to interleave them. Otherwise, blend the low and high\n/// halves of the inputs separately (making them have relatively few inputs)\n/// and then concatenate them.\nstatic SDValue lowerV8I16Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v8i16 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v8i16 && \"Bad operand type!\");\n  assert(Mask.size() == 8 && \"Unexpected mask size for v8 shuffle!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(DL, MVT::v8i16, V1, V2, Mask,\n                                                   Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Try to use lower using a truncation.\n  if (SDValue V = lowerShuffleWithVPMOV(DL, MVT::v8i16, V1, V2, Mask, Zeroable,\n                                        Subtarget, DAG))\n    return V;\n\n  int NumV2Inputs = count_if(Mask, [](int M) { return M >= 8; });\n\n  if (NumV2Inputs == 0) {\n    // Try to use shift instructions.\n    if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v8i16, V1, V1, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Shift;\n\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v8i16, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Try to use bit rotation instructions.\n    if (SDValue Rotate = lowerShuffleAsBitRotate(DL, MVT::v8i16, V1, Mask,\n                                                 Subtarget, DAG))\n      return Rotate;\n\n    // Use dedicated unpack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v8i16, Mask, V1, V2, DAG))\n      return V;\n\n    // Use dedicated pack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithPACK(DL, MVT::v8i16, Mask, V1, V2, DAG,\n                                         Subtarget))\n      return V;\n\n    // Try to use byte rotation instructions.\n    if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v8i16, V1, V1, Mask,\n                                                  Subtarget, DAG))\n      return Rotate;\n\n    // Make a copy of the mask so it can be modified.\n    SmallVector<int, 8> MutableMask(Mask.begin(), Mask.end());\n    return lowerV8I16GeneralSingleInputShuffle(DL, MVT::v8i16, V1, MutableMask,\n                                               Subtarget, DAG);\n  }\n\n  assert(llvm::any_of(Mask, [](int M) { return M >= 0 && M < 8; }) &&\n         \"All single-input shuffles should be canonicalized to be V1-input \"\n         \"shuffles.\");\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v8i16, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // See if we can use SSE4A Extraction / Insertion.\n  if (Subtarget.hasSSE4A())\n    if (SDValue V = lowerShuffleWithSSE4A(DL, MVT::v8i16, V1, V2, Mask,\n                                          Zeroable, DAG))\n      return V;\n\n  // There are special ways we can lower some single-element blends.\n  if (NumV2Inputs == 1)\n    if (SDValue V = lowerShuffleAsElementInsertion(\n            DL, MVT::v8i16, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return V;\n\n  // We have different paths for blend lowering, but they all must use the\n  // *exact* same predicate.\n  bool IsBlendSupported = Subtarget.hasSSE41();\n  if (IsBlendSupported)\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v8i16, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n  if (SDValue Masked = lowerShuffleAsBitMask(DL, MVT::v8i16, V1, V2, Mask,\n                                             Zeroable, Subtarget, DAG))\n    return Masked;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v8i16, Mask, V1, V2, DAG))\n    return V;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithPACK(DL, MVT::v8i16, Mask, V1, V2, DAG,\n                                       Subtarget))\n    return V;\n\n  // Try to use lower using a truncation.\n  if (SDValue V = lowerShuffleAsVTRUNC(DL, MVT::v8i16, V1, V2, Mask, Zeroable,\n                                       Subtarget, DAG))\n    return V;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v8i16, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  if (SDValue BitBlend =\n          lowerShuffleAsBitBlend(DL, MVT::v8i16, V1, V2, Mask, DAG))\n    return BitBlend;\n\n  // Try to use byte shift instructions to mask.\n  if (SDValue V = lowerShuffleAsByteShiftMask(DL, MVT::v8i16, V1, V2, Mask,\n                                              Zeroable, Subtarget, DAG))\n    return V;\n\n  // Attempt to lower using compaction, SSE41 is necessary for PACKUSDW.\n  // We could use SIGN_EXTEND_INREG+PACKSSDW for older targets but this seems to\n  // be slower than a PSHUFLW+PSHUFHW+PSHUFD chain.\n  int NumEvenDrops = canLowerByDroppingEvenElements(Mask, false);\n  if ((NumEvenDrops == 1 || NumEvenDrops == 2) && Subtarget.hasSSE41() &&\n      !Subtarget.hasVLX()) {\n    SmallVector<SDValue, 8> DWordClearOps(4, DAG.getConstant(0, DL, MVT::i32));\n    for (unsigned i = 0; i != 4; i += 1 << (NumEvenDrops - 1))\n      DWordClearOps[i] = DAG.getConstant(0xFFFF, DL, MVT::i32);\n    SDValue DWordClearMask = DAG.getBuildVector(MVT::v4i32, DL, DWordClearOps);\n    V1 = DAG.getNode(ISD::AND, DL, MVT::v4i32, DAG.getBitcast(MVT::v4i32, V1),\n                     DWordClearMask);\n    V2 = DAG.getNode(ISD::AND, DL, MVT::v4i32, DAG.getBitcast(MVT::v4i32, V2),\n                     DWordClearMask);\n    // Now pack things back together.\n    SDValue Result = DAG.getNode(X86ISD::PACKUS, DL, MVT::v8i16, V1, V2);\n    if (NumEvenDrops == 2) {\n      Result = DAG.getBitcast(MVT::v4i32, Result);\n      Result = DAG.getNode(X86ISD::PACKUS, DL, MVT::v8i16, Result, Result);\n    }\n    return Result;\n  }\n\n  // Try to lower by permuting the inputs into an unpack instruction.\n  if (SDValue Unpack = lowerShuffleAsPermuteAndUnpack(DL, MVT::v8i16, V1, V2,\n                                                      Mask, Subtarget, DAG))\n    return Unpack;\n\n  // If we can't directly blend but can use PSHUFB, that will be better as it\n  // can both shuffle and set up the inefficient blend.\n  if (!IsBlendSupported && Subtarget.hasSSSE3()) {\n    bool V1InUse, V2InUse;\n    return lowerShuffleAsBlendOfPSHUFBs(DL, MVT::v8i16, V1, V2, Mask,\n                                        Zeroable, DAG, V1InUse, V2InUse);\n  }\n\n  // We can always bit-blend if we have to so the fallback strategy is to\n  // decompose into single-input permutes and blends/unpacks.\n  return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v8i16, V1, V2,\n                                              Mask, Subtarget, DAG);\n}\n\n// Lowers unary/binary shuffle as VPERMV/VPERMV3, for non-VLX targets,\n// sub-512-bit shuffles are padded to 512-bits for the shuffle and then\n// the active subvector is extracted.\nstatic SDValue lowerShuffleWithPERMV(const SDLoc &DL, MVT VT,\n                                     ArrayRef<int> Mask, SDValue V1, SDValue V2,\n                                     const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  MVT MaskVT = VT.changeTypeToInteger();\n  SDValue MaskNode;\n  MVT ShuffleVT = VT;\n  if (!VT.is512BitVector() && !Subtarget.hasVLX()) {\n    V1 = widenSubVector(V1, false, Subtarget, DAG, DL, 512);\n    V2 = widenSubVector(V2, false, Subtarget, DAG, DL, 512);\n    ShuffleVT = V1.getSimpleValueType();\n\n    // Adjust mask to correct indices for the second input.\n    int NumElts = VT.getVectorNumElements();\n    unsigned Scale = 512 / VT.getSizeInBits();\n    SmallVector<int, 32> AdjustedMask(Mask.begin(), Mask.end());\n    for (int &M : AdjustedMask)\n      if (NumElts <= M)\n        M += (Scale - 1) * NumElts;\n    MaskNode = getConstVector(AdjustedMask, MaskVT, DAG, DL, true);\n    MaskNode = widenSubVector(MaskNode, false, Subtarget, DAG, DL, 512);\n  } else {\n    MaskNode = getConstVector(Mask, MaskVT, DAG, DL, true);\n  }\n\n  SDValue Result;\n  if (V2.isUndef())\n    Result = DAG.getNode(X86ISD::VPERMV, DL, ShuffleVT, MaskNode, V1);\n  else\n    Result = DAG.getNode(X86ISD::VPERMV3, DL, ShuffleVT, V1, MaskNode, V2);\n\n  if (VT != ShuffleVT)\n    Result = extractSubVector(Result, 0, DAG, DL, VT.getSizeInBits());\n\n  return Result;\n}\n\n/// Generic lowering of v16i8 shuffles.\n///\n/// This is a hybrid strategy to lower v16i8 vectors. It first attempts to\n/// detect any complexity reducing interleaving. If that doesn't help, it uses\n/// UNPCK to spread the i8 elements across two i16-element vectors, and uses\n/// the existing lowering for v8i16 blends on each half, finally PACK-ing them\n/// back together.\nstatic SDValue lowerV16I8Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v16i8 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v16i8 && \"Bad operand type!\");\n  assert(Mask.size() == 16 && \"Unexpected mask size for v16 shuffle!\");\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v16i8, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v16i8, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithPACK(DL, MVT::v16i8, Mask, V1, V2, DAG,\n                                       Subtarget))\n    return V;\n\n  // Try to use a zext lowering.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(DL, MVT::v16i8, V1, V2, Mask,\n                                                   Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Try to use lower using a truncation.\n  if (SDValue V = lowerShuffleWithVPMOV(DL, MVT::v16i8, V1, V2, Mask, Zeroable,\n                                        Subtarget, DAG))\n    return V;\n\n  if (SDValue V = lowerShuffleAsVTRUNC(DL, MVT::v16i8, V1, V2, Mask, Zeroable,\n                                       Subtarget, DAG))\n    return V;\n\n  // See if we can use SSE4A Extraction / Insertion.\n  if (Subtarget.hasSSE4A())\n    if (SDValue V = lowerShuffleWithSSE4A(DL, MVT::v16i8, V1, V2, Mask,\n                                          Zeroable, DAG))\n      return V;\n\n  int NumV2Elements = count_if(Mask, [](int M) { return M >= 16; });\n\n  // For single-input shuffles, there are some nicer lowering tricks we can use.\n  if (NumV2Elements == 0) {\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v16i8, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Try to use bit rotation instructions.\n    if (SDValue Rotate = lowerShuffleAsBitRotate(DL, MVT::v16i8, V1, Mask,\n                                                 Subtarget, DAG))\n      return Rotate;\n\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v16i8, Mask, V1, V2, DAG))\n      return V;\n\n    // Check whether we can widen this to an i16 shuffle by duplicating bytes.\n    // Notably, this handles splat and partial-splat shuffles more efficiently.\n    // However, it only makes sense if the pre-duplication shuffle simplifies\n    // things significantly. Currently, this means we need to be able to\n    // express the pre-duplication shuffle as an i16 shuffle.\n    //\n    // FIXME: We should check for other patterns which can be widened into an\n    // i16 shuffle as well.\n    auto canWidenViaDuplication = [](ArrayRef<int> Mask) {\n      for (int i = 0; i < 16; i += 2)\n        if (Mask[i] >= 0 && Mask[i + 1] >= 0 && Mask[i] != Mask[i + 1])\n          return false;\n\n      return true;\n    };\n    auto tryToWidenViaDuplication = [&]() -> SDValue {\n      if (!canWidenViaDuplication(Mask))\n        return SDValue();\n      SmallVector<int, 4> LoInputs;\n      copy_if(Mask, std::back_inserter(LoInputs),\n              [](int M) { return M >= 0 && M < 8; });\n      array_pod_sort(LoInputs.begin(), LoInputs.end());\n      LoInputs.erase(std::unique(LoInputs.begin(), LoInputs.end()),\n                     LoInputs.end());\n      SmallVector<int, 4> HiInputs;\n      copy_if(Mask, std::back_inserter(HiInputs), [](int M) { return M >= 8; });\n      array_pod_sort(HiInputs.begin(), HiInputs.end());\n      HiInputs.erase(std::unique(HiInputs.begin(), HiInputs.end()),\n                     HiInputs.end());\n\n      bool TargetLo = LoInputs.size() >= HiInputs.size();\n      ArrayRef<int> InPlaceInputs = TargetLo ? LoInputs : HiInputs;\n      ArrayRef<int> MovingInputs = TargetLo ? HiInputs : LoInputs;\n\n      int PreDupI16Shuffle[] = {-1, -1, -1, -1, -1, -1, -1, -1};\n      SmallDenseMap<int, int, 8> LaneMap;\n      for (int I : InPlaceInputs) {\n        PreDupI16Shuffle[I/2] = I/2;\n        LaneMap[I] = I;\n      }\n      int j = TargetLo ? 0 : 4, je = j + 4;\n      for (int i = 0, ie = MovingInputs.size(); i < ie; ++i) {\n        // Check if j is already a shuffle of this input. This happens when\n        // there are two adjacent bytes after we move the low one.\n        if (PreDupI16Shuffle[j] != MovingInputs[i] / 2) {\n          // If we haven't yet mapped the input, search for a slot into which\n          // we can map it.\n          while (j < je && PreDupI16Shuffle[j] >= 0)\n            ++j;\n\n          if (j == je)\n            // We can't place the inputs into a single half with a simple i16 shuffle, so bail.\n            return SDValue();\n\n          // Map this input with the i16 shuffle.\n          PreDupI16Shuffle[j] = MovingInputs[i] / 2;\n        }\n\n        // Update the lane map based on the mapping we ended up with.\n        LaneMap[MovingInputs[i]] = 2 * j + MovingInputs[i] % 2;\n      }\n      V1 = DAG.getBitcast(\n          MVT::v16i8,\n          DAG.getVectorShuffle(MVT::v8i16, DL, DAG.getBitcast(MVT::v8i16, V1),\n                               DAG.getUNDEF(MVT::v8i16), PreDupI16Shuffle));\n\n      // Unpack the bytes to form the i16s that will be shuffled into place.\n      bool EvenInUse = false, OddInUse = false;\n      for (int i = 0; i < 16; i += 2) {\n        EvenInUse |= (Mask[i + 0] >= 0);\n        OddInUse |= (Mask[i + 1] >= 0);\n        if (EvenInUse && OddInUse)\n          break;\n      }\n      V1 = DAG.getNode(TargetLo ? X86ISD::UNPCKL : X86ISD::UNPCKH, DL,\n                       MVT::v16i8, EvenInUse ? V1 : DAG.getUNDEF(MVT::v16i8),\n                       OddInUse ? V1 : DAG.getUNDEF(MVT::v16i8));\n\n      int PostDupI16Shuffle[8] = {-1, -1, -1, -1, -1, -1, -1, -1};\n      for (int i = 0; i < 16; ++i)\n        if (Mask[i] >= 0) {\n          int MappedMask = LaneMap[Mask[i]] - (TargetLo ? 0 : 8);\n          assert(MappedMask < 8 && \"Invalid v8 shuffle mask!\");\n          if (PostDupI16Shuffle[i / 2] < 0)\n            PostDupI16Shuffle[i / 2] = MappedMask;\n          else\n            assert(PostDupI16Shuffle[i / 2] == MappedMask &&\n                   \"Conflicting entries in the original shuffle!\");\n        }\n      return DAG.getBitcast(\n          MVT::v16i8,\n          DAG.getVectorShuffle(MVT::v8i16, DL, DAG.getBitcast(MVT::v8i16, V1),\n                               DAG.getUNDEF(MVT::v8i16), PostDupI16Shuffle));\n    };\n    if (SDValue V = tryToWidenViaDuplication())\n      return V;\n  }\n\n  if (SDValue Masked = lowerShuffleAsBitMask(DL, MVT::v16i8, V1, V2, Mask,\n                                             Zeroable, Subtarget, DAG))\n    return Masked;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v16i8, Mask, V1, V2, DAG))\n    return V;\n\n  // Try to use byte shift instructions to mask.\n  if (SDValue V = lowerShuffleAsByteShiftMask(DL, MVT::v16i8, V1, V2, Mask,\n                                              Zeroable, Subtarget, DAG))\n    return V;\n\n  // Check for compaction patterns.\n  bool IsSingleInput = V2.isUndef();\n  int NumEvenDrops = canLowerByDroppingEvenElements(Mask, IsSingleInput);\n\n  // Check for SSSE3 which lets us lower all v16i8 shuffles much more directly\n  // with PSHUFB. It is important to do this before we attempt to generate any\n  // blends but after all of the single-input lowerings. If the single input\n  // lowerings can find an instruction sequence that is faster than a PSHUFB, we\n  // want to preserve that and we can DAG combine any longer sequences into\n  // a PSHUFB in the end. But once we start blending from multiple inputs,\n  // the complexity of DAG combining bad patterns back into PSHUFB is too high,\n  // and there are *very* few patterns that would actually be faster than the\n  // PSHUFB approach because of its ability to zero lanes.\n  //\n  // If the mask is a binary compaction, we can more efficiently perform this\n  // as a PACKUS(AND(),AND()) - which is quicker than UNPACK(PSHUFB(),PSHUFB()).\n  //\n  // FIXME: The only exceptions to the above are blends which are exact\n  // interleavings with direct instructions supporting them. We currently don't\n  // handle those well here.\n  if (Subtarget.hasSSSE3() && (IsSingleInput || NumEvenDrops != 1)) {\n    bool V1InUse = false;\n    bool V2InUse = false;\n\n    SDValue PSHUFB = lowerShuffleAsBlendOfPSHUFBs(\n        DL, MVT::v16i8, V1, V2, Mask, Zeroable, DAG, V1InUse, V2InUse);\n\n    // If both V1 and V2 are in use and we can use a direct blend or an unpack,\n    // do so. This avoids using them to handle blends-with-zero which is\n    // important as a single pshufb is significantly faster for that.\n    if (V1InUse && V2InUse) {\n      if (Subtarget.hasSSE41())\n        if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v16i8, V1, V2, Mask,\n                                                Zeroable, Subtarget, DAG))\n          return Blend;\n\n      // We can use an unpack to do the blending rather than an or in some\n      // cases. Even though the or may be (very minorly) more efficient, we\n      // preference this lowering because there are common cases where part of\n      // the complexity of the shuffles goes away when we do the final blend as\n      // an unpack.\n      // FIXME: It might be worth trying to detect if the unpack-feeding\n      // shuffles will both be pshufb, in which case we shouldn't bother with\n      // this.\n      if (SDValue Unpack = lowerShuffleAsPermuteAndUnpack(\n              DL, MVT::v16i8, V1, V2, Mask, Subtarget, DAG))\n        return Unpack;\n\n      // AVX512VBMI can lower to VPERMB (non-VLX will pad to v64i8).\n      if (Subtarget.hasVBMI())\n        return lowerShuffleWithPERMV(DL, MVT::v16i8, Mask, V1, V2, Subtarget,\n                                     DAG);\n\n      // If we have XOP we can use one VPPERM instead of multiple PSHUFBs.\n      if (Subtarget.hasXOP()) {\n        SDValue MaskNode = getConstVector(Mask, MVT::v16i8, DAG, DL, true);\n        return DAG.getNode(X86ISD::VPPERM, DL, MVT::v16i8, V1, V2, MaskNode);\n      }\n\n      // Use PALIGNR+Permute if possible - permute might become PSHUFB but the\n      // PALIGNR will be cheaper than the second PSHUFB+OR.\n      if (SDValue V = lowerShuffleAsByteRotateAndPermute(\n              DL, MVT::v16i8, V1, V2, Mask, Subtarget, DAG))\n        return V;\n    }\n\n    return PSHUFB;\n  }\n\n  // There are special ways we can lower some single-element blends.\n  if (NumV2Elements == 1)\n    if (SDValue V = lowerShuffleAsElementInsertion(\n            DL, MVT::v16i8, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return V;\n\n  if (SDValue Blend = lowerShuffleAsBitBlend(DL, MVT::v16i8, V1, V2, Mask, DAG))\n    return Blend;\n\n  // Check whether a compaction lowering can be done. This handles shuffles\n  // which take every Nth element for some even N. See the helper function for\n  // details.\n  //\n  // We special case these as they can be particularly efficiently handled with\n  // the PACKUSB instruction on x86 and they show up in common patterns of\n  // rearranging bytes to truncate wide elements.\n  if (NumEvenDrops) {\n    // NumEvenDrops is the power of two stride of the elements. Another way of\n    // thinking about it is that we need to drop the even elements this many\n    // times to get the original input.\n\n    // First we need to zero all the dropped bytes.\n    assert(NumEvenDrops <= 3 &&\n           \"No support for dropping even elements more than 3 times.\");\n    SmallVector<SDValue, 8> WordClearOps(8, DAG.getConstant(0, DL, MVT::i16));\n    for (unsigned i = 0; i != 8; i += 1 << (NumEvenDrops - 1))\n      WordClearOps[i] = DAG.getConstant(0xFF, DL, MVT::i16);\n    SDValue WordClearMask = DAG.getBuildVector(MVT::v8i16, DL, WordClearOps);\n    V1 = DAG.getNode(ISD::AND, DL, MVT::v8i16, DAG.getBitcast(MVT::v8i16, V1),\n                     WordClearMask);\n    if (!IsSingleInput)\n      V2 = DAG.getNode(ISD::AND, DL, MVT::v8i16, DAG.getBitcast(MVT::v8i16, V2),\n                       WordClearMask);\n\n    // Now pack things back together.\n    SDValue Result = DAG.getNode(X86ISD::PACKUS, DL, MVT::v16i8, V1,\n                                 IsSingleInput ? V1 : V2);\n    for (int i = 1; i < NumEvenDrops; ++i) {\n      Result = DAG.getBitcast(MVT::v8i16, Result);\n      Result = DAG.getNode(X86ISD::PACKUS, DL, MVT::v16i8, Result, Result);\n    }\n    return Result;\n  }\n\n  // Handle multi-input cases by blending/unpacking single-input shuffles.\n  if (NumV2Elements > 0)\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v16i8, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // The fallback path for single-input shuffles widens this into two v8i16\n  // vectors with unpacks, shuffles those, and then pulls them back together\n  // with a pack.\n  SDValue V = V1;\n\n  std::array<int, 8> LoBlendMask = {{-1, -1, -1, -1, -1, -1, -1, -1}};\n  std::array<int, 8> HiBlendMask = {{-1, -1, -1, -1, -1, -1, -1, -1}};\n  for (int i = 0; i < 16; ++i)\n    if (Mask[i] >= 0)\n      (i < 8 ? LoBlendMask[i] : HiBlendMask[i % 8]) = Mask[i];\n\n  SDValue VLoHalf, VHiHalf;\n  // Check if any of the odd lanes in the v16i8 are used. If not, we can mask\n  // them out and avoid using UNPCK{L,H} to extract the elements of V as\n  // i16s.\n  if (none_of(LoBlendMask, [](int M) { return M >= 0 && M % 2 == 1; }) &&\n      none_of(HiBlendMask, [](int M) { return M >= 0 && M % 2 == 1; })) {\n    // Use a mask to drop the high bytes.\n    VLoHalf = DAG.getBitcast(MVT::v8i16, V);\n    VLoHalf = DAG.getNode(ISD::AND, DL, MVT::v8i16, VLoHalf,\n                          DAG.getConstant(0x00FF, DL, MVT::v8i16));\n\n    // This will be a single vector shuffle instead of a blend so nuke VHiHalf.\n    VHiHalf = DAG.getUNDEF(MVT::v8i16);\n\n    // Squash the masks to point directly into VLoHalf.\n    for (int &M : LoBlendMask)\n      if (M >= 0)\n        M /= 2;\n    for (int &M : HiBlendMask)\n      if (M >= 0)\n        M /= 2;\n  } else {\n    // Otherwise just unpack the low half of V into VLoHalf and the high half into\n    // VHiHalf so that we can blend them as i16s.\n    SDValue Zero = getZeroVector(MVT::v16i8, Subtarget, DAG, DL);\n\n    VLoHalf = DAG.getBitcast(\n        MVT::v8i16, DAG.getNode(X86ISD::UNPCKL, DL, MVT::v16i8, V, Zero));\n    VHiHalf = DAG.getBitcast(\n        MVT::v8i16, DAG.getNode(X86ISD::UNPCKH, DL, MVT::v16i8, V, Zero));\n  }\n\n  SDValue LoV = DAG.getVectorShuffle(MVT::v8i16, DL, VLoHalf, VHiHalf, LoBlendMask);\n  SDValue HiV = DAG.getVectorShuffle(MVT::v8i16, DL, VLoHalf, VHiHalf, HiBlendMask);\n\n  return DAG.getNode(X86ISD::PACKUS, DL, MVT::v16i8, LoV, HiV);\n}\n\n/// Dispatching routine to lower various 128-bit x86 vector shuffles.\n///\n/// This routine breaks down the specific type of 128-bit shuffle and\n/// dispatches to the lowering routines accordingly.\nstatic SDValue lower128BitShuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  MVT VT, SDValue V1, SDValue V2,\n                                  const APInt &Zeroable,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  switch (VT.SimpleTy) {\n  case MVT::v2i64:\n    return lowerV2I64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v2f64:\n    return lowerV2F64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v4i32:\n    return lowerV4I32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v4f32:\n    return lowerV4F32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v8i16:\n    return lowerV8I16Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v16i8:\n    return lowerV16I8Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n\n  default:\n    llvm_unreachable(\"Unimplemented!\");\n  }\n}\n\n/// Generic routine to split vector shuffle into half-sized shuffles.\n///\n/// This routine just extracts two subvectors, shuffles them independently, and\n/// then concatenates them back together. This should work effectively with all\n/// AVX vector shuffle types.\nstatic SDValue splitAndLowerShuffle(const SDLoc &DL, MVT VT, SDValue V1,\n                                    SDValue V2, ArrayRef<int> Mask,\n                                    SelectionDAG &DAG) {\n  assert(VT.getSizeInBits() >= 256 &&\n         \"Only for 256-bit or wider vector shuffles!\");\n  assert(V1.getSimpleValueType() == VT && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == VT && \"Bad operand type!\");\n\n  ArrayRef<int> LoMask = Mask.slice(0, Mask.size() / 2);\n  ArrayRef<int> HiMask = Mask.slice(Mask.size() / 2);\n\n  int NumElements = VT.getVectorNumElements();\n  int SplitNumElements = NumElements / 2;\n  MVT ScalarVT = VT.getVectorElementType();\n  MVT SplitVT = MVT::getVectorVT(ScalarVT, SplitNumElements);\n\n  // Use splitVector/extractSubVector so that split build-vectors just build two\n  // narrower build vectors. This helps shuffling with splats and zeros.\n  auto SplitVector = [&](SDValue V) {\n    SDValue LoV, HiV;\n    std::tie(LoV, HiV) = splitVector(peekThroughBitcasts(V), DAG, DL);\n    return std::make_pair(DAG.getBitcast(SplitVT, LoV),\n                          DAG.getBitcast(SplitVT, HiV));\n  };\n\n  SDValue LoV1, HiV1, LoV2, HiV2;\n  std::tie(LoV1, HiV1) = SplitVector(V1);\n  std::tie(LoV2, HiV2) = SplitVector(V2);\n\n  // Now create two 4-way blends of these half-width vectors.\n  auto HalfBlend = [&](ArrayRef<int> HalfMask) {\n    bool UseLoV1 = false, UseHiV1 = false, UseLoV2 = false, UseHiV2 = false;\n    SmallVector<int, 32> V1BlendMask((unsigned)SplitNumElements, -1);\n    SmallVector<int, 32> V2BlendMask((unsigned)SplitNumElements, -1);\n    SmallVector<int, 32> BlendMask((unsigned)SplitNumElements, -1);\n    for (int i = 0; i < SplitNumElements; ++i) {\n      int M = HalfMask[i];\n      if (M >= NumElements) {\n        if (M >= NumElements + SplitNumElements)\n          UseHiV2 = true;\n        else\n          UseLoV2 = true;\n        V2BlendMask[i] = M - NumElements;\n        BlendMask[i] = SplitNumElements + i;\n      } else if (M >= 0) {\n        if (M >= SplitNumElements)\n          UseHiV1 = true;\n        else\n          UseLoV1 = true;\n        V1BlendMask[i] = M;\n        BlendMask[i] = i;\n      }\n    }\n\n    // Because the lowering happens after all combining takes place, we need to\n    // manually combine these blend masks as much as possible so that we create\n    // a minimal number of high-level vector shuffle nodes.\n\n    // First try just blending the halves of V1 or V2.\n    if (!UseLoV1 && !UseHiV1 && !UseLoV2 && !UseHiV2)\n      return DAG.getUNDEF(SplitVT);\n    if (!UseLoV2 && !UseHiV2)\n      return DAG.getVectorShuffle(SplitVT, DL, LoV1, HiV1, V1BlendMask);\n    if (!UseLoV1 && !UseHiV1)\n      return DAG.getVectorShuffle(SplitVT, DL, LoV2, HiV2, V2BlendMask);\n\n    SDValue V1Blend, V2Blend;\n    if (UseLoV1 && UseHiV1) {\n      V1Blend =\n        DAG.getVectorShuffle(SplitVT, DL, LoV1, HiV1, V1BlendMask);\n    } else {\n      // We only use half of V1 so map the usage down into the final blend mask.\n      V1Blend = UseLoV1 ? LoV1 : HiV1;\n      for (int i = 0; i < SplitNumElements; ++i)\n        if (BlendMask[i] >= 0 && BlendMask[i] < SplitNumElements)\n          BlendMask[i] = V1BlendMask[i] - (UseLoV1 ? 0 : SplitNumElements);\n    }\n    if (UseLoV2 && UseHiV2) {\n      V2Blend =\n        DAG.getVectorShuffle(SplitVT, DL, LoV2, HiV2, V2BlendMask);\n    } else {\n      // We only use half of V2 so map the usage down into the final blend mask.\n      V2Blend = UseLoV2 ? LoV2 : HiV2;\n      for (int i = 0; i < SplitNumElements; ++i)\n        if (BlendMask[i] >= SplitNumElements)\n          BlendMask[i] = V2BlendMask[i] + (UseLoV2 ? SplitNumElements : 0);\n    }\n    return DAG.getVectorShuffle(SplitVT, DL, V1Blend, V2Blend, BlendMask);\n  };\n  SDValue Lo = HalfBlend(LoMask);\n  SDValue Hi = HalfBlend(HiMask);\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Lo, Hi);\n}\n\n/// Either split a vector in halves or decompose the shuffles and the\n/// blend/unpack.\n///\n/// This is provided as a good fallback for many lowerings of non-single-input\n/// shuffles with more than one 128-bit lane. In those cases, we want to select\n/// between splitting the shuffle into 128-bit components and stitching those\n/// back together vs. extracting the single-input shuffles and blending those\n/// results.\nstatic SDValue lowerShuffleAsSplitOrBlend(const SDLoc &DL, MVT VT, SDValue V1,\n                                          SDValue V2, ArrayRef<int> Mask,\n                                          const X86Subtarget &Subtarget,\n                                          SelectionDAG &DAG) {\n  assert(!V2.isUndef() && \"This routine must not be used to lower single-input \"\n         \"shuffles as it could then recurse on itself.\");\n  int Size = Mask.size();\n\n  // If this can be modeled as a broadcast of two elements followed by a blend,\n  // prefer that lowering. This is especially important because broadcasts can\n  // often fold with memory operands.\n  auto DoBothBroadcast = [&] {\n    int V1BroadcastIdx = -1, V2BroadcastIdx = -1;\n    for (int M : Mask)\n      if (M >= Size) {\n        if (V2BroadcastIdx < 0)\n          V2BroadcastIdx = M - Size;\n        else if (M - Size != V2BroadcastIdx)\n          return false;\n      } else if (M >= 0) {\n        if (V1BroadcastIdx < 0)\n          V1BroadcastIdx = M;\n        else if (M != V1BroadcastIdx)\n          return false;\n      }\n    return true;\n  };\n  if (DoBothBroadcast())\n    return lowerShuffleAsDecomposedShuffleMerge(DL, VT, V1, V2, Mask, Subtarget,\n                                                DAG);\n\n  // If the inputs all stem from a single 128-bit lane of each input, then we\n  // split them rather than blending because the split will decompose to\n  // unusually few instructions.\n  int LaneCount = VT.getSizeInBits() / 128;\n  int LaneSize = Size / LaneCount;\n  SmallBitVector LaneInputs[2];\n  LaneInputs[0].resize(LaneCount, false);\n  LaneInputs[1].resize(LaneCount, false);\n  for (int i = 0; i < Size; ++i)\n    if (Mask[i] >= 0)\n      LaneInputs[Mask[i] / Size][(Mask[i] % Size) / LaneSize] = true;\n  if (LaneInputs[0].count() <= 1 && LaneInputs[1].count() <= 1)\n    return splitAndLowerShuffle(DL, VT, V1, V2, Mask, DAG);\n\n  // Otherwise, just fall back to decomposed shuffles and a blend/unpack. This\n  // requires that the decomposed single-input shuffles don't end up here.\n  return lowerShuffleAsDecomposedShuffleMerge(DL, VT, V1, V2, Mask, Subtarget,\n                                              DAG);\n}\n\n// Lower as SHUFPD(VPERM2F128(V1, V2), VPERM2F128(V1, V2)).\n// TODO: Extend to support v8f32 (+ 512-bit shuffles).\nstatic SDValue lowerShuffleAsLanePermuteAndSHUFP(const SDLoc &DL, MVT VT,\n                                                 SDValue V1, SDValue V2,\n                                                 ArrayRef<int> Mask,\n                                                 SelectionDAG &DAG) {\n  assert(VT == MVT::v4f64 && \"Only for v4f64 shuffles\");\n\n  int LHSMask[4] = {-1, -1, -1, -1};\n  int RHSMask[4] = {-1, -1, -1, -1};\n  unsigned SHUFPMask = 0;\n\n  // As SHUFPD uses a single LHS/RHS element per lane, we can always\n  // perform the shuffle once the lanes have been shuffled in place.\n  for (int i = 0; i != 4; ++i) {\n    int M = Mask[i];\n    if (M < 0)\n      continue;\n    int LaneBase = i & ~1;\n    auto &LaneMask = (i & 1) ? RHSMask : LHSMask;\n    LaneMask[LaneBase + (M & 1)] = M;\n    SHUFPMask |= (M & 1) << i;\n  }\n\n  SDValue LHS = DAG.getVectorShuffle(VT, DL, V1, V2, LHSMask);\n  SDValue RHS = DAG.getVectorShuffle(VT, DL, V1, V2, RHSMask);\n  return DAG.getNode(X86ISD::SHUFP, DL, VT, LHS, RHS,\n                     DAG.getTargetConstant(SHUFPMask, DL, MVT::i8));\n}\n\n/// Lower a vector shuffle crossing multiple 128-bit lanes as\n/// a lane permutation followed by a per-lane permutation.\n///\n/// This is mainly for cases where we can have non-repeating permutes\n/// in each lane.\n///\n/// TODO: This is very similar to lowerShuffleAsLanePermuteAndRepeatedMask,\n/// we should investigate merging them.\nstatic SDValue lowerShuffleAsLanePermuteAndPermute(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  int NumElts = VT.getVectorNumElements();\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumEltsPerLane = NumElts / NumLanes;\n  bool CanUseSublanes = Subtarget.hasAVX2() && V2.isUndef();\n\n  /// Attempts to find a sublane permute with the given size\n  /// that gets all elements into their target lanes.\n  ///\n  /// If successful, fills CrossLaneMask and InLaneMask and returns true.\n  /// If unsuccessful, returns false and may overwrite InLaneMask.\n  auto getSublanePermute = [&](int NumSublanes) -> SDValue {\n    int NumSublanesPerLane = NumSublanes / NumLanes;\n    int NumEltsPerSublane = NumElts / NumSublanes;\n\n    SmallVector<int, 16> CrossLaneMask;\n    SmallVector<int, 16> InLaneMask(NumElts, SM_SentinelUndef);\n    // CrossLaneMask but one entry == one sublane.\n    SmallVector<int, 16> CrossLaneMaskLarge(NumSublanes, SM_SentinelUndef);\n\n    for (int i = 0; i != NumElts; ++i) {\n      int M = Mask[i];\n      if (M < 0)\n        continue;\n\n      int SrcSublane = M / NumEltsPerSublane;\n      int DstLane = i / NumEltsPerLane;\n\n      // We only need to get the elements into the right lane, not sublane.\n      // So search all sublanes that make up the destination lane.\n      bool Found = false;\n      int DstSubStart = DstLane * NumSublanesPerLane;\n      int DstSubEnd = DstSubStart + NumSublanesPerLane;\n      for (int DstSublane = DstSubStart; DstSublane < DstSubEnd; ++DstSublane) {\n        if (!isUndefOrEqual(CrossLaneMaskLarge[DstSublane], SrcSublane))\n          continue;\n\n        Found = true;\n        CrossLaneMaskLarge[DstSublane] = SrcSublane;\n        int DstSublaneOffset = DstSublane * NumEltsPerSublane;\n        InLaneMask[i] = DstSublaneOffset + M % NumEltsPerSublane;\n        break;\n      }\n      if (!Found)\n        return SDValue();\n    }\n\n    // Fill CrossLaneMask using CrossLaneMaskLarge.\n    narrowShuffleMaskElts(NumEltsPerSublane, CrossLaneMaskLarge, CrossLaneMask);\n\n    if (!CanUseSublanes) {\n      // If we're only shuffling a single lowest lane and the rest are identity\n      // then don't bother.\n      // TODO - isShuffleMaskInputInPlace could be extended to something like\n      // this.\n      int NumIdentityLanes = 0;\n      bool OnlyShuffleLowestLane = true;\n      for (int i = 0; i != NumLanes; ++i) {\n        int LaneOffset = i * NumEltsPerLane;\n        if (isSequentialOrUndefInRange(InLaneMask, LaneOffset, NumEltsPerLane,\n                                       i * NumEltsPerLane))\n          NumIdentityLanes++;\n        else if (CrossLaneMask[LaneOffset] != 0)\n          OnlyShuffleLowestLane = false;\n      }\n      if (OnlyShuffleLowestLane && NumIdentityLanes == (NumLanes - 1))\n        return SDValue();\n    }\n\n    SDValue CrossLane = DAG.getVectorShuffle(VT, DL, V1, V2, CrossLaneMask);\n    return DAG.getVectorShuffle(VT, DL, CrossLane, DAG.getUNDEF(VT),\n                                InLaneMask);\n  };\n\n  // First attempt a solution with full lanes.\n  if (SDValue V = getSublanePermute(/*NumSublanes=*/NumLanes))\n    return V;\n\n  // The rest of the solutions use sublanes.\n  if (!CanUseSublanes)\n    return SDValue();\n\n  // Then attempt a solution with 64-bit sublanes (vpermq).\n  if (SDValue V = getSublanePermute(/*NumSublanes=*/NumLanes * 2))\n    return V;\n\n  // If that doesn't work and we have fast variable shuffle,\n  // attempt 32-bit sublanes (vpermd).\n  if (!Subtarget.hasFastVariableShuffle())\n    return SDValue();\n\n  return getSublanePermute(/*NumSublanes=*/NumLanes * 4);\n}\n\n/// Lower a vector shuffle crossing multiple 128-bit lanes by shuffling one\n/// source with a lane permutation.\n///\n/// This lowering strategy results in four instructions in the worst case for a\n/// single-input cross lane shuffle which is lower than any other fully general\n/// cross-lane shuffle strategy I'm aware of. Special cases for each particular\n/// shuffle pattern should be handled prior to trying this lowering.\nstatic SDValue lowerShuffleAsLanePermuteAndShuffle(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  // FIXME: This should probably be generalized for 512-bit vectors as well.\n  assert(VT.is256BitVector() && \"Only for 256-bit vector shuffles!\");\n  int Size = Mask.size();\n  int LaneSize = Size / 2;\n\n  // Fold to SHUFPD(VPERM2F128(V1, V2), VPERM2F128(V1, V2)).\n  // Only do this if the elements aren't all from the lower lane,\n  // otherwise we're (probably) better off doing a split.\n  if (VT == MVT::v4f64 &&\n      !all_of(Mask, [LaneSize](int M) { return M < LaneSize; }))\n    if (SDValue V =\n            lowerShuffleAsLanePermuteAndSHUFP(DL, VT, V1, V2, Mask, DAG))\n      return V;\n\n  // If there are only inputs from one 128-bit lane, splitting will in fact be\n  // less expensive. The flags track whether the given lane contains an element\n  // that crosses to another lane.\n  if (!Subtarget.hasAVX2()) {\n    bool LaneCrossing[2] = {false, false};\n    for (int i = 0; i < Size; ++i)\n      if (Mask[i] >= 0 && ((Mask[i] % Size) / LaneSize) != (i / LaneSize))\n        LaneCrossing[(Mask[i] % Size) / LaneSize] = true;\n    if (!LaneCrossing[0] || !LaneCrossing[1])\n      return splitAndLowerShuffle(DL, VT, V1, V2, Mask, DAG);\n  } else {\n    bool LaneUsed[2] = {false, false};\n    for (int i = 0; i < Size; ++i)\n      if (Mask[i] >= 0)\n        LaneUsed[(Mask[i] % Size) / LaneSize] = true;\n    if (!LaneUsed[0] || !LaneUsed[1])\n      return splitAndLowerShuffle(DL, VT, V1, V2, Mask, DAG);\n  }\n\n  // TODO - we could support shuffling V2 in the Flipped input.\n  assert(V2.isUndef() &&\n         \"This last part of this routine only works on single input shuffles\");\n\n  SmallVector<int, 32> InLaneMask(Mask.begin(), Mask.end());\n  for (int i = 0; i < Size; ++i) {\n    int &M = InLaneMask[i];\n    if (M < 0)\n      continue;\n    if (((M % Size) / LaneSize) != (i / LaneSize))\n      M = (M % LaneSize) + ((i / LaneSize) * LaneSize) + Size;\n  }\n  assert(!is128BitLaneCrossingShuffleMask(VT, InLaneMask) &&\n         \"In-lane shuffle mask expected\");\n\n  // Flip the lanes, and shuffle the results which should now be in-lane.\n  MVT PVT = VT.isFloatingPoint() ? MVT::v4f64 : MVT::v4i64;\n  SDValue Flipped = DAG.getBitcast(PVT, V1);\n  Flipped =\n      DAG.getVectorShuffle(PVT, DL, Flipped, DAG.getUNDEF(PVT), {2, 3, 0, 1});\n  Flipped = DAG.getBitcast(VT, Flipped);\n  return DAG.getVectorShuffle(VT, DL, V1, Flipped, InLaneMask);\n}\n\n/// Handle lowering 2-lane 128-bit shuffles.\nstatic SDValue lowerV2X128Shuffle(const SDLoc &DL, MVT VT, SDValue V1,\n                                  SDValue V2, ArrayRef<int> Mask,\n                                  const APInt &Zeroable,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  // With AVX2, use VPERMQ/VPERMPD for unary shuffles to allow memory folding.\n  if (Subtarget.hasAVX2() && V2.isUndef())\n    return SDValue();\n\n  bool V2IsZero = !V2.isUndef() && ISD::isBuildVectorAllZeros(V2.getNode());\n\n  SmallVector<int, 4> WidenedMask;\n  if (!canWidenShuffleElements(Mask, Zeroable, V2IsZero, WidenedMask))\n    return SDValue();\n\n  bool IsLowZero = (Zeroable & 0x3) == 0x3;\n  bool IsHighZero = (Zeroable & 0xc) == 0xc;\n\n  // Try to use an insert into a zero vector.\n  if (WidenedMask[0] == 0 && IsHighZero) {\n    MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(), 2);\n    SDValue LoV = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVT, V1,\n                              DAG.getIntPtrConstant(0, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                       getZeroVector(VT, Subtarget, DAG, DL), LoV,\n                       DAG.getIntPtrConstant(0, DL));\n  }\n\n  // TODO: If minimizing size and one of the inputs is a zero vector and the\n  // the zero vector has only one use, we could use a VPERM2X128 to save the\n  // instruction bytes needed to explicitly generate the zero vector.\n\n  // Blends are faster and handle all the non-lane-crossing cases.\n  if (SDValue Blend = lowerShuffleAsBlend(DL, VT, V1, V2, Mask, Zeroable,\n                                          Subtarget, DAG))\n    return Blend;\n\n  // If either input operand is a zero vector, use VPERM2X128 because its mask\n  // allows us to replace the zero input with an implicit zero.\n  if (!IsLowZero && !IsHighZero) {\n    // Check for patterns which can be matched with a single insert of a 128-bit\n    // subvector.\n    bool OnlyUsesV1 = isShuffleEquivalent(Mask, {0, 1, 0, 1}, V1, V2);\n    if (OnlyUsesV1 || isShuffleEquivalent(Mask, {0, 1, 4, 5}, V1, V2)) {\n\n      // With AVX1, use vperm2f128 (below) to allow load folding. Otherwise,\n      // this will likely become vinsertf128 which can't fold a 256-bit memop.\n      if (!isa<LoadSDNode>(peekThroughBitcasts(V1))) {\n        MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(), 2);\n        SDValue SubVec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVT,\n                                     OnlyUsesV1 ? V1 : V2,\n                                     DAG.getIntPtrConstant(0, DL));\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, V1, SubVec,\n                           DAG.getIntPtrConstant(2, DL));\n      }\n    }\n\n    // Try to use SHUF128 if possible.\n    if (Subtarget.hasVLX()) {\n      if (WidenedMask[0] < 2 && WidenedMask[1] >= 2) {\n        unsigned PermMask = ((WidenedMask[0] % 2) << 0) |\n                            ((WidenedMask[1] % 2) << 1);\n        return DAG.getNode(X86ISD::SHUF128, DL, VT, V1, V2,\n                           DAG.getTargetConstant(PermMask, DL, MVT::i8));\n      }\n    }\n  }\n\n  // Otherwise form a 128-bit permutation. After accounting for undefs,\n  // convert the 64-bit shuffle mask selection values into 128-bit\n  // selection bits by dividing the indexes by 2 and shifting into positions\n  // defined by a vperm2*128 instruction's immediate control byte.\n\n  // The immediate permute control byte looks like this:\n  //    [1:0] - select 128 bits from sources for low half of destination\n  //    [2]   - ignore\n  //    [3]   - zero low half of destination\n  //    [5:4] - select 128 bits from sources for high half of destination\n  //    [6]   - ignore\n  //    [7]   - zero high half of destination\n\n  assert((WidenedMask[0] >= 0 || IsLowZero) &&\n         (WidenedMask[1] >= 0 || IsHighZero) && \"Undef half?\");\n\n  unsigned PermMask = 0;\n  PermMask |= IsLowZero  ? 0x08 : (WidenedMask[0] << 0);\n  PermMask |= IsHighZero ? 0x80 : (WidenedMask[1] << 4);\n\n  // Check the immediate mask and replace unused sources with undef.\n  if ((PermMask & 0x0a) != 0x00 && (PermMask & 0xa0) != 0x00)\n    V1 = DAG.getUNDEF(VT);\n  if ((PermMask & 0x0a) != 0x02 && (PermMask & 0xa0) != 0x20)\n    V2 = DAG.getUNDEF(VT);\n\n  return DAG.getNode(X86ISD::VPERM2X128, DL, VT, V1, V2,\n                     DAG.getTargetConstant(PermMask, DL, MVT::i8));\n}\n\n/// Lower a vector shuffle by first fixing the 128-bit lanes and then\n/// shuffling each lane.\n///\n/// This attempts to create a repeated lane shuffle where each lane uses one\n/// or two of the lanes of the inputs. The lanes of the input vectors are\n/// shuffled in one or two independent shuffles to get the lanes into the\n/// position needed by the final shuffle.\nstatic SDValue lowerShuffleAsLanePermuteAndRepeatedMask(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  assert(!V2.isUndef() && \"This is only useful with multiple inputs.\");\n\n  if (is128BitLaneRepeatedShuffleMask(VT, Mask))\n    return SDValue();\n\n  int NumElts = Mask.size();\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumLaneElts = 128 / VT.getScalarSizeInBits();\n  SmallVector<int, 16> RepeatMask(NumLaneElts, -1);\n  SmallVector<std::array<int, 2>, 2> LaneSrcs(NumLanes, {{-1, -1}});\n\n  // First pass will try to fill in the RepeatMask from lanes that need two\n  // sources.\n  for (int Lane = 0; Lane != NumLanes; ++Lane) {\n    int Srcs[2] = {-1, -1};\n    SmallVector<int, 16> InLaneMask(NumLaneElts, -1);\n    for (int i = 0; i != NumLaneElts; ++i) {\n      int M = Mask[(Lane * NumLaneElts) + i];\n      if (M < 0)\n        continue;\n      // Determine which of the possible input lanes (NumLanes from each source)\n      // this element comes from. Assign that as one of the sources for this\n      // lane. We can assign up to 2 sources for this lane. If we run out\n      // sources we can't do anything.\n      int LaneSrc = M / NumLaneElts;\n      int Src;\n      if (Srcs[0] < 0 || Srcs[0] == LaneSrc)\n        Src = 0;\n      else if (Srcs[1] < 0 || Srcs[1] == LaneSrc)\n        Src = 1;\n      else\n        return SDValue();\n\n      Srcs[Src] = LaneSrc;\n      InLaneMask[i] = (M % NumLaneElts) + Src * NumElts;\n    }\n\n    // If this lane has two sources, see if it fits with the repeat mask so far.\n    if (Srcs[1] < 0)\n      continue;\n\n    LaneSrcs[Lane][0] = Srcs[0];\n    LaneSrcs[Lane][1] = Srcs[1];\n\n    auto MatchMasks = [](ArrayRef<int> M1, ArrayRef<int> M2) {\n      assert(M1.size() == M2.size() && \"Unexpected mask size\");\n      for (int i = 0, e = M1.size(); i != e; ++i)\n        if (M1[i] >= 0 && M2[i] >= 0 && M1[i] != M2[i])\n          return false;\n      return true;\n    };\n\n    auto MergeMasks = [](ArrayRef<int> Mask, MutableArrayRef<int> MergedMask) {\n      assert(Mask.size() == MergedMask.size() && \"Unexpected mask size\");\n      for (int i = 0, e = MergedMask.size(); i != e; ++i) {\n        int M = Mask[i];\n        if (M < 0)\n          continue;\n        assert((MergedMask[i] < 0 || MergedMask[i] == M) &&\n               \"Unexpected mask element\");\n        MergedMask[i] = M;\n      }\n    };\n\n    if (MatchMasks(InLaneMask, RepeatMask)) {\n      // Merge this lane mask into the final repeat mask.\n      MergeMasks(InLaneMask, RepeatMask);\n      continue;\n    }\n\n    // Didn't find a match. Swap the operands and try again.\n    std::swap(LaneSrcs[Lane][0], LaneSrcs[Lane][1]);\n    ShuffleVectorSDNode::commuteMask(InLaneMask);\n\n    if (MatchMasks(InLaneMask, RepeatMask)) {\n      // Merge this lane mask into the final repeat mask.\n      MergeMasks(InLaneMask, RepeatMask);\n      continue;\n    }\n\n    // Couldn't find a match with the operands in either order.\n    return SDValue();\n  }\n\n  // Now handle any lanes with only one source.\n  for (int Lane = 0; Lane != NumLanes; ++Lane) {\n    // If this lane has already been processed, skip it.\n    if (LaneSrcs[Lane][0] >= 0)\n      continue;\n\n    for (int i = 0; i != NumLaneElts; ++i) {\n      int M = Mask[(Lane * NumLaneElts) + i];\n      if (M < 0)\n        continue;\n\n      // If RepeatMask isn't defined yet we can define it ourself.\n      if (RepeatMask[i] < 0)\n        RepeatMask[i] = M % NumLaneElts;\n\n      if (RepeatMask[i] < NumElts) {\n        if (RepeatMask[i] != M % NumLaneElts)\n          return SDValue();\n        LaneSrcs[Lane][0] = M / NumLaneElts;\n      } else {\n        if (RepeatMask[i] != ((M % NumLaneElts) + NumElts))\n          return SDValue();\n        LaneSrcs[Lane][1] = M / NumLaneElts;\n      }\n    }\n\n    if (LaneSrcs[Lane][0] < 0 && LaneSrcs[Lane][1] < 0)\n      return SDValue();\n  }\n\n  SmallVector<int, 16> NewMask(NumElts, -1);\n  for (int Lane = 0; Lane != NumLanes; ++Lane) {\n    int Src = LaneSrcs[Lane][0];\n    for (int i = 0; i != NumLaneElts; ++i) {\n      int M = -1;\n      if (Src >= 0)\n        M = Src * NumLaneElts + i;\n      NewMask[Lane * NumLaneElts + i] = M;\n    }\n  }\n  SDValue NewV1 = DAG.getVectorShuffle(VT, DL, V1, V2, NewMask);\n  // Ensure we didn't get back the shuffle we started with.\n  // FIXME: This is a hack to make up for some splat handling code in\n  // getVectorShuffle.\n  if (isa<ShuffleVectorSDNode>(NewV1) &&\n      cast<ShuffleVectorSDNode>(NewV1)->getMask() == Mask)\n    return SDValue();\n\n  for (int Lane = 0; Lane != NumLanes; ++Lane) {\n    int Src = LaneSrcs[Lane][1];\n    for (int i = 0; i != NumLaneElts; ++i) {\n      int M = -1;\n      if (Src >= 0)\n        M = Src * NumLaneElts + i;\n      NewMask[Lane * NumLaneElts + i] = M;\n    }\n  }\n  SDValue NewV2 = DAG.getVectorShuffle(VT, DL, V1, V2, NewMask);\n  // Ensure we didn't get back the shuffle we started with.\n  // FIXME: This is a hack to make up for some splat handling code in\n  // getVectorShuffle.\n  if (isa<ShuffleVectorSDNode>(NewV2) &&\n      cast<ShuffleVectorSDNode>(NewV2)->getMask() == Mask)\n    return SDValue();\n\n  for (int i = 0; i != NumElts; ++i) {\n    NewMask[i] = RepeatMask[i % NumLaneElts];\n    if (NewMask[i] < 0)\n      continue;\n\n    NewMask[i] += (i / NumLaneElts) * NumLaneElts;\n  }\n  return DAG.getVectorShuffle(VT, DL, NewV1, NewV2, NewMask);\n}\n\n/// If the input shuffle mask results in a vector that is undefined in all upper\n/// or lower half elements and that mask accesses only 2 halves of the\n/// shuffle's operands, return true. A mask of half the width with mask indexes\n/// adjusted to access the extracted halves of the original shuffle operands is\n/// returned in HalfMask. HalfIdx1 and HalfIdx2 return whether the upper or\n/// lower half of each input operand is accessed.\nstatic bool\ngetHalfShuffleMask(ArrayRef<int> Mask, MutableArrayRef<int> HalfMask,\n                   int &HalfIdx1, int &HalfIdx2) {\n  assert((Mask.size() == HalfMask.size() * 2) &&\n         \"Expected input mask to be twice as long as output\");\n\n  // Exactly one half of the result must be undef to allow narrowing.\n  bool UndefLower = isUndefLowerHalf(Mask);\n  bool UndefUpper = isUndefUpperHalf(Mask);\n  if (UndefLower == UndefUpper)\n    return false;\n\n  unsigned HalfNumElts = HalfMask.size();\n  unsigned MaskIndexOffset = UndefLower ? HalfNumElts : 0;\n  HalfIdx1 = -1;\n  HalfIdx2 = -1;\n  for (unsigned i = 0; i != HalfNumElts; ++i) {\n    int M = Mask[i + MaskIndexOffset];\n    if (M < 0) {\n      HalfMask[i] = M;\n      continue;\n    }\n\n    // Determine which of the 4 half vectors this element is from.\n    // i.e. 0 = Lower V1, 1 = Upper V1, 2 = Lower V2, 3 = Upper V2.\n    int HalfIdx = M / HalfNumElts;\n\n    // Determine the element index into its half vector source.\n    int HalfElt = M % HalfNumElts;\n\n    // We can shuffle with up to 2 half vectors, set the new 'half'\n    // shuffle mask accordingly.\n    if (HalfIdx1 < 0 || HalfIdx1 == HalfIdx) {\n      HalfMask[i] = HalfElt;\n      HalfIdx1 = HalfIdx;\n      continue;\n    }\n    if (HalfIdx2 < 0 || HalfIdx2 == HalfIdx) {\n      HalfMask[i] = HalfElt + HalfNumElts;\n      HalfIdx2 = HalfIdx;\n      continue;\n    }\n\n    // Too many half vectors referenced.\n    return false;\n  }\n\n  return true;\n}\n\n/// Given the output values from getHalfShuffleMask(), create a half width\n/// shuffle of extracted vectors followed by an insert back to full width.\nstatic SDValue getShuffleHalfVectors(const SDLoc &DL, SDValue V1, SDValue V2,\n                                     ArrayRef<int> HalfMask, int HalfIdx1,\n                                     int HalfIdx2, bool UndefLower,\n                                     SelectionDAG &DAG, bool UseConcat = false) {\n  assert(V1.getValueType() == V2.getValueType() && \"Different sized vectors?\");\n  assert(V1.getValueType().isSimple() && \"Expecting only simple types\");\n\n  MVT VT = V1.getSimpleValueType();\n  MVT HalfVT = VT.getHalfNumVectorElementsVT();\n  unsigned HalfNumElts = HalfVT.getVectorNumElements();\n\n  auto getHalfVector = [&](int HalfIdx) {\n    if (HalfIdx < 0)\n      return DAG.getUNDEF(HalfVT);\n    SDValue V = (HalfIdx < 2 ? V1 : V2);\n    HalfIdx = (HalfIdx % 2) * HalfNumElts;\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, HalfVT, V,\n                       DAG.getIntPtrConstant(HalfIdx, DL));\n  };\n\n  // ins undef, (shuf (ext V1, HalfIdx1), (ext V2, HalfIdx2), HalfMask), Offset\n  SDValue Half1 = getHalfVector(HalfIdx1);\n  SDValue Half2 = getHalfVector(HalfIdx2);\n  SDValue V = DAG.getVectorShuffle(HalfVT, DL, Half1, Half2, HalfMask);\n  if (UseConcat) {\n    SDValue Op0 = V;\n    SDValue Op1 = DAG.getUNDEF(HalfVT);\n    if (UndefLower)\n      std::swap(Op0, Op1);\n    return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Op0, Op1);\n  }\n\n  unsigned Offset = UndefLower ? HalfNumElts : 0;\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, DAG.getUNDEF(VT), V,\n                     DAG.getIntPtrConstant(Offset, DL));\n}\n\n/// Lower shuffles where an entire half of a 256 or 512-bit vector is UNDEF.\n/// This allows for fast cases such as subvector extraction/insertion\n/// or shuffling smaller vector types which can lower more efficiently.\nstatic SDValue lowerShuffleWithUndefHalf(const SDLoc &DL, MVT VT, SDValue V1,\n                                         SDValue V2, ArrayRef<int> Mask,\n                                         const X86Subtarget &Subtarget,\n                                         SelectionDAG &DAG) {\n  assert((VT.is256BitVector() || VT.is512BitVector()) &&\n         \"Expected 256-bit or 512-bit vector\");\n\n  bool UndefLower = isUndefLowerHalf(Mask);\n  if (!UndefLower && !isUndefUpperHalf(Mask))\n    return SDValue();\n\n  assert((!UndefLower || !isUndefUpperHalf(Mask)) &&\n         \"Completely undef shuffle mask should have been simplified already\");\n\n  // Upper half is undef and lower half is whole upper subvector.\n  // e.g. vector_shuffle <4, 5, 6, 7, u, u, u, u> or <2, 3, u, u>\n  MVT HalfVT = VT.getHalfNumVectorElementsVT();\n  unsigned HalfNumElts = HalfVT.getVectorNumElements();\n  if (!UndefLower &&\n      isSequentialOrUndefInRange(Mask, 0, HalfNumElts, HalfNumElts)) {\n    SDValue Hi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, HalfVT, V1,\n                             DAG.getIntPtrConstant(HalfNumElts, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, DAG.getUNDEF(VT), Hi,\n                       DAG.getIntPtrConstant(0, DL));\n  }\n\n  // Lower half is undef and upper half is whole lower subvector.\n  // e.g. vector_shuffle <u, u, u, u, 0, 1, 2, 3> or <u, u, 0, 1>\n  if (UndefLower &&\n      isSequentialOrUndefInRange(Mask, HalfNumElts, HalfNumElts, 0)) {\n    SDValue Hi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, HalfVT, V1,\n                             DAG.getIntPtrConstant(0, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, DAG.getUNDEF(VT), Hi,\n                       DAG.getIntPtrConstant(HalfNumElts, DL));\n  }\n\n  int HalfIdx1, HalfIdx2;\n  SmallVector<int, 8> HalfMask(HalfNumElts);\n  if (!getHalfShuffleMask(Mask, HalfMask, HalfIdx1, HalfIdx2))\n    return SDValue();\n\n  assert(HalfMask.size() == HalfNumElts && \"Unexpected shuffle mask length\");\n\n  // Only shuffle the halves of the inputs when useful.\n  unsigned NumLowerHalves =\n      (HalfIdx1 == 0 || HalfIdx1 == 2) + (HalfIdx2 == 0 || HalfIdx2 == 2);\n  unsigned NumUpperHalves =\n      (HalfIdx1 == 1 || HalfIdx1 == 3) + (HalfIdx2 == 1 || HalfIdx2 == 3);\n  assert(NumLowerHalves + NumUpperHalves <= 2 && \"Only 1 or 2 halves allowed\");\n\n  // Determine the larger pattern of undef/halves, then decide if it's worth\n  // splitting the shuffle based on subtarget capabilities and types.\n  unsigned EltWidth = VT.getVectorElementType().getSizeInBits();\n  if (!UndefLower) {\n    // XXXXuuuu: no insert is needed.\n    // Always extract lowers when setting lower - these are all free subreg ops.\n    if (NumUpperHalves == 0)\n      return getShuffleHalfVectors(DL, V1, V2, HalfMask, HalfIdx1, HalfIdx2,\n                                   UndefLower, DAG);\n\n    if (NumUpperHalves == 1) {\n      // AVX2 has efficient 32/64-bit element cross-lane shuffles.\n      if (Subtarget.hasAVX2()) {\n        // extract128 + vunpckhps/vshufps, is better than vblend + vpermps.\n        if (EltWidth == 32 && NumLowerHalves && HalfVT.is128BitVector() &&\n            !is128BitUnpackShuffleMask(HalfMask) &&\n            (!isSingleSHUFPSMask(HalfMask) ||\n             Subtarget.hasFastVariableShuffle()))\n          return SDValue();\n        // If this is a unary shuffle (assume that the 2nd operand is\n        // canonicalized to undef), then we can use vpermpd. Otherwise, we\n        // are better off extracting the upper half of 1 operand and using a\n        // narrow shuffle.\n        if (EltWidth == 64 && V2.isUndef())\n          return SDValue();\n      }\n      // AVX512 has efficient cross-lane shuffles for all legal 512-bit types.\n      if (Subtarget.hasAVX512() && VT.is512BitVector())\n        return SDValue();\n      // Extract + narrow shuffle is better than the wide alternative.\n      return getShuffleHalfVectors(DL, V1, V2, HalfMask, HalfIdx1, HalfIdx2,\n                                   UndefLower, DAG);\n    }\n\n    // Don't extract both uppers, instead shuffle and then extract.\n    assert(NumUpperHalves == 2 && \"Half vector count went wrong\");\n    return SDValue();\n  }\n\n  // UndefLower - uuuuXXXX: an insert to high half is required if we split this.\n  if (NumUpperHalves == 0) {\n    // AVX2 has efficient 64-bit element cross-lane shuffles.\n    // TODO: Refine to account for unary shuffle, splat, and other masks?\n    if (Subtarget.hasAVX2() && EltWidth == 64)\n      return SDValue();\n    // AVX512 has efficient cross-lane shuffles for all legal 512-bit types.\n    if (Subtarget.hasAVX512() && VT.is512BitVector())\n      return SDValue();\n    // Narrow shuffle + insert is better than the wide alternative.\n    return getShuffleHalfVectors(DL, V1, V2, HalfMask, HalfIdx1, HalfIdx2,\n                                 UndefLower, DAG);\n  }\n\n  // NumUpperHalves != 0: don't bother with extract, shuffle, and then insert.\n  return SDValue();\n}\n\n/// Test whether the specified input (0 or 1) is in-place blended by the\n/// given mask.\n///\n/// This returns true if the elements from a particular input are already in the\n/// slot required by the given mask and require no permutation.\nstatic bool isShuffleMaskInputInPlace(int Input, ArrayRef<int> Mask) {\n  assert((Input == 0 || Input == 1) && \"Only two inputs to shuffles.\");\n  int Size = Mask.size();\n  for (int i = 0; i < Size; ++i)\n    if (Mask[i] >= 0 && Mask[i] / Size == Input && Mask[i] % Size != i)\n      return false;\n\n  return true;\n}\n\n/// Handle case where shuffle sources are coming from the same 128-bit lane and\n/// every lane can be represented as the same repeating mask - allowing us to\n/// shuffle the sources with the repeating shuffle and then permute the result\n/// to the destination lanes.\nstatic SDValue lowerShuffleAsRepeatedMaskAndLanePermute(\n    const SDLoc &DL, MVT VT, SDValue V1, SDValue V2, ArrayRef<int> Mask,\n    const X86Subtarget &Subtarget, SelectionDAG &DAG) {\n  int NumElts = VT.getVectorNumElements();\n  int NumLanes = VT.getSizeInBits() / 128;\n  int NumLaneElts = NumElts / NumLanes;\n\n  // On AVX2 we may be able to just shuffle the lowest elements and then\n  // broadcast the result.\n  if (Subtarget.hasAVX2()) {\n    for (unsigned BroadcastSize : {16, 32, 64}) {\n      if (BroadcastSize <= VT.getScalarSizeInBits())\n        continue;\n      int NumBroadcastElts = BroadcastSize / VT.getScalarSizeInBits();\n\n      // Attempt to match a repeating pattern every NumBroadcastElts,\n      // accounting for UNDEFs but only references the lowest 128-bit\n      // lane of the inputs.\n      auto FindRepeatingBroadcastMask = [&](SmallVectorImpl<int> &RepeatMask) {\n        for (int i = 0; i != NumElts; i += NumBroadcastElts)\n          for (int j = 0; j != NumBroadcastElts; ++j) {\n            int M = Mask[i + j];\n            if (M < 0)\n              continue;\n            int &R = RepeatMask[j];\n            if (0 != ((M % NumElts) / NumLaneElts))\n              return false;\n            if (0 <= R && R != M)\n              return false;\n            R = M;\n          }\n        return true;\n      };\n\n      SmallVector<int, 8> RepeatMask((unsigned)NumElts, -1);\n      if (!FindRepeatingBroadcastMask(RepeatMask))\n        continue;\n\n      // Shuffle the (lowest) repeated elements in place for broadcast.\n      SDValue RepeatShuf = DAG.getVectorShuffle(VT, DL, V1, V2, RepeatMask);\n\n      // Shuffle the actual broadcast.\n      SmallVector<int, 8> BroadcastMask((unsigned)NumElts, -1);\n      for (int i = 0; i != NumElts; i += NumBroadcastElts)\n        for (int j = 0; j != NumBroadcastElts; ++j)\n          BroadcastMask[i + j] = j;\n      return DAG.getVectorShuffle(VT, DL, RepeatShuf, DAG.getUNDEF(VT),\n                                  BroadcastMask);\n    }\n  }\n\n  // Bail if the shuffle mask doesn't cross 128-bit lanes.\n  if (!is128BitLaneCrossingShuffleMask(VT, Mask))\n    return SDValue();\n\n  // Bail if we already have a repeated lane shuffle mask.\n  SmallVector<int, 8> RepeatedShuffleMask;\n  if (is128BitLaneRepeatedShuffleMask(VT, Mask, RepeatedShuffleMask))\n    return SDValue();\n\n  // On AVX2 targets we can permute 256-bit vectors as 64-bit sub-lanes\n  // (with PERMQ/PERMPD), otherwise we can only permute whole 128-bit lanes.\n  int SubLaneScale = Subtarget.hasAVX2() && VT.is256BitVector() ? 2 : 1;\n  int NumSubLanes = NumLanes * SubLaneScale;\n  int NumSubLaneElts = NumLaneElts / SubLaneScale;\n\n  // Check that all the sources are coming from the same lane and see if we can\n  // form a repeating shuffle mask (local to each sub-lane). At the same time,\n  // determine the source sub-lane for each destination sub-lane.\n  int TopSrcSubLane = -1;\n  SmallVector<int, 8> Dst2SrcSubLanes((unsigned)NumSubLanes, -1);\n  SmallVector<int, 8> RepeatedSubLaneMasks[2] = {\n      SmallVector<int, 8>((unsigned)NumSubLaneElts, SM_SentinelUndef),\n      SmallVector<int, 8>((unsigned)NumSubLaneElts, SM_SentinelUndef)};\n\n  for (int DstSubLane = 0; DstSubLane != NumSubLanes; ++DstSubLane) {\n    // Extract the sub-lane mask, check that it all comes from the same lane\n    // and normalize the mask entries to come from the first lane.\n    int SrcLane = -1;\n    SmallVector<int, 8> SubLaneMask((unsigned)NumSubLaneElts, -1);\n    for (int Elt = 0; Elt != NumSubLaneElts; ++Elt) {\n      int M = Mask[(DstSubLane * NumSubLaneElts) + Elt];\n      if (M < 0)\n        continue;\n      int Lane = (M % NumElts) / NumLaneElts;\n      if ((0 <= SrcLane) && (SrcLane != Lane))\n        return SDValue();\n      SrcLane = Lane;\n      int LocalM = (M % NumLaneElts) + (M < NumElts ? 0 : NumElts);\n      SubLaneMask[Elt] = LocalM;\n    }\n\n    // Whole sub-lane is UNDEF.\n    if (SrcLane < 0)\n      continue;\n\n    // Attempt to match against the candidate repeated sub-lane masks.\n    for (int SubLane = 0; SubLane != SubLaneScale; ++SubLane) {\n      auto MatchMasks = [NumSubLaneElts](ArrayRef<int> M1, ArrayRef<int> M2) {\n        for (int i = 0; i != NumSubLaneElts; ++i) {\n          if (M1[i] < 0 || M2[i] < 0)\n            continue;\n          if (M1[i] != M2[i])\n            return false;\n        }\n        return true;\n      };\n\n      auto &RepeatedSubLaneMask = RepeatedSubLaneMasks[SubLane];\n      if (!MatchMasks(SubLaneMask, RepeatedSubLaneMask))\n        continue;\n\n      // Merge the sub-lane mask into the matching repeated sub-lane mask.\n      for (int i = 0; i != NumSubLaneElts; ++i) {\n        int M = SubLaneMask[i];\n        if (M < 0)\n          continue;\n        assert((RepeatedSubLaneMask[i] < 0 || RepeatedSubLaneMask[i] == M) &&\n               \"Unexpected mask element\");\n        RepeatedSubLaneMask[i] = M;\n      }\n\n      // Track the top most source sub-lane - by setting the remaining to UNDEF\n      // we can greatly simplify shuffle matching.\n      int SrcSubLane = (SrcLane * SubLaneScale) + SubLane;\n      TopSrcSubLane = std::max(TopSrcSubLane, SrcSubLane);\n      Dst2SrcSubLanes[DstSubLane] = SrcSubLane;\n      break;\n    }\n\n    // Bail if we failed to find a matching repeated sub-lane mask.\n    if (Dst2SrcSubLanes[DstSubLane] < 0)\n      return SDValue();\n  }\n  assert(0 <= TopSrcSubLane && TopSrcSubLane < NumSubLanes &&\n         \"Unexpected source lane\");\n\n  // Create a repeating shuffle mask for the entire vector.\n  SmallVector<int, 8> RepeatedMask((unsigned)NumElts, -1);\n  for (int SubLane = 0; SubLane <= TopSrcSubLane; ++SubLane) {\n    int Lane = SubLane / SubLaneScale;\n    auto &RepeatedSubLaneMask = RepeatedSubLaneMasks[SubLane % SubLaneScale];\n    for (int Elt = 0; Elt != NumSubLaneElts; ++Elt) {\n      int M = RepeatedSubLaneMask[Elt];\n      if (M < 0)\n        continue;\n      int Idx = (SubLane * NumSubLaneElts) + Elt;\n      RepeatedMask[Idx] = M + (Lane * NumLaneElts);\n    }\n  }\n  SDValue RepeatedShuffle = DAG.getVectorShuffle(VT, DL, V1, V2, RepeatedMask);\n\n  // Shuffle each source sub-lane to its destination.\n  SmallVector<int, 8> SubLaneMask((unsigned)NumElts, -1);\n  for (int i = 0; i != NumElts; i += NumSubLaneElts) {\n    int SrcSubLane = Dst2SrcSubLanes[i / NumSubLaneElts];\n    if (SrcSubLane < 0)\n      continue;\n    for (int j = 0; j != NumSubLaneElts; ++j)\n      SubLaneMask[i + j] = j + (SrcSubLane * NumSubLaneElts);\n  }\n\n  return DAG.getVectorShuffle(VT, DL, RepeatedShuffle, DAG.getUNDEF(VT),\n                              SubLaneMask);\n}\n\nstatic bool matchShuffleWithSHUFPD(MVT VT, SDValue &V1, SDValue &V2,\n                                   bool &ForceV1Zero, bool &ForceV2Zero,\n                                   unsigned &ShuffleImm, ArrayRef<int> Mask,\n                                   const APInt &Zeroable) {\n  int NumElts = VT.getVectorNumElements();\n  assert(VT.getScalarSizeInBits() == 64 &&\n         (NumElts == 2 || NumElts == 4 || NumElts == 8) &&\n         \"Unexpected data type for VSHUFPD\");\n  assert(isUndefOrZeroOrInRange(Mask, 0, 2 * NumElts) &&\n         \"Illegal shuffle mask\");\n\n  bool ZeroLane[2] = { true, true };\n  for (int i = 0; i < NumElts; ++i)\n    ZeroLane[i & 1] &= Zeroable[i];\n\n  // Mask for V8F64: 0/1,  8/9,  2/3,  10/11, 4/5, ..\n  // Mask for V4F64; 0/1,  4/5,  2/3,  6/7..\n  ShuffleImm = 0;\n  bool ShufpdMask = true;\n  bool CommutableMask = true;\n  for (int i = 0; i < NumElts; ++i) {\n    if (Mask[i] == SM_SentinelUndef || ZeroLane[i & 1])\n      continue;\n    if (Mask[i] < 0)\n      return false;\n    int Val = (i & 6) + NumElts * (i & 1);\n    int CommutVal = (i & 0xe) + NumElts * ((i & 1) ^ 1);\n    if (Mask[i] < Val || Mask[i] > Val + 1)\n      ShufpdMask = false;\n    if (Mask[i] < CommutVal || Mask[i] > CommutVal + 1)\n      CommutableMask = false;\n    ShuffleImm |= (Mask[i] % 2) << i;\n  }\n\n  if (!ShufpdMask && !CommutableMask)\n    return false;\n\n  if (!ShufpdMask && CommutableMask)\n    std::swap(V1, V2);\n\n  ForceV1Zero = ZeroLane[0];\n  ForceV2Zero = ZeroLane[1];\n  return true;\n}\n\nstatic SDValue lowerShuffleWithSHUFPD(const SDLoc &DL, MVT VT, SDValue V1,\n                                      SDValue V2, ArrayRef<int> Mask,\n                                      const APInt &Zeroable,\n                                      const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  assert((VT == MVT::v2f64 || VT == MVT::v4f64 || VT == MVT::v8f64) &&\n         \"Unexpected data type for VSHUFPD\");\n\n  unsigned Immediate = 0;\n  bool ForceV1Zero = false, ForceV2Zero = false;\n  if (!matchShuffleWithSHUFPD(VT, V1, V2, ForceV1Zero, ForceV2Zero, Immediate,\n                              Mask, Zeroable))\n    return SDValue();\n\n  // Create a REAL zero vector - ISD::isBuildVectorAllZeros allows UNDEFs.\n  if (ForceV1Zero)\n    V1 = getZeroVector(VT, Subtarget, DAG, DL);\n  if (ForceV2Zero)\n    V2 = getZeroVector(VT, Subtarget, DAG, DL);\n\n  return DAG.getNode(X86ISD::SHUFP, DL, VT, V1, V2,\n                     DAG.getTargetConstant(Immediate, DL, MVT::i8));\n}\n\n// Look for {0, 8, 16, 24, 32, 40, 48, 56 } in the first 8 elements. Followed\n// by zeroable elements in the remaining 24 elements. Turn this into two\n// vmovqb instructions shuffled together.\nstatic SDValue lowerShuffleAsVTRUNCAndUnpack(const SDLoc &DL, MVT VT,\n                                             SDValue V1, SDValue V2,\n                                             ArrayRef<int> Mask,\n                                             const APInt &Zeroable,\n                                             SelectionDAG &DAG) {\n  assert(VT == MVT::v32i8 && \"Unexpected type!\");\n\n  // The first 8 indices should be every 8th element.\n  if (!isSequentialOrUndefInRange(Mask, 0, 8, 0, 8))\n    return SDValue();\n\n  // Remaining elements need to be zeroable.\n  if (Zeroable.countLeadingOnes() < (Mask.size() - 8))\n    return SDValue();\n\n  V1 = DAG.getBitcast(MVT::v4i64, V1);\n  V2 = DAG.getBitcast(MVT::v4i64, V2);\n\n  V1 = DAG.getNode(X86ISD::VTRUNC, DL, MVT::v16i8, V1);\n  V2 = DAG.getNode(X86ISD::VTRUNC, DL, MVT::v16i8, V2);\n\n  // The VTRUNCs will put 0s in the upper 12 bytes. Use them to put zeroes in\n  // the upper bits of the result using an unpckldq.\n  SDValue Unpack = DAG.getVectorShuffle(MVT::v16i8, DL, V1, V2,\n                                        { 0, 1, 2, 3, 16, 17, 18, 19,\n                                          4, 5, 6, 7, 20, 21, 22, 23 });\n  // Insert the unpckldq into a zero vector to widen to v32i8.\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, MVT::v32i8,\n                     DAG.getConstant(0, DL, MVT::v32i8), Unpack,\n                     DAG.getIntPtrConstant(0, DL));\n}\n\n\n/// Handle lowering of 4-lane 64-bit floating point shuffles.\n///\n/// Also ends up handling lowering of 4-lane 64-bit integer shuffles when AVX2\n/// isn't available.\nstatic SDValue lowerV4F64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v4f64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v4f64 && \"Bad operand type!\");\n  assert(Mask.size() == 4 && \"Unexpected mask size for v4 shuffle!\");\n\n  if (SDValue V = lowerV2X128Shuffle(DL, MVT::v4f64, V1, V2, Mask, Zeroable,\n                                     Subtarget, DAG))\n    return V;\n\n  if (V2.isUndef()) {\n    // Check for being able to broadcast a single element.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v4f64, V1, V2,\n                                                    Mask, Subtarget, DAG))\n      return Broadcast;\n\n    // Use low duplicate instructions for masks that match their pattern.\n    if (isShuffleEquivalent(Mask, {0, 0, 2, 2}, V1, V2))\n      return DAG.getNode(X86ISD::MOVDDUP, DL, MVT::v4f64, V1);\n\n    if (!is128BitLaneCrossingShuffleMask(MVT::v4f64, Mask)) {\n      // Non-half-crossing single input shuffles can be lowered with an\n      // interleaved permutation.\n      unsigned VPERMILPMask = (Mask[0] == 1) | ((Mask[1] == 1) << 1) |\n                              ((Mask[2] == 3) << 2) | ((Mask[3] == 3) << 3);\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v4f64, V1,\n                         DAG.getTargetConstant(VPERMILPMask, DL, MVT::i8));\n    }\n\n    // With AVX2 we have direct support for this permutation.\n    if (Subtarget.hasAVX2())\n      return DAG.getNode(X86ISD::VPERMI, DL, MVT::v4f64, V1,\n                         getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n\n    // Try to create an in-lane repeating shuffle mask and then shuffle the\n    // results into the target lanes.\n    if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n            DL, MVT::v4f64, V1, V2, Mask, Subtarget, DAG))\n      return V;\n\n    // Try to permute the lanes and then use a per-lane permute.\n    if (SDValue V = lowerShuffleAsLanePermuteAndPermute(DL, MVT::v4f64, V1, V2,\n                                                        Mask, DAG, Subtarget))\n      return V;\n\n    // Otherwise, fall back.\n    return lowerShuffleAsLanePermuteAndShuffle(DL, MVT::v4f64, V1, V2, Mask,\n                                               DAG, Subtarget);\n  }\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v4f64, Mask, V1, V2, DAG))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v4f64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Check if the blend happens to exactly fit that of SHUFPD.\n  if (SDValue Op = lowerShuffleWithSHUFPD(DL, MVT::v4f64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Op;\n\n  // If we have lane crossing shuffles AND they don't all come from the lower\n  // lane elements, lower to SHUFPD(VPERM2F128(V1, V2), VPERM2F128(V1, V2)).\n  // TODO: Handle BUILD_VECTOR sources which getVectorShuffle currently\n  // canonicalize to a blend of splat which isn't necessary for this combine.\n  if (is128BitLaneCrossingShuffleMask(MVT::v4f64, Mask) &&\n      !all_of(Mask, [](int M) { return M < 2 || (4 <= M && M < 6); }) &&\n      (V1.getOpcode() != ISD::BUILD_VECTOR) &&\n      (V2.getOpcode() != ISD::BUILD_VECTOR))\n    if (SDValue Op = lowerShuffleAsLanePermuteAndSHUFP(DL, MVT::v4f64, V1, V2,\n                                                       Mask, DAG))\n      return Op;\n\n  // If we have one input in place, then we can permute the other input and\n  // blend the result.\n  if (isShuffleMaskInputInPlace(0, Mask) || isShuffleMaskInputInPlace(1, Mask))\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v4f64, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v4f64, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle. However, if we have AVX2 and either inputs are already in place,\n  // we will be able to shuffle even across lanes the other input in a single\n  // instruction so skip this pattern.\n  if (!(Subtarget.hasAVX2() && (isShuffleMaskInputInPlace(0, Mask) ||\n                                isShuffleMaskInputInPlace(1, Mask))))\n    if (SDValue V = lowerShuffleAsLanePermuteAndRepeatedMask(\n            DL, MVT::v4f64, V1, V2, Mask, Subtarget, DAG))\n      return V;\n\n  // If we have VLX support, we can use VEXPAND.\n  if (Subtarget.hasVLX())\n    if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v4f64, Zeroable, Mask, V1, V2,\n                                         DAG, Subtarget))\n      return V;\n\n  // If we have AVX2 then we always want to lower with a blend because an v4 we\n  // can fully permute the elements.\n  if (Subtarget.hasAVX2())\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v4f64, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // Otherwise fall back on generic lowering.\n  return lowerShuffleAsSplitOrBlend(DL, MVT::v4f64, V1, V2, Mask,\n                                    Subtarget, DAG);\n}\n\n/// Handle lowering of 4-lane 64-bit integer shuffles.\n///\n/// This routine is only called when we have AVX2 and thus a reasonable\n/// instruction set for v4i64 shuffling..\nstatic SDValue lowerV4I64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v4i64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v4i64 && \"Bad operand type!\");\n  assert(Mask.size() == 4 && \"Unexpected mask size for v4 shuffle!\");\n  assert(Subtarget.hasAVX2() && \"We can only lower v4i64 with AVX2!\");\n\n  if (SDValue V = lowerV2X128Shuffle(DL, MVT::v4i64, V1, V2, Mask, Zeroable,\n                                     Subtarget, DAG))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v4i64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v4i64, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  if (V2.isUndef()) {\n    // When the shuffle is mirrored between the 128-bit lanes of the unit, we\n    // can use lower latency instructions that will operate on both lanes.\n    SmallVector<int, 2> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MVT::v4i64, Mask, RepeatedMask)) {\n      SmallVector<int, 4> PSHUFDMask;\n      narrowShuffleMaskElts(2, RepeatedMask, PSHUFDMask);\n      return DAG.getBitcast(\n          MVT::v4i64,\n          DAG.getNode(X86ISD::PSHUFD, DL, MVT::v8i32,\n                      DAG.getBitcast(MVT::v8i32, V1),\n                      getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG)));\n    }\n\n    // AVX2 provides a direct instruction for permuting a single input across\n    // lanes.\n    return DAG.getNode(X86ISD::VPERMI, DL, MVT::v4i64, V1,\n                       getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n  }\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v4i64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // If we have VLX support, we can use VALIGN or VEXPAND.\n  if (Subtarget.hasVLX()) {\n    if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v4i64, V1, V2, Mask,\n                                              Subtarget, DAG))\n      return Rotate;\n\n    if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v4i64, Zeroable, Mask, V1, V2,\n                                         DAG, Subtarget))\n      return V;\n  }\n\n  // Try to use PALIGNR.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v4i64, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v4i64, Mask, V1, V2, DAG))\n    return V;\n\n  // If we have one input in place, then we can permute the other input and\n  // blend the result.\n  if (isShuffleMaskInputInPlace(0, Mask) || isShuffleMaskInputInPlace(1, Mask))\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v4i64, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v4i64, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle. However, if we have AVX2 and either inputs are already in place,\n  // we will be able to shuffle even across lanes the other input in a single\n  // instruction so skip this pattern.\n  if (!isShuffleMaskInputInPlace(0, Mask) &&\n      !isShuffleMaskInputInPlace(1, Mask))\n    if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n            DL, MVT::v4i64, V1, V2, Mask, Subtarget, DAG))\n      return Result;\n\n  // Otherwise fall back on generic blend lowering.\n  return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v4i64, V1, V2, Mask,\n                                              Subtarget, DAG);\n}\n\n/// Handle lowering of 8-lane 32-bit floating point shuffles.\n///\n/// Also ends up handling lowering of 8-lane 32-bit integer shuffles when AVX2\n/// isn't available.\nstatic SDValue lowerV8F32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v8f32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v8f32 && \"Bad operand type!\");\n  assert(Mask.size() == 8 && \"Unexpected mask size for v8 shuffle!\");\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v8f32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v8f32, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  // If the shuffle mask is repeated in each 128-bit lane, we have many more\n  // options to efficiently lower the shuffle.\n  SmallVector<int, 4> RepeatedMask;\n  if (is128BitLaneRepeatedShuffleMask(MVT::v8f32, Mask, RepeatedMask)) {\n    assert(RepeatedMask.size() == 4 &&\n           \"Repeated masks must be half the mask width!\");\n\n    // Use even/odd duplicate instructions for masks that match their pattern.\n    if (isShuffleEquivalent(RepeatedMask, {0, 0, 2, 2}, V1, V2))\n      return DAG.getNode(X86ISD::MOVSLDUP, DL, MVT::v8f32, V1);\n    if (isShuffleEquivalent(RepeatedMask, {1, 1, 3, 3}, V1, V2))\n      return DAG.getNode(X86ISD::MOVSHDUP, DL, MVT::v8f32, V1);\n\n    if (V2.isUndef())\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v8f32, V1,\n                         getV4X86ShuffleImm8ForMask(RepeatedMask, DL, DAG));\n\n    // Use dedicated unpack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v8f32, Mask, V1, V2, DAG))\n      return V;\n\n    // Otherwise, fall back to a SHUFPS sequence. Here it is important that we\n    // have already handled any direct blends.\n    return lowerShuffleWithSHUFPS(DL, MVT::v8f32, RepeatedMask, V1, V2, DAG);\n  }\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v8f32, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // If we have a single input shuffle with different shuffle patterns in the\n  // two 128-bit lanes use the variable mask to VPERMILPS.\n  if (V2.isUndef()) {\n    if (!is128BitLaneCrossingShuffleMask(MVT::v8f32, Mask)) {\n      SDValue VPermMask = getConstVector(Mask, MVT::v8i32, DAG, DL, true);\n      return DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v8f32, V1, VPermMask);\n    }\n    if (Subtarget.hasAVX2()) {\n      SDValue VPermMask = getConstVector(Mask, MVT::v8i32, DAG, DL, true);\n      return DAG.getNode(X86ISD::VPERMV, DL, MVT::v8f32, VPermMask, V1);\n    }\n    // Otherwise, fall back.\n    return lowerShuffleAsLanePermuteAndShuffle(DL, MVT::v8f32, V1, V2, Mask,\n                                               DAG, Subtarget);\n  }\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle.\n  if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n          DL, MVT::v8f32, V1, V2, Mask, Subtarget, DAG))\n    return Result;\n\n  // If we have VLX support, we can use VEXPAND.\n  if (Subtarget.hasVLX())\n    if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v8f32, Zeroable, Mask, V1, V2,\n                                         DAG, Subtarget))\n      return V;\n\n  // For non-AVX512 if the Mask is of 16bit elements in lane then try to split\n  // since after split we get a more efficient code using vpunpcklwd and\n  // vpunpckhwd instrs than vblend.\n  if (!Subtarget.hasAVX512() && isUnpackWdShuffleMask(Mask, MVT::v8f32))\n    return lowerShuffleAsSplitOrBlend(DL, MVT::v8f32, V1, V2, Mask, Subtarget,\n                                      DAG);\n\n  // If we have AVX2 then we always want to lower with a blend because at v8 we\n  // can fully permute the elements.\n  if (Subtarget.hasAVX2())\n    return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v8f32, V1, V2, Mask,\n                                                Subtarget, DAG);\n\n  // Otherwise fall back on generic lowering.\n  return lowerShuffleAsSplitOrBlend(DL, MVT::v8f32, V1, V2, Mask,\n                                    Subtarget, DAG);\n}\n\n/// Handle lowering of 8-lane 32-bit integer shuffles.\n///\n/// This routine is only called when we have AVX2 and thus a reasonable\n/// instruction set for v8i32 shuffling..\nstatic SDValue lowerV8I32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v8i32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v8i32 && \"Bad operand type!\");\n  assert(Mask.size() == 8 && \"Unexpected mask size for v8 shuffle!\");\n  assert(Subtarget.hasAVX2() && \"We can only lower v8i32 with AVX2!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(DL, MVT::v8i32, V1, V2, Mask,\n                                                   Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // For non-AVX512 if the Mask is of 16bit elements in lane then try to split\n  // since after split we get a more efficient code than vblend by using\n  // vpunpcklwd and vpunpckhwd instrs.\n  if (isUnpackWdShuffleMask(Mask, MVT::v8i32) && !V2.isUndef() &&\n      !Subtarget.hasAVX512())\n    return lowerShuffleAsSplitOrBlend(DL, MVT::v8i32, V1, V2, Mask, Subtarget,\n                                      DAG);\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v8i32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v8i32, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  // If the shuffle mask is repeated in each 128-bit lane we can use more\n  // efficient instructions that mirror the shuffles across the two 128-bit\n  // lanes.\n  SmallVector<int, 4> RepeatedMask;\n  bool Is128BitLaneRepeatedShuffle =\n      is128BitLaneRepeatedShuffleMask(MVT::v8i32, Mask, RepeatedMask);\n  if (Is128BitLaneRepeatedShuffle) {\n    assert(RepeatedMask.size() == 4 && \"Unexpected repeated mask size!\");\n    if (V2.isUndef())\n      return DAG.getNode(X86ISD::PSHUFD, DL, MVT::v8i32, V1,\n                         getV4X86ShuffleImm8ForMask(RepeatedMask, DL, DAG));\n\n    // Use dedicated unpack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v8i32, Mask, V1, V2, DAG))\n      return V;\n  }\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v8i32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // If we have VLX support, we can use VALIGN or EXPAND.\n  if (Subtarget.hasVLX()) {\n    if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v8i32, V1, V2, Mask,\n                                              Subtarget, DAG))\n      return Rotate;\n\n    if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v8i32, Zeroable, Mask, V1, V2,\n                                         DAG, Subtarget))\n      return V;\n  }\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v8i32, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v8i32, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  if (V2.isUndef()) {\n    // Try to produce a fixed cross-128-bit lane permute followed by unpack\n    // because that should be faster than the variable permute alternatives.\n    if (SDValue V = lowerShuffleWithUNPCK256(DL, MVT::v8i32, Mask, V1, V2, DAG))\n      return V;\n\n    // If the shuffle patterns aren't repeated but it's a single input, directly\n    // generate a cross-lane VPERMD instruction.\n    SDValue VPermMask = getConstVector(Mask, MVT::v8i32, DAG, DL, true);\n    return DAG.getNode(X86ISD::VPERMV, DL, MVT::v8i32, VPermMask, V1);\n  }\n\n  // Assume that a single SHUFPS is faster than an alternative sequence of\n  // multiple instructions (even if the CPU has a domain penalty).\n  // If some CPU is harmed by the domain switch, we can fix it in a later pass.\n  if (Is128BitLaneRepeatedShuffle && isSingleSHUFPSMask(RepeatedMask)) {\n    SDValue CastV1 = DAG.getBitcast(MVT::v8f32, V1);\n    SDValue CastV2 = DAG.getBitcast(MVT::v8f32, V2);\n    SDValue ShufPS = lowerShuffleWithSHUFPS(DL, MVT::v8f32, RepeatedMask,\n                                            CastV1, CastV2, DAG);\n    return DAG.getBitcast(MVT::v8i32, ShufPS);\n  }\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle.\n  if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n          DL, MVT::v8i32, V1, V2, Mask, Subtarget, DAG))\n    return Result;\n\n  // Otherwise fall back on generic blend lowering.\n  return lowerShuffleAsDecomposedShuffleMerge(DL, MVT::v8i32, V1, V2, Mask,\n                                              Subtarget, DAG);\n}\n\n/// Handle lowering of 16-lane 16-bit integer shuffles.\n///\n/// This routine is only called when we have AVX2 and thus a reasonable\n/// instruction set for v16i16 shuffling..\nstatic SDValue lowerV16I16Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  const APInt &Zeroable, SDValue V1, SDValue V2,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v16i16 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v16i16 && \"Bad operand type!\");\n  assert(Mask.size() == 16 && \"Unexpected mask size for v16 shuffle!\");\n  assert(Subtarget.hasAVX2() && \"We can only lower v16i16 with AVX2!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(\n          DL, MVT::v16i16, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v16i16, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v16i16, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v16i16, Mask, V1, V2, DAG))\n    return V;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithPACK(DL, MVT::v16i16, Mask, V1, V2, DAG,\n                                       Subtarget))\n    return V;\n\n  // Try to use lower using a truncation.\n  if (SDValue V = lowerShuffleAsVTRUNC(DL, MVT::v16i16, V1, V2, Mask, Zeroable,\n                                       Subtarget, DAG))\n    return V;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v16i16, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v16i16, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v16i16, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  if (V2.isUndef()) {\n    // Try to use bit rotation instructions.\n    if (SDValue Rotate =\n            lowerShuffleAsBitRotate(DL, MVT::v16i16, V1, Mask, Subtarget, DAG))\n      return Rotate;\n\n    // Try to produce a fixed cross-128-bit lane permute followed by unpack\n    // because that should be faster than the variable permute alternatives.\n    if (SDValue V = lowerShuffleWithUNPCK256(DL, MVT::v16i16, Mask, V1, V2, DAG))\n      return V;\n\n    // There are no generalized cross-lane shuffle operations available on i16\n    // element types.\n    if (is128BitLaneCrossingShuffleMask(MVT::v16i16, Mask)) {\n      if (SDValue V = lowerShuffleAsLanePermuteAndPermute(\n              DL, MVT::v16i16, V1, V2, Mask, DAG, Subtarget))\n        return V;\n\n      return lowerShuffleAsLanePermuteAndShuffle(DL, MVT::v16i16, V1, V2, Mask,\n                                                 DAG, Subtarget);\n    }\n\n    SmallVector<int, 8> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MVT::v16i16, Mask, RepeatedMask)) {\n      // As this is a single-input shuffle, the repeated mask should be\n      // a strictly valid v8i16 mask that we can pass through to the v8i16\n      // lowering to handle even the v16 case.\n      return lowerV8I16GeneralSingleInputShuffle(\n          DL, MVT::v16i16, V1, RepeatedMask, Subtarget, DAG);\n    }\n  }\n\n  if (SDValue PSHUFB = lowerShuffleWithPSHUFB(DL, MVT::v16i16, Mask, V1, V2,\n                                              Zeroable, Subtarget, DAG))\n    return PSHUFB;\n\n  // AVX512BW can lower to VPERMW (non-VLX will pad to v32i16).\n  if (Subtarget.hasBWI())\n    return lowerShuffleWithPERMV(DL, MVT::v16i16, Mask, V1, V2, Subtarget, DAG);\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle.\n  if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n          DL, MVT::v16i16, V1, V2, Mask, Subtarget, DAG))\n    return Result;\n\n  // Try to permute the lanes and then use a per-lane permute.\n  if (SDValue V = lowerShuffleAsLanePermuteAndPermute(\n          DL, MVT::v16i16, V1, V2, Mask, DAG, Subtarget))\n    return V;\n\n  // Otherwise fall back on generic lowering.\n  return lowerShuffleAsSplitOrBlend(DL, MVT::v16i16, V1, V2, Mask,\n                                    Subtarget, DAG);\n}\n\n/// Handle lowering of 32-lane 8-bit integer shuffles.\n///\n/// This routine is only called when we have AVX2 and thus a reasonable\n/// instruction set for v32i8 shuffling..\nstatic SDValue lowerV32I8Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v32i8 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v32i8 && \"Bad operand type!\");\n  assert(Mask.size() == 32 && \"Unexpected mask size for v32 shuffle!\");\n  assert(Subtarget.hasAVX2() && \"We can only lower v32i8 with AVX2!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(DL, MVT::v32i8, V1, V2, Mask,\n                                                   Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, MVT::v32i8, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v32i8, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v32i8, Mask, V1, V2, DAG))\n    return V;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithPACK(DL, MVT::v32i8, Mask, V1, V2, DAG,\n                                       Subtarget))\n    return V;\n\n  // Try to use lower using a truncation.\n  if (SDValue V = lowerShuffleAsVTRUNC(DL, MVT::v32i8, V1, V2, Mask, Zeroable,\n                                       Subtarget, DAG))\n    return V;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v32i8, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v32i8, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Try to use bit rotation instructions.\n  if (V2.isUndef())\n    if (SDValue Rotate =\n            lowerShuffleAsBitRotate(DL, MVT::v32i8, V1, Mask, Subtarget, DAG))\n      return Rotate;\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v32i8, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // There are no generalized cross-lane shuffle operations available on i8\n  // element types.\n  if (V2.isUndef() && is128BitLaneCrossingShuffleMask(MVT::v32i8, Mask)) {\n    // Try to produce a fixed cross-128-bit lane permute followed by unpack\n    // because that should be faster than the variable permute alternatives.\n    if (SDValue V = lowerShuffleWithUNPCK256(DL, MVT::v32i8, Mask, V1, V2, DAG))\n      return V;\n\n    if (SDValue V = lowerShuffleAsLanePermuteAndPermute(\n            DL, MVT::v32i8, V1, V2, Mask, DAG, Subtarget))\n      return V;\n\n    return lowerShuffleAsLanePermuteAndShuffle(DL, MVT::v32i8, V1, V2, Mask,\n                                               DAG, Subtarget);\n  }\n\n  if (SDValue PSHUFB = lowerShuffleWithPSHUFB(DL, MVT::v32i8, Mask, V1, V2,\n                                              Zeroable, Subtarget, DAG))\n    return PSHUFB;\n\n  // AVX512VBMI can lower to VPERMB (non-VLX will pad to v64i8).\n  if (Subtarget.hasVBMI())\n    return lowerShuffleWithPERMV(DL, MVT::v32i8, Mask, V1, V2, Subtarget, DAG);\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle.\n  if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n          DL, MVT::v32i8, V1, V2, Mask, Subtarget, DAG))\n    return Result;\n\n  // Try to permute the lanes and then use a per-lane permute.\n  if (SDValue V = lowerShuffleAsLanePermuteAndPermute(\n          DL, MVT::v32i8, V1, V2, Mask, DAG, Subtarget))\n    return V;\n\n  // Look for {0, 8, 16, 24, 32, 40, 48, 56 } in the first 8 elements. Followed\n  // by zeroable elements in the remaining 24 elements. Turn this into two\n  // vmovqb instructions shuffled together.\n  if (Subtarget.hasVLX())\n    if (SDValue V = lowerShuffleAsVTRUNCAndUnpack(DL, MVT::v32i8, V1, V2,\n                                                  Mask, Zeroable, DAG))\n      return V;\n\n  // Otherwise fall back on generic lowering.\n  return lowerShuffleAsSplitOrBlend(DL, MVT::v32i8, V1, V2, Mask,\n                                    Subtarget, DAG);\n}\n\n/// High-level routine to lower various 256-bit x86 vector shuffles.\n///\n/// This routine either breaks down the specific type of a 256-bit x86 vector\n/// shuffle or splits it into two 128-bit shuffles and fuses the results back\n/// together based on the available instructions.\nstatic SDValue lower256BitShuffle(const SDLoc &DL, ArrayRef<int> Mask, MVT VT,\n                                  SDValue V1, SDValue V2, const APInt &Zeroable,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  // If we have a single input to the zero element, insert that into V1 if we\n  // can do so cheaply.\n  int NumElts = VT.getVectorNumElements();\n  int NumV2Elements = count_if(Mask, [NumElts](int M) { return M >= NumElts; });\n\n  if (NumV2Elements == 1 && Mask[0] >= NumElts)\n    if (SDValue Insertion = lowerShuffleAsElementInsertion(\n            DL, VT, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return Insertion;\n\n  // Handle special cases where the lower or upper half is UNDEF.\n  if (SDValue V =\n          lowerShuffleWithUndefHalf(DL, VT, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // There is a really nice hard cut-over between AVX1 and AVX2 that means we\n  // can check for those subtargets here and avoid much of the subtarget\n  // querying in the per-vector-type lowering routines. With AVX1 we have\n  // essentially *zero* ability to manipulate a 256-bit vector with integer\n  // types. Since we'll use floating point types there eventually, just\n  // immediately cast everything to a float and operate entirely in that domain.\n  if (VT.isInteger() && !Subtarget.hasAVX2()) {\n    int ElementBits = VT.getScalarSizeInBits();\n    if (ElementBits < 32) {\n      // No floating point type available, if we can't use the bit operations\n      // for masking/blending then decompose into 128-bit vectors.\n      if (SDValue V = lowerShuffleAsBitMask(DL, VT, V1, V2, Mask, Zeroable,\n                                            Subtarget, DAG))\n        return V;\n      if (SDValue V = lowerShuffleAsBitBlend(DL, VT, V1, V2, Mask, DAG))\n        return V;\n      return splitAndLowerShuffle(DL, VT, V1, V2, Mask, DAG);\n    }\n\n    MVT FpVT = MVT::getVectorVT(MVT::getFloatingPointVT(ElementBits),\n                                VT.getVectorNumElements());\n    V1 = DAG.getBitcast(FpVT, V1);\n    V2 = DAG.getBitcast(FpVT, V2);\n    return DAG.getBitcast(VT, DAG.getVectorShuffle(FpVT, DL, V1, V2, Mask));\n  }\n\n  switch (VT.SimpleTy) {\n  case MVT::v4f64:\n    return lowerV4F64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v4i64:\n    return lowerV4I64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v8f32:\n    return lowerV8F32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v8i32:\n    return lowerV8I32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v16i16:\n    return lowerV16I16Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v32i8:\n    return lowerV32I8Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n\n  default:\n    llvm_unreachable(\"Not a valid 256-bit x86 vector type!\");\n  }\n}\n\n/// Try to lower a vector shuffle as a 128-bit shuffles.\nstatic SDValue lowerV4X128Shuffle(const SDLoc &DL, MVT VT, ArrayRef<int> Mask,\n                                  const APInt &Zeroable, SDValue V1, SDValue V2,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(VT.getScalarSizeInBits() == 64 &&\n         \"Unexpected element type size for 128bit shuffle.\");\n\n  // To handle 256 bit vector requires VLX and most probably\n  // function lowerV2X128VectorShuffle() is better solution.\n  assert(VT.is512BitVector() && \"Unexpected vector size for 512bit shuffle.\");\n\n  // TODO - use Zeroable like we do for lowerV2X128VectorShuffle?\n  SmallVector<int, 4> Widened128Mask;\n  if (!canWidenShuffleElements(Mask, Widened128Mask))\n    return SDValue();\n  assert(Widened128Mask.size() == 4 && \"Shuffle widening mismatch\");\n\n  // Try to use an insert into a zero vector.\n  if (Widened128Mask[0] == 0 && (Zeroable & 0xf0) == 0xf0 &&\n      (Widened128Mask[1] == 1 || (Zeroable & 0x0c) == 0x0c)) {\n    unsigned NumElts = ((Zeroable & 0x0c) == 0x0c) ? 2 : 4;\n    MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(), NumElts);\n    SDValue LoV = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVT, V1,\n                              DAG.getIntPtrConstant(0, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                       getZeroVector(VT, Subtarget, DAG, DL), LoV,\n                       DAG.getIntPtrConstant(0, DL));\n  }\n\n  // Check for patterns which can be matched with a single insert of a 256-bit\n  // subvector.\n  bool OnlyUsesV1 = isShuffleEquivalent(Mask, {0, 1, 2, 3, 0, 1, 2, 3}, V1, V2);\n  if (OnlyUsesV1 ||\n      isShuffleEquivalent(Mask, {0, 1, 2, 3, 8, 9, 10, 11}, V1, V2)) {\n    MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(), 4);\n    SDValue SubVec =\n        DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVT, OnlyUsesV1 ? V1 : V2,\n                    DAG.getIntPtrConstant(0, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT, V1, SubVec,\n                       DAG.getIntPtrConstant(4, DL));\n  }\n\n  // See if this is an insertion of the lower 128-bits of V2 into V1.\n  bool IsInsert = true;\n  int V2Index = -1;\n  for (int i = 0; i < 4; ++i) {\n    assert(Widened128Mask[i] >= -1 && \"Illegal shuffle sentinel value\");\n    if (Widened128Mask[i] < 0)\n      continue;\n\n    // Make sure all V1 subvectors are in place.\n    if (Widened128Mask[i] < 4) {\n      if (Widened128Mask[i] != i) {\n        IsInsert = false;\n        break;\n      }\n    } else {\n      // Make sure we only have a single V2 index and its the lowest 128-bits.\n      if (V2Index >= 0 || Widened128Mask[i] != 4) {\n        IsInsert = false;\n        break;\n      }\n      V2Index = i;\n    }\n  }\n  if (IsInsert && V2Index >= 0) {\n    MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(), 2);\n    SDValue Subvec = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, SubVT, V2,\n                                 DAG.getIntPtrConstant(0, DL));\n    return insert128BitVector(V1, Subvec, V2Index * 2, DAG, DL);\n  }\n\n  // See if we can widen to a 256-bit lane shuffle, we're going to lose 128-lane\n  // UNDEF info by lowering to X86ISD::SHUF128 anyway, so by widening where\n  // possible we at least ensure the lanes stay sequential to help later\n  // combines.\n  SmallVector<int, 2> Widened256Mask;\n  if (canWidenShuffleElements(Widened128Mask, Widened256Mask)) {\n    Widened128Mask.clear();\n    narrowShuffleMaskElts(2, Widened256Mask, Widened128Mask);\n  }\n\n  // Try to lower to vshuf64x2/vshuf32x4.\n  SDValue Ops[2] = {DAG.getUNDEF(VT), DAG.getUNDEF(VT)};\n  unsigned PermMask = 0;\n  // Insure elements came from the same Op.\n  for (int i = 0; i < 4; ++i) {\n    assert(Widened128Mask[i] >= -1 && \"Illegal shuffle sentinel value\");\n    if (Widened128Mask[i] < 0)\n      continue;\n\n    SDValue Op = Widened128Mask[i] >= 4 ? V2 : V1;\n    unsigned OpIndex = i / 2;\n    if (Ops[OpIndex].isUndef())\n      Ops[OpIndex] = Op;\n    else if (Ops[OpIndex] != Op)\n      return SDValue();\n\n    // Convert the 128-bit shuffle mask selection values into 128-bit selection\n    // bits defined by a vshuf64x2 instruction's immediate control byte.\n    PermMask |= (Widened128Mask[i] % 4) << (i * 2);\n  }\n\n  return DAG.getNode(X86ISD::SHUF128, DL, VT, Ops[0], Ops[1],\n                     DAG.getTargetConstant(PermMask, DL, MVT::i8));\n}\n\n/// Handle lowering of 8-lane 64-bit floating point shuffles.\nstatic SDValue lowerV8F64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v8f64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v8f64 && \"Bad operand type!\");\n  assert(Mask.size() == 8 && \"Unexpected mask size for v8 shuffle!\");\n\n  if (V2.isUndef()) {\n    // Use low duplicate instructions for masks that match their pattern.\n    if (isShuffleEquivalent(Mask, {0, 0, 2, 2, 4, 4, 6, 6}, V1, V2))\n      return DAG.getNode(X86ISD::MOVDDUP, DL, MVT::v8f64, V1);\n\n    if (!is128BitLaneCrossingShuffleMask(MVT::v8f64, Mask)) {\n      // Non-half-crossing single input shuffles can be lowered with an\n      // interleaved permutation.\n      unsigned VPERMILPMask = (Mask[0] == 1) | ((Mask[1] == 1) << 1) |\n                              ((Mask[2] == 3) << 2) | ((Mask[3] == 3) << 3) |\n                              ((Mask[4] == 5) << 4) | ((Mask[5] == 5) << 5) |\n                              ((Mask[6] == 7) << 6) | ((Mask[7] == 7) << 7);\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v8f64, V1,\n                         DAG.getTargetConstant(VPERMILPMask, DL, MVT::i8));\n    }\n\n    SmallVector<int, 4> RepeatedMask;\n    if (is256BitLaneRepeatedShuffleMask(MVT::v8f64, Mask, RepeatedMask))\n      return DAG.getNode(X86ISD::VPERMI, DL, MVT::v8f64, V1,\n                         getV4X86ShuffleImm8ForMask(RepeatedMask, DL, DAG));\n  }\n\n  if (SDValue Shuf128 = lowerV4X128Shuffle(DL, MVT::v8f64, Mask, Zeroable, V1,\n                                           V2, Subtarget, DAG))\n    return Shuf128;\n\n  if (SDValue Unpck = lowerShuffleWithUNPCK(DL, MVT::v8f64, Mask, V1, V2, DAG))\n    return Unpck;\n\n  // Check if the blend happens to exactly fit that of SHUFPD.\n  if (SDValue Op = lowerShuffleWithSHUFPD(DL, MVT::v8f64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Op;\n\n  if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v8f64, Zeroable, Mask, V1, V2,\n                                       DAG, Subtarget))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v8f64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  return lowerShuffleWithPERMV(DL, MVT::v8f64, Mask, V1, V2, Subtarget, DAG);\n}\n\n/// Handle lowering of 16-lane 32-bit floating point shuffles.\nstatic SDValue lowerV16F32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  const APInt &Zeroable, SDValue V1, SDValue V2,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v16f32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v16f32 && \"Bad operand type!\");\n  assert(Mask.size() == 16 && \"Unexpected mask size for v16 shuffle!\");\n\n  // If the shuffle mask is repeated in each 128-bit lane, we have many more\n  // options to efficiently lower the shuffle.\n  SmallVector<int, 4> RepeatedMask;\n  if (is128BitLaneRepeatedShuffleMask(MVT::v16f32, Mask, RepeatedMask)) {\n    assert(RepeatedMask.size() == 4 && \"Unexpected repeated mask size!\");\n\n    // Use even/odd duplicate instructions for masks that match their pattern.\n    if (isShuffleEquivalent(RepeatedMask, {0, 0, 2, 2}, V1, V2))\n      return DAG.getNode(X86ISD::MOVSLDUP, DL, MVT::v16f32, V1);\n    if (isShuffleEquivalent(RepeatedMask, {1, 1, 3, 3}, V1, V2))\n      return DAG.getNode(X86ISD::MOVSHDUP, DL, MVT::v16f32, V1);\n\n    if (V2.isUndef())\n      return DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v16f32, V1,\n                         getV4X86ShuffleImm8ForMask(RepeatedMask, DL, DAG));\n\n    // Use dedicated unpack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v16f32, Mask, V1, V2, DAG))\n      return V;\n\n    if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v16f32, V1, V2, Mask,\n                                            Zeroable, Subtarget, DAG))\n      return Blend;\n\n    // Otherwise, fall back to a SHUFPS sequence.\n    return lowerShuffleWithSHUFPS(DL, MVT::v16f32, RepeatedMask, V1, V2, DAG);\n  }\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v16f32, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // If we have a single input shuffle with different shuffle patterns in the\n  // 128-bit lanes and don't lane cross, use variable mask VPERMILPS.\n  if (V2.isUndef() &&\n      !is128BitLaneCrossingShuffleMask(MVT::v16f32, Mask)) {\n    SDValue VPermMask = getConstVector(Mask, MVT::v16i32, DAG, DL, true);\n    return DAG.getNode(X86ISD::VPERMILPV, DL, MVT::v16f32, V1, VPermMask);\n  }\n\n  // If we have AVX512F support, we can use VEXPAND.\n  if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v16f32, Zeroable, Mask,\n                                             V1, V2, DAG, Subtarget))\n    return V;\n\n  return lowerShuffleWithPERMV(DL, MVT::v16f32, Mask, V1, V2, Subtarget, DAG);\n}\n\n/// Handle lowering of 8-lane 64-bit integer shuffles.\nstatic SDValue lowerV8I64Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v8i64 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v8i64 && \"Bad operand type!\");\n  assert(Mask.size() == 8 && \"Unexpected mask size for v8 shuffle!\");\n\n  if (V2.isUndef()) {\n    // When the shuffle is mirrored between the 128-bit lanes of the unit, we\n    // can use lower latency instructions that will operate on all four\n    // 128-bit lanes.\n    SmallVector<int, 2> Repeated128Mask;\n    if (is128BitLaneRepeatedShuffleMask(MVT::v8i64, Mask, Repeated128Mask)) {\n      SmallVector<int, 4> PSHUFDMask;\n      narrowShuffleMaskElts(2, Repeated128Mask, PSHUFDMask);\n      return DAG.getBitcast(\n          MVT::v8i64,\n          DAG.getNode(X86ISD::PSHUFD, DL, MVT::v16i32,\n                      DAG.getBitcast(MVT::v16i32, V1),\n                      getV4X86ShuffleImm8ForMask(PSHUFDMask, DL, DAG)));\n    }\n\n    SmallVector<int, 4> Repeated256Mask;\n    if (is256BitLaneRepeatedShuffleMask(MVT::v8i64, Mask, Repeated256Mask))\n      return DAG.getNode(X86ISD::VPERMI, DL, MVT::v8i64, V1,\n                         getV4X86ShuffleImm8ForMask(Repeated256Mask, DL, DAG));\n  }\n\n  if (SDValue Shuf128 = lowerV4X128Shuffle(DL, MVT::v8i64, Mask, Zeroable, V1,\n                                           V2, Subtarget, DAG))\n    return Shuf128;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v8i64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use VALIGN.\n  if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v8i64, V1, V2, Mask,\n                                            Subtarget, DAG))\n    return Rotate;\n\n  // Try to use PALIGNR.\n  if (Subtarget.hasBWI())\n    if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v8i64, V1, V2, Mask,\n                                                  Subtarget, DAG))\n      return Rotate;\n\n  if (SDValue Unpck = lowerShuffleWithUNPCK(DL, MVT::v8i64, Mask, V1, V2, DAG))\n    return Unpck;\n\n  // If we have AVX512F support, we can use VEXPAND.\n  if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v8i64, Zeroable, Mask, V1, V2,\n                                       DAG, Subtarget))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v8i64, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  return lowerShuffleWithPERMV(DL, MVT::v8i64, Mask, V1, V2, Subtarget, DAG);\n}\n\n/// Handle lowering of 16-lane 32-bit integer shuffles.\nstatic SDValue lowerV16I32Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  const APInt &Zeroable, SDValue V1, SDValue V2,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v16i32 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v16i32 && \"Bad operand type!\");\n  assert(Mask.size() == 16 && \"Unexpected mask size for v16 shuffle!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(\n          DL, MVT::v16i32, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // If the shuffle mask is repeated in each 128-bit lane we can use more\n  // efficient instructions that mirror the shuffles across the four 128-bit\n  // lanes.\n  SmallVector<int, 4> RepeatedMask;\n  bool Is128BitLaneRepeatedShuffle =\n      is128BitLaneRepeatedShuffleMask(MVT::v16i32, Mask, RepeatedMask);\n  if (Is128BitLaneRepeatedShuffle) {\n    assert(RepeatedMask.size() == 4 && \"Unexpected repeated mask size!\");\n    if (V2.isUndef())\n      return DAG.getNode(X86ISD::PSHUFD, DL, MVT::v16i32, V1,\n                         getV4X86ShuffleImm8ForMask(RepeatedMask, DL, DAG));\n\n    // Use dedicated unpack instructions for masks that match their pattern.\n    if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v16i32, Mask, V1, V2, DAG))\n      return V;\n  }\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v16i32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use VALIGN.\n  if (SDValue Rotate = lowerShuffleAsVALIGN(DL, MVT::v16i32, V1, V2, Mask,\n                                            Subtarget, DAG))\n    return Rotate;\n\n  // Try to use byte rotation instructions.\n  if (Subtarget.hasBWI())\n    if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v16i32, V1, V2, Mask,\n                                                  Subtarget, DAG))\n      return Rotate;\n\n  // Assume that a single SHUFPS is faster than using a permv shuffle.\n  // If some CPU is harmed by the domain switch, we can fix it in a later pass.\n  if (Is128BitLaneRepeatedShuffle && isSingleSHUFPSMask(RepeatedMask)) {\n    SDValue CastV1 = DAG.getBitcast(MVT::v16f32, V1);\n    SDValue CastV2 = DAG.getBitcast(MVT::v16f32, V2);\n    SDValue ShufPS = lowerShuffleWithSHUFPS(DL, MVT::v16f32, RepeatedMask,\n                                            CastV1, CastV2, DAG);\n    return DAG.getBitcast(MVT::v16i32, ShufPS);\n  }\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v16i32, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // If we have AVX512F support, we can use VEXPAND.\n  if (SDValue V = lowerShuffleToEXPAND(DL, MVT::v16i32, Zeroable, Mask, V1, V2,\n                                       DAG, Subtarget))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v16i32, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  return lowerShuffleWithPERMV(DL, MVT::v16i32, Mask, V1, V2, Subtarget, DAG);\n}\n\n/// Handle lowering of 32-lane 16-bit integer shuffles.\nstatic SDValue lowerV32I16Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  const APInt &Zeroable, SDValue V1, SDValue V2,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v32i16 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v32i16 && \"Bad operand type!\");\n  assert(Mask.size() == 32 && \"Unexpected mask size for v32 shuffle!\");\n  assert(Subtarget.hasBWI() && \"We can only lower v32i16 with AVX-512-BWI!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(\n          DL, MVT::v32i16, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v32i16, Mask, V1, V2, DAG))\n    return V;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V =\n          lowerShuffleWithPACK(DL, MVT::v32i16, Mask, V1, V2, DAG, Subtarget))\n    return V;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v32i16, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v32i16, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  if (V2.isUndef()) {\n    // Try to use bit rotation instructions.\n    if (SDValue Rotate =\n            lowerShuffleAsBitRotate(DL, MVT::v32i16, V1, Mask, Subtarget, DAG))\n      return Rotate;\n\n    SmallVector<int, 8> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MVT::v32i16, Mask, RepeatedMask)) {\n      // As this is a single-input shuffle, the repeated mask should be\n      // a strictly valid v8i16 mask that we can pass through to the v8i16\n      // lowering to handle even the v32 case.\n      return lowerV8I16GeneralSingleInputShuffle(DL, MVT::v32i16, V1,\n                                                 RepeatedMask, Subtarget, DAG);\n    }\n  }\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v32i16, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  if (SDValue PSHUFB = lowerShuffleWithPSHUFB(DL, MVT::v32i16, Mask, V1, V2,\n                                              Zeroable, Subtarget, DAG))\n    return PSHUFB;\n\n  return lowerShuffleWithPERMV(DL, MVT::v32i16, Mask, V1, V2, Subtarget, DAG);\n}\n\n/// Handle lowering of 64-lane 8-bit integer shuffles.\nstatic SDValue lowerV64I8Shuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                 const APInt &Zeroable, SDValue V1, SDValue V2,\n                                 const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  assert(V1.getSimpleValueType() == MVT::v64i8 && \"Bad operand type!\");\n  assert(V2.getSimpleValueType() == MVT::v64i8 && \"Bad operand type!\");\n  assert(Mask.size() == 64 && \"Unexpected mask size for v64 shuffle!\");\n  assert(Subtarget.hasBWI() && \"We can only lower v64i8 with AVX-512-BWI!\");\n\n  // Whenever we can lower this as a zext, that instruction is strictly faster\n  // than any alternative. It also allows us to fold memory operands into the\n  // shuffle in many cases.\n  if (SDValue ZExt = lowerShuffleAsZeroOrAnyExtend(\n          DL, MVT::v64i8, V1, V2, Mask, Zeroable, Subtarget, DAG))\n    return ZExt;\n\n  // Use dedicated unpack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithUNPCK(DL, MVT::v64i8, Mask, V1, V2, DAG))\n    return V;\n\n  // Use dedicated pack instructions for masks that match their pattern.\n  if (SDValue V = lowerShuffleWithPACK(DL, MVT::v64i8, Mask, V1, V2, DAG,\n                                       Subtarget))\n    return V;\n\n  // Try to use shift instructions.\n  if (SDValue Shift = lowerShuffleAsShift(DL, MVT::v64i8, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Shift;\n\n  // Try to use byte rotation instructions.\n  if (SDValue Rotate = lowerShuffleAsByteRotate(DL, MVT::v64i8, V1, V2, Mask,\n                                                Subtarget, DAG))\n    return Rotate;\n\n  // Try to use bit rotation instructions.\n  if (V2.isUndef())\n    if (SDValue Rotate =\n            lowerShuffleAsBitRotate(DL, MVT::v64i8, V1, Mask, Subtarget, DAG))\n      return Rotate;\n\n  // Lower as AND if possible.\n  if (SDValue Masked = lowerShuffleAsBitMask(DL, MVT::v64i8, V1, V2, Mask,\n                                             Zeroable, Subtarget, DAG))\n    return Masked;\n\n  if (SDValue PSHUFB = lowerShuffleWithPSHUFB(DL, MVT::v64i8, Mask, V1, V2,\n                                              Zeroable, Subtarget, DAG))\n    return PSHUFB;\n\n  // VBMI can use VPERMV/VPERMV3 byte shuffles.\n  if (Subtarget.hasVBMI())\n    return lowerShuffleWithPERMV(DL, MVT::v64i8, Mask, V1, V2, Subtarget, DAG);\n\n  // Try to create an in-lane repeating shuffle mask and then shuffle the\n  // results into the target lanes.\n  if (SDValue V = lowerShuffleAsRepeatedMaskAndLanePermute(\n          DL, MVT::v64i8, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  if (SDValue Blend = lowerShuffleAsBlend(DL, MVT::v64i8, V1, V2, Mask,\n                                          Zeroable, Subtarget, DAG))\n    return Blend;\n\n  // Try to simplify this by merging 128-bit lanes to enable a lane-based\n  // shuffle.\n  if (!V2.isUndef())\n    if (SDValue Result = lowerShuffleAsLanePermuteAndRepeatedMask(\n            DL, MVT::v64i8, V1, V2, Mask, Subtarget, DAG))\n      return Result;\n\n  // FIXME: Implement direct support for this type!\n  return splitAndLowerShuffle(DL, MVT::v64i8, V1, V2, Mask, DAG);\n}\n\n/// High-level routine to lower various 512-bit x86 vector shuffles.\n///\n/// This routine either breaks down the specific type of a 512-bit x86 vector\n/// shuffle or splits it into two 256-bit shuffles and fuses the results back\n/// together based on the available instructions.\nstatic SDValue lower512BitShuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                  MVT VT, SDValue V1, SDValue V2,\n                                  const APInt &Zeroable,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG) {\n  assert(Subtarget.hasAVX512() &&\n         \"Cannot lower 512-bit vectors w/ basic ISA!\");\n\n  // If we have a single input to the zero element, insert that into V1 if we\n  // can do so cheaply.\n  int NumElts = Mask.size();\n  int NumV2Elements = count_if(Mask, [NumElts](int M) { return M >= NumElts; });\n\n  if (NumV2Elements == 1 && Mask[0] >= NumElts)\n    if (SDValue Insertion = lowerShuffleAsElementInsertion(\n            DL, VT, V1, V2, Mask, Zeroable, Subtarget, DAG))\n      return Insertion;\n\n  // Handle special cases where the lower or upper half is UNDEF.\n  if (SDValue V =\n          lowerShuffleWithUndefHalf(DL, VT, V1, V2, Mask, Subtarget, DAG))\n    return V;\n\n  // Check for being able to broadcast a single element.\n  if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, VT, V1, V2, Mask,\n                                                  Subtarget, DAG))\n    return Broadcast;\n\n  if ((VT == MVT::v32i16 || VT == MVT::v64i8) && !Subtarget.hasBWI()) {\n    // Try using bit ops for masking and blending before falling back to\n    // splitting.\n    if (SDValue V = lowerShuffleAsBitMask(DL, VT, V1, V2, Mask, Zeroable,\n                                          Subtarget, DAG))\n      return V;\n    if (SDValue V = lowerShuffleAsBitBlend(DL, VT, V1, V2, Mask, DAG))\n      return V;\n\n    return splitAndLowerShuffle(DL, VT, V1, V2, Mask, DAG);\n  }\n\n  // Dispatch to each element type for lowering. If we don't have support for\n  // specific element type shuffles at 512 bits, immediately split them and\n  // lower them. Each lowering routine of a given type is allowed to assume that\n  // the requisite ISA extensions for that element type are available.\n  switch (VT.SimpleTy) {\n  case MVT::v8f64:\n    return lowerV8F64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v16f32:\n    return lowerV16F32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v8i64:\n    return lowerV8I64Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v16i32:\n    return lowerV16I32Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v32i16:\n    return lowerV32I16Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n  case MVT::v64i8:\n    return lowerV64I8Shuffle(DL, Mask, Zeroable, V1, V2, Subtarget, DAG);\n\n  default:\n    llvm_unreachable(\"Not a valid 512-bit x86 vector type!\");\n  }\n}\n\nstatic SDValue lower1BitShuffleAsKSHIFTR(const SDLoc &DL, ArrayRef<int> Mask,\n                                         MVT VT, SDValue V1, SDValue V2,\n                                         const X86Subtarget &Subtarget,\n                                         SelectionDAG &DAG) {\n  // Shuffle should be unary.\n  if (!V2.isUndef())\n    return SDValue();\n\n  int ShiftAmt = -1;\n  int NumElts = Mask.size();\n  for (int i = 0; i != NumElts; ++i) {\n    int M = Mask[i];\n    assert((M == SM_SentinelUndef || (0 <= M && M < NumElts)) &&\n           \"Unexpected mask index.\");\n    if (M < 0)\n      continue;\n\n    // The first non-undef element determines our shift amount.\n    if (ShiftAmt < 0) {\n      ShiftAmt = M - i;\n      // Need to be shifting right.\n      if (ShiftAmt <= 0)\n        return SDValue();\n    }\n    // All non-undef elements must shift by the same amount.\n    if (ShiftAmt != M - i)\n      return SDValue();\n  }\n  assert(ShiftAmt >= 0 && \"All undef?\");\n\n  // Great we found a shift right.\n  MVT WideVT = VT;\n  if ((!Subtarget.hasDQI() && NumElts == 8) || NumElts < 8)\n    WideVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n  SDValue Res = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, WideVT,\n                            DAG.getUNDEF(WideVT), V1,\n                            DAG.getIntPtrConstant(0, DL));\n  Res = DAG.getNode(X86ISD::KSHIFTR, DL, WideVT, Res,\n                    DAG.getTargetConstant(ShiftAmt, DL, MVT::i8));\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                     DAG.getIntPtrConstant(0, DL));\n}\n\n// Determine if this shuffle can be implemented with a KSHIFT instruction.\n// Returns the shift amount if possible or -1 if not. This is a simplified\n// version of matchShuffleAsShift.\nstatic int match1BitShuffleAsKSHIFT(unsigned &Opcode, ArrayRef<int> Mask,\n                                    int MaskOffset, const APInt &Zeroable) {\n  int Size = Mask.size();\n\n  auto CheckZeros = [&](int Shift, bool Left) {\n    for (int j = 0; j < Shift; ++j)\n      if (!Zeroable[j + (Left ? 0 : (Size - Shift))])\n        return false;\n\n    return true;\n  };\n\n  auto MatchShift = [&](int Shift, bool Left) {\n    unsigned Pos = Left ? Shift : 0;\n    unsigned Low = Left ? 0 : Shift;\n    unsigned Len = Size - Shift;\n    return isSequentialOrUndefInRange(Mask, Pos, Len, Low + MaskOffset);\n  };\n\n  for (int Shift = 1; Shift != Size; ++Shift)\n    for (bool Left : {true, false})\n      if (CheckZeros(Shift, Left) && MatchShift(Shift, Left)) {\n        Opcode = Left ? X86ISD::KSHIFTL : X86ISD::KSHIFTR;\n        return Shift;\n      }\n\n  return -1;\n}\n\n\n// Lower vXi1 vector shuffles.\n// There is no a dedicated instruction on AVX-512 that shuffles the masks.\n// The only way to shuffle bits is to sign-extend the mask vector to SIMD\n// vector, shuffle and then truncate it back.\nstatic SDValue lower1BitShuffle(const SDLoc &DL, ArrayRef<int> Mask,\n                                MVT VT, SDValue V1, SDValue V2,\n                                const APInt &Zeroable,\n                                const X86Subtarget &Subtarget,\n                                SelectionDAG &DAG) {\n  assert(Subtarget.hasAVX512() &&\n         \"Cannot lower 512-bit vectors w/o basic ISA!\");\n\n  int NumElts = Mask.size();\n\n  // Try to recognize shuffles that are just padding a subvector with zeros.\n  int SubvecElts = 0;\n  int Src = -1;\n  for (int i = 0; i != NumElts; ++i) {\n    if (Mask[i] >= 0) {\n      // Grab the source from the first valid mask. All subsequent elements need\n      // to use this same source.\n      if (Src < 0)\n        Src = Mask[i] / NumElts;\n      if (Src != (Mask[i] / NumElts) || (Mask[i] % NumElts) != i)\n        break;\n    }\n\n    ++SubvecElts;\n  }\n  assert(SubvecElts != NumElts && \"Identity shuffle?\");\n\n  // Clip to a power 2.\n  SubvecElts = PowerOf2Floor(SubvecElts);\n\n  // Make sure the number of zeroable bits in the top at least covers the bits\n  // not covered by the subvector.\n  if ((int)Zeroable.countLeadingOnes() >= (NumElts - SubvecElts)) {\n    assert(Src >= 0 && \"Expected a source!\");\n    MVT ExtractVT = MVT::getVectorVT(MVT::i1, SubvecElts);\n    SDValue Extract = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, ExtractVT,\n                                  Src == 0 ? V1 : V2,\n                                  DAG.getIntPtrConstant(0, DL));\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                       DAG.getConstant(0, DL, VT),\n                       Extract, DAG.getIntPtrConstant(0, DL));\n  }\n\n  // Try a simple shift right with undef elements. Later we'll try with zeros.\n  if (SDValue Shift = lower1BitShuffleAsKSHIFTR(DL, Mask, VT, V1, V2, Subtarget,\n                                                DAG))\n    return Shift;\n\n  // Try to match KSHIFTs.\n  unsigned Offset = 0;\n  for (SDValue V : { V1, V2 }) {\n    unsigned Opcode;\n    int ShiftAmt = match1BitShuffleAsKSHIFT(Opcode, Mask, Offset, Zeroable);\n    if (ShiftAmt >= 0) {\n      MVT WideVT = VT;\n      if ((!Subtarget.hasDQI() && NumElts == 8) || NumElts < 8)\n        WideVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n      SDValue Res = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, WideVT,\n                                DAG.getUNDEF(WideVT), V,\n                                DAG.getIntPtrConstant(0, DL));\n      // Widened right shifts need two shifts to ensure we shift in zeroes.\n      if (Opcode == X86ISD::KSHIFTR && WideVT != VT) {\n        int WideElts = WideVT.getVectorNumElements();\n        // Shift left to put the original vector in the MSBs of the new size.\n        Res = DAG.getNode(X86ISD::KSHIFTL, DL, WideVT, Res,\n                          DAG.getTargetConstant(WideElts - NumElts, DL, MVT::i8));\n        // Increase the shift amount to account for the left shift.\n        ShiftAmt += WideElts - NumElts;\n      }\n\n      Res = DAG.getNode(Opcode, DL, WideVT, Res,\n                        DAG.getTargetConstant(ShiftAmt, DL, MVT::i8));\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                         DAG.getIntPtrConstant(0, DL));\n    }\n    Offset += NumElts; // Increment for next iteration.\n  }\n\n\n\n  MVT ExtVT;\n  switch (VT.SimpleTy) {\n  default:\n    llvm_unreachable(\"Expected a vector of i1 elements\");\n  case MVT::v2i1:\n    ExtVT = MVT::v2i64;\n    break;\n  case MVT::v4i1:\n    ExtVT = MVT::v4i32;\n    break;\n  case MVT::v8i1:\n    // Take 512-bit type, more shuffles on KNL. If we have VLX use a 256-bit\n    // shuffle.\n    ExtVT = Subtarget.hasVLX() ? MVT::v8i32 : MVT::v8i64;\n    break;\n  case MVT::v16i1:\n    // Take 512-bit type, unless we are avoiding 512-bit types and have the\n    // 256-bit operation available.\n    ExtVT = Subtarget.canExtendTo512DQ() ? MVT::v16i32 : MVT::v16i16;\n    break;\n  case MVT::v32i1:\n    // Take 512-bit type, unless we are avoiding 512-bit types and have the\n    // 256-bit operation available.\n    assert(Subtarget.hasBWI() && \"Expected AVX512BW support\");\n    ExtVT = Subtarget.canExtendTo512BW() ? MVT::v32i16 : MVT::v32i8;\n    break;\n  case MVT::v64i1:\n    // Fall back to scalarization. FIXME: We can do better if the shuffle\n    // can be partitioned cleanly.\n    if (!Subtarget.useBWIRegs())\n      return SDValue();\n    ExtVT = MVT::v64i8;\n    break;\n  }\n\n  V1 = DAG.getNode(ISD::SIGN_EXTEND, DL, ExtVT, V1);\n  V2 = DAG.getNode(ISD::SIGN_EXTEND, DL, ExtVT, V2);\n\n  SDValue Shuffle = DAG.getVectorShuffle(ExtVT, DL, V1, V2, Mask);\n  // i1 was sign extended we can use X86ISD::CVT2MASK.\n  int NumElems = VT.getVectorNumElements();\n  if ((Subtarget.hasBWI() && (NumElems >= 32)) ||\n      (Subtarget.hasDQI() && (NumElems < 32)))\n    return DAG.getSetCC(DL, VT, DAG.getConstant(0, DL, ExtVT),\n                       Shuffle, ISD::SETGT);\n\n  return DAG.getNode(ISD::TRUNCATE, DL, VT, Shuffle);\n}\n\n/// Helper function that returns true if the shuffle mask should be\n/// commuted to improve canonicalization.\nstatic bool canonicalizeShuffleMaskWithCommute(ArrayRef<int> Mask) {\n  int NumElements = Mask.size();\n\n  int NumV1Elements = 0, NumV2Elements = 0;\n  for (int M : Mask)\n    if (M < 0)\n      continue;\n    else if (M < NumElements)\n      ++NumV1Elements;\n    else\n      ++NumV2Elements;\n\n  // Commute the shuffle as needed such that more elements come from V1 than\n  // V2. This allows us to match the shuffle pattern strictly on how many\n  // elements come from V1 without handling the symmetric cases.\n  if (NumV2Elements > NumV1Elements)\n    return true;\n\n  assert(NumV1Elements > 0 && \"No V1 indices\");\n\n  if (NumV2Elements == 0)\n    return false;\n\n  // When the number of V1 and V2 elements are the same, try to minimize the\n  // number of uses of V2 in the low half of the vector. When that is tied,\n  // ensure that the sum of indices for V1 is equal to or lower than the sum\n  // indices for V2. When those are equal, try to ensure that the number of odd\n  // indices for V1 is lower than the number of odd indices for V2.\n  if (NumV1Elements == NumV2Elements) {\n    int LowV1Elements = 0, LowV2Elements = 0;\n    for (int M : Mask.slice(0, NumElements / 2))\n      if (M >= NumElements)\n        ++LowV2Elements;\n      else if (M >= 0)\n        ++LowV1Elements;\n    if (LowV2Elements > LowV1Elements)\n      return true;\n    if (LowV2Elements == LowV1Elements) {\n      int SumV1Indices = 0, SumV2Indices = 0;\n      for (int i = 0, Size = Mask.size(); i < Size; ++i)\n        if (Mask[i] >= NumElements)\n          SumV2Indices += i;\n        else if (Mask[i] >= 0)\n          SumV1Indices += i;\n      if (SumV2Indices < SumV1Indices)\n        return true;\n      if (SumV2Indices == SumV1Indices) {\n        int NumV1OddIndices = 0, NumV2OddIndices = 0;\n        for (int i = 0, Size = Mask.size(); i < Size; ++i)\n          if (Mask[i] >= NumElements)\n            NumV2OddIndices += i % 2;\n          else if (Mask[i] >= 0)\n            NumV1OddIndices += i % 2;\n        if (NumV2OddIndices < NumV1OddIndices)\n          return true;\n      }\n    }\n  }\n\n  return false;\n}\n\n/// Top-level lowering for x86 vector shuffles.\n///\n/// This handles decomposition, canonicalization, and lowering of all x86\n/// vector shuffles. Most of the specific lowering strategies are encapsulated\n/// above in helper routines. The canonicalization attempts to widen shuffles\n/// to involve fewer lanes of wider elements, consolidate symmetric patterns\n/// s.t. only one of the two inputs needs to be tested, etc.\nstatic SDValue lowerVECTOR_SHUFFLE(SDValue Op, const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  ShuffleVectorSDNode *SVOp = cast<ShuffleVectorSDNode>(Op);\n  ArrayRef<int> OrigMask = SVOp->getMask();\n  SDValue V1 = Op.getOperand(0);\n  SDValue V2 = Op.getOperand(1);\n  MVT VT = Op.getSimpleValueType();\n  int NumElements = VT.getVectorNumElements();\n  SDLoc DL(Op);\n  bool Is1BitVector = (VT.getVectorElementType() == MVT::i1);\n\n  assert((VT.getSizeInBits() != 64 || Is1BitVector) &&\n         \"Can't lower MMX shuffles\");\n\n  bool V1IsUndef = V1.isUndef();\n  bool V2IsUndef = V2.isUndef();\n  if (V1IsUndef && V2IsUndef)\n    return DAG.getUNDEF(VT);\n\n  // When we create a shuffle node we put the UNDEF node to second operand,\n  // but in some cases the first operand may be transformed to UNDEF.\n  // In this case we should just commute the node.\n  if (V1IsUndef)\n    return DAG.getCommutedVectorShuffle(*SVOp);\n\n  // Check for non-undef masks pointing at an undef vector and make the masks\n  // undef as well. This makes it easier to match the shuffle based solely on\n  // the mask.\n  if (V2IsUndef &&\n      any_of(OrigMask, [NumElements](int M) { return M >= NumElements; })) {\n    SmallVector<int, 8> NewMask(OrigMask.begin(), OrigMask.end());\n    for (int &M : NewMask)\n      if (M >= NumElements)\n        M = -1;\n    return DAG.getVectorShuffle(VT, DL, V1, V2, NewMask);\n  }\n\n  // Check for illegal shuffle mask element index values.\n  int MaskUpperLimit = OrigMask.size() * (V2IsUndef ? 1 : 2);\n  (void)MaskUpperLimit;\n  assert(llvm::all_of(OrigMask,\n                      [&](int M) { return -1 <= M && M < MaskUpperLimit; }) &&\n         \"Out of bounds shuffle index\");\n\n  // We actually see shuffles that are entirely re-arrangements of a set of\n  // zero inputs. This mostly happens while decomposing complex shuffles into\n  // simple ones. Directly lower these as a buildvector of zeros.\n  APInt KnownUndef, KnownZero;\n  computeZeroableShuffleElements(OrigMask, V1, V2, KnownUndef, KnownZero);\n\n  APInt Zeroable = KnownUndef | KnownZero;\n  if (Zeroable.isAllOnesValue())\n    return getZeroVector(VT, Subtarget, DAG, DL);\n\n  bool V2IsZero = !V2IsUndef && ISD::isBuildVectorAllZeros(V2.getNode());\n\n  // Try to collapse shuffles into using a vector type with fewer elements but\n  // wider element types. We cap this to not form integers or floating point\n  // elements wider than 64 bits, but it might be interesting to form i128\n  // integers to handle flipping the low and high halves of AVX 256-bit vectors.\n  SmallVector<int, 16> WidenedMask;\n  if (VT.getScalarSizeInBits() < 64 && !Is1BitVector &&\n      canWidenShuffleElements(OrigMask, Zeroable, V2IsZero, WidenedMask)) {\n    // Shuffle mask widening should not interfere with a broadcast opportunity\n    // by obfuscating the operands with bitcasts.\n    // TODO: Avoid lowering directly from this top-level function: make this\n    // a query (canLowerAsBroadcast) and defer lowering to the type-based calls.\n    if (SDValue Broadcast = lowerShuffleAsBroadcast(DL, VT, V1, V2, OrigMask,\n                                                    Subtarget, DAG))\n      return Broadcast;\n\n    MVT NewEltVT = VT.isFloatingPoint()\n                       ? MVT::getFloatingPointVT(VT.getScalarSizeInBits() * 2)\n                       : MVT::getIntegerVT(VT.getScalarSizeInBits() * 2);\n    int NewNumElts = NumElements / 2;\n    MVT NewVT = MVT::getVectorVT(NewEltVT, NewNumElts);\n    // Make sure that the new vector type is legal. For example, v2f64 isn't\n    // legal on SSE1.\n    if (DAG.getTargetLoweringInfo().isTypeLegal(NewVT)) {\n      if (V2IsZero) {\n        // Modify the new Mask to take all zeros from the all-zero vector.\n        // Choose indices that are blend-friendly.\n        bool UsedZeroVector = false;\n        assert(is_contained(WidenedMask, SM_SentinelZero) &&\n               \"V2's non-undef elements are used?!\");\n        for (int i = 0; i != NewNumElts; ++i)\n          if (WidenedMask[i] == SM_SentinelZero) {\n            WidenedMask[i] = i + NewNumElts;\n            UsedZeroVector = true;\n          }\n        // Ensure all elements of V2 are zero - isBuildVectorAllZeros permits\n        // some elements to be undef.\n        if (UsedZeroVector)\n          V2 = getZeroVector(NewVT, Subtarget, DAG, DL);\n      }\n      V1 = DAG.getBitcast(NewVT, V1);\n      V2 = DAG.getBitcast(NewVT, V2);\n      return DAG.getBitcast(\n          VT, DAG.getVectorShuffle(NewVT, DL, V1, V2, WidenedMask));\n    }\n  }\n\n  // Commute the shuffle if it will improve canonicalization.\n  SmallVector<int, 64> Mask(OrigMask.begin(), OrigMask.end());\n  if (canonicalizeShuffleMaskWithCommute(Mask)) {\n    ShuffleVectorSDNode::commuteMask(Mask);\n    std::swap(V1, V2);\n  }\n\n  // For each vector width, delegate to a specialized lowering routine.\n  if (VT.is128BitVector())\n    return lower128BitShuffle(DL, Mask, VT, V1, V2, Zeroable, Subtarget, DAG);\n\n  if (VT.is256BitVector())\n    return lower256BitShuffle(DL, Mask, VT, V1, V2, Zeroable, Subtarget, DAG);\n\n  if (VT.is512BitVector())\n    return lower512BitShuffle(DL, Mask, VT, V1, V2, Zeroable, Subtarget, DAG);\n\n  if (Is1BitVector)\n    return lower1BitShuffle(DL, Mask, VT, V1, V2, Zeroable, Subtarget, DAG);\n\n  llvm_unreachable(\"Unimplemented!\");\n}\n\n/// Try to lower a VSELECT instruction to a vector shuffle.\nstatic SDValue lowerVSELECTtoVectorShuffle(SDValue Op,\n                                           const X86Subtarget &Subtarget,\n                                           SelectionDAG &DAG) {\n  SDValue Cond = Op.getOperand(0);\n  SDValue LHS = Op.getOperand(1);\n  SDValue RHS = Op.getOperand(2);\n  MVT VT = Op.getSimpleValueType();\n\n  // Only non-legal VSELECTs reach this lowering, convert those into generic\n  // shuffles and re-use the shuffle lowering path for blends.\n  if (ISD::isBuildVectorOfConstantSDNodes(Cond.getNode())) {\n    SmallVector<int, 32> Mask;\n    if (createShuffleMaskFromVSELECT(Mask, Cond))\n      return DAG.getVectorShuffle(VT, SDLoc(Op), LHS, RHS, Mask);\n  }\n\n  return SDValue();\n}\n\nSDValue X86TargetLowering::LowerVSELECT(SDValue Op, SelectionDAG &DAG) const {\n  SDValue Cond = Op.getOperand(0);\n  SDValue LHS = Op.getOperand(1);\n  SDValue RHS = Op.getOperand(2);\n\n  // A vselect where all conditions and data are constants can be optimized into\n  // a single vector load by SelectionDAGLegalize::ExpandBUILD_VECTOR().\n  if (ISD::isBuildVectorOfConstantSDNodes(Cond.getNode()) &&\n      ISD::isBuildVectorOfConstantSDNodes(LHS.getNode()) &&\n      ISD::isBuildVectorOfConstantSDNodes(RHS.getNode()))\n    return SDValue();\n\n  // Try to lower this to a blend-style vector shuffle. This can handle all\n  // constant condition cases.\n  if (SDValue BlendOp = lowerVSELECTtoVectorShuffle(Op, Subtarget, DAG))\n    return BlendOp;\n\n  // If this VSELECT has a vector if i1 as a mask, it will be directly matched\n  // with patterns on the mask registers on AVX-512.\n  MVT CondVT = Cond.getSimpleValueType();\n  unsigned CondEltSize = Cond.getScalarValueSizeInBits();\n  if (CondEltSize == 1)\n    return Op;\n\n  // Variable blends are only legal from SSE4.1 onward.\n  if (!Subtarget.hasSSE41())\n    return SDValue();\n\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n  unsigned EltSize = VT.getScalarSizeInBits();\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // Expand v32i16/v64i8 without BWI.\n  if ((VT == MVT::v32i16 || VT == MVT::v64i8) && !Subtarget.hasBWI())\n    return SDValue();\n\n  // If the VSELECT is on a 512-bit type, we have to convert a non-i1 condition\n  // into an i1 condition so that we can use the mask-based 512-bit blend\n  // instructions.\n  if (VT.getSizeInBits() == 512) {\n    // Build a mask by testing the condition against zero.\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, NumElts);\n    SDValue Mask = DAG.getSetCC(dl, MaskVT, Cond,\n                                DAG.getConstant(0, dl, CondVT),\n                                ISD::SETNE);\n    // Now return a new VSELECT using the mask.\n    return DAG.getSelect(dl, VT, Mask, LHS, RHS);\n  }\n\n  // SEXT/TRUNC cases where the mask doesn't match the destination size.\n  if (CondEltSize != EltSize) {\n    // If we don't have a sign splat, rely on the expansion.\n    if (CondEltSize != DAG.ComputeNumSignBits(Cond))\n      return SDValue();\n\n    MVT NewCondSVT = MVT::getIntegerVT(EltSize);\n    MVT NewCondVT = MVT::getVectorVT(NewCondSVT, NumElts);\n    Cond = DAG.getSExtOrTrunc(Cond, dl, NewCondVT);\n    return DAG.getNode(ISD::VSELECT, dl, VT, Cond, LHS, RHS);\n  }\n\n  // Only some types will be legal on some subtargets. If we can emit a legal\n  // VSELECT-matching blend, return Op, and but if we need to expand, return\n  // a null value.\n  switch (VT.SimpleTy) {\n  default:\n    // Most of the vector types have blends past SSE4.1.\n    return Op;\n\n  case MVT::v32i8:\n    // The byte blends for AVX vectors were introduced only in AVX2.\n    if (Subtarget.hasAVX2())\n      return Op;\n\n    return SDValue();\n\n  case MVT::v8i16:\n  case MVT::v16i16: {\n    // Bitcast everything to the vXi8 type and use a vXi8 vselect.\n    MVT CastVT = MVT::getVectorVT(MVT::i8, NumElts * 2);\n    Cond = DAG.getBitcast(CastVT, Cond);\n    LHS = DAG.getBitcast(CastVT, LHS);\n    RHS = DAG.getBitcast(CastVT, RHS);\n    SDValue Select = DAG.getNode(ISD::VSELECT, dl, CastVT, Cond, LHS, RHS);\n    return DAG.getBitcast(VT, Select);\n  }\n  }\n}\n\nstatic SDValue LowerEXTRACT_VECTOR_ELT_SSE4(SDValue Op, SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  SDValue Vec = Op.getOperand(0);\n  SDValue Idx = Op.getOperand(1);\n  assert(isa<ConstantSDNode>(Idx) && \"Constant index expected\");\n  SDLoc dl(Op);\n\n  if (!Vec.getSimpleValueType().is128BitVector())\n    return SDValue();\n\n  if (VT.getSizeInBits() == 8) {\n    // If IdxVal is 0, it's cheaper to do a move instead of a pextrb, unless\n    // we're going to zero extend the register or fold the store.\n    if (llvm::isNullConstant(Idx) && !MayFoldIntoZeroExtend(Op) &&\n        !MayFoldIntoStore(Op))\n      return DAG.getNode(ISD::TRUNCATE, dl, MVT::i8,\n                         DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i32,\n                                     DAG.getBitcast(MVT::v4i32, Vec), Idx));\n\n    unsigned IdxVal = cast<ConstantSDNode>(Idx)->getZExtValue();\n    SDValue Extract = DAG.getNode(X86ISD::PEXTRB, dl, MVT::i32, Vec,\n                                  DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n    return DAG.getNode(ISD::TRUNCATE, dl, VT, Extract);\n  }\n\n  if (VT == MVT::f32) {\n    // EXTRACTPS outputs to a GPR32 register which will require a movd to copy\n    // the result back to FR32 register. It's only worth matching if the\n    // result has a single use which is a store or a bitcast to i32.  And in\n    // the case of a store, it's not worth it if the index is a constant 0,\n    // because a MOVSSmr can be used instead, which is smaller and faster.\n    if (!Op.hasOneUse())\n      return SDValue();\n    SDNode *User = *Op.getNode()->use_begin();\n    if ((User->getOpcode() != ISD::STORE || isNullConstant(Idx)) &&\n        (User->getOpcode() != ISD::BITCAST ||\n         User->getValueType(0) != MVT::i32))\n      return SDValue();\n    SDValue Extract = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i32,\n                                  DAG.getBitcast(MVT::v4i32, Vec), Idx);\n    return DAG.getBitcast(MVT::f32, Extract);\n  }\n\n  if (VT == MVT::i32 || VT == MVT::i64)\n      return Op;\n\n  return SDValue();\n}\n\n/// Extract one bit from mask vector, like v16i1 or v8i1.\n/// AVX-512 feature.\nstatic SDValue ExtractBitFromMaskVector(SDValue Op, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  SDValue Vec = Op.getOperand(0);\n  SDLoc dl(Vec);\n  MVT VecVT = Vec.getSimpleValueType();\n  SDValue Idx = Op.getOperand(1);\n  auto* IdxC = dyn_cast<ConstantSDNode>(Idx);\n  MVT EltVT = Op.getSimpleValueType();\n\n  assert((VecVT.getVectorNumElements() <= 16 || Subtarget.hasBWI()) &&\n         \"Unexpected vector type in ExtractBitFromMaskVector\");\n\n  // variable index can't be handled in mask registers,\n  // extend vector to VR512/128\n  if (!IdxC) {\n    unsigned NumElts = VecVT.getVectorNumElements();\n    // Extending v8i1/v16i1 to 512-bit get better performance on KNL\n    // than extending to 128/256bit.\n    MVT ExtEltVT = (NumElts <= 8) ? MVT::getIntegerVT(128 / NumElts) : MVT::i8;\n    MVT ExtVecVT = MVT::getVectorVT(ExtEltVT, NumElts);\n    SDValue Ext = DAG.getNode(ISD::SIGN_EXTEND, dl, ExtVecVT, Vec);\n    SDValue Elt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, ExtEltVT, Ext, Idx);\n    return DAG.getNode(ISD::TRUNCATE, dl, EltVT, Elt);\n  }\n\n  unsigned IdxVal = IdxC->getZExtValue();\n  if (IdxVal == 0) // the operation is legal\n    return Op;\n\n  // Extend to natively supported kshift.\n  unsigned NumElems = VecVT.getVectorNumElements();\n  MVT WideVecVT = VecVT;\n  if ((!Subtarget.hasDQI() && NumElems == 8) || NumElems < 8) {\n    WideVecVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n    Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideVecVT,\n                      DAG.getUNDEF(WideVecVT), Vec,\n                      DAG.getIntPtrConstant(0, dl));\n  }\n\n  // Use kshiftr instruction to move to the lower element.\n  Vec = DAG.getNode(X86ISD::KSHIFTR, dl, WideVecVT, Vec,\n                    DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, Op.getValueType(), Vec,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\nSDValue\nX86TargetLowering::LowerEXTRACT_VECTOR_ELT(SDValue Op,\n                                           SelectionDAG &DAG) const {\n  SDLoc dl(Op);\n  SDValue Vec = Op.getOperand(0);\n  MVT VecVT = Vec.getSimpleValueType();\n  SDValue Idx = Op.getOperand(1);\n  auto* IdxC = dyn_cast<ConstantSDNode>(Idx);\n\n  if (VecVT.getVectorElementType() == MVT::i1)\n    return ExtractBitFromMaskVector(Op, DAG, Subtarget);\n\n  if (!IdxC) {\n    // Its more profitable to go through memory (1 cycles throughput)\n    // than using VMOVD + VPERMV/PSHUFB sequence ( 2/3 cycles throughput)\n    // IACA tool was used to get performance estimation\n    // (https://software.intel.com/en-us/articles/intel-architecture-code-analyzer)\n    //\n    // example : extractelement <16 x i8> %a, i32 %i\n    //\n    // Block Throughput: 3.00 Cycles\n    // Throughput Bottleneck: Port5\n    //\n    // | Num Of |   Ports pressure in cycles  |    |\n    // |  Uops  |  0  - DV  |  5  |  6  |  7  |    |\n    // ---------------------------------------------\n    // |   1    |           | 1.0 |     |     | CP | vmovd xmm1, edi\n    // |   1    |           | 1.0 |     |     | CP | vpshufb xmm0, xmm0, xmm1\n    // |   2    | 1.0       | 1.0 |     |     | CP | vpextrb eax, xmm0, 0x0\n    // Total Num Of Uops: 4\n    //\n    //\n    // Block Throughput: 1.00 Cycles\n    // Throughput Bottleneck: PORT2_AGU, PORT3_AGU, Port4\n    //\n    // |    |  Ports pressure in cycles   |  |\n    // |Uops| 1 | 2 - D  |3 -  D  | 4 | 5 |  |\n    // ---------------------------------------------------------\n    // |2^  |   | 0.5    | 0.5    |1.0|   |CP| vmovaps xmmword ptr [rsp-0x18], xmm0\n    // |1   |0.5|        |        |   |0.5|  | lea rax, ptr [rsp-0x18]\n    // |1   |   |0.5, 0.5|0.5, 0.5|   |   |CP| mov al, byte ptr [rdi+rax*1]\n    // Total Num Of Uops: 4\n\n    return SDValue();\n  }\n\n  unsigned IdxVal = IdxC->getZExtValue();\n\n  // If this is a 256-bit vector result, first extract the 128-bit vector and\n  // then extract the element from the 128-bit vector.\n  if (VecVT.is256BitVector() || VecVT.is512BitVector()) {\n    // Get the 128-bit vector.\n    Vec = extract128BitVector(Vec, IdxVal, DAG, dl);\n    MVT EltVT = VecVT.getVectorElementType();\n\n    unsigned ElemsPerChunk = 128 / EltVT.getSizeInBits();\n    assert(isPowerOf2_32(ElemsPerChunk) && \"Elements per chunk not power of 2\");\n\n    // Find IdxVal modulo ElemsPerChunk. Since ElemsPerChunk is a power of 2\n    // this can be done with a mask.\n    IdxVal &= ElemsPerChunk - 1;\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, Op.getValueType(), Vec,\n                       DAG.getIntPtrConstant(IdxVal, dl));\n  }\n\n  assert(VecVT.is128BitVector() && \"Unexpected vector length\");\n\n  MVT VT = Op.getSimpleValueType();\n\n  if (VT.getSizeInBits() == 16) {\n    // If IdxVal is 0, it's cheaper to do a move instead of a pextrw, unless\n    // we're going to zero extend the register or fold the store (SSE41 only).\n    if (IdxVal == 0 && !MayFoldIntoZeroExtend(Op) &&\n        !(Subtarget.hasSSE41() && MayFoldIntoStore(Op)))\n      return DAG.getNode(ISD::TRUNCATE, dl, MVT::i16,\n                         DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i32,\n                                     DAG.getBitcast(MVT::v4i32, Vec), Idx));\n\n    SDValue Extract = DAG.getNode(X86ISD::PEXTRW, dl, MVT::i32, Vec,\n                                  DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n    return DAG.getNode(ISD::TRUNCATE, dl, VT, Extract);\n  }\n\n  if (Subtarget.hasSSE41())\n    if (SDValue Res = LowerEXTRACT_VECTOR_ELT_SSE4(Op, DAG))\n      return Res;\n\n  // TODO: We only extract a single element from v16i8, we can probably afford\n  // to be more aggressive here before using the default approach of spilling to\n  // stack.\n  if (VT.getSizeInBits() == 8 && Op->isOnlyUserOf(Vec.getNode())) {\n    // Extract either the lowest i32 or any i16, and extract the sub-byte.\n    int DWordIdx = IdxVal / 4;\n    if (DWordIdx == 0) {\n      SDValue Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i32,\n                                DAG.getBitcast(MVT::v4i32, Vec),\n                                DAG.getIntPtrConstant(DWordIdx, dl));\n      int ShiftVal = (IdxVal % 4) * 8;\n      if (ShiftVal != 0)\n        Res = DAG.getNode(ISD::SRL, dl, MVT::i32, Res,\n                          DAG.getConstant(ShiftVal, dl, MVT::i8));\n      return DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n    }\n\n    int WordIdx = IdxVal / 2;\n    SDValue Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i16,\n                              DAG.getBitcast(MVT::v8i16, Vec),\n                              DAG.getIntPtrConstant(WordIdx, dl));\n    int ShiftVal = (IdxVal % 2) * 8;\n    if (ShiftVal != 0)\n      Res = DAG.getNode(ISD::SRL, dl, MVT::i16, Res,\n                        DAG.getConstant(ShiftVal, dl, MVT::i8));\n    return DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n  }\n\n  if (VT.getSizeInBits() == 32) {\n    if (IdxVal == 0)\n      return Op;\n\n    // SHUFPS the element to the lowest double word, then movss.\n    int Mask[4] = { static_cast<int>(IdxVal), -1, -1, -1 };\n    Vec = DAG.getVectorShuffle(VecVT, dl, Vec, DAG.getUNDEF(VecVT), Mask);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, Vec,\n                       DAG.getIntPtrConstant(0, dl));\n  }\n\n  if (VT.getSizeInBits() == 64) {\n    // FIXME: .td only matches this for <2 x f64>, not <2 x i64> on 32b\n    // FIXME: seems like this should be unnecessary if mov{h,l}pd were taught\n    //        to match extract_elt for f64.\n    if (IdxVal == 0)\n      return Op;\n\n    // UNPCKHPD the element to the lowest double word, then movsd.\n    // Note if the lower 64 bits of the result of the UNPCKHPD is then stored\n    // to a f64mem, the whole operation is folded into a single MOVHPDmr.\n    int Mask[2] = { 1, -1 };\n    Vec = DAG.getVectorShuffle(VecVT, dl, Vec, DAG.getUNDEF(VecVT), Mask);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, Vec,\n                       DAG.getIntPtrConstant(0, dl));\n  }\n\n  return SDValue();\n}\n\n/// Insert one bit to mask vector, like v16i1 or v8i1.\n/// AVX-512 feature.\nstatic SDValue InsertBitToMaskVector(SDValue Op, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  SDLoc dl(Op);\n  SDValue Vec = Op.getOperand(0);\n  SDValue Elt = Op.getOperand(1);\n  SDValue Idx = Op.getOperand(2);\n  MVT VecVT = Vec.getSimpleValueType();\n\n  if (!isa<ConstantSDNode>(Idx)) {\n    // Non constant index. Extend source and destination,\n    // insert element and then truncate the result.\n    unsigned NumElts = VecVT.getVectorNumElements();\n    MVT ExtEltVT = (NumElts <= 8) ? MVT::getIntegerVT(128 / NumElts) : MVT::i8;\n    MVT ExtVecVT = MVT::getVectorVT(ExtEltVT, NumElts);\n    SDValue ExtOp = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, ExtVecVT,\n      DAG.getNode(ISD::SIGN_EXTEND, dl, ExtVecVT, Vec),\n      DAG.getNode(ISD::SIGN_EXTEND, dl, ExtEltVT, Elt), Idx);\n    return DAG.getNode(ISD::TRUNCATE, dl, VecVT, ExtOp);\n  }\n\n  // Copy into a k-register, extract to v1i1 and insert_subvector.\n  SDValue EltInVec = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v1i1, Elt);\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, VecVT, Vec, EltInVec, Idx);\n}\n\nSDValue X86TargetLowering::LowerINSERT_VECTOR_ELT(SDValue Op,\n                                                  SelectionDAG &DAG) const {\n  MVT VT = Op.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  unsigned NumElts = VT.getVectorNumElements();\n\n  if (EltVT == MVT::i1)\n    return InsertBitToMaskVector(Op, DAG, Subtarget);\n\n  SDLoc dl(Op);\n  SDValue N0 = Op.getOperand(0);\n  SDValue N1 = Op.getOperand(1);\n  SDValue N2 = Op.getOperand(2);\n\n  auto *N2C = dyn_cast<ConstantSDNode>(N2);\n  if (!N2C || N2C->getAPIntValue().uge(NumElts))\n    return SDValue();\n  uint64_t IdxVal = N2C->getZExtValue();\n\n  bool IsZeroElt = X86::isZeroNode(N1);\n  bool IsAllOnesElt = VT.isInteger() && llvm::isAllOnesConstant(N1);\n\n  // If we are inserting a element, see if we can do this more efficiently with\n  // a blend shuffle with a rematerializable vector than a costly integer\n  // insertion.\n  if ((IsZeroElt || IsAllOnesElt) && Subtarget.hasSSE41() &&\n      16 <= EltVT.getSizeInBits()) {\n    SmallVector<int, 8> BlendMask;\n    for (unsigned i = 0; i != NumElts; ++i)\n      BlendMask.push_back(i == IdxVal ? i + NumElts : i);\n    SDValue CstVector = IsZeroElt ? getZeroVector(VT, Subtarget, DAG, dl)\n                                  : getOnesVector(VT, DAG, dl);\n    return DAG.getVectorShuffle(VT, dl, N0, CstVector, BlendMask);\n  }\n\n  // If the vector is wider than 128 bits, extract the 128-bit subvector, insert\n  // into that, and then insert the subvector back into the result.\n  if (VT.is256BitVector() || VT.is512BitVector()) {\n    // With a 256-bit vector, we can insert into the zero element efficiently\n    // using a blend if we have AVX or AVX2 and the right data type.\n    if (VT.is256BitVector() && IdxVal == 0) {\n      // TODO: It is worthwhile to cast integer to floating point and back\n      // and incur a domain crossing penalty if that's what we'll end up\n      // doing anyway after extracting to a 128-bit vector.\n      if ((Subtarget.hasAVX() && (EltVT == MVT::f64 || EltVT == MVT::f32)) ||\n          (Subtarget.hasAVX2() && EltVT == MVT::i32)) {\n        SDValue N1Vec = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, N1);\n        return DAG.getNode(X86ISD::BLENDI, dl, VT, N0, N1Vec,\n                           DAG.getTargetConstant(1, dl, MVT::i8));\n      }\n    }\n\n    // Get the desired 128-bit vector chunk.\n    SDValue V = extract128BitVector(N0, IdxVal, DAG, dl);\n\n    // Insert the element into the desired chunk.\n    unsigned NumEltsIn128 = 128 / EltVT.getSizeInBits();\n    assert(isPowerOf2_32(NumEltsIn128));\n    // Since NumEltsIn128 is a power of 2 we can use mask instead of modulo.\n    unsigned IdxIn128 = IdxVal & (NumEltsIn128 - 1);\n\n    V = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, V.getValueType(), V, N1,\n                    DAG.getIntPtrConstant(IdxIn128, dl));\n\n    // Insert the changed part back into the bigger vector\n    return insert128BitVector(N0, V, IdxVal, DAG, dl);\n  }\n  assert(VT.is128BitVector() && \"Only 128-bit vector types should be left!\");\n\n  // This will be just movd/movq/movss/movsd.\n  if (IdxVal == 0 && ISD::isBuildVectorAllZeros(N0.getNode())) {\n    if (EltVT == MVT::i32 || EltVT == MVT::f32 || EltVT == MVT::f64 ||\n        EltVT == MVT::i64) {\n      N1 = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT, N1);\n      return getShuffleVectorZeroOrUndef(N1, 0, true, Subtarget, DAG);\n    }\n\n    // We can't directly insert an i8 or i16 into a vector, so zero extend\n    // it to i32 first.\n    if (EltVT == MVT::i16 || EltVT == MVT::i8) {\n      N1 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, N1);\n      MVT ShufVT = MVT::getVectorVT(MVT::i32, VT.getSizeInBits()/32);\n      N1 = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, ShufVT, N1);\n      N1 = getShuffleVectorZeroOrUndef(N1, 0, true, Subtarget, DAG);\n      return DAG.getBitcast(VT, N1);\n    }\n  }\n\n  // Transform it so it match pinsr{b,w} which expects a GR32 as its second\n  // argument. SSE41 required for pinsrb.\n  if (VT == MVT::v8i16 || (VT == MVT::v16i8 && Subtarget.hasSSE41())) {\n    unsigned Opc;\n    if (VT == MVT::v8i16) {\n      assert(Subtarget.hasSSE2() && \"SSE2 required for PINSRW\");\n      Opc = X86ISD::PINSRW;\n    } else {\n      assert(VT == MVT::v16i8 && \"PINSRB requires v16i8 vector\");\n      assert(Subtarget.hasSSE41() && \"SSE41 required for PINSRB\");\n      Opc = X86ISD::PINSRB;\n    }\n\n    assert(N1.getValueType() != MVT::i32 && \"Unexpected VT\");\n    N1 = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i32, N1);\n    N2 = DAG.getTargetConstant(IdxVal, dl, MVT::i8);\n    return DAG.getNode(Opc, dl, VT, N0, N1, N2);\n  }\n\n  if (Subtarget.hasSSE41()) {\n    if (EltVT == MVT::f32) {\n      // Bits [7:6] of the constant are the source select. This will always be\n      //   zero here. The DAG Combiner may combine an extract_elt index into\n      //   these bits. For example (insert (extract, 3), 2) could be matched by\n      //   putting the '3' into bits [7:6] of X86ISD::INSERTPS.\n      // Bits [5:4] of the constant are the destination select. This is the\n      //   value of the incoming immediate.\n      // Bits [3:0] of the constant are the zero mask. The DAG Combiner may\n      //   combine either bitwise AND or insert of float 0.0 to set these bits.\n\n      bool MinSize = DAG.getMachineFunction().getFunction().hasMinSize();\n      if (IdxVal == 0 && (!MinSize || !MayFoldLoad(N1))) {\n        // If this is an insertion of 32-bits into the low 32-bits of\n        // a vector, we prefer to generate a blend with immediate rather\n        // than an insertps. Blends are simpler operations in hardware and so\n        // will always have equal or better performance than insertps.\n        // But if optimizing for size and there's a load folding opportunity,\n        // generate insertps because blendps does not have a 32-bit memory\n        // operand form.\n        N1 = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4f32, N1);\n        return DAG.getNode(X86ISD::BLENDI, dl, VT, N0, N1,\n                           DAG.getTargetConstant(1, dl, MVT::i8));\n      }\n      // Create this as a scalar to vector..\n      N1 = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4f32, N1);\n      return DAG.getNode(X86ISD::INSERTPS, dl, VT, N0, N1,\n                         DAG.getTargetConstant(IdxVal << 4, dl, MVT::i8));\n    }\n\n    // PINSR* works with constant index.\n    if (EltVT == MVT::i32 || EltVT == MVT::i64)\n      return Op;\n  }\n\n  return SDValue();\n}\n\nstatic SDValue LowerSCALAR_TO_VECTOR(SDValue Op, const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  MVT OpVT = Op.getSimpleValueType();\n\n  // It's always cheaper to replace a xor+movd with xorps and simplifies further\n  // combines.\n  if (X86::isZeroNode(Op.getOperand(0)))\n    return getZeroVector(OpVT, Subtarget, DAG, dl);\n\n  // If this is a 256-bit vector result, first insert into a 128-bit\n  // vector and then insert into the 256-bit vector.\n  if (!OpVT.is128BitVector()) {\n    // Insert into a 128-bit vector.\n    unsigned SizeFactor = OpVT.getSizeInBits() / 128;\n    MVT VT128 = MVT::getVectorVT(OpVT.getVectorElementType(),\n                                 OpVT.getVectorNumElements() / SizeFactor);\n\n    Op = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VT128, Op.getOperand(0));\n\n    // Insert the 128-bit vector.\n    return insert128BitVector(DAG.getUNDEF(OpVT), Op, 0, DAG, dl);\n  }\n  assert(OpVT.is128BitVector() && OpVT.isInteger() && OpVT != MVT::v2i64 &&\n         \"Expected an SSE type!\");\n\n  // Pass through a v4i32 SCALAR_TO_VECTOR as that's what we use in tblgen.\n  if (OpVT == MVT::v4i32)\n    return Op;\n\n  SDValue AnyExt = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i32, Op.getOperand(0));\n  return DAG.getBitcast(\n      OpVT, DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4i32, AnyExt));\n}\n\n// Lower a node with an INSERT_SUBVECTOR opcode.  This may result in a\n// simple superregister reference or explicit instructions to insert\n// the upper bits of a vector.\nstatic SDValue LowerINSERT_SUBVECTOR(SDValue Op, const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  assert(Op.getSimpleValueType().getVectorElementType() == MVT::i1);\n\n  return insert1BitVector(Op, DAG, Subtarget);\n}\n\nstatic SDValue LowerEXTRACT_SUBVECTOR(SDValue Op, const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  assert(Op.getSimpleValueType().getVectorElementType() == MVT::i1 &&\n         \"Only vXi1 extract_subvectors need custom lowering\");\n\n  SDLoc dl(Op);\n  SDValue Vec = Op.getOperand(0);\n  uint64_t IdxVal = Op.getConstantOperandVal(1);\n\n  if (IdxVal == 0) // the operation is legal\n    return Op;\n\n  MVT VecVT = Vec.getSimpleValueType();\n  unsigned NumElems = VecVT.getVectorNumElements();\n\n  // Extend to natively supported kshift.\n  MVT WideVecVT = VecVT;\n  if ((!Subtarget.hasDQI() && NumElems == 8) || NumElems < 8) {\n    WideVecVT = Subtarget.hasDQI() ? MVT::v8i1 : MVT::v16i1;\n    Vec = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideVecVT,\n                      DAG.getUNDEF(WideVecVT), Vec,\n                      DAG.getIntPtrConstant(0, dl));\n  }\n\n  // Shift to the LSB.\n  Vec = DAG.getNode(X86ISD::KSHIFTR, dl, WideVecVT, Vec,\n                    DAG.getTargetConstant(IdxVal, dl, MVT::i8));\n\n  return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, Op.getValueType(), Vec,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\n// Returns the appropriate wrapper opcode for a global reference.\nunsigned X86TargetLowering::getGlobalWrapperKind(\n    const GlobalValue *GV, const unsigned char OpFlags) const {\n  // References to absolute symbols are never PC-relative.\n  if (GV && GV->isAbsoluteSymbolRef())\n    return X86ISD::Wrapper;\n\n  CodeModel::Model M = getTargetMachine().getCodeModel();\n  if (Subtarget.isPICStyleRIPRel() &&\n      (M == CodeModel::Small || M == CodeModel::Kernel))\n    return X86ISD::WrapperRIP;\n\n  // GOTPCREL references must always use RIP.\n  if (OpFlags == X86II::MO_GOTPCREL)\n    return X86ISD::WrapperRIP;\n\n  return X86ISD::Wrapper;\n}\n\n// ConstantPool, JumpTable, GlobalAddress, and ExternalSymbol are lowered as\n// their target counterpart wrapped in the X86ISD::Wrapper node. Suppose N is\n// one of the above mentioned nodes. It has to be wrapped because otherwise\n// Select(N) returns N. So the raw TargetGlobalAddress nodes, etc. can only\n// be used to form addressing mode. These wrapped nodes will be selected\n// into MOV32ri.\nSDValue\nX86TargetLowering::LowerConstantPool(SDValue Op, SelectionDAG &DAG) const {\n  ConstantPoolSDNode *CP = cast<ConstantPoolSDNode>(Op);\n\n  // In PIC mode (unless we're in RIPRel PIC mode) we add an offset to the\n  // global base reg.\n  unsigned char OpFlag = Subtarget.classifyLocalReference(nullptr);\n\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  SDValue Result = DAG.getTargetConstantPool(\n      CP->getConstVal(), PtrVT, CP->getAlign(), CP->getOffset(), OpFlag);\n  SDLoc DL(CP);\n  Result = DAG.getNode(getGlobalWrapperKind(), DL, PtrVT, Result);\n  // With PIC, the address is actually $g + Offset.\n  if (OpFlag) {\n    Result =\n        DAG.getNode(ISD::ADD, DL, PtrVT,\n                    DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(), PtrVT), Result);\n  }\n\n  return Result;\n}\n\nSDValue X86TargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) const {\n  JumpTableSDNode *JT = cast<JumpTableSDNode>(Op);\n\n  // In PIC mode (unless we're in RIPRel PIC mode) we add an offset to the\n  // global base reg.\n  unsigned char OpFlag = Subtarget.classifyLocalReference(nullptr);\n\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  SDValue Result = DAG.getTargetJumpTable(JT->getIndex(), PtrVT, OpFlag);\n  SDLoc DL(JT);\n  Result = DAG.getNode(getGlobalWrapperKind(), DL, PtrVT, Result);\n\n  // With PIC, the address is actually $g + Offset.\n  if (OpFlag)\n    Result =\n        DAG.getNode(ISD::ADD, DL, PtrVT,\n                    DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(), PtrVT), Result);\n\n  return Result;\n}\n\nSDValue X86TargetLowering::LowerExternalSymbol(SDValue Op,\n                                               SelectionDAG &DAG) const {\n  return LowerGlobalOrExternal(Op, DAG, /*ForCall=*/false);\n}\n\nSDValue\nX86TargetLowering::LowerBlockAddress(SDValue Op, SelectionDAG &DAG) const {\n  // Create the TargetBlockAddressAddress node.\n  unsigned char OpFlags =\n    Subtarget.classifyBlockAddressReference();\n  const BlockAddress *BA = cast<BlockAddressSDNode>(Op)->getBlockAddress();\n  int64_t Offset = cast<BlockAddressSDNode>(Op)->getOffset();\n  SDLoc dl(Op);\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  SDValue Result = DAG.getTargetBlockAddress(BA, PtrVT, Offset, OpFlags);\n  Result = DAG.getNode(getGlobalWrapperKind(), dl, PtrVT, Result);\n\n  // With PIC, the address is actually $g + Offset.\n  if (isGlobalRelativeToPICBase(OpFlags)) {\n    Result = DAG.getNode(ISD::ADD, dl, PtrVT,\n                         DAG.getNode(X86ISD::GlobalBaseReg, dl, PtrVT), Result);\n  }\n\n  return Result;\n}\n\n/// Creates target global address or external symbol nodes for calls or\n/// other uses.\nSDValue X86TargetLowering::LowerGlobalOrExternal(SDValue Op, SelectionDAG &DAG,\n                                                 bool ForCall) const {\n  // Unpack the global address or external symbol.\n  const SDLoc &dl = SDLoc(Op);\n  const GlobalValue *GV = nullptr;\n  int64_t Offset = 0;\n  const char *ExternalSym = nullptr;\n  if (const auto *G = dyn_cast<GlobalAddressSDNode>(Op)) {\n    GV = G->getGlobal();\n    Offset = G->getOffset();\n  } else {\n    const auto *ES = cast<ExternalSymbolSDNode>(Op);\n    ExternalSym = ES->getSymbol();\n  }\n\n  // Calculate some flags for address lowering.\n  const Module &Mod = *DAG.getMachineFunction().getFunction().getParent();\n  unsigned char OpFlags;\n  if (ForCall)\n    OpFlags = Subtarget.classifyGlobalFunctionReference(GV, Mod);\n  else\n    OpFlags = Subtarget.classifyGlobalReference(GV, Mod);\n  bool HasPICReg = isGlobalRelativeToPICBase(OpFlags);\n  bool NeedsLoad = isGlobalStubReference(OpFlags);\n\n  CodeModel::Model M = DAG.getTarget().getCodeModel();\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  SDValue Result;\n\n  if (GV) {\n    // Create a target global address if this is a global. If possible, fold the\n    // offset into the global address reference. Otherwise, ADD it on later.\n    // Suppress the folding if Offset is negative: movl foo-1, %eax is not\n    // allowed because if the address of foo is 0, the ELF R_X86_64_32\n    // relocation will compute to a negative value, which is invalid.\n    int64_t GlobalOffset = 0;\n    if (OpFlags == X86II::MO_NO_FLAG && Offset >= 0 &&\n        X86::isOffsetSuitableForCodeModel(Offset, M, true)) {\n      std::swap(GlobalOffset, Offset);\n    }\n    Result = DAG.getTargetGlobalAddress(GV, dl, PtrVT, GlobalOffset, OpFlags);\n  } else {\n    // If this is not a global address, this must be an external symbol.\n    Result = DAG.getTargetExternalSymbol(ExternalSym, PtrVT, OpFlags);\n  }\n\n  // If this is a direct call, avoid the wrapper if we don't need to do any\n  // loads or adds. This allows SDAG ISel to match direct calls.\n  if (ForCall && !NeedsLoad && !HasPICReg && Offset == 0)\n    return Result;\n\n  Result = DAG.getNode(getGlobalWrapperKind(GV, OpFlags), dl, PtrVT, Result);\n\n  // With PIC, the address is actually $g + Offset.\n  if (HasPICReg) {\n    Result = DAG.getNode(ISD::ADD, dl, PtrVT,\n                         DAG.getNode(X86ISD::GlobalBaseReg, dl, PtrVT), Result);\n  }\n\n  // For globals that require a load from a stub to get the address, emit the\n  // load.\n  if (NeedsLoad)\n    Result = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), Result,\n                         MachinePointerInfo::getGOT(DAG.getMachineFunction()));\n\n  // If there was a non-zero offset that we didn't fold, create an explicit\n  // addition for it.\n  if (Offset != 0)\n    Result = DAG.getNode(ISD::ADD, dl, PtrVT, Result,\n                         DAG.getConstant(Offset, dl, PtrVT));\n\n  return Result;\n}\n\nSDValue\nX86TargetLowering::LowerGlobalAddress(SDValue Op, SelectionDAG &DAG) const {\n  return LowerGlobalOrExternal(Op, DAG, /*ForCall=*/false);\n}\n\nstatic SDValue\nGetTLSADDR(SelectionDAG &DAG, SDValue Chain, GlobalAddressSDNode *GA,\n           SDValue *InFlag, const EVT PtrVT, unsigned ReturnReg,\n           unsigned char OperandFlags, bool LocalDynamic = false) {\n  MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();\n  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDLoc dl(GA);\n  SDValue TGA = DAG.getTargetGlobalAddress(GA->getGlobal(), dl,\n                                           GA->getValueType(0),\n                                           GA->getOffset(),\n                                           OperandFlags);\n\n  X86ISD::NodeType CallType = LocalDynamic ? X86ISD::TLSBASEADDR\n                                           : X86ISD::TLSADDR;\n\n  if (InFlag) {\n    SDValue Ops[] = { Chain,  TGA, *InFlag };\n    Chain = DAG.getNode(CallType, dl, NodeTys, Ops);\n  } else {\n    SDValue Ops[]  = { Chain, TGA };\n    Chain = DAG.getNode(CallType, dl, NodeTys, Ops);\n  }\n\n  // TLSADDR will be codegen'ed as call. Inform MFI that function has calls.\n  MFI.setAdjustsStack(true);\n  MFI.setHasCalls(true);\n\n  SDValue Flag = Chain.getValue(1);\n  return DAG.getCopyFromReg(Chain, dl, ReturnReg, PtrVT, Flag);\n}\n\n// Lower ISD::GlobalTLSAddress using the \"general dynamic\" model, 32 bit\nstatic SDValue\nLowerToTLSGeneralDynamicModel32(GlobalAddressSDNode *GA, SelectionDAG &DAG,\n                                const EVT PtrVT) {\n  SDValue InFlag;\n  SDLoc dl(GA);  // ? function entry point might be better\n  SDValue Chain = DAG.getCopyToReg(DAG.getEntryNode(), dl, X86::EBX,\n                                   DAG.getNode(X86ISD::GlobalBaseReg,\n                                               SDLoc(), PtrVT), InFlag);\n  InFlag = Chain.getValue(1);\n\n  return GetTLSADDR(DAG, Chain, GA, &InFlag, PtrVT, X86::EAX, X86II::MO_TLSGD);\n}\n\n// Lower ISD::GlobalTLSAddress using the \"general dynamic\" model, 64 bit LP64\nstatic SDValue\nLowerToTLSGeneralDynamicModel64(GlobalAddressSDNode *GA, SelectionDAG &DAG,\n                                const EVT PtrVT) {\n  return GetTLSADDR(DAG, DAG.getEntryNode(), GA, nullptr, PtrVT,\n                    X86::RAX, X86II::MO_TLSGD);\n}\n\n// Lower ISD::GlobalTLSAddress using the \"general dynamic\" model, 64 bit ILP32\nstatic SDValue\nLowerToTLSGeneralDynamicModelX32(GlobalAddressSDNode *GA, SelectionDAG &DAG,\n                                 const EVT PtrVT) {\n  return GetTLSADDR(DAG, DAG.getEntryNode(), GA, nullptr, PtrVT,\n                    X86::EAX, X86II::MO_TLSGD);\n}\n\nstatic SDValue LowerToTLSLocalDynamicModel(GlobalAddressSDNode *GA,\n                                           SelectionDAG &DAG, const EVT PtrVT,\n                                           bool Is64Bit, bool Is64BitLP64) {\n  SDLoc dl(GA);\n\n  // Get the start address of the TLS block for this module.\n  X86MachineFunctionInfo *MFI = DAG.getMachineFunction()\n      .getInfo<X86MachineFunctionInfo>();\n  MFI->incNumLocalDynamicTLSAccesses();\n\n  SDValue Base;\n  if (Is64Bit) {\n    unsigned ReturnReg = Is64BitLP64 ? X86::RAX : X86::EAX;\n    Base = GetTLSADDR(DAG, DAG.getEntryNode(), GA, nullptr, PtrVT, ReturnReg,\n                      X86II::MO_TLSLD, /*LocalDynamic=*/true);\n  } else {\n    SDValue InFlag;\n    SDValue Chain = DAG.getCopyToReg(DAG.getEntryNode(), dl, X86::EBX,\n        DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(), PtrVT), InFlag);\n    InFlag = Chain.getValue(1);\n    Base = GetTLSADDR(DAG, Chain, GA, &InFlag, PtrVT, X86::EAX,\n                      X86II::MO_TLSLDM, /*LocalDynamic=*/true);\n  }\n\n  // Note: the CleanupLocalDynamicTLSPass will remove redundant computations\n  // of Base.\n\n  // Build x@dtpoff.\n  unsigned char OperandFlags = X86II::MO_DTPOFF;\n  unsigned WrapperKind = X86ISD::Wrapper;\n  SDValue TGA = DAG.getTargetGlobalAddress(GA->getGlobal(), dl,\n                                           GA->getValueType(0),\n                                           GA->getOffset(), OperandFlags);\n  SDValue Offset = DAG.getNode(WrapperKind, dl, PtrVT, TGA);\n\n  // Add x@dtpoff with the base.\n  return DAG.getNode(ISD::ADD, dl, PtrVT, Offset, Base);\n}\n\n// Lower ISD::GlobalTLSAddress using the \"initial exec\" or \"local exec\" model.\nstatic SDValue LowerToTLSExecModel(GlobalAddressSDNode *GA, SelectionDAG &DAG,\n                                   const EVT PtrVT, TLSModel::Model model,\n                                   bool is64Bit, bool isPIC) {\n  SDLoc dl(GA);\n\n  // Get the Thread Pointer, which is %gs:0 (32-bit) or %fs:0 (64-bit).\n  Value *Ptr = Constant::getNullValue(Type::getInt8PtrTy(*DAG.getContext(),\n                                                         is64Bit ? 257 : 256));\n\n  SDValue ThreadPointer =\n      DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), DAG.getIntPtrConstant(0, dl),\n                  MachinePointerInfo(Ptr));\n\n  unsigned char OperandFlags = 0;\n  // Most TLS accesses are not RIP relative, even on x86-64.  One exception is\n  // initialexec.\n  unsigned WrapperKind = X86ISD::Wrapper;\n  if (model == TLSModel::LocalExec) {\n    OperandFlags = is64Bit ? X86II::MO_TPOFF : X86II::MO_NTPOFF;\n  } else if (model == TLSModel::InitialExec) {\n    if (is64Bit) {\n      OperandFlags = X86II::MO_GOTTPOFF;\n      WrapperKind = X86ISD::WrapperRIP;\n    } else {\n      OperandFlags = isPIC ? X86II::MO_GOTNTPOFF : X86II::MO_INDNTPOFF;\n    }\n  } else {\n    llvm_unreachable(\"Unexpected model\");\n  }\n\n  // emit \"addl x@ntpoff,%eax\" (local exec)\n  // or \"addl x@indntpoff,%eax\" (initial exec)\n  // or \"addl x@gotntpoff(%ebx) ,%eax\" (initial exec, 32-bit pic)\n  SDValue TGA =\n      DAG.getTargetGlobalAddress(GA->getGlobal(), dl, GA->getValueType(0),\n                                 GA->getOffset(), OperandFlags);\n  SDValue Offset = DAG.getNode(WrapperKind, dl, PtrVT, TGA);\n\n  if (model == TLSModel::InitialExec) {\n    if (isPIC && !is64Bit) {\n      Offset = DAG.getNode(ISD::ADD, dl, PtrVT,\n                           DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(), PtrVT),\n                           Offset);\n    }\n\n    Offset = DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), Offset,\n                         MachinePointerInfo::getGOT(DAG.getMachineFunction()));\n  }\n\n  // The address of the thread local variable is the add of the thread\n  // pointer with the offset of the variable.\n  return DAG.getNode(ISD::ADD, dl, PtrVT, ThreadPointer, Offset);\n}\n\nSDValue\nX86TargetLowering::LowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const {\n\n  GlobalAddressSDNode *GA = cast<GlobalAddressSDNode>(Op);\n\n  if (DAG.getTarget().useEmulatedTLS())\n    return LowerToTLSEmulatedModel(GA, DAG);\n\n  const GlobalValue *GV = GA->getGlobal();\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  bool PositionIndependent = isPositionIndependent();\n\n  if (Subtarget.isTargetELF()) {\n    TLSModel::Model model = DAG.getTarget().getTLSModel(GV);\n    switch (model) {\n      case TLSModel::GeneralDynamic:\n        if (Subtarget.is64Bit()) {\n          if (Subtarget.isTarget64BitLP64())\n            return LowerToTLSGeneralDynamicModel64(GA, DAG, PtrVT);\n          return LowerToTLSGeneralDynamicModelX32(GA, DAG, PtrVT);\n        }\n        return LowerToTLSGeneralDynamicModel32(GA, DAG, PtrVT);\n      case TLSModel::LocalDynamic:\n        return LowerToTLSLocalDynamicModel(GA, DAG, PtrVT, Subtarget.is64Bit(),\n                                           Subtarget.isTarget64BitLP64());\n      case TLSModel::InitialExec:\n      case TLSModel::LocalExec:\n        return LowerToTLSExecModel(GA, DAG, PtrVT, model, Subtarget.is64Bit(),\n                                   PositionIndependent);\n    }\n    llvm_unreachable(\"Unknown TLS model.\");\n  }\n\n  if (Subtarget.isTargetDarwin()) {\n    // Darwin only has one model of TLS.  Lower to that.\n    unsigned char OpFlag = 0;\n    unsigned WrapperKind = Subtarget.isPICStyleRIPRel() ?\n                           X86ISD::WrapperRIP : X86ISD::Wrapper;\n\n    // In PIC mode (unless we're in RIPRel PIC mode) we add an offset to the\n    // global base reg.\n    bool PIC32 = PositionIndependent && !Subtarget.is64Bit();\n    if (PIC32)\n      OpFlag = X86II::MO_TLVP_PIC_BASE;\n    else\n      OpFlag = X86II::MO_TLVP;\n    SDLoc DL(Op);\n    SDValue Result = DAG.getTargetGlobalAddress(GA->getGlobal(), DL,\n                                                GA->getValueType(0),\n                                                GA->getOffset(), OpFlag);\n    SDValue Offset = DAG.getNode(WrapperKind, DL, PtrVT, Result);\n\n    // With PIC32, the address is actually $g + Offset.\n    if (PIC32)\n      Offset = DAG.getNode(ISD::ADD, DL, PtrVT,\n                           DAG.getNode(X86ISD::GlobalBaseReg, SDLoc(), PtrVT),\n                           Offset);\n\n    // Lowering the machine isd will make sure everything is in the right\n    // location.\n    SDValue Chain = DAG.getEntryNode();\n    SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);\n    Chain = DAG.getCALLSEQ_START(Chain, 0, 0, DL);\n    SDValue Args[] = { Chain, Offset };\n    Chain = DAG.getNode(X86ISD::TLSCALL, DL, NodeTys, Args);\n    Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(0, DL, true),\n                               DAG.getIntPtrConstant(0, DL, true),\n                               Chain.getValue(1), DL);\n\n    // TLSCALL will be codegen'ed as call. Inform MFI that function has calls.\n    MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();\n    MFI.setAdjustsStack(true);\n\n    // And our return value (tls address) is in the standard call return value\n    // location.\n    unsigned Reg = Subtarget.is64Bit() ? X86::RAX : X86::EAX;\n    return DAG.getCopyFromReg(Chain, DL, Reg, PtrVT, Chain.getValue(1));\n  }\n\n  if (Subtarget.isOSWindows()) {\n    // Just use the implicit TLS architecture\n    // Need to generate something similar to:\n    //   mov     rdx, qword [gs:abs 58H]; Load pointer to ThreadLocalStorage\n    //                                  ; from TEB\n    //   mov     ecx, dword [rel _tls_index]: Load index (from C runtime)\n    //   mov     rcx, qword [rdx+rcx*8]\n    //   mov     eax, .tls$:tlsvar\n    //   [rax+rcx] contains the address\n    // Windows 64bit: gs:0x58\n    // Windows 32bit: fs:__tls_array\n\n    SDLoc dl(GA);\n    SDValue Chain = DAG.getEntryNode();\n\n    // Get the Thread Pointer, which is %fs:__tls_array (32-bit) or\n    // %gs:0x58 (64-bit). On MinGW, __tls_array is not available, so directly\n    // use its literal value of 0x2C.\n    Value *Ptr = Constant::getNullValue(Subtarget.is64Bit()\n                                        ? Type::getInt8PtrTy(*DAG.getContext(),\n                                                             256)\n                                        : Type::getInt32PtrTy(*DAG.getContext(),\n                                                              257));\n\n    SDValue TlsArray = Subtarget.is64Bit()\n                           ? DAG.getIntPtrConstant(0x58, dl)\n                           : (Subtarget.isTargetWindowsGNU()\n                                  ? DAG.getIntPtrConstant(0x2C, dl)\n                                  : DAG.getExternalSymbol(\"_tls_array\", PtrVT));\n\n    SDValue ThreadPointer =\n        DAG.getLoad(PtrVT, dl, Chain, TlsArray, MachinePointerInfo(Ptr));\n\n    SDValue res;\n    if (GV->getThreadLocalMode() == GlobalVariable::LocalExecTLSModel) {\n      res = ThreadPointer;\n    } else {\n      // Load the _tls_index variable\n      SDValue IDX = DAG.getExternalSymbol(\"_tls_index\", PtrVT);\n      if (Subtarget.is64Bit())\n        IDX = DAG.getExtLoad(ISD::ZEXTLOAD, dl, PtrVT, Chain, IDX,\n                             MachinePointerInfo(), MVT::i32);\n      else\n        IDX = DAG.getLoad(PtrVT, dl, Chain, IDX, MachinePointerInfo());\n\n      const DataLayout &DL = DAG.getDataLayout();\n      SDValue Scale =\n          DAG.getConstant(Log2_64_Ceil(DL.getPointerSize()), dl, MVT::i8);\n      IDX = DAG.getNode(ISD::SHL, dl, PtrVT, IDX, Scale);\n\n      res = DAG.getNode(ISD::ADD, dl, PtrVT, ThreadPointer, IDX);\n    }\n\n    res = DAG.getLoad(PtrVT, dl, Chain, res, MachinePointerInfo());\n\n    // Get the offset of start of .tls section\n    SDValue TGA = DAG.getTargetGlobalAddress(GA->getGlobal(), dl,\n                                             GA->getValueType(0),\n                                             GA->getOffset(), X86II::MO_SECREL);\n    SDValue Offset = DAG.getNode(X86ISD::Wrapper, dl, PtrVT, TGA);\n\n    // The address of the thread local variable is the add of the thread\n    // pointer with the offset of the variable.\n    return DAG.getNode(ISD::ADD, dl, PtrVT, res, Offset);\n  }\n\n  llvm_unreachable(\"TLS not implemented for this target.\");\n}\n\n/// Lower SRA_PARTS and friends, which return two i32 values\n/// and take a 2 x i32 value to shift plus a shift amount.\n/// TODO: Can this be moved to general expansion code?\nstatic SDValue LowerShiftParts(SDValue Op, SelectionDAG &DAG) {\n  assert(Op.getNumOperands() == 3 && \"Not a double-shift!\");\n  MVT VT = Op.getSimpleValueType();\n  unsigned VTBits = VT.getSizeInBits();\n  SDLoc dl(Op);\n  bool isSRA = Op.getOpcode() == ISD::SRA_PARTS;\n  SDValue ShOpLo = Op.getOperand(0);\n  SDValue ShOpHi = Op.getOperand(1);\n  SDValue ShAmt  = Op.getOperand(2);\n  // ISD::FSHL and ISD::FSHR have defined overflow behavior but ISD::SHL and\n  // ISD::SRA/L nodes haven't. Insert an AND to be safe, it's optimized away\n  // during isel.\n  SDValue SafeShAmt = DAG.getNode(ISD::AND, dl, MVT::i8, ShAmt,\n                                  DAG.getConstant(VTBits - 1, dl, MVT::i8));\n  SDValue Tmp1 = isSRA ? DAG.getNode(ISD::SRA, dl, VT, ShOpHi,\n                                     DAG.getConstant(VTBits - 1, dl, MVT::i8))\n                       : DAG.getConstant(0, dl, VT);\n\n  SDValue Tmp2, Tmp3;\n  if (Op.getOpcode() == ISD::SHL_PARTS) {\n    Tmp2 = DAG.getNode(ISD::FSHL, dl, VT, ShOpHi, ShOpLo, ShAmt);\n    Tmp3 = DAG.getNode(ISD::SHL, dl, VT, ShOpLo, SafeShAmt);\n  } else {\n    Tmp2 = DAG.getNode(ISD::FSHR, dl, VT, ShOpHi, ShOpLo, ShAmt);\n    Tmp3 = DAG.getNode(isSRA ? ISD::SRA : ISD::SRL, dl, VT, ShOpHi, SafeShAmt);\n  }\n\n  // If the shift amount is larger or equal than the width of a part we can't\n  // rely on the results of shld/shrd. Insert a test and select the appropriate\n  // values for large shift amounts.\n  SDValue AndNode = DAG.getNode(ISD::AND, dl, MVT::i8, ShAmt,\n                                DAG.getConstant(VTBits, dl, MVT::i8));\n  SDValue Cond = DAG.getSetCC(dl, MVT::i8, AndNode,\n                             DAG.getConstant(0, dl, MVT::i8), ISD::SETNE);\n\n  SDValue Hi, Lo;\n  if (Op.getOpcode() == ISD::SHL_PARTS) {\n    Hi = DAG.getNode(ISD::SELECT, dl, VT, Cond, Tmp3, Tmp2);\n    Lo = DAG.getNode(ISD::SELECT, dl, VT, Cond, Tmp1, Tmp3);\n  } else {\n    Lo = DAG.getNode(ISD::SELECT, dl, VT, Cond, Tmp3, Tmp2);\n    Hi = DAG.getNode(ISD::SELECT, dl, VT, Cond, Tmp1, Tmp3);\n  }\n\n  return DAG.getMergeValues({ Lo, Hi }, dl);\n}\n\nstatic SDValue LowerFunnelShift(SDValue Op, const X86Subtarget &Subtarget,\n                                SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  assert((Op.getOpcode() == ISD::FSHL || Op.getOpcode() == ISD::FSHR) &&\n         \"Unexpected funnel shift opcode!\");\n\n  SDLoc DL(Op);\n  SDValue Op0 = Op.getOperand(0);\n  SDValue Op1 = Op.getOperand(1);\n  SDValue Amt = Op.getOperand(2);\n\n  bool IsFSHR = Op.getOpcode() == ISD::FSHR;\n\n  if (VT.isVector()) {\n    assert(Subtarget.hasVBMI2() && \"Expected VBMI2\");\n\n    if (IsFSHR)\n      std::swap(Op0, Op1);\n\n    // With AVX512, but not VLX we need to widen to get a 512-bit result type.\n    if (!Subtarget.hasVLX() && !VT.is512BitVector()) {\n      Op0 = widenSubVector(Op0, false, Subtarget, DAG, DL, 512);\n      Op1 = widenSubVector(Op1, false, Subtarget, DAG, DL, 512);\n    }\n\n    SDValue Funnel;\n    APInt APIntShiftAmt;\n    MVT ResultVT = Op0.getSimpleValueType();\n    if (X86::isConstantSplat(Amt, APIntShiftAmt)) {\n      uint64_t ShiftAmt = APIntShiftAmt.urem(VT.getScalarSizeInBits());\n      Funnel =\n          DAG.getNode(IsFSHR ? X86ISD::VSHRD : X86ISD::VSHLD, DL, ResultVT, Op0,\n                      Op1, DAG.getTargetConstant(ShiftAmt, DL, MVT::i8));\n    } else {\n      if (!Subtarget.hasVLX() && !VT.is512BitVector())\n        Amt = widenSubVector(Amt, false, Subtarget, DAG, DL, 512);\n      Funnel = DAG.getNode(IsFSHR ? X86ISD::VSHRDV : X86ISD::VSHLDV, DL,\n                           ResultVT, Op0, Op1, Amt);\n    }\n    if (!Subtarget.hasVLX() && !VT.is512BitVector())\n      Funnel = extractSubVector(Funnel, 0, DAG, DL, VT.getSizeInBits());\n    return Funnel;\n  }\n  assert(\n      (VT == MVT::i8 || VT == MVT::i16 || VT == MVT::i32 || VT == MVT::i64) &&\n      \"Unexpected funnel shift type!\");\n\n  // Expand slow SHLD/SHRD cases if we are not optimizing for size.\n  bool OptForSize = DAG.shouldOptForSize();\n  bool ExpandFunnel = !OptForSize && Subtarget.isSHLDSlow();\n\n  // fshl(x,y,z) -> (((aext(x) << bw) | zext(y)) << (z & (bw-1))) >> bw.\n  // fshr(x,y,z) -> (((aext(x) << bw) | zext(y)) >> (z & (bw-1))).\n  if ((VT == MVT::i8 || (ExpandFunnel && VT == MVT::i16)) &&\n      !isa<ConstantSDNode>(Amt)) {\n    unsigned EltSizeInBits = VT.getScalarSizeInBits();\n    SDValue Mask = DAG.getConstant(EltSizeInBits - 1, DL, Amt.getValueType());\n    SDValue HiShift = DAG.getConstant(EltSizeInBits, DL, Amt.getValueType());\n    Op0 = DAG.getAnyExtOrTrunc(Op0, DL, MVT::i32);\n    Op1 = DAG.getZExtOrTrunc(Op1, DL, MVT::i32);\n    Amt = DAG.getNode(ISD::AND, DL, Amt.getValueType(), Amt, Mask);\n    SDValue Res = DAG.getNode(ISD::SHL, DL, MVT::i32, Op0, HiShift);\n    Res = DAG.getNode(ISD::OR, DL, MVT::i32, Res, Op1);\n    if (IsFSHR) {\n      Res = DAG.getNode(ISD::SRL, DL, MVT::i32, Res, Amt);\n    } else {\n      Res = DAG.getNode(ISD::SHL, DL, MVT::i32, Res, Amt);\n      Res = DAG.getNode(ISD::SRL, DL, MVT::i32, Res, HiShift);\n    }\n    return DAG.getZExtOrTrunc(Res, DL, VT);\n  }\n\n  if (VT == MVT::i8 || ExpandFunnel)\n    return SDValue();\n\n  // i16 needs to modulo the shift amount, but i32/i64 have implicit modulo.\n  if (VT == MVT::i16) {\n    Amt = DAG.getNode(ISD::AND, DL, Amt.getValueType(), Amt,\n                      DAG.getConstant(15, DL, Amt.getValueType()));\n    unsigned FSHOp = (IsFSHR ? X86ISD::FSHR : X86ISD::FSHL);\n    return DAG.getNode(FSHOp, DL, VT, Op0, Op1, Amt);\n  }\n\n  return Op;\n}\n\n// Try to use a packed vector operation to handle i64 on 32-bit targets when\n// AVX512DQ is enabled.\nstatic SDValue LowerI64IntToFP_AVX512DQ(SDValue Op, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  assert((Op.getOpcode() == ISD::SINT_TO_FP ||\n          Op.getOpcode() == ISD::STRICT_SINT_TO_FP ||\n          Op.getOpcode() == ISD::STRICT_UINT_TO_FP ||\n          Op.getOpcode() == ISD::UINT_TO_FP) &&\n         \"Unexpected opcode!\");\n  bool IsStrict = Op->isStrictFPOpcode();\n  unsigned OpNo = IsStrict ? 1 : 0;\n  SDValue Src = Op.getOperand(OpNo);\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT VT = Op.getSimpleValueType();\n\n   if (!Subtarget.hasDQI() || SrcVT != MVT::i64 || Subtarget.is64Bit() ||\n       (VT != MVT::f32 && VT != MVT::f64))\n    return SDValue();\n\n  // Pack the i64 into a vector, do the operation and extract.\n\n  // Using 256-bit to ensure result is 128-bits for f32 case.\n  unsigned NumElts = Subtarget.hasVLX() ? 4 : 8;\n  MVT VecInVT = MVT::getVectorVT(MVT::i64, NumElts);\n  MVT VecVT = MVT::getVectorVT(VT, NumElts);\n\n  SDLoc dl(Op);\n  SDValue InVec = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VecInVT, Src);\n  if (IsStrict) {\n    SDValue CvtVec = DAG.getNode(Op.getOpcode(), dl, {VecVT, MVT::Other},\n                                 {Op.getOperand(0), InVec});\n    SDValue Chain = CvtVec.getValue(1);\n    SDValue Value = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, CvtVec,\n                                DAG.getIntPtrConstant(0, dl));\n    return DAG.getMergeValues({Value, Chain}, dl);\n  }\n\n  SDValue CvtVec = DAG.getNode(Op.getOpcode(), dl, VecVT, InVec);\n\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, CvtVec,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\nstatic bool useVectorCast(unsigned Opcode, MVT FromVT, MVT ToVT,\n                          const X86Subtarget &Subtarget) {\n  switch (Opcode) {\n    case ISD::SINT_TO_FP:\n      // TODO: Handle wider types with AVX/AVX512.\n      if (!Subtarget.hasSSE2() || FromVT != MVT::v4i32)\n        return false;\n      // CVTDQ2PS or (V)CVTDQ2PD\n      return ToVT == MVT::v4f32 || (Subtarget.hasAVX() && ToVT == MVT::v4f64);\n\n    case ISD::UINT_TO_FP:\n      // TODO: Handle wider types and i64 elements.\n      if (!Subtarget.hasAVX512() || FromVT != MVT::v4i32)\n        return false;\n      // VCVTUDQ2PS or VCVTUDQ2PD\n      return ToVT == MVT::v4f32 || ToVT == MVT::v4f64;\n\n    default:\n      return false;\n  }\n}\n\n/// Given a scalar cast operation that is extracted from a vector, try to\n/// vectorize the cast op followed by extraction. This will avoid an expensive\n/// round-trip between XMM and GPR.\nstatic SDValue vectorizeExtractedCast(SDValue Cast, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  // TODO: This could be enhanced to handle smaller integer types by peeking\n  // through an extend.\n  SDValue Extract = Cast.getOperand(0);\n  MVT DestVT = Cast.getSimpleValueType();\n  if (Extract.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n      !isa<ConstantSDNode>(Extract.getOperand(1)))\n    return SDValue();\n\n  // See if we have a 128-bit vector cast op for this type of cast.\n  SDValue VecOp = Extract.getOperand(0);\n  MVT FromVT = VecOp.getSimpleValueType();\n  unsigned NumEltsInXMM = 128 / FromVT.getScalarSizeInBits();\n  MVT Vec128VT = MVT::getVectorVT(FromVT.getScalarType(), NumEltsInXMM);\n  MVT ToVT = MVT::getVectorVT(DestVT, NumEltsInXMM);\n  if (!useVectorCast(Cast.getOpcode(), Vec128VT, ToVT, Subtarget))\n    return SDValue();\n\n  // If we are extracting from a non-zero element, first shuffle the source\n  // vector to allow extracting from element zero.\n  SDLoc DL(Cast);\n  if (!isNullConstant(Extract.getOperand(1))) {\n    SmallVector<int, 16> Mask(FromVT.getVectorNumElements(), -1);\n    Mask[0] = Extract.getConstantOperandVal(1);\n    VecOp = DAG.getVectorShuffle(FromVT, DL, VecOp, DAG.getUNDEF(FromVT), Mask);\n  }\n  // If the source vector is wider than 128-bits, extract the low part. Do not\n  // create an unnecessarily wide vector cast op.\n  if (FromVT != Vec128VT)\n    VecOp = extract128BitVector(VecOp, 0, DAG, DL);\n\n  // cast (extelt V, 0) --> extelt (cast (extract_subv V)), 0\n  // cast (extelt V, C) --> extelt (cast (extract_subv (shuffle V, [C...]))), 0\n  SDValue VCast = DAG.getNode(Cast.getOpcode(), DL, ToVT, VecOp);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, DestVT, VCast,\n                     DAG.getIntPtrConstant(0, DL));\n}\n\n/// Given a scalar cast to FP with a cast to integer operand (almost an ftrunc),\n/// try to vectorize the cast ops. This will avoid an expensive round-trip\n/// between XMM and GPR.\nstatic SDValue lowerFPToIntToFP(SDValue CastToFP, SelectionDAG &DAG,\n                                const X86Subtarget &Subtarget) {\n  // TODO: Allow FP_TO_UINT.\n  SDValue CastToInt = CastToFP.getOperand(0);\n  MVT VT = CastToFP.getSimpleValueType();\n  if (CastToInt.getOpcode() != ISD::FP_TO_SINT || VT.isVector())\n    return SDValue();\n\n  MVT IntVT = CastToInt.getSimpleValueType();\n  SDValue X = CastToInt.getOperand(0);\n  MVT SrcVT = X.getSimpleValueType();\n  if (SrcVT != MVT::f32 && SrcVT != MVT::f64)\n    return SDValue();\n\n  // See if we have 128-bit vector cast instructions for this type of cast.\n  // We need cvttps2dq/cvttpd2dq and cvtdq2ps/cvtdq2pd.\n  if (!Subtarget.hasSSE2() || (VT != MVT::f32 && VT != MVT::f64) ||\n      IntVT != MVT::i32)\n    return SDValue();\n\n  unsigned SrcSize = SrcVT.getSizeInBits();\n  unsigned IntSize = IntVT.getSizeInBits();\n  unsigned VTSize = VT.getSizeInBits();\n  MVT VecSrcVT = MVT::getVectorVT(SrcVT, 128 / SrcSize);\n  MVT VecIntVT = MVT::getVectorVT(IntVT, 128 / IntSize);\n  MVT VecVT = MVT::getVectorVT(VT, 128 / VTSize);\n\n  // We need target-specific opcodes if this is v2f64 -> v4i32 -> v2f64.\n  unsigned ToIntOpcode =\n      SrcSize != IntSize ? X86ISD::CVTTP2SI : (unsigned)ISD::FP_TO_SINT;\n  unsigned ToFPOpcode =\n      IntSize != VTSize ? X86ISD::CVTSI2P : (unsigned)ISD::SINT_TO_FP;\n\n  // sint_to_fp (fp_to_sint X) --> extelt (sint_to_fp (fp_to_sint (s2v X))), 0\n  //\n  // We are not defining the high elements (for example, zero them) because\n  // that could nullify any performance advantage that we hoped to gain from\n  // this vector op hack. We do not expect any adverse effects (like denorm\n  // penalties) with cast ops.\n  SDLoc DL(CastToFP);\n  SDValue ZeroIdx = DAG.getIntPtrConstant(0, DL);\n  SDValue VecX = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecSrcVT, X);\n  SDValue VCastToInt = DAG.getNode(ToIntOpcode, DL, VecIntVT, VecX);\n  SDValue VCastToFP = DAG.getNode(ToFPOpcode, DL, VecVT, VCastToInt);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, VCastToFP, ZeroIdx);\n}\n\nstatic SDValue lowerINT_TO_FP_vXi64(SDValue Op, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  SDLoc DL(Op);\n  bool IsStrict = Op->isStrictFPOpcode();\n  MVT VT = Op->getSimpleValueType(0);\n  SDValue Src = Op->getOperand(IsStrict ? 1 : 0);\n\n  if (Subtarget.hasDQI()) {\n    assert(!Subtarget.hasVLX() && \"Unexpected features\");\n\n    assert((Src.getSimpleValueType() == MVT::v2i64 ||\n            Src.getSimpleValueType() == MVT::v4i64) &&\n           \"Unsupported custom type\");\n\n    // With AVX512DQ, but not VLX we need to widen to get a 512-bit result type.\n    assert((VT == MVT::v4f32 || VT == MVT::v2f64 || VT == MVT::v4f64) &&\n           \"Unexpected VT!\");\n    MVT WideVT = VT == MVT::v4f32 ? MVT::v8f32 : MVT::v8f64;\n\n    // Need to concat with zero vector for strict fp to avoid spurious\n    // exceptions.\n    SDValue Tmp = IsStrict ? DAG.getConstant(0, DL, MVT::v8i64)\n                           : DAG.getUNDEF(MVT::v8i64);\n    Src = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, MVT::v8i64, Tmp, Src,\n                      DAG.getIntPtrConstant(0, DL));\n    SDValue Res, Chain;\n    if (IsStrict) {\n      Res = DAG.getNode(Op.getOpcode(), DL, {WideVT, MVT::Other},\n                        {Op->getOperand(0), Src});\n      Chain = Res.getValue(1);\n    } else {\n      Res = DAG.getNode(Op.getOpcode(), DL, WideVT, Src);\n    }\n\n    Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                      DAG.getIntPtrConstant(0, DL));\n\n    if (IsStrict)\n      return DAG.getMergeValues({Res, Chain}, DL);\n    return Res;\n  }\n\n  bool IsSigned = Op->getOpcode() == ISD::SINT_TO_FP ||\n                  Op->getOpcode() == ISD::STRICT_SINT_TO_FP;\n  if (VT != MVT::v4f32 || IsSigned)\n    return SDValue();\n\n  SDValue Zero = DAG.getConstant(0, DL, MVT::v4i64);\n  SDValue One  = DAG.getConstant(1, DL, MVT::v4i64);\n  SDValue Sign = DAG.getNode(ISD::OR, DL, MVT::v4i64,\n                             DAG.getNode(ISD::SRL, DL, MVT::v4i64, Src, One),\n                             DAG.getNode(ISD::AND, DL, MVT::v4i64, Src, One));\n  SDValue IsNeg = DAG.getSetCC(DL, MVT::v4i64, Src, Zero, ISD::SETLT);\n  SDValue SignSrc = DAG.getSelect(DL, MVT::v4i64, IsNeg, Sign, Src);\n  SmallVector<SDValue, 4> SignCvts(4);\n  SmallVector<SDValue, 4> Chains(4);\n  for (int i = 0; i != 4; ++i) {\n    SDValue Elt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, MVT::i64, SignSrc,\n                              DAG.getIntPtrConstant(i, DL));\n    if (IsStrict) {\n      SignCvts[i] =\n          DAG.getNode(ISD::STRICT_SINT_TO_FP, DL, {MVT::f32, MVT::Other},\n                      {Op.getOperand(0), Elt});\n      Chains[i] = SignCvts[i].getValue(1);\n    } else {\n      SignCvts[i] = DAG.getNode(ISD::SINT_TO_FP, DL, MVT::f32, Elt);\n    }\n  }\n  SDValue SignCvt = DAG.getBuildVector(VT, DL, SignCvts);\n\n  SDValue Slow, Chain;\n  if (IsStrict) {\n    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Chains);\n    Slow = DAG.getNode(ISD::STRICT_FADD, DL, {MVT::v4f32, MVT::Other},\n                       {Chain, SignCvt, SignCvt});\n    Chain = Slow.getValue(1);\n  } else {\n    Slow = DAG.getNode(ISD::FADD, DL, MVT::v4f32, SignCvt, SignCvt);\n  }\n\n  IsNeg = DAG.getNode(ISD::TRUNCATE, DL, MVT::v4i32, IsNeg);\n  SDValue Cvt = DAG.getSelect(DL, MVT::v4f32, IsNeg, Slow, SignCvt);\n\n  if (IsStrict)\n    return DAG.getMergeValues({Cvt, Chain}, DL);\n\n  return Cvt;\n}\n\nSDValue X86TargetLowering::LowerSINT_TO_FP(SDValue Op,\n                                           SelectionDAG &DAG) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n  unsigned OpNo = IsStrict ? 1 : 0;\n  SDValue Src = Op.getOperand(OpNo);\n  SDValue Chain = IsStrict ? Op->getOperand(0) : DAG.getEntryNode();\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n\n  if (SDValue Extract = vectorizeExtractedCast(Op, DAG, Subtarget))\n    return Extract;\n\n  if (SDValue R = lowerFPToIntToFP(Op, DAG, Subtarget))\n    return R;\n\n  if (SrcVT.isVector()) {\n    if (SrcVT == MVT::v2i32 && VT == MVT::v2f64) {\n      // Note: Since v2f64 is a legal type. We don't need to zero extend the\n      // source for strict FP.\n      if (IsStrict)\n        return DAG.getNode(\n            X86ISD::STRICT_CVTSI2P, dl, {VT, MVT::Other},\n            {Chain, DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4i32, Src,\n                                DAG.getUNDEF(SrcVT))});\n      return DAG.getNode(X86ISD::CVTSI2P, dl, VT,\n                         DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4i32, Src,\n                                     DAG.getUNDEF(SrcVT)));\n    }\n    if (SrcVT == MVT::v2i64 || SrcVT == MVT::v4i64)\n      return lowerINT_TO_FP_vXi64(Op, DAG, Subtarget);\n\n    return SDValue();\n  }\n\n  assert(SrcVT <= MVT::i64 && SrcVT >= MVT::i16 &&\n         \"Unknown SINT_TO_FP to lower!\");\n\n  bool UseSSEReg = isScalarFPTypeInSSEReg(VT);\n\n  // These are really Legal; return the operand so the caller accepts it as\n  // Legal.\n  if (SrcVT == MVT::i32 && UseSSEReg)\n    return Op;\n  if (SrcVT == MVT::i64 && UseSSEReg && Subtarget.is64Bit())\n    return Op;\n\n  if (SDValue V = LowerI64IntToFP_AVX512DQ(Op, DAG, Subtarget))\n    return V;\n\n  // SSE doesn't have an i16 conversion so we need to promote.\n  if (SrcVT == MVT::i16 && (UseSSEReg || VT == MVT::f128)) {\n    SDValue Ext = DAG.getNode(ISD::SIGN_EXTEND, dl, MVT::i32, Src);\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {VT, MVT::Other},\n                         {Chain, Ext});\n\n    return DAG.getNode(ISD::SINT_TO_FP, dl, VT, Ext);\n  }\n\n  if (VT == MVT::f128)\n    return SDValue();\n\n  SDValue ValueToStore = Src;\n  if (SrcVT == MVT::i64 && Subtarget.hasSSE2() && !Subtarget.is64Bit())\n    // Bitcasting to f64 here allows us to do a single 64-bit store from\n    // an SSE register, avoiding the store forwarding penalty that would come\n    // with two 32-bit stores.\n    ValueToStore = DAG.getBitcast(MVT::f64, ValueToStore);\n\n  unsigned Size = SrcVT.getStoreSize();\n  Align Alignment(Size);\n  MachineFunction &MF = DAG.getMachineFunction();\n  auto PtrVT = getPointerTy(MF.getDataLayout());\n  int SSFI = MF.getFrameInfo().CreateStackObject(Size, Alignment, false);\n  MachinePointerInfo MPI =\n      MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SSFI);\n  SDValue StackSlot = DAG.getFrameIndex(SSFI, PtrVT);\n  Chain = DAG.getStore(Chain, dl, ValueToStore, StackSlot, MPI, Alignment);\n  std::pair<SDValue, SDValue> Tmp =\n      BuildFILD(VT, SrcVT, dl, Chain, StackSlot, MPI, Alignment, DAG);\n\n  if (IsStrict)\n    return DAG.getMergeValues({Tmp.first, Tmp.second}, dl);\n\n  return Tmp.first;\n}\n\nstd::pair<SDValue, SDValue> X86TargetLowering::BuildFILD(\n    EVT DstVT, EVT SrcVT, const SDLoc &DL, SDValue Chain, SDValue Pointer,\n    MachinePointerInfo PtrInfo, Align Alignment, SelectionDAG &DAG) const {\n  // Build the FILD\n  SDVTList Tys;\n  bool useSSE = isScalarFPTypeInSSEReg(DstVT);\n  if (useSSE)\n    Tys = DAG.getVTList(MVT::f80, MVT::Other);\n  else\n    Tys = DAG.getVTList(DstVT, MVT::Other);\n\n  SDValue FILDOps[] = {Chain, Pointer};\n  SDValue Result =\n      DAG.getMemIntrinsicNode(X86ISD::FILD, DL, Tys, FILDOps, SrcVT, PtrInfo,\n                              Alignment, MachineMemOperand::MOLoad);\n  Chain = Result.getValue(1);\n\n  if (useSSE) {\n    MachineFunction &MF = DAG.getMachineFunction();\n    unsigned SSFISize = DstVT.getStoreSize();\n    int SSFI =\n        MF.getFrameInfo().CreateStackObject(SSFISize, Align(SSFISize), false);\n    auto PtrVT = getPointerTy(MF.getDataLayout());\n    SDValue StackSlot = DAG.getFrameIndex(SSFI, PtrVT);\n    Tys = DAG.getVTList(MVT::Other);\n    SDValue FSTOps[] = {Chain, Result, StackSlot};\n    MachineMemOperand *StoreMMO = DAG.getMachineFunction().getMachineMemOperand(\n        MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SSFI),\n        MachineMemOperand::MOStore, SSFISize, Align(SSFISize));\n\n    Chain =\n        DAG.getMemIntrinsicNode(X86ISD::FST, DL, Tys, FSTOps, DstVT, StoreMMO);\n    Result = DAG.getLoad(\n        DstVT, DL, Chain, StackSlot,\n        MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SSFI));\n    Chain = Result.getValue(1);\n  }\n\n  return { Result, Chain };\n}\n\n/// Horizontal vector math instructions may be slower than normal math with\n/// shuffles. Limit horizontal op codegen based on size/speed trade-offs, uarch\n/// implementation, and likely shuffle complexity of the alternate sequence.\nstatic bool shouldUseHorizontalOp(bool IsSingleSource, SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  bool IsOptimizingSize = DAG.shouldOptForSize();\n  bool HasFastHOps = Subtarget.hasFastHorizontalOps();\n  return !IsSingleSource || IsOptimizingSize || HasFastHOps;\n}\n\n/// 64-bit unsigned integer to double expansion.\nstatic SDValue LowerUINT_TO_FP_i64(SDValue Op, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  // We can't use this algorithm for strict fp. It produces -0.0 instead of +0.0\n  // when converting 0 when rounding toward negative infinity. Caller will\n  // fall back to Expand for when i64 or is legal or use FILD in 32-bit mode.\n  assert(!Op->isStrictFPOpcode() && \"Expected non-strict uint_to_fp!\");\n  // This algorithm is not obvious. Here it is what we're trying to output:\n  /*\n     movq       %rax,  %xmm0\n     punpckldq  (c0),  %xmm0  // c0: (uint4){ 0x43300000U, 0x45300000U, 0U, 0U }\n     subpd      (c1),  %xmm0  // c1: (double2){ 0x1.0p52, 0x1.0p52 * 0x1.0p32 }\n     #ifdef __SSE3__\n       haddpd   %xmm0, %xmm0\n     #else\n       pshufd   $0x4e, %xmm0, %xmm1\n       addpd    %xmm1, %xmm0\n     #endif\n  */\n\n  SDLoc dl(Op);\n  LLVMContext *Context = DAG.getContext();\n\n  // Build some magic constants.\n  static const uint32_t CV0[] = { 0x43300000, 0x45300000, 0, 0 };\n  Constant *C0 = ConstantDataVector::get(*Context, CV0);\n  auto PtrVT = DAG.getTargetLoweringInfo().getPointerTy(DAG.getDataLayout());\n  SDValue CPIdx0 = DAG.getConstantPool(C0, PtrVT, Align(16));\n\n  SmallVector<Constant*,2> CV1;\n  CV1.push_back(\n    ConstantFP::get(*Context, APFloat(APFloat::IEEEdouble(),\n                                      APInt(64, 0x4330000000000000ULL))));\n  CV1.push_back(\n    ConstantFP::get(*Context, APFloat(APFloat::IEEEdouble(),\n                                      APInt(64, 0x4530000000000000ULL))));\n  Constant *C1 = ConstantVector::get(CV1);\n  SDValue CPIdx1 = DAG.getConstantPool(C1, PtrVT, Align(16));\n\n  // Load the 64-bit value into an XMM register.\n  SDValue XR1 =\n      DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64, Op.getOperand(0));\n  SDValue CLod0 = DAG.getLoad(\n      MVT::v4i32, dl, DAG.getEntryNode(), CPIdx0,\n      MachinePointerInfo::getConstantPool(DAG.getMachineFunction()), Align(16));\n  SDValue Unpck1 =\n      getUnpackl(DAG, dl, MVT::v4i32, DAG.getBitcast(MVT::v4i32, XR1), CLod0);\n\n  SDValue CLod1 = DAG.getLoad(\n      MVT::v2f64, dl, CLod0.getValue(1), CPIdx1,\n      MachinePointerInfo::getConstantPool(DAG.getMachineFunction()), Align(16));\n  SDValue XR2F = DAG.getBitcast(MVT::v2f64, Unpck1);\n  // TODO: Are there any fast-math-flags to propagate here?\n  SDValue Sub = DAG.getNode(ISD::FSUB, dl, MVT::v2f64, XR2F, CLod1);\n  SDValue Result;\n\n  if (Subtarget.hasSSE3() &&\n      shouldUseHorizontalOp(true, DAG, Subtarget)) {\n    Result = DAG.getNode(X86ISD::FHADD, dl, MVT::v2f64, Sub, Sub);\n  } else {\n    SDValue Shuffle = DAG.getVectorShuffle(MVT::v2f64, dl, Sub, Sub, {1,-1});\n    Result = DAG.getNode(ISD::FADD, dl, MVT::v2f64, Shuffle, Sub);\n  }\n  Result = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::f64, Result,\n                       DAG.getIntPtrConstant(0, dl));\n  return Result;\n}\n\n/// 32-bit unsigned integer to float expansion.\nstatic SDValue LowerUINT_TO_FP_i32(SDValue Op, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  unsigned OpNo = Op.getNode()->isStrictFPOpcode() ? 1 : 0;\n  SDLoc dl(Op);\n  // FP constant to bias correct the final result.\n  SDValue Bias = DAG.getConstantFP(BitsToDouble(0x4330000000000000ULL), dl,\n                                   MVT::f64);\n\n  // Load the 32-bit value into an XMM register.\n  SDValue Load =\n      DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4i32, Op.getOperand(OpNo));\n\n  // Zero out the upper parts of the register.\n  Load = getShuffleVectorZeroOrUndef(Load, 0, true, Subtarget, DAG);\n\n  // Or the load with the bias.\n  SDValue Or = DAG.getNode(\n      ISD::OR, dl, MVT::v2i64,\n      DAG.getBitcast(MVT::v2i64, Load),\n      DAG.getBitcast(MVT::v2i64,\n                     DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2f64, Bias)));\n  Or =\n      DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::f64,\n                  DAG.getBitcast(MVT::v2f64, Or), DAG.getIntPtrConstant(0, dl));\n\n  if (Op.getNode()->isStrictFPOpcode()) {\n    // Subtract the bias.\n    // TODO: Are there any fast-math-flags to propagate here?\n    SDValue Chain = Op.getOperand(0);\n    SDValue Sub = DAG.getNode(ISD::STRICT_FSUB, dl, {MVT::f64, MVT::Other},\n                              {Chain, Or, Bias});\n\n    if (Op.getValueType() == Sub.getValueType())\n      return Sub;\n\n    // Handle final rounding.\n    std::pair<SDValue, SDValue> ResultPair = DAG.getStrictFPExtendOrRound(\n        Sub, Sub.getValue(1), dl, Op.getSimpleValueType());\n\n    return DAG.getMergeValues({ResultPair.first, ResultPair.second}, dl);\n  }\n\n  // Subtract the bias.\n  // TODO: Are there any fast-math-flags to propagate here?\n  SDValue Sub = DAG.getNode(ISD::FSUB, dl, MVT::f64, Or, Bias);\n\n  // Handle final rounding.\n  return DAG.getFPExtendOrRound(Sub, dl, Op.getSimpleValueType());\n}\n\nstatic SDValue lowerUINT_TO_FP_v2i32(SDValue Op, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget,\n                                     const SDLoc &DL) {\n  if (Op.getSimpleValueType() != MVT::v2f64)\n    return SDValue();\n\n  bool IsStrict = Op->isStrictFPOpcode();\n\n  SDValue N0 = Op.getOperand(IsStrict ? 1 : 0);\n  assert(N0.getSimpleValueType() == MVT::v2i32 && \"Unexpected input type\");\n\n  if (Subtarget.hasAVX512()) {\n    if (!Subtarget.hasVLX()) {\n      // Let generic type legalization widen this.\n      if (!IsStrict)\n        return SDValue();\n      // Otherwise pad the integer input with 0s and widen the operation.\n      N0 = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v4i32, N0,\n                       DAG.getConstant(0, DL, MVT::v2i32));\n      SDValue Res = DAG.getNode(Op->getOpcode(), DL, {MVT::v4f64, MVT::Other},\n                                {Op.getOperand(0), N0});\n      SDValue Chain = Res.getValue(1);\n      Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v2f64, Res,\n                        DAG.getIntPtrConstant(0, DL));\n      return DAG.getMergeValues({Res, Chain}, DL);\n    }\n\n    // Legalize to v4i32 type.\n    N0 = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v4i32, N0,\n                     DAG.getUNDEF(MVT::v2i32));\n    if (IsStrict)\n      return DAG.getNode(X86ISD::STRICT_CVTUI2P, DL, {MVT::v2f64, MVT::Other},\n                         {Op.getOperand(0), N0});\n    return DAG.getNode(X86ISD::CVTUI2P, DL, MVT::v2f64, N0);\n  }\n\n  // Zero extend to 2i64, OR with the floating point representation of 2^52.\n  // This gives us the floating point equivalent of 2^52 + the i32 integer\n  // since double has 52-bits of mantissa. Then subtract 2^52 in floating\n  // point leaving just our i32 integers in double format.\n  SDValue ZExtIn = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::v2i64, N0);\n  SDValue VBias =\n      DAG.getConstantFP(BitsToDouble(0x4330000000000000ULL), DL, MVT::v2f64);\n  SDValue Or = DAG.getNode(ISD::OR, DL, MVT::v2i64, ZExtIn,\n                           DAG.getBitcast(MVT::v2i64, VBias));\n  Or = DAG.getBitcast(MVT::v2f64, Or);\n\n  if (IsStrict)\n    return DAG.getNode(ISD::STRICT_FSUB, DL, {MVT::v2f64, MVT::Other},\n                       {Op.getOperand(0), Or, VBias});\n  return DAG.getNode(ISD::FSUB, DL, MVT::v2f64, Or, VBias);\n}\n\nstatic SDValue lowerUINT_TO_FP_vXi32(SDValue Op, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  SDLoc DL(Op);\n  bool IsStrict = Op->isStrictFPOpcode();\n  SDValue V = Op->getOperand(IsStrict ? 1 : 0);\n  MVT VecIntVT = V.getSimpleValueType();\n  assert((VecIntVT == MVT::v4i32 || VecIntVT == MVT::v8i32) &&\n         \"Unsupported custom type\");\n\n  if (Subtarget.hasAVX512()) {\n    // With AVX512, but not VLX we need to widen to get a 512-bit result type.\n    assert(!Subtarget.hasVLX() && \"Unexpected features\");\n    MVT VT = Op->getSimpleValueType(0);\n\n    // v8i32->v8f64 is legal with AVX512 so just return it.\n    if (VT == MVT::v8f64)\n      return Op;\n\n    assert((VT == MVT::v4f32 || VT == MVT::v8f32 || VT == MVT::v4f64) &&\n           \"Unexpected VT!\");\n    MVT WideVT = VT == MVT::v4f64 ? MVT::v8f64 : MVT::v16f32;\n    MVT WideIntVT = VT == MVT::v4f64 ? MVT::v8i32 : MVT::v16i32;\n    // Need to concat with zero vector for strict fp to avoid spurious\n    // exceptions.\n    SDValue Tmp =\n        IsStrict ? DAG.getConstant(0, DL, WideIntVT) : DAG.getUNDEF(WideIntVT);\n    V = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, WideIntVT, Tmp, V,\n                    DAG.getIntPtrConstant(0, DL));\n    SDValue Res, Chain;\n    if (IsStrict) {\n      Res = DAG.getNode(ISD::STRICT_UINT_TO_FP, DL, {WideVT, MVT::Other},\n                        {Op->getOperand(0), V});\n      Chain = Res.getValue(1);\n    } else {\n      Res = DAG.getNode(ISD::UINT_TO_FP, DL, WideVT, V);\n    }\n\n    Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                      DAG.getIntPtrConstant(0, DL));\n\n    if (IsStrict)\n      return DAG.getMergeValues({Res, Chain}, DL);\n    return Res;\n  }\n\n  if (Subtarget.hasAVX() && VecIntVT == MVT::v4i32 &&\n      Op->getSimpleValueType(0) == MVT::v4f64) {\n    SDValue ZExtIn = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::v4i64, V);\n    Constant *Bias = ConstantFP::get(\n        *DAG.getContext(),\n        APFloat(APFloat::IEEEdouble(), APInt(64, 0x4330000000000000ULL)));\n    auto PtrVT = DAG.getTargetLoweringInfo().getPointerTy(DAG.getDataLayout());\n    SDValue CPIdx = DAG.getConstantPool(Bias, PtrVT, Align(8));\n    SDVTList Tys = DAG.getVTList(MVT::v4f64, MVT::Other);\n    SDValue Ops[] = {DAG.getEntryNode(), CPIdx};\n    SDValue VBias = DAG.getMemIntrinsicNode(\n        X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, MVT::f64,\n        MachinePointerInfo::getConstantPool(DAG.getMachineFunction()), Align(8),\n        MachineMemOperand::MOLoad);\n\n    SDValue Or = DAG.getNode(ISD::OR, DL, MVT::v4i64, ZExtIn,\n                             DAG.getBitcast(MVT::v4i64, VBias));\n    Or = DAG.getBitcast(MVT::v4f64, Or);\n\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_FSUB, DL, {MVT::v4f64, MVT::Other},\n                         {Op.getOperand(0), Or, VBias});\n    return DAG.getNode(ISD::FSUB, DL, MVT::v4f64, Or, VBias);\n  }\n\n  // The algorithm is the following:\n  // #ifdef __SSE4_1__\n  //     uint4 lo = _mm_blend_epi16( v, (uint4) 0x4b000000, 0xaa);\n  //     uint4 hi = _mm_blend_epi16( _mm_srli_epi32(v,16),\n  //                                 (uint4) 0x53000000, 0xaa);\n  // #else\n  //     uint4 lo = (v & (uint4) 0xffff) | (uint4) 0x4b000000;\n  //     uint4 hi = (v >> 16) | (uint4) 0x53000000;\n  // #endif\n  //     float4 fhi = (float4) hi - (0x1.0p39f + 0x1.0p23f);\n  //     return (float4) lo + fhi;\n\n  bool Is128 = VecIntVT == MVT::v4i32;\n  MVT VecFloatVT = Is128 ? MVT::v4f32 : MVT::v8f32;\n  // If we convert to something else than the supported type, e.g., to v4f64,\n  // abort early.\n  if (VecFloatVT != Op->getSimpleValueType(0))\n    return SDValue();\n\n  // In the #idef/#else code, we have in common:\n  // - The vector of constants:\n  // -- 0x4b000000\n  // -- 0x53000000\n  // - A shift:\n  // -- v >> 16\n\n  // Create the splat vector for 0x4b000000.\n  SDValue VecCstLow = DAG.getConstant(0x4b000000, DL, VecIntVT);\n  // Create the splat vector for 0x53000000.\n  SDValue VecCstHigh = DAG.getConstant(0x53000000, DL, VecIntVT);\n\n  // Create the right shift.\n  SDValue VecCstShift = DAG.getConstant(16, DL, VecIntVT);\n  SDValue HighShift = DAG.getNode(ISD::SRL, DL, VecIntVT, V, VecCstShift);\n\n  SDValue Low, High;\n  if (Subtarget.hasSSE41()) {\n    MVT VecI16VT = Is128 ? MVT::v8i16 : MVT::v16i16;\n    //     uint4 lo = _mm_blend_epi16( v, (uint4) 0x4b000000, 0xaa);\n    SDValue VecCstLowBitcast = DAG.getBitcast(VecI16VT, VecCstLow);\n    SDValue VecBitcast = DAG.getBitcast(VecI16VT, V);\n    // Low will be bitcasted right away, so do not bother bitcasting back to its\n    // original type.\n    Low = DAG.getNode(X86ISD::BLENDI, DL, VecI16VT, VecBitcast,\n                      VecCstLowBitcast, DAG.getTargetConstant(0xaa, DL, MVT::i8));\n    //     uint4 hi = _mm_blend_epi16( _mm_srli_epi32(v,16),\n    //                                 (uint4) 0x53000000, 0xaa);\n    SDValue VecCstHighBitcast = DAG.getBitcast(VecI16VT, VecCstHigh);\n    SDValue VecShiftBitcast = DAG.getBitcast(VecI16VT, HighShift);\n    // High will be bitcasted right away, so do not bother bitcasting back to\n    // its original type.\n    High = DAG.getNode(X86ISD::BLENDI, DL, VecI16VT, VecShiftBitcast,\n                       VecCstHighBitcast, DAG.getTargetConstant(0xaa, DL, MVT::i8));\n  } else {\n    SDValue VecCstMask = DAG.getConstant(0xffff, DL, VecIntVT);\n    //     uint4 lo = (v & (uint4) 0xffff) | (uint4) 0x4b000000;\n    SDValue LowAnd = DAG.getNode(ISD::AND, DL, VecIntVT, V, VecCstMask);\n    Low = DAG.getNode(ISD::OR, DL, VecIntVT, LowAnd, VecCstLow);\n\n    //     uint4 hi = (v >> 16) | (uint4) 0x53000000;\n    High = DAG.getNode(ISD::OR, DL, VecIntVT, HighShift, VecCstHigh);\n  }\n\n  // Create the vector constant for (0x1.0p39f + 0x1.0p23f).\n  SDValue VecCstFSub = DAG.getConstantFP(\n      APFloat(APFloat::IEEEsingle(), APInt(32, 0x53000080)), DL, VecFloatVT);\n\n  //     float4 fhi = (float4) hi - (0x1.0p39f + 0x1.0p23f);\n  // NOTE: By using fsub of a positive constant instead of fadd of a negative\n  // constant, we avoid reassociation in MachineCombiner when unsafe-fp-math is\n  // enabled. See PR24512.\n  SDValue HighBitcast = DAG.getBitcast(VecFloatVT, High);\n  // TODO: Are there any fast-math-flags to propagate here?\n  //     (float4) lo;\n  SDValue LowBitcast = DAG.getBitcast(VecFloatVT, Low);\n  //     return (float4) lo + fhi;\n  if (IsStrict) {\n    SDValue FHigh = DAG.getNode(ISD::STRICT_FSUB, DL, {VecFloatVT, MVT::Other},\n                                {Op.getOperand(0), HighBitcast, VecCstFSub});\n    return DAG.getNode(ISD::STRICT_FADD, DL, {VecFloatVT, MVT::Other},\n                       {FHigh.getValue(1), LowBitcast, FHigh});\n  }\n\n  SDValue FHigh =\n      DAG.getNode(ISD::FSUB, DL, VecFloatVT, HighBitcast, VecCstFSub);\n  return DAG.getNode(ISD::FADD, DL, VecFloatVT, LowBitcast, FHigh);\n}\n\nstatic SDValue lowerUINT_TO_FP_vec(SDValue Op, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  unsigned OpNo = Op.getNode()->isStrictFPOpcode() ? 1 : 0;\n  SDValue N0 = Op.getOperand(OpNo);\n  MVT SrcVT = N0.getSimpleValueType();\n  SDLoc dl(Op);\n\n  switch (SrcVT.SimpleTy) {\n  default:\n    llvm_unreachable(\"Custom UINT_TO_FP is not supported!\");\n  case MVT::v2i32:\n    return lowerUINT_TO_FP_v2i32(Op, DAG, Subtarget, dl);\n  case MVT::v4i32:\n  case MVT::v8i32:\n    return lowerUINT_TO_FP_vXi32(Op, DAG, Subtarget);\n  case MVT::v2i64:\n  case MVT::v4i64:\n    return lowerINT_TO_FP_vXi64(Op, DAG, Subtarget);\n  }\n}\n\nSDValue X86TargetLowering::LowerUINT_TO_FP(SDValue Op,\n                                           SelectionDAG &DAG) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n  unsigned OpNo = IsStrict ? 1 : 0;\n  SDValue Src = Op.getOperand(OpNo);\n  SDLoc dl(Op);\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT DstVT = Op->getSimpleValueType(0);\n  SDValue Chain = IsStrict ? Op.getOperand(0) : DAG.getEntryNode();\n\n  if (DstVT == MVT::f128)\n    return SDValue();\n\n  if (DstVT.isVector())\n    return lowerUINT_TO_FP_vec(Op, DAG, Subtarget);\n\n  if (SDValue Extract = vectorizeExtractedCast(Op, DAG, Subtarget))\n    return Extract;\n\n  if (Subtarget.hasAVX512() && isScalarFPTypeInSSEReg(DstVT) &&\n      (SrcVT == MVT::i32 || (SrcVT == MVT::i64 && Subtarget.is64Bit()))) {\n    // Conversions from unsigned i32 to f32/f64 are legal,\n    // using VCVTUSI2SS/SD.  Same for i64 in 64-bit mode.\n    return Op;\n  }\n\n  // Promote i32 to i64 and use a signed conversion on 64-bit targets.\n  if (SrcVT == MVT::i32 && Subtarget.is64Bit()) {\n    Src = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i64, Src);\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {DstVT, MVT::Other},\n                         {Chain, Src});\n    return DAG.getNode(ISD::SINT_TO_FP, dl, DstVT, Src);\n  }\n\n  if (SDValue V = LowerI64IntToFP_AVX512DQ(Op, DAG, Subtarget))\n    return V;\n\n  // The transform for i64->f64 isn't correct for 0 when rounding to negative\n  // infinity. It produces -0.0, so disable under strictfp.\n  if (SrcVT == MVT::i64 && DstVT == MVT::f64 && X86ScalarSSEf64 && !IsStrict)\n    return LowerUINT_TO_FP_i64(Op, DAG, Subtarget);\n  if (SrcVT == MVT::i32 && X86ScalarSSEf64 && DstVT != MVT::f80)\n    return LowerUINT_TO_FP_i32(Op, DAG, Subtarget);\n  if (Subtarget.is64Bit() && SrcVT == MVT::i64 &&\n      (DstVT == MVT::f32 || DstVT == MVT::f64))\n    return SDValue();\n\n  // Make a 64-bit buffer, and use it to build an FILD.\n  SDValue StackSlot = DAG.CreateStackTemporary(MVT::i64, 8);\n  int SSFI = cast<FrameIndexSDNode>(StackSlot)->getIndex();\n  Align SlotAlign(8);\n  MachinePointerInfo MPI =\n    MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SSFI);\n  if (SrcVT == MVT::i32) {\n    SDValue OffsetSlot =\n        DAG.getMemBasePlusOffset(StackSlot, TypeSize::Fixed(4), dl);\n    SDValue Store1 = DAG.getStore(Chain, dl, Src, StackSlot, MPI, SlotAlign);\n    SDValue Store2 = DAG.getStore(Store1, dl, DAG.getConstant(0, dl, MVT::i32),\n                                  OffsetSlot, MPI.getWithOffset(4), SlotAlign);\n    std::pair<SDValue, SDValue> Tmp =\n        BuildFILD(DstVT, MVT::i64, dl, Store2, StackSlot, MPI, SlotAlign, DAG);\n    if (IsStrict)\n      return DAG.getMergeValues({Tmp.first, Tmp.second}, dl);\n\n    return Tmp.first;\n  }\n\n  assert(SrcVT == MVT::i64 && \"Unexpected type in UINT_TO_FP\");\n  SDValue ValueToStore = Src;\n  if (isScalarFPTypeInSSEReg(Op.getValueType()) && !Subtarget.is64Bit()) {\n    // Bitcasting to f64 here allows us to do a single 64-bit store from\n    // an SSE register, avoiding the store forwarding penalty that would come\n    // with two 32-bit stores.\n    ValueToStore = DAG.getBitcast(MVT::f64, ValueToStore);\n  }\n  SDValue Store =\n      DAG.getStore(Chain, dl, ValueToStore, StackSlot, MPI, SlotAlign);\n  // For i64 source, we need to add the appropriate power of 2 if the input\n  // was negative. We must be careful to do the computation in x87 extended\n  // precision, not in SSE.\n  SDVTList Tys = DAG.getVTList(MVT::f80, MVT::Other);\n  SDValue Ops[] = { Store, StackSlot };\n  SDValue Fild =\n      DAG.getMemIntrinsicNode(X86ISD::FILD, dl, Tys, Ops, MVT::i64, MPI,\n                              SlotAlign, MachineMemOperand::MOLoad);\n  Chain = Fild.getValue(1);\n\n\n  // Check whether the sign bit is set.\n  SDValue SignSet = DAG.getSetCC(\n      dl, getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(), MVT::i64),\n      Op.getOperand(OpNo), DAG.getConstant(0, dl, MVT::i64), ISD::SETLT);\n\n  // Build a 64 bit pair (FF, 0) in the constant pool, with FF in the hi bits.\n  APInt FF(64, 0x5F80000000000000ULL);\n  SDValue FudgePtr = DAG.getConstantPool(\n      ConstantInt::get(*DAG.getContext(), FF), PtrVT);\n  Align CPAlignment = cast<ConstantPoolSDNode>(FudgePtr)->getAlign();\n\n  // Get a pointer to FF if the sign bit was set, or to 0 otherwise.\n  SDValue Zero = DAG.getIntPtrConstant(0, dl);\n  SDValue Four = DAG.getIntPtrConstant(4, dl);\n  SDValue Offset = DAG.getSelect(dl, Zero.getValueType(), SignSet, Four, Zero);\n  FudgePtr = DAG.getNode(ISD::ADD, dl, PtrVT, FudgePtr, Offset);\n\n  // Load the value out, extending it from f32 to f80.\n  SDValue Fudge = DAG.getExtLoad(\n      ISD::EXTLOAD, dl, MVT::f80, Chain, FudgePtr,\n      MachinePointerInfo::getConstantPool(DAG.getMachineFunction()), MVT::f32,\n      CPAlignment);\n  Chain = Fudge.getValue(1);\n  // Extend everything to 80 bits to force it to be done on x87.\n  // TODO: Are there any fast-math-flags to propagate here?\n  if (IsStrict) {\n    SDValue Add = DAG.getNode(ISD::STRICT_FADD, dl, {MVT::f80, MVT::Other},\n                              {Chain, Fild, Fudge});\n    // STRICT_FP_ROUND can't handle equal types.\n    if (DstVT == MVT::f80)\n      return Add;\n    return DAG.getNode(ISD::STRICT_FP_ROUND, dl, {DstVT, MVT::Other},\n                       {Add.getValue(1), Add, DAG.getIntPtrConstant(0, dl)});\n  }\n  SDValue Add = DAG.getNode(ISD::FADD, dl, MVT::f80, Fild, Fudge);\n  return DAG.getNode(ISD::FP_ROUND, dl, DstVT, Add,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\n// If the given FP_TO_SINT (IsSigned) or FP_TO_UINT (!IsSigned) operation\n// is legal, or has an fp128 or f16 source (which needs to be promoted to f32),\n// just return an SDValue().\n// Otherwise it is assumed to be a conversion from one of f32, f64 or f80\n// to i16, i32 or i64, and we lower it to a legal sequence and return the\n// result.\nSDValue\nX86TargetLowering::FP_TO_INTHelper(SDValue Op, SelectionDAG &DAG,\n                                   bool IsSigned, SDValue &Chain) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n  SDLoc DL(Op);\n\n  EVT DstTy = Op.getValueType();\n  SDValue Value = Op.getOperand(IsStrict ? 1 : 0);\n  EVT TheVT = Value.getValueType();\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n\n  if (TheVT != MVT::f32 && TheVT != MVT::f64 && TheVT != MVT::f80) {\n    // f16 must be promoted before using the lowering in this routine.\n    // fp128 does not use this lowering.\n    return SDValue();\n  }\n\n  // If using FIST to compute an unsigned i64, we'll need some fixup\n  // to handle values above the maximum signed i64.  A FIST is always\n  // used for the 32-bit subtarget, but also for f80 on a 64-bit target.\n  bool UnsignedFixup = !IsSigned && DstTy == MVT::i64;\n\n  // FIXME: This does not generate an invalid exception if the input does not\n  // fit in i32. PR44019\n  if (!IsSigned && DstTy != MVT::i64) {\n    // Replace the fp-to-uint32 operation with an fp-to-sint64 FIST.\n    // The low 32 bits of the fist result will have the correct uint32 result.\n    assert(DstTy == MVT::i32 && \"Unexpected FP_TO_UINT\");\n    DstTy = MVT::i64;\n  }\n\n  assert(DstTy.getSimpleVT() <= MVT::i64 &&\n         DstTy.getSimpleVT() >= MVT::i16 &&\n         \"Unknown FP_TO_INT to lower!\");\n\n  // We lower FP->int64 into FISTP64 followed by a load from a temporary\n  // stack slot.\n  MachineFunction &MF = DAG.getMachineFunction();\n  unsigned MemSize = DstTy.getStoreSize();\n  int SSFI =\n      MF.getFrameInfo().CreateStackObject(MemSize, Align(MemSize), false);\n  SDValue StackSlot = DAG.getFrameIndex(SSFI, PtrVT);\n\n  Chain = IsStrict ? Op.getOperand(0) : DAG.getEntryNode();\n\n  SDValue Adjust; // 0x0 or 0x80000000, for result sign bit adjustment.\n\n  if (UnsignedFixup) {\n    //\n    // Conversion to unsigned i64 is implemented with a select,\n    // depending on whether the source value fits in the range\n    // of a signed i64.  Let Thresh be the FP equivalent of\n    // 0x8000000000000000ULL.\n    //\n    //  Adjust = (Value >= Thresh) ? 0x80000000 : 0;\n    //  FltOfs = (Value >= Thresh) ? 0x80000000 : 0;\n    //  FistSrc = (Value - FltOfs);\n    //  Fist-to-mem64 FistSrc\n    //  Add 0 or 0x800...0ULL to the 64-bit result, which is equivalent\n    //  to XOR'ing the high 32 bits with Adjust.\n    //\n    // Being a power of 2, Thresh is exactly representable in all FP formats.\n    // For X87 we'd like to use the smallest FP type for this constant, but\n    // for DAG type consistency we have to match the FP operand type.\n\n    APFloat Thresh(APFloat::IEEEsingle(), APInt(32, 0x5f000000));\n    LLVM_ATTRIBUTE_UNUSED APFloat::opStatus Status = APFloat::opOK;\n    bool LosesInfo = false;\n    if (TheVT == MVT::f64)\n      // The rounding mode is irrelevant as the conversion should be exact.\n      Status = Thresh.convert(APFloat::IEEEdouble(), APFloat::rmNearestTiesToEven,\n                              &LosesInfo);\n    else if (TheVT == MVT::f80)\n      Status = Thresh.convert(APFloat::x87DoubleExtended(),\n                              APFloat::rmNearestTiesToEven, &LosesInfo);\n\n    assert(Status == APFloat::opOK && !LosesInfo &&\n           \"FP conversion should have been exact\");\n\n    SDValue ThreshVal = DAG.getConstantFP(Thresh, DL, TheVT);\n\n    EVT ResVT = getSetCCResultType(DAG.getDataLayout(),\n                                   *DAG.getContext(), TheVT);\n    SDValue Cmp;\n    if (IsStrict) {\n      Cmp = DAG.getSetCC(DL, ResVT, Value, ThreshVal, ISD::SETGE, Chain,\n                         /*IsSignaling*/ true);\n      Chain = Cmp.getValue(1);\n    } else {\n      Cmp = DAG.getSetCC(DL, ResVT, Value, ThreshVal, ISD::SETGE);\n    }\n\n    // Our preferred lowering of\n    //\n    // (Value >= Thresh) ? 0x8000000000000000ULL : 0\n    //\n    // is\n    //\n    // (Value >= Thresh) << 63\n    //\n    // but since we can get here after LegalOperations, DAGCombine might do the\n    // wrong thing if we create a select. So, directly create the preferred\n    // version.\n    SDValue Zext = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, Cmp);\n    SDValue Const63 = DAG.getConstant(63, DL, MVT::i8);\n    Adjust = DAG.getNode(ISD::SHL, DL, MVT::i64, Zext, Const63);\n\n    SDValue FltOfs = DAG.getSelect(DL, TheVT, Cmp, ThreshVal,\n                                   DAG.getConstantFP(0.0, DL, TheVT));\n\n    if (IsStrict) {\n      Value = DAG.getNode(ISD::STRICT_FSUB, DL, { TheVT, MVT::Other},\n                          { Chain, Value, FltOfs });\n      Chain = Value.getValue(1);\n    } else\n      Value = DAG.getNode(ISD::FSUB, DL, TheVT, Value, FltOfs);\n  }\n\n  MachinePointerInfo MPI = MachinePointerInfo::getFixedStack(MF, SSFI);\n\n  // FIXME This causes a redundant load/store if the SSE-class value is already\n  // in memory, such as if it is on the callstack.\n  if (isScalarFPTypeInSSEReg(TheVT)) {\n    assert(DstTy == MVT::i64 && \"Invalid FP_TO_SINT to lower!\");\n    Chain = DAG.getStore(Chain, DL, Value, StackSlot, MPI);\n    SDVTList Tys = DAG.getVTList(MVT::f80, MVT::Other);\n    SDValue Ops[] = { Chain, StackSlot };\n\n    unsigned FLDSize = TheVT.getStoreSize();\n    assert(FLDSize <= MemSize && \"Stack slot not big enough\");\n    MachineMemOperand *MMO = MF.getMachineMemOperand(\n        MPI, MachineMemOperand::MOLoad, FLDSize, Align(FLDSize));\n    Value = DAG.getMemIntrinsicNode(X86ISD::FLD, DL, Tys, Ops, TheVT, MMO);\n    Chain = Value.getValue(1);\n  }\n\n  // Build the FP_TO_INT*_IN_MEM\n  MachineMemOperand *MMO = MF.getMachineMemOperand(\n      MPI, MachineMemOperand::MOStore, MemSize, Align(MemSize));\n  SDValue Ops[] = { Chain, Value, StackSlot };\n  SDValue FIST = DAG.getMemIntrinsicNode(X86ISD::FP_TO_INT_IN_MEM, DL,\n                                         DAG.getVTList(MVT::Other),\n                                         Ops, DstTy, MMO);\n\n  SDValue Res = DAG.getLoad(Op.getValueType(), SDLoc(Op), FIST, StackSlot, MPI);\n  Chain = Res.getValue(1);\n\n  // If we need an unsigned fixup, XOR the result with adjust.\n  if (UnsignedFixup)\n    Res = DAG.getNode(ISD::XOR, DL, MVT::i64, Res, Adjust);\n\n  return Res;\n}\n\nstatic SDValue LowerAVXExtend(SDValue Op, SelectionDAG &DAG,\n                              const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  SDValue In = Op.getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  SDLoc dl(Op);\n  unsigned Opc = Op.getOpcode();\n\n  assert(VT.isVector() && InVT.isVector() && \"Expected vector type\");\n  assert((Opc == ISD::ANY_EXTEND || Opc == ISD::ZERO_EXTEND) &&\n         \"Unexpected extension opcode\");\n  assert(VT.getVectorNumElements() == InVT.getVectorNumElements() &&\n         \"Expected same number of elements\");\n  assert((VT.getVectorElementType() == MVT::i16 ||\n          VT.getVectorElementType() == MVT::i32 ||\n          VT.getVectorElementType() == MVT::i64) &&\n         \"Unexpected element type\");\n  assert((InVT.getVectorElementType() == MVT::i8 ||\n          InVT.getVectorElementType() == MVT::i16 ||\n          InVT.getVectorElementType() == MVT::i32) &&\n         \"Unexpected element type\");\n\n  unsigned ExtendInVecOpc = getOpcode_EXTEND_VECTOR_INREG(Opc);\n\n  if (VT == MVT::v32i16 && !Subtarget.hasBWI()) {\n    assert(InVT == MVT::v32i8 && \"Unexpected VT!\");\n    return splitVectorIntUnary(Op, DAG);\n  }\n\n  if (Subtarget.hasInt256())\n    return Op;\n\n  // Optimize vectors in AVX mode:\n  //\n  //   v8i16 -> v8i32\n  //   Use vpmovzwd for 4 lower elements  v8i16 -> v4i32.\n  //   Use vpunpckhwd for 4 upper elements  v8i16 -> v4i32.\n  //   Concat upper and lower parts.\n  //\n  //   v4i32 -> v4i64\n  //   Use vpmovzdq for 4 lower elements  v4i32 -> v2i64.\n  //   Use vpunpckhdq for 4 upper elements  v4i32 -> v2i64.\n  //   Concat upper and lower parts.\n  //\n  MVT HalfVT = VT.getHalfNumVectorElementsVT();\n  SDValue OpLo = DAG.getNode(ExtendInVecOpc, dl, HalfVT, In);\n\n  // Short-circuit if we can determine that each 128-bit half is the same value.\n  // Otherwise, this is difficult to match and optimize.\n  if (auto *Shuf = dyn_cast<ShuffleVectorSDNode>(In))\n    if (hasIdenticalHalvesShuffleMask(Shuf->getMask()))\n      return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, OpLo, OpLo);\n\n  SDValue ZeroVec = DAG.getConstant(0, dl, InVT);\n  SDValue Undef = DAG.getUNDEF(InVT);\n  bool NeedZero = Opc == ISD::ZERO_EXTEND;\n  SDValue OpHi = getUnpackh(DAG, dl, InVT, In, NeedZero ? ZeroVec : Undef);\n  OpHi = DAG.getBitcast(HalfVT, OpHi);\n\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, OpLo, OpHi);\n}\n\n// Helper to split and extend a v16i1 mask to v16i8 or v16i16.\nstatic SDValue SplitAndExtendv16i1(unsigned ExtOpc, MVT VT, SDValue In,\n                                   const SDLoc &dl, SelectionDAG &DAG) {\n  assert((VT == MVT::v16i8 || VT == MVT::v16i16) && \"Unexpected VT.\");\n  SDValue Lo = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v8i1, In,\n                           DAG.getIntPtrConstant(0, dl));\n  SDValue Hi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v8i1, In,\n                           DAG.getIntPtrConstant(8, dl));\n  Lo = DAG.getNode(ExtOpc, dl, MVT::v8i16, Lo);\n  Hi = DAG.getNode(ExtOpc, dl, MVT::v8i16, Hi);\n  SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v16i16, Lo, Hi);\n  return DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n}\n\nstatic  SDValue LowerZERO_EXTEND_Mask(SDValue Op,\n                                      const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  MVT VT = Op->getSimpleValueType(0);\n  SDValue In = Op->getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  assert(InVT.getVectorElementType() == MVT::i1 && \"Unexpected input type!\");\n  SDLoc DL(Op);\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // For all vectors, but vXi8 we can just emit a sign_extend and a shift. This\n  // avoids a constant pool load.\n  if (VT.getVectorElementType() != MVT::i8) {\n    SDValue Extend = DAG.getNode(ISD::SIGN_EXTEND, DL, VT, In);\n    return DAG.getNode(ISD::SRL, DL, VT, Extend,\n                       DAG.getConstant(VT.getScalarSizeInBits() - 1, DL, VT));\n  }\n\n  // Extend VT if BWI is not supported.\n  MVT ExtVT = VT;\n  if (!Subtarget.hasBWI()) {\n    // If v16i32 is to be avoided, we'll need to split and concatenate.\n    if (NumElts == 16 && !Subtarget.canExtendTo512DQ())\n      return SplitAndExtendv16i1(ISD::ZERO_EXTEND, VT, In, DL, DAG);\n\n    ExtVT = MVT::getVectorVT(MVT::i32, NumElts);\n  }\n\n  // Widen to 512-bits if VLX is not supported.\n  MVT WideVT = ExtVT;\n  if (!ExtVT.is512BitVector() && !Subtarget.hasVLX()) {\n    NumElts *= 512 / ExtVT.getSizeInBits();\n    InVT = MVT::getVectorVT(MVT::i1, NumElts);\n    In = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, InVT, DAG.getUNDEF(InVT),\n                     In, DAG.getIntPtrConstant(0, DL));\n    WideVT = MVT::getVectorVT(ExtVT.getVectorElementType(),\n                              NumElts);\n  }\n\n  SDValue One = DAG.getConstant(1, DL, WideVT);\n  SDValue Zero = DAG.getConstant(0, DL, WideVT);\n\n  SDValue SelectedVal = DAG.getSelect(DL, WideVT, In, One, Zero);\n\n  // Truncate if we had to extend above.\n  if (VT != ExtVT) {\n    WideVT = MVT::getVectorVT(MVT::i8, NumElts);\n    SelectedVal = DAG.getNode(ISD::TRUNCATE, DL, WideVT, SelectedVal);\n  }\n\n  // Extract back to 128/256-bit if we widened.\n  if (WideVT != VT)\n    SelectedVal = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, SelectedVal,\n                              DAG.getIntPtrConstant(0, DL));\n\n  return SelectedVal;\n}\n\nstatic SDValue LowerZERO_EXTEND(SDValue Op, const X86Subtarget &Subtarget,\n                                SelectionDAG &DAG) {\n  SDValue In = Op.getOperand(0);\n  MVT SVT = In.getSimpleValueType();\n\n  if (SVT.getVectorElementType() == MVT::i1)\n    return LowerZERO_EXTEND_Mask(Op, Subtarget, DAG);\n\n  assert(Subtarget.hasAVX() && \"Expected AVX support\");\n  return LowerAVXExtend(Op, DAG, Subtarget);\n}\n\n/// Helper to recursively truncate vector elements in half with PACKSS/PACKUS.\n/// It makes use of the fact that vectors with enough leading sign/zero bits\n/// prevent the PACKSS/PACKUS from saturating the results.\n/// AVX2 (Int256) sub-targets require extra shuffling as the PACK*S operates\n/// within each 128-bit lane.\nstatic SDValue truncateVectorWithPACK(unsigned Opcode, EVT DstVT, SDValue In,\n                                      const SDLoc &DL, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  assert((Opcode == X86ISD::PACKSS || Opcode == X86ISD::PACKUS) &&\n         \"Unexpected PACK opcode\");\n  assert(DstVT.isVector() && \"VT not a vector?\");\n\n  // Requires SSE2 for PACKSS (SSE41 PACKUSDW is handled below).\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  EVT SrcVT = In.getValueType();\n\n  // No truncation required, we might get here due to recursive calls.\n  if (SrcVT == DstVT)\n    return In;\n\n  // We only support vector truncation to 64bits or greater from a\n  // 128bits or greater source.\n  unsigned DstSizeInBits = DstVT.getSizeInBits();\n  unsigned SrcSizeInBits = SrcVT.getSizeInBits();\n  if ((DstSizeInBits % 64) != 0 || (SrcSizeInBits % 128) != 0)\n    return SDValue();\n\n  unsigned NumElems = SrcVT.getVectorNumElements();\n  if (!isPowerOf2_32(NumElems))\n    return SDValue();\n\n  LLVMContext &Ctx = *DAG.getContext();\n  assert(DstVT.getVectorNumElements() == NumElems && \"Illegal truncation\");\n  assert(SrcSizeInBits > DstSizeInBits && \"Illegal truncation\");\n\n  EVT PackedSVT = EVT::getIntegerVT(Ctx, SrcVT.getScalarSizeInBits() / 2);\n\n  // Pack to the largest type possible:\n  // vXi64/vXi32 -> PACK*SDW and vXi16 -> PACK*SWB.\n  EVT InVT = MVT::i16, OutVT = MVT::i8;\n  if (SrcVT.getScalarSizeInBits() > 16 &&\n      (Opcode == X86ISD::PACKSS || Subtarget.hasSSE41())) {\n    InVT = MVT::i32;\n    OutVT = MVT::i16;\n  }\n\n  // 128bit -> 64bit truncate - PACK 128-bit src in the lower subvector.\n  if (SrcVT.is128BitVector()) {\n    InVT = EVT::getVectorVT(Ctx, InVT, 128 / InVT.getSizeInBits());\n    OutVT = EVT::getVectorVT(Ctx, OutVT, 128 / OutVT.getSizeInBits());\n    In = DAG.getBitcast(InVT, In);\n    SDValue Res = DAG.getNode(Opcode, DL, OutVT, In, DAG.getUNDEF(InVT));\n    Res = extractSubVector(Res, 0, DAG, DL, 64);\n    return DAG.getBitcast(DstVT, Res);\n  }\n\n  // Split lower/upper subvectors.\n  SDValue Lo, Hi;\n  std::tie(Lo, Hi) = splitVector(In, DAG, DL);\n\n  unsigned SubSizeInBits = SrcSizeInBits / 2;\n  InVT = EVT::getVectorVT(Ctx, InVT, SubSizeInBits / InVT.getSizeInBits());\n  OutVT = EVT::getVectorVT(Ctx, OutVT, SubSizeInBits / OutVT.getSizeInBits());\n\n  // 256bit -> 128bit truncate - PACK lower/upper 128-bit subvectors.\n  if (SrcVT.is256BitVector() && DstVT.is128BitVector()) {\n    Lo = DAG.getBitcast(InVT, Lo);\n    Hi = DAG.getBitcast(InVT, Hi);\n    SDValue Res = DAG.getNode(Opcode, DL, OutVT, Lo, Hi);\n    return DAG.getBitcast(DstVT, Res);\n  }\n\n  // AVX2: 512bit -> 256bit truncate - PACK lower/upper 256-bit subvectors.\n  // AVX2: 512bit -> 128bit truncate - PACK(PACK, PACK).\n  if (SrcVT.is512BitVector() && Subtarget.hasInt256()) {\n    Lo = DAG.getBitcast(InVT, Lo);\n    Hi = DAG.getBitcast(InVT, Hi);\n    SDValue Res = DAG.getNode(Opcode, DL, OutVT, Lo, Hi);\n\n    // 256-bit PACK(ARG0, ARG1) leaves us with ((LO0,LO1),(HI0,HI1)),\n    // so we need to shuffle to get ((LO0,HI0),(LO1,HI1)).\n    // Scale shuffle mask to avoid bitcasts and help ComputeNumSignBits.\n    SmallVector<int, 64> Mask;\n    int Scale = 64 / OutVT.getScalarSizeInBits();\n    narrowShuffleMaskElts(Scale, { 0, 2, 1, 3 }, Mask);\n    Res = DAG.getVectorShuffle(OutVT, DL, Res, Res, Mask);\n\n    if (DstVT.is256BitVector())\n      return DAG.getBitcast(DstVT, Res);\n\n    // If 512bit -> 128bit truncate another stage.\n    EVT PackedVT = EVT::getVectorVT(Ctx, PackedSVT, NumElems);\n    Res = DAG.getBitcast(PackedVT, Res);\n    return truncateVectorWithPACK(Opcode, DstVT, Res, DL, DAG, Subtarget);\n  }\n\n  // Recursively pack lower/upper subvectors, concat result and pack again.\n  assert(SrcSizeInBits >= 256 && \"Expected 256-bit vector or greater\");\n  EVT PackedVT = EVT::getVectorVT(Ctx, PackedSVT, NumElems / 2);\n  Lo = truncateVectorWithPACK(Opcode, PackedVT, Lo, DL, DAG, Subtarget);\n  Hi = truncateVectorWithPACK(Opcode, PackedVT, Hi, DL, DAG, Subtarget);\n\n  PackedVT = EVT::getVectorVT(Ctx, PackedSVT, NumElems);\n  SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, DL, PackedVT, Lo, Hi);\n  return truncateVectorWithPACK(Opcode, DstVT, Res, DL, DAG, Subtarget);\n}\n\nstatic SDValue LowerTruncateVecI1(SDValue Op, SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue In = Op.getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n\n  assert(VT.getVectorElementType() == MVT::i1 && \"Unexpected vector type.\");\n\n  // Shift LSB to MSB and use VPMOVB/W2M or TESTD/Q.\n  unsigned ShiftInx = InVT.getScalarSizeInBits() - 1;\n  if (InVT.getScalarSizeInBits() <= 16) {\n    if (Subtarget.hasBWI()) {\n      // legal, will go to VPMOVB2M, VPMOVW2M\n      if (DAG.ComputeNumSignBits(In) < InVT.getScalarSizeInBits()) {\n        // We need to shift to get the lsb into sign position.\n        // Shift packed bytes not supported natively, bitcast to word\n        MVT ExtVT = MVT::getVectorVT(MVT::i16, InVT.getSizeInBits()/16);\n        In = DAG.getNode(ISD::SHL, DL, ExtVT,\n                         DAG.getBitcast(ExtVT, In),\n                         DAG.getConstant(ShiftInx, DL, ExtVT));\n        In = DAG.getBitcast(InVT, In);\n      }\n      return DAG.getSetCC(DL, VT, DAG.getConstant(0, DL, InVT),\n                          In, ISD::SETGT);\n    }\n    // Use TESTD/Q, extended vector to packed dword/qword.\n    assert((InVT.is256BitVector() || InVT.is128BitVector()) &&\n           \"Unexpected vector type.\");\n    unsigned NumElts = InVT.getVectorNumElements();\n    assert((NumElts == 8 || NumElts == 16) && \"Unexpected number of elements\");\n    // We need to change to a wider element type that we have support for.\n    // For 8 element vectors this is easy, we either extend to v8i32 or v8i64.\n    // For 16 element vectors we extend to v16i32 unless we are explicitly\n    // trying to avoid 512-bit vectors. If we are avoiding 512-bit vectors\n    // we need to split into two 8 element vectors which we can extend to v8i32,\n    // truncate and concat the results. There's an additional complication if\n    // the original type is v16i8. In that case we can't split the v16i8\n    // directly, so we need to shuffle high elements to low and use\n    // sign_extend_vector_inreg.\n    if (NumElts == 16 && !Subtarget.canExtendTo512DQ()) {\n      SDValue Lo, Hi;\n      if (InVT == MVT::v16i8) {\n        Lo = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, DL, MVT::v8i32, In);\n        Hi = DAG.getVectorShuffle(\n            InVT, DL, In, In,\n            {8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1});\n        Hi = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, DL, MVT::v8i32, Hi);\n      } else {\n        assert(InVT == MVT::v16i16 && \"Unexpected VT!\");\n        Lo = extract128BitVector(In, 0, DAG, DL);\n        Hi = extract128BitVector(In, 8, DAG, DL);\n      }\n      // We're split now, just emit two truncates and a concat. The two\n      // truncates will trigger legalization to come back to this function.\n      Lo = DAG.getNode(ISD::TRUNCATE, DL, MVT::v8i1, Lo);\n      Hi = DAG.getNode(ISD::TRUNCATE, DL, MVT::v8i1, Hi);\n      return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Lo, Hi);\n    }\n    // We either have 8 elements or we're allowed to use 512-bit vectors.\n    // If we have VLX, we want to use the narrowest vector that can get the\n    // job done so we use vXi32.\n    MVT EltVT = Subtarget.hasVLX() ? MVT::i32 : MVT::getIntegerVT(512/NumElts);\n    MVT ExtVT = MVT::getVectorVT(EltVT, NumElts);\n    In = DAG.getNode(ISD::SIGN_EXTEND, DL, ExtVT, In);\n    InVT = ExtVT;\n    ShiftInx = InVT.getScalarSizeInBits() - 1;\n  }\n\n  if (DAG.ComputeNumSignBits(In) < InVT.getScalarSizeInBits()) {\n    // We need to shift to get the lsb into sign position.\n    In = DAG.getNode(ISD::SHL, DL, InVT, In,\n                     DAG.getConstant(ShiftInx, DL, InVT));\n  }\n  // If we have DQI, emit a pattern that will be iseled as vpmovq2m/vpmovd2m.\n  if (Subtarget.hasDQI())\n    return DAG.getSetCC(DL, VT, DAG.getConstant(0, DL, InVT), In, ISD::SETGT);\n  return DAG.getSetCC(DL, VT, In, DAG.getConstant(0, DL, InVT), ISD::SETNE);\n}\n\nSDValue X86TargetLowering::LowerTRUNCATE(SDValue Op, SelectionDAG &DAG) const {\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue In = Op.getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  unsigned InNumEltBits = InVT.getScalarSizeInBits();\n\n  assert(VT.getVectorNumElements() == InVT.getVectorNumElements() &&\n         \"Invalid TRUNCATE operation\");\n\n  // If we're called by the type legalizer, handle a few cases.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!TLI.isTypeLegal(InVT)) {\n    if ((InVT == MVT::v8i64 || InVT == MVT::v16i32 || InVT == MVT::v16i64) &&\n        VT.is128BitVector()) {\n      assert((InVT == MVT::v16i64 || Subtarget.hasVLX()) &&\n             \"Unexpected subtarget!\");\n      // The default behavior is to truncate one step, concatenate, and then\n      // truncate the remainder. We'd rather produce two 64-bit results and\n      // concatenate those.\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = DAG.SplitVector(In, DL);\n\n      EVT LoVT, HiVT;\n      std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n\n      Lo = DAG.getNode(ISD::TRUNCATE, DL, LoVT, Lo);\n      Hi = DAG.getNode(ISD::TRUNCATE, DL, HiVT, Hi);\n      return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Lo, Hi);\n    }\n\n    // Otherwise let default legalization handle it.\n    return SDValue();\n  }\n\n  if (VT.getVectorElementType() == MVT::i1)\n    return LowerTruncateVecI1(Op, DAG, Subtarget);\n\n  // vpmovqb/w/d, vpmovdb/w, vpmovwb\n  if (Subtarget.hasAVX512()) {\n    if (InVT == MVT::v32i16 && !Subtarget.hasBWI()) {\n      assert(VT == MVT::v32i8 && \"Unexpected VT!\");\n      return splitVectorIntUnary(Op, DAG);\n    }\n\n    // word to byte only under BWI. Otherwise we have to promoted to v16i32\n    // and then truncate that. But we should only do that if we haven't been\n    // asked to avoid 512-bit vectors. The actual promotion to v16i32 will be\n    // handled by isel patterns.\n    if (InVT != MVT::v16i16 || Subtarget.hasBWI() ||\n        Subtarget.canExtendTo512DQ())\n      return Op;\n  }\n\n  unsigned NumPackedSignBits = std::min<unsigned>(VT.getScalarSizeInBits(), 16);\n  unsigned NumPackedZeroBits = Subtarget.hasSSE41() ? NumPackedSignBits : 8;\n\n  // Truncate with PACKUS if we are truncating a vector with leading zero bits\n  // that extend all the way to the packed/truncated value.\n  // Pre-SSE41 we can only use PACKUSWB.\n  KnownBits Known = DAG.computeKnownBits(In);\n  if ((InNumEltBits - NumPackedZeroBits) <= Known.countMinLeadingZeros())\n    if (SDValue V =\n            truncateVectorWithPACK(X86ISD::PACKUS, VT, In, DL, DAG, Subtarget))\n      return V;\n\n  // Truncate with PACKSS if we are truncating a vector with sign-bits that\n  // extend all the way to the packed/truncated value.\n  if ((InNumEltBits - NumPackedSignBits) < DAG.ComputeNumSignBits(In))\n    if (SDValue V =\n            truncateVectorWithPACK(X86ISD::PACKSS, VT, In, DL, DAG, Subtarget))\n      return V;\n\n  // Handle truncation of V256 to V128 using shuffles.\n  assert(VT.is128BitVector() && InVT.is256BitVector() && \"Unexpected types!\");\n\n  if ((VT == MVT::v4i32) && (InVT == MVT::v4i64)) {\n    In = DAG.getBitcast(MVT::v8i32, In);\n\n    // On AVX2, v4i64 -> v4i32 becomes VPERMD.\n    if (Subtarget.hasInt256()) {\n      static const int ShufMask[] = {0, 2, 4, 6, -1, -1, -1, -1};\n      In = DAG.getVectorShuffle(MVT::v8i32, DL, In, In, ShufMask);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, In,\n                         DAG.getIntPtrConstant(0, DL));\n    }\n\n    SDValue OpLo = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v4i32, In,\n                               DAG.getIntPtrConstant(0, DL));\n    SDValue OpHi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v4i32, In,\n                               DAG.getIntPtrConstant(4, DL));\n    static const int ShufMask[] = {0, 2, 4, 6};\n    return DAG.getVectorShuffle(VT, DL, OpLo, OpHi, ShufMask);\n  }\n\n  if ((VT == MVT::v8i16) && (InVT == MVT::v8i32)) {\n    In = DAG.getBitcast(MVT::v32i8, In);\n\n    // On AVX2, v8i32 -> v8i16 becomes PSHUFB.\n    if (Subtarget.hasInt256()) {\n      // The PSHUFB mask:\n      static const int ShufMask1[] = { 0,  1,  4,  5,  8,  9, 12, 13,\n                                      -1, -1, -1, -1, -1, -1, -1, -1,\n                                      16, 17, 20, 21, 24, 25, 28, 29,\n                                      -1, -1, -1, -1, -1, -1, -1, -1 };\n      In = DAG.getVectorShuffle(MVT::v32i8, DL, In, In, ShufMask1);\n      In = DAG.getBitcast(MVT::v4i64, In);\n\n      static const int ShufMask2[] = {0, 2, -1, -1};\n      In = DAG.getVectorShuffle(MVT::v4i64, DL, In, In, ShufMask2);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v8i16,\n                         DAG.getBitcast(MVT::v16i16, In),\n                         DAG.getIntPtrConstant(0, DL));\n    }\n\n    SDValue OpLo = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v16i8, In,\n                               DAG.getIntPtrConstant(0, DL));\n    SDValue OpHi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v16i8, In,\n                               DAG.getIntPtrConstant(16, DL));\n\n    // The PSHUFB mask:\n    static const int ShufMask1[] = {0,  1,  4,  5,  8,  9, 12, 13,\n                                   -1, -1, -1, -1, -1, -1, -1, -1};\n\n    OpLo = DAG.getVectorShuffle(MVT::v16i8, DL, OpLo, OpLo, ShufMask1);\n    OpHi = DAG.getVectorShuffle(MVT::v16i8, DL, OpHi, OpHi, ShufMask1);\n\n    OpLo = DAG.getBitcast(MVT::v4i32, OpLo);\n    OpHi = DAG.getBitcast(MVT::v4i32, OpHi);\n\n    // The MOVLHPS Mask:\n    static const int ShufMask2[] = {0, 1, 4, 5};\n    SDValue res = DAG.getVectorShuffle(MVT::v4i32, DL, OpLo, OpHi, ShufMask2);\n    return DAG.getBitcast(MVT::v8i16, res);\n  }\n\n  if (VT == MVT::v16i8 && InVT == MVT::v16i16) {\n    // Use an AND to zero uppper bits for PACKUS.\n    In = DAG.getNode(ISD::AND, DL, InVT, In, DAG.getConstant(255, DL, InVT));\n\n    SDValue InLo = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v8i16, In,\n                               DAG.getIntPtrConstant(0, DL));\n    SDValue InHi = DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, MVT::v8i16, In,\n                               DAG.getIntPtrConstant(8, DL));\n    return DAG.getNode(X86ISD::PACKUS, DL, VT, InLo, InHi);\n  }\n\n  llvm_unreachable(\"All 256->128 cases should have been handled above!\");\n}\n\nSDValue X86TargetLowering::LowerFP_TO_INT(SDValue Op, SelectionDAG &DAG) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n  bool IsSigned = Op.getOpcode() == ISD::FP_TO_SINT ||\n                  Op.getOpcode() == ISD::STRICT_FP_TO_SINT;\n  MVT VT = Op->getSimpleValueType(0);\n  SDValue Src = Op.getOperand(IsStrict ? 1 : 0);\n  MVT SrcVT = Src.getSimpleValueType();\n  SDLoc dl(Op);\n\n  if (VT.isVector()) {\n    if (VT == MVT::v2i1 && SrcVT == MVT::v2f64) {\n      MVT ResVT = MVT::v4i32;\n      MVT TruncVT = MVT::v4i1;\n      unsigned Opc;\n      if (IsStrict)\n        Opc = IsSigned ? X86ISD::STRICT_CVTTP2SI : X86ISD::STRICT_CVTTP2UI;\n      else\n        Opc = IsSigned ? X86ISD::CVTTP2SI : X86ISD::CVTTP2UI;\n\n      if (!IsSigned && !Subtarget.hasVLX()) {\n        assert(Subtarget.useAVX512Regs() && \"Unexpected features!\");\n        // Widen to 512-bits.\n        ResVT = MVT::v8i32;\n        TruncVT = MVT::v8i1;\n        Opc = Op.getOpcode();\n        // Need to concat with zero vector for strict fp to avoid spurious\n        // exceptions.\n        // TODO: Should we just do this for non-strict as well?\n        SDValue Tmp = IsStrict ? DAG.getConstantFP(0.0, dl, MVT::v8f64)\n                               : DAG.getUNDEF(MVT::v8f64);\n        Src = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, MVT::v8f64, Tmp, Src,\n                          DAG.getIntPtrConstant(0, dl));\n      }\n      SDValue Res, Chain;\n      if (IsStrict) {\n        Res =\n            DAG.getNode(Opc, dl, {ResVT, MVT::Other}, {Op->getOperand(0), Src});\n        Chain = Res.getValue(1);\n      } else {\n        Res = DAG.getNode(Opc, dl, ResVT, Src);\n      }\n\n      Res = DAG.getNode(ISD::TRUNCATE, dl, TruncVT, Res);\n      Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v2i1, Res,\n                        DAG.getIntPtrConstant(0, dl));\n      if (IsStrict)\n        return DAG.getMergeValues({Res, Chain}, dl);\n      return Res;\n    }\n\n    // v8f64->v8i32 is legal, but we need v8i32 to be custom for v8f32.\n    if (VT == MVT::v8i32 && SrcVT == MVT::v8f64) {\n      assert(!IsSigned && \"Expected unsigned conversion!\");\n      assert(Subtarget.useAVX512Regs() && \"Requires avx512f\");\n      return Op;\n    }\n\n    // Widen vXi32 fp_to_uint with avx512f to 512-bit source.\n    if ((VT == MVT::v4i32 || VT == MVT::v8i32) &&\n        (SrcVT == MVT::v4f64 || SrcVT == MVT::v4f32 || SrcVT == MVT::v8f32)) {\n      assert(!IsSigned && \"Expected unsigned conversion!\");\n      assert(Subtarget.useAVX512Regs() && !Subtarget.hasVLX() &&\n             \"Unexpected features!\");\n      MVT WideVT = SrcVT == MVT::v4f64 ? MVT::v8f64 : MVT::v16f32;\n      MVT ResVT = SrcVT == MVT::v4f64 ? MVT::v8i32 : MVT::v16i32;\n      // Need to concat with zero vector for strict fp to avoid spurious\n      // exceptions.\n      // TODO: Should we just do this for non-strict as well?\n      SDValue Tmp =\n          IsStrict ? DAG.getConstantFP(0.0, dl, WideVT) : DAG.getUNDEF(WideVT);\n      Src = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideVT, Tmp, Src,\n                        DAG.getIntPtrConstant(0, dl));\n\n      SDValue Res, Chain;\n      if (IsStrict) {\n        Res = DAG.getNode(ISD::STRICT_FP_TO_UINT, dl, {ResVT, MVT::Other},\n                          {Op->getOperand(0), Src});\n        Chain = Res.getValue(1);\n      } else {\n        Res = DAG.getNode(ISD::FP_TO_UINT, dl, ResVT, Src);\n      }\n\n      Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, Res,\n                        DAG.getIntPtrConstant(0, dl));\n\n      if (IsStrict)\n        return DAG.getMergeValues({Res, Chain}, dl);\n      return Res;\n    }\n\n    // Widen vXi64 fp_to_uint/fp_to_sint with avx512dq to 512-bit source.\n    if ((VT == MVT::v2i64 || VT == MVT::v4i64) &&\n        (SrcVT == MVT::v2f64 || SrcVT == MVT::v4f64 || SrcVT == MVT::v4f32)) {\n      assert(Subtarget.useAVX512Regs() && Subtarget.hasDQI() &&\n             !Subtarget.hasVLX() && \"Unexpected features!\");\n      MVT WideVT = SrcVT == MVT::v4f32 ? MVT::v8f32 : MVT::v8f64;\n      // Need to concat with zero vector for strict fp to avoid spurious\n      // exceptions.\n      // TODO: Should we just do this for non-strict as well?\n      SDValue Tmp =\n          IsStrict ? DAG.getConstantFP(0.0, dl, WideVT) : DAG.getUNDEF(WideVT);\n      Src = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, WideVT, Tmp, Src,\n                        DAG.getIntPtrConstant(0, dl));\n\n      SDValue Res, Chain;\n      if (IsStrict) {\n        Res = DAG.getNode(Op.getOpcode(), dl, {MVT::v8i64, MVT::Other},\n                          {Op->getOperand(0), Src});\n        Chain = Res.getValue(1);\n      } else {\n        Res = DAG.getNode(Op.getOpcode(), dl, MVT::v8i64, Src);\n      }\n\n      Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, Res,\n                        DAG.getIntPtrConstant(0, dl));\n\n      if (IsStrict)\n        return DAG.getMergeValues({Res, Chain}, dl);\n      return Res;\n    }\n\n    if (VT == MVT::v2i64 && SrcVT  == MVT::v2f32) {\n      if (!Subtarget.hasVLX()) {\n        // Non-strict nodes without VLX can we widened to v4f32->v4i64 by type\n        // legalizer and then widened again by vector op legalization.\n        if (!IsStrict)\n          return SDValue();\n\n        SDValue Zero = DAG.getConstantFP(0.0, dl, MVT::v2f32);\n        SDValue Tmp = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8f32,\n                                  {Src, Zero, Zero, Zero});\n        Tmp = DAG.getNode(Op.getOpcode(), dl, {MVT::v8i64, MVT::Other},\n                          {Op->getOperand(0), Tmp});\n        SDValue Chain = Tmp.getValue(1);\n        Tmp = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v2i64, Tmp,\n                          DAG.getIntPtrConstant(0, dl));\n        if (IsStrict)\n          return DAG.getMergeValues({Tmp, Chain}, dl);\n        return Tmp;\n      }\n\n      assert(Subtarget.hasDQI() && Subtarget.hasVLX() && \"Requires AVX512DQVL\");\n      SDValue Tmp = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f32, Src,\n                                DAG.getUNDEF(MVT::v2f32));\n      if (IsStrict) {\n        unsigned Opc = IsSigned ? X86ISD::STRICT_CVTTP2SI\n                                : X86ISD::STRICT_CVTTP2UI;\n        return DAG.getNode(Opc, dl, {VT, MVT::Other}, {Op->getOperand(0), Tmp});\n      }\n      unsigned Opc = IsSigned ? X86ISD::CVTTP2SI : X86ISD::CVTTP2UI;\n      return DAG.getNode(Opc, dl, VT, Tmp);\n    }\n\n    return SDValue();\n  }\n\n  assert(!VT.isVector());\n\n  bool UseSSEReg = isScalarFPTypeInSSEReg(SrcVT);\n\n  if (!IsSigned && UseSSEReg) {\n    // Conversions from f32/f64 with AVX512 should be legal.\n    if (Subtarget.hasAVX512())\n      return Op;\n\n    // Use default expansion for i64.\n    if (VT == MVT::i64)\n      return SDValue();\n\n    assert(VT == MVT::i32 && \"Unexpected VT!\");\n\n    // Promote i32 to i64 and use a signed operation on 64-bit targets.\n    // FIXME: This does not generate an invalid exception if the input does not\n    // fit in i32. PR44019\n    if (Subtarget.is64Bit()) {\n      SDValue Res, Chain;\n      if (IsStrict) {\n        Res = DAG.getNode(ISD::STRICT_FP_TO_SINT, dl, { MVT::i64, MVT::Other},\n                          { Op.getOperand(0), Src });\n        Chain = Res.getValue(1);\n      } else\n        Res = DAG.getNode(ISD::FP_TO_SINT, dl, MVT::i64, Src);\n\n      Res = DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n      if (IsStrict)\n        return DAG.getMergeValues({ Res, Chain }, dl);\n      return Res;\n    }\n\n    // Use default expansion for SSE1/2 targets without SSE3. With SSE3 we can\n    // use fisttp which will be handled later.\n    if (!Subtarget.hasSSE3())\n      return SDValue();\n  }\n\n  // Promote i16 to i32 if we can use a SSE operation or the type is f128.\n  // FIXME: This does not generate an invalid exception if the input does not\n  // fit in i16. PR44019\n  if (VT == MVT::i16 && (UseSSEReg || SrcVT == MVT::f128)) {\n    assert(IsSigned && \"Expected i16 FP_TO_UINT to have been promoted!\");\n    SDValue Res, Chain;\n    if (IsStrict) {\n      Res = DAG.getNode(ISD::STRICT_FP_TO_SINT, dl, { MVT::i32, MVT::Other},\n                        { Op.getOperand(0), Src });\n      Chain = Res.getValue(1);\n    } else\n      Res = DAG.getNode(ISD::FP_TO_SINT, dl, MVT::i32, Src);\n\n    Res = DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n    if (IsStrict)\n      return DAG.getMergeValues({ Res, Chain }, dl);\n    return Res;\n  }\n\n  // If this is a FP_TO_SINT using SSEReg we're done.\n  if (UseSSEReg && IsSigned)\n    return Op;\n\n  // fp128 needs to use a libcall.\n  if (SrcVT == MVT::f128) {\n    RTLIB::Libcall LC;\n    if (IsSigned)\n      LC = RTLIB::getFPTOSINT(SrcVT, VT);\n    else\n      LC = RTLIB::getFPTOUINT(SrcVT, VT);\n\n    SDValue Chain = IsStrict ? Op.getOperand(0) : SDValue();\n    MakeLibCallOptions CallOptions;\n    std::pair<SDValue, SDValue> Tmp = makeLibCall(DAG, LC, VT, Src, CallOptions,\n                                                  SDLoc(Op), Chain);\n\n    if (IsStrict)\n      return DAG.getMergeValues({ Tmp.first, Tmp.second }, dl);\n\n    return Tmp.first;\n  }\n\n  // Fall back to X87.\n  SDValue Chain;\n  if (SDValue V = FP_TO_INTHelper(Op, DAG, IsSigned, Chain)) {\n    if (IsStrict)\n      return DAG.getMergeValues({V, Chain}, dl);\n    return V;\n  }\n\n  llvm_unreachable(\"Expected FP_TO_INTHelper to handle all remaining cases.\");\n}\n\nSDValue X86TargetLowering::LowerLRINT_LLRINT(SDValue Op,\n                                             SelectionDAG &DAG) const {\n  SDValue Src = Op.getOperand(0);\n  MVT SrcVT = Src.getSimpleValueType();\n\n  // If the source is in an SSE register, the node is Legal.\n  if (isScalarFPTypeInSSEReg(SrcVT))\n    return Op;\n\n  return LRINT_LLRINTHelper(Op.getNode(), DAG);\n}\n\nSDValue X86TargetLowering::LRINT_LLRINTHelper(SDNode *N,\n                                              SelectionDAG &DAG) const {\n  EVT DstVT = N->getValueType(0);\n  SDValue Src = N->getOperand(0);\n  EVT SrcVT = Src.getValueType();\n\n  if (SrcVT != MVT::f32 && SrcVT != MVT::f64 && SrcVT != MVT::f80) {\n    // f16 must be promoted before using the lowering in this routine.\n    // fp128 does not use this lowering.\n    return SDValue();\n  }\n\n  SDLoc DL(N);\n  SDValue Chain = DAG.getEntryNode();\n\n  bool UseSSE = isScalarFPTypeInSSEReg(SrcVT);\n\n  // If we're converting from SSE, the stack slot needs to hold both types.\n  // Otherwise it only needs to hold the DstVT.\n  EVT OtherVT = UseSSE ? SrcVT : DstVT;\n  SDValue StackPtr = DAG.CreateStackTemporary(DstVT, OtherVT);\n  int SPFI = cast<FrameIndexSDNode>(StackPtr.getNode())->getIndex();\n  MachinePointerInfo MPI =\n      MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SPFI);\n\n  if (UseSSE) {\n    assert(DstVT == MVT::i64 && \"Invalid LRINT/LLRINT to lower!\");\n    Chain = DAG.getStore(Chain, DL, Src, StackPtr, MPI);\n    SDVTList Tys = DAG.getVTList(MVT::f80, MVT::Other);\n    SDValue Ops[] = { Chain, StackPtr };\n\n    Src = DAG.getMemIntrinsicNode(X86ISD::FLD, DL, Tys, Ops, SrcVT, MPI,\n                                  /*Align*/ None, MachineMemOperand::MOLoad);\n    Chain = Src.getValue(1);\n  }\n\n  SDValue StoreOps[] = { Chain, Src, StackPtr };\n  Chain = DAG.getMemIntrinsicNode(X86ISD::FIST, DL, DAG.getVTList(MVT::Other),\n                                  StoreOps, DstVT, MPI, /*Align*/ None,\n                                  MachineMemOperand::MOStore);\n\n  return DAG.getLoad(DstVT, DL, Chain, StackPtr, MPI);\n}\n\nSDValue\nX86TargetLowering::LowerFP_TO_INT_SAT(SDValue Op, SelectionDAG &DAG) const {\n  // This is based on the TargetLowering::expandFP_TO_INT_SAT implementation,\n  // but making use of X86 specifics to produce better instruction sequences.\n  SDNode *Node = Op.getNode();\n  bool IsSigned = Node->getOpcode() == ISD::FP_TO_SINT_SAT;\n  unsigned FpToIntOpcode = IsSigned ? ISD::FP_TO_SINT : ISD::FP_TO_UINT;\n  SDLoc dl(SDValue(Node, 0));\n  SDValue Src = Node->getOperand(0);\n\n  // There are three types involved here: SrcVT is the source floating point\n  // type, DstVT is the type of the result, and TmpVT is the result of the\n  // intermediate FP_TO_*INT operation we'll use (which may be a promotion of\n  // DstVT).\n  EVT SrcVT = Src.getValueType();\n  EVT DstVT = Node->getValueType(0);\n  EVT TmpVT = DstVT;\n\n  // This code is only for floats and doubles. Fall back to generic code for\n  // anything else.\n  if (!isScalarFPTypeInSSEReg(SrcVT))\n    return SDValue();\n\n  unsigned SatWidth = Node->getConstantOperandVal(1);\n  unsigned DstWidth = DstVT.getScalarSizeInBits();\n  unsigned TmpWidth = TmpVT.getScalarSizeInBits();\n  assert(SatWidth <= DstWidth && SatWidth <= TmpWidth &&\n         \"Expected saturation width smaller than result width\");\n\n  // Promote result of FP_TO_*INT to at least 32 bits.\n  if (TmpWidth < 32) {\n    TmpVT = MVT::i32;\n    TmpWidth = 32;\n  }\n\n  // Promote conversions to unsigned 32-bit to 64-bit, because it will allow\n  // us to use a native signed conversion instead.\n  if (SatWidth == 32 && !IsSigned && Subtarget.is64Bit()) {\n    TmpVT = MVT::i64;\n    TmpWidth = 64;\n  }\n\n  // If the saturation width is smaller than the size of the temporary result,\n  // we can always use signed conversion, which is native.\n  if (SatWidth < TmpWidth)\n    FpToIntOpcode = ISD::FP_TO_SINT;\n\n  // Determine minimum and maximum integer values and their corresponding\n  // floating-point values.\n  APInt MinInt, MaxInt;\n  if (IsSigned) {\n    MinInt = APInt::getSignedMinValue(SatWidth).sextOrSelf(DstWidth);\n    MaxInt = APInt::getSignedMaxValue(SatWidth).sextOrSelf(DstWidth);\n  } else {\n    MinInt = APInt::getMinValue(SatWidth).zextOrSelf(DstWidth);\n    MaxInt = APInt::getMaxValue(SatWidth).zextOrSelf(DstWidth);\n  }\n\n  APFloat MinFloat(DAG.EVTToAPFloatSemantics(SrcVT));\n  APFloat MaxFloat(DAG.EVTToAPFloatSemantics(SrcVT));\n\n  APFloat::opStatus MinStatus = MinFloat.convertFromAPInt(\n    MinInt, IsSigned, APFloat::rmTowardZero);\n  APFloat::opStatus MaxStatus = MaxFloat.convertFromAPInt(\n    MaxInt, IsSigned, APFloat::rmTowardZero);\n  bool AreExactFloatBounds = !(MinStatus & APFloat::opStatus::opInexact)\n                          && !(MaxStatus & APFloat::opStatus::opInexact);\n\n  SDValue MinFloatNode = DAG.getConstantFP(MinFloat, dl, SrcVT);\n  SDValue MaxFloatNode = DAG.getConstantFP(MaxFloat, dl, SrcVT);\n\n  // If the integer bounds are exactly representable as floats, emit a\n  // min+max+fptoi sequence. Otherwise use comparisons and selects.\n  if (AreExactFloatBounds) {\n    if (DstVT != TmpVT) {\n      // Clamp by MinFloat from below. If Src is NaN, propagate NaN.\n      SDValue MinClamped = DAG.getNode(\n        X86ISD::FMAX, dl, SrcVT, MinFloatNode, Src);\n      // Clamp by MaxFloat from above. If Src is NaN, propagate NaN.\n      SDValue BothClamped = DAG.getNode(\n        X86ISD::FMIN, dl, SrcVT, MaxFloatNode, MinClamped);\n      // Convert clamped value to integer.\n      SDValue FpToInt = DAG.getNode(FpToIntOpcode, dl, TmpVT, BothClamped);\n\n      // NaN will become INDVAL, with the top bit set and the rest zero.\n      // Truncation will discard the top bit, resulting in zero.\n      return DAG.getNode(ISD::TRUNCATE, dl, DstVT, FpToInt);\n    }\n\n    // Clamp by MinFloat from below. If Src is NaN, the result is MinFloat.\n    SDValue MinClamped = DAG.getNode(\n      X86ISD::FMAX, dl, SrcVT, Src, MinFloatNode);\n    // Clamp by MaxFloat from above. NaN cannot occur.\n    SDValue BothClamped = DAG.getNode(\n      X86ISD::FMINC, dl, SrcVT, MinClamped, MaxFloatNode);\n    // Convert clamped value to integer.\n    SDValue FpToInt = DAG.getNode(FpToIntOpcode, dl, DstVT, BothClamped);\n\n    if (!IsSigned) {\n      // In the unsigned case we're done, because we mapped NaN to MinFloat,\n      // which is zero.\n      return FpToInt;\n    }\n\n    // Otherwise, select zero if Src is NaN.\n    SDValue ZeroInt = DAG.getConstant(0, dl, DstVT);\n    return DAG.getSelectCC(\n      dl, Src, Src, ZeroInt, FpToInt, ISD::CondCode::SETUO);\n  }\n\n  SDValue MinIntNode = DAG.getConstant(MinInt, dl, DstVT);\n  SDValue MaxIntNode = DAG.getConstant(MaxInt, dl, DstVT);\n\n  // Result of direct conversion, which may be selected away.\n  SDValue FpToInt = DAG.getNode(FpToIntOpcode, dl, TmpVT, Src);\n\n  if (DstVT != TmpVT) {\n    // NaN will become INDVAL, with the top bit set and the rest zero.\n    // Truncation will discard the top bit, resulting in zero.\n    FpToInt = DAG.getNode(ISD::TRUNCATE, dl, DstVT, FpToInt);\n  }\n\n  SDValue Select = FpToInt;\n  // For signed conversions where we saturate to the same size as the\n  // result type of the fptoi instructions, INDVAL coincides with integer\n  // minimum, so we don't need to explicitly check it.\n  if (!IsSigned || SatWidth != TmpVT.getScalarSizeInBits()) {\n    // If Src ULT MinFloat, select MinInt. In particular, this also selects\n    // MinInt if Src is NaN.\n    Select = DAG.getSelectCC(\n      dl, Src, MinFloatNode, MinIntNode, Select, ISD::CondCode::SETULT);\n  }\n\n  // If Src OGT MaxFloat, select MaxInt.\n  Select = DAG.getSelectCC(\n    dl, Src, MaxFloatNode, MaxIntNode, Select, ISD::CondCode::SETOGT);\n\n  // In the unsigned case we are done, because we mapped NaN to MinInt, which\n  // is already zero. The promoted case was already handled above.\n  if (!IsSigned || DstVT != TmpVT) {\n    return Select;\n  }\n\n  // Otherwise, select 0 if Src is NaN.\n  SDValue ZeroInt = DAG.getConstant(0, dl, DstVT);\n  return DAG.getSelectCC(\n    dl, Src, Src, ZeroInt, Select, ISD::CondCode::SETUO);\n}\n\nSDValue X86TargetLowering::LowerFP_EXTEND(SDValue Op, SelectionDAG &DAG) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n\n  SDLoc DL(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue In = Op.getOperand(IsStrict ? 1 : 0);\n  MVT SVT = In.getSimpleValueType();\n\n  if (VT == MVT::f128)\n    return SDValue();\n\n  assert(SVT == MVT::v2f32 && \"Only customize MVT::v2f32 type legalization!\");\n\n  SDValue Res =\n      DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v4f32, In, DAG.getUNDEF(SVT));\n  if (IsStrict)\n    return DAG.getNode(X86ISD::STRICT_VFPEXT, DL, {VT, MVT::Other},\n                       {Op->getOperand(0), Res});\n  return DAG.getNode(X86ISD::VFPEXT, DL, VT, Res);\n}\n\nSDValue X86TargetLowering::LowerFP_ROUND(SDValue Op, SelectionDAG &DAG) const {\n  bool IsStrict = Op->isStrictFPOpcode();\n  SDValue In = Op.getOperand(IsStrict ? 1 : 0);\n  // It's legal except when f128 is involved\n  if (In.getSimpleValueType() != MVT::f128)\n    return Op;\n\n  return SDValue();\n}\n\nstatic SDValue LowerFP16_TO_FP(SDValue Op, SelectionDAG &DAG) {\n  bool IsStrict = Op->isStrictFPOpcode();\n  SDValue Src = Op.getOperand(IsStrict ? 1 : 0);\n  assert(Src.getValueType() == MVT::i16 && Op.getValueType() == MVT::f32 &&\n         \"Unexpected VT!\");\n\n  SDLoc dl(Op);\n  SDValue Res = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, MVT::v8i16,\n                            DAG.getConstant(0, dl, MVT::v8i16), Src,\n                            DAG.getIntPtrConstant(0, dl));\n\n  SDValue Chain;\n  if (IsStrict) {\n    Res = DAG.getNode(X86ISD::STRICT_CVTPH2PS, dl, {MVT::v4f32, MVT::Other},\n                      {Op.getOperand(0), Res});\n    Chain = Res.getValue(1);\n  } else {\n    Res = DAG.getNode(X86ISD::CVTPH2PS, dl, MVT::v4f32, Res);\n  }\n\n  Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::f32, Res,\n                    DAG.getIntPtrConstant(0, dl));\n\n  if (IsStrict)\n    return DAG.getMergeValues({Res, Chain}, dl);\n\n  return Res;\n}\n\nstatic SDValue LowerFP_TO_FP16(SDValue Op, SelectionDAG &DAG) {\n  bool IsStrict = Op->isStrictFPOpcode();\n  SDValue Src = Op.getOperand(IsStrict ? 1 : 0);\n  assert(Src.getValueType() == MVT::f32 && Op.getValueType() == MVT::i16 &&\n         \"Unexpected VT!\");\n\n  SDLoc dl(Op);\n  SDValue Res, Chain;\n  if (IsStrict) {\n    Res = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, MVT::v4f32,\n                      DAG.getConstantFP(0, dl, MVT::v4f32), Src,\n                      DAG.getIntPtrConstant(0, dl));\n    Res = DAG.getNode(\n        X86ISD::STRICT_CVTPS2PH, dl, {MVT::v8i16, MVT::Other},\n        {Op.getOperand(0), Res, DAG.getTargetConstant(4, dl, MVT::i32)});\n    Chain = Res.getValue(1);\n  } else {\n    // FIXME: Should we use zeros for upper elements for non-strict?\n    Res = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4f32, Src);\n    Res = DAG.getNode(X86ISD::CVTPS2PH, dl, MVT::v8i16, Res,\n                      DAG.getTargetConstant(4, dl, MVT::i32));\n  }\n\n  Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i16, Res,\n                    DAG.getIntPtrConstant(0, dl));\n\n  if (IsStrict)\n    return DAG.getMergeValues({Res, Chain}, dl);\n\n  return Res;\n}\n\n/// Depending on uarch and/or optimizing for size, we might prefer to use a\n/// vector operation in place of the typical scalar operation.\nstatic SDValue lowerAddSubToHorizontalOp(SDValue Op, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  // If both operands have other uses, this is probably not profitable.\n  SDValue LHS = Op.getOperand(0);\n  SDValue RHS = Op.getOperand(1);\n  if (!LHS.hasOneUse() && !RHS.hasOneUse())\n    return Op;\n\n  // FP horizontal add/sub were added with SSE3. Integer with SSSE3.\n  bool IsFP = Op.getSimpleValueType().isFloatingPoint();\n  if (IsFP && !Subtarget.hasSSE3())\n    return Op;\n  if (!IsFP && !Subtarget.hasSSSE3())\n    return Op;\n\n  // Extract from a common vector.\n  if (LHS.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n      RHS.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n      LHS.getOperand(0) != RHS.getOperand(0) ||\n      !isa<ConstantSDNode>(LHS.getOperand(1)) ||\n      !isa<ConstantSDNode>(RHS.getOperand(1)) ||\n      !shouldUseHorizontalOp(true, DAG, Subtarget))\n    return Op;\n\n  // Allow commuted 'hadd' ops.\n  // TODO: Allow commuted (f)sub by negating the result of (F)HSUB?\n  unsigned HOpcode;\n  switch (Op.getOpcode()) {\n    case ISD::ADD: HOpcode = X86ISD::HADD; break;\n    case ISD::SUB: HOpcode = X86ISD::HSUB; break;\n    case ISD::FADD: HOpcode = X86ISD::FHADD; break;\n    case ISD::FSUB: HOpcode = X86ISD::FHSUB; break;\n    default:\n      llvm_unreachable(\"Trying to lower unsupported opcode to horizontal op\");\n  }\n  unsigned LExtIndex = LHS.getConstantOperandVal(1);\n  unsigned RExtIndex = RHS.getConstantOperandVal(1);\n  if ((LExtIndex & 1) == 1 && (RExtIndex & 1) == 0 &&\n      (HOpcode == X86ISD::HADD || HOpcode == X86ISD::FHADD))\n    std::swap(LExtIndex, RExtIndex);\n\n  if ((LExtIndex & 1) != 0 || RExtIndex != (LExtIndex + 1))\n    return Op;\n\n  SDValue X = LHS.getOperand(0);\n  EVT VecVT = X.getValueType();\n  unsigned BitWidth = VecVT.getSizeInBits();\n  unsigned NumLanes = BitWidth / 128;\n  unsigned NumEltsPerLane = VecVT.getVectorNumElements() / NumLanes;\n  assert((BitWidth == 128 || BitWidth == 256 || BitWidth == 512) &&\n         \"Not expecting illegal vector widths here\");\n\n  // Creating a 256-bit horizontal op would be wasteful, and there is no 512-bit\n  // equivalent, so extract the 256/512-bit source op to 128-bit if we can.\n  SDLoc DL(Op);\n  if (BitWidth == 256 || BitWidth == 512) {\n    unsigned LaneIdx = LExtIndex / NumEltsPerLane;\n    X = extract128BitVector(X, LaneIdx * NumEltsPerLane, DAG, DL);\n    LExtIndex %= NumEltsPerLane;\n  }\n\n  // add (extractelt (X, 0), extractelt (X, 1)) --> extractelt (hadd X, X), 0\n  // add (extractelt (X, 1), extractelt (X, 0)) --> extractelt (hadd X, X), 0\n  // add (extractelt (X, 2), extractelt (X, 3)) --> extractelt (hadd X, X), 1\n  // sub (extractelt (X, 0), extractelt (X, 1)) --> extractelt (hsub X, X), 0\n  SDValue HOp = DAG.getNode(HOpcode, DL, X.getValueType(), X, X);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, Op.getSimpleValueType(), HOp,\n                     DAG.getIntPtrConstant(LExtIndex / 2, DL));\n}\n\n/// Depending on uarch and/or optimizing for size, we might prefer to use a\n/// vector operation in place of the typical scalar operation.\nSDValue X86TargetLowering::lowerFaddFsub(SDValue Op, SelectionDAG &DAG) const {\n  assert((Op.getValueType() == MVT::f32 || Op.getValueType() == MVT::f64) &&\n         \"Only expecting float/double\");\n  return lowerAddSubToHorizontalOp(Op, DAG, Subtarget);\n}\n\n/// ISD::FROUND is defined to round to nearest with ties rounding away from 0.\n/// This mode isn't supported in hardware on X86. But as long as we aren't\n/// compiling with trapping math, we can emulate this with\n/// floor(X + copysign(nextafter(0.5, 0.0), X)).\nstatic SDValue LowerFROUND(SDValue Op, SelectionDAG &DAG) {\n  SDValue N0 = Op.getOperand(0);\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  // N0 += copysign(nextafter(0.5, 0.0), N0)\n  const fltSemantics &Sem = SelectionDAG::EVTToAPFloatSemantics(VT);\n  bool Ignored;\n  APFloat Point5Pred = APFloat(0.5f);\n  Point5Pred.convert(Sem, APFloat::rmNearestTiesToEven, &Ignored);\n  Point5Pred.next(/*nextDown*/true);\n\n  SDValue Adder = DAG.getNode(ISD::FCOPYSIGN, dl, VT,\n                              DAG.getConstantFP(Point5Pred, dl, VT), N0);\n  N0 = DAG.getNode(ISD::FADD, dl, VT, N0, Adder);\n\n  // Truncate the result to remove fraction.\n  return DAG.getNode(ISD::FTRUNC, dl, VT, N0);\n}\n\n/// The only differences between FABS and FNEG are the mask and the logic op.\n/// FNEG also has a folding opportunity for FNEG(FABS(x)).\nstatic SDValue LowerFABSorFNEG(SDValue Op, SelectionDAG &DAG) {\n  assert((Op.getOpcode() == ISD::FABS || Op.getOpcode() == ISD::FNEG) &&\n         \"Wrong opcode for lowering FABS or FNEG.\");\n\n  bool IsFABS = (Op.getOpcode() == ISD::FABS);\n\n  // If this is a FABS and it has an FNEG user, bail out to fold the combination\n  // into an FNABS. We'll lower the FABS after that if it is still in use.\n  if (IsFABS)\n    for (SDNode *User : Op->uses())\n      if (User->getOpcode() == ISD::FNEG)\n        return Op;\n\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  bool IsF128 = (VT == MVT::f128);\n  assert((VT == MVT::f64 || VT == MVT::f32 || VT == MVT::f128 ||\n          VT == MVT::v2f64 || VT == MVT::v4f64 || VT == MVT::v4f32 ||\n          VT == MVT::v8f32 || VT == MVT::v8f64 || VT == MVT::v16f32) &&\n         \"Unexpected type in LowerFABSorFNEG\");\n\n  // FIXME: Use function attribute \"OptimizeForSize\" and/or CodeGenOpt::Level to\n  // decide if we should generate a 16-byte constant mask when we only need 4 or\n  // 8 bytes for the scalar case.\n\n  // There are no scalar bitwise logical SSE/AVX instructions, so we\n  // generate a 16-byte vector constant and logic op even for the scalar case.\n  // Using a 16-byte mask allows folding the load of the mask with\n  // the logic op, so it can save (~4 bytes) on code size.\n  bool IsFakeVector = !VT.isVector() && !IsF128;\n  MVT LogicVT = VT;\n  if (IsFakeVector)\n    LogicVT = (VT == MVT::f64) ? MVT::v2f64 : MVT::v4f32;\n\n  unsigned EltBits = VT.getScalarSizeInBits();\n  // For FABS, mask is 0x7f...; for FNEG, mask is 0x80...\n  APInt MaskElt = IsFABS ? APInt::getSignedMaxValue(EltBits) :\n                           APInt::getSignMask(EltBits);\n  const fltSemantics &Sem = SelectionDAG::EVTToAPFloatSemantics(VT);\n  SDValue Mask = DAG.getConstantFP(APFloat(Sem, MaskElt), dl, LogicVT);\n\n  SDValue Op0 = Op.getOperand(0);\n  bool IsFNABS = !IsFABS && (Op0.getOpcode() == ISD::FABS);\n  unsigned LogicOp = IsFABS  ? X86ISD::FAND :\n                     IsFNABS ? X86ISD::FOR  :\n                               X86ISD::FXOR;\n  SDValue Operand = IsFNABS ? Op0.getOperand(0) : Op0;\n\n  if (VT.isVector() || IsF128)\n    return DAG.getNode(LogicOp, dl, LogicVT, Operand, Mask);\n\n  // For the scalar case extend to a 128-bit vector, perform the logic op,\n  // and extract the scalar result back out.\n  Operand = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, LogicVT, Operand);\n  SDValue LogicNode = DAG.getNode(LogicOp, dl, LogicVT, Operand, Mask);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, LogicNode,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\nstatic SDValue LowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) {\n  SDValue Mag = Op.getOperand(0);\n  SDValue Sign = Op.getOperand(1);\n  SDLoc dl(Op);\n\n  // If the sign operand is smaller, extend it first.\n  MVT VT = Op.getSimpleValueType();\n  if (Sign.getSimpleValueType().bitsLT(VT))\n    Sign = DAG.getNode(ISD::FP_EXTEND, dl, VT, Sign);\n\n  // And if it is bigger, shrink it first.\n  if (Sign.getSimpleValueType().bitsGT(VT))\n    Sign = DAG.getNode(ISD::FP_ROUND, dl, VT, Sign, DAG.getIntPtrConstant(1, dl));\n\n  // At this point the operands and the result should have the same\n  // type, and that won't be f80 since that is not custom lowered.\n  bool IsF128 = (VT == MVT::f128);\n  assert((VT == MVT::f64 || VT == MVT::f32 || VT == MVT::f128 ||\n          VT == MVT::v2f64 || VT == MVT::v4f64 || VT == MVT::v4f32 ||\n          VT == MVT::v8f32 || VT == MVT::v8f64 || VT == MVT::v16f32) &&\n         \"Unexpected type in LowerFCOPYSIGN\");\n\n  const fltSemantics &Sem = SelectionDAG::EVTToAPFloatSemantics(VT);\n\n  // Perform all scalar logic operations as 16-byte vectors because there are no\n  // scalar FP logic instructions in SSE.\n  // TODO: This isn't necessary. If we used scalar types, we might avoid some\n  // unnecessary splats, but we might miss load folding opportunities. Should\n  // this decision be based on OptimizeForSize?\n  bool IsFakeVector = !VT.isVector() && !IsF128;\n  MVT LogicVT = VT;\n  if (IsFakeVector)\n    LogicVT = (VT == MVT::f64) ? MVT::v2f64 : MVT::v4f32;\n\n  // The mask constants are automatically splatted for vector types.\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  SDValue SignMask = DAG.getConstantFP(\n      APFloat(Sem, APInt::getSignMask(EltSizeInBits)), dl, LogicVT);\n  SDValue MagMask = DAG.getConstantFP(\n      APFloat(Sem, APInt::getSignedMaxValue(EltSizeInBits)), dl, LogicVT);\n\n  // First, clear all bits but the sign bit from the second operand (sign).\n  if (IsFakeVector)\n    Sign = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, LogicVT, Sign);\n  SDValue SignBit = DAG.getNode(X86ISD::FAND, dl, LogicVT, Sign, SignMask);\n\n  // Next, clear the sign bit from the first operand (magnitude).\n  // TODO: If we had general constant folding for FP logic ops, this check\n  // wouldn't be necessary.\n  SDValue MagBits;\n  if (ConstantFPSDNode *Op0CN = isConstOrConstSplatFP(Mag)) {\n    APFloat APF = Op0CN->getValueAPF();\n    APF.clearSign();\n    MagBits = DAG.getConstantFP(APF, dl, LogicVT);\n  } else {\n    // If the magnitude operand wasn't a constant, we need to AND out the sign.\n    if (IsFakeVector)\n      Mag = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, LogicVT, Mag);\n    MagBits = DAG.getNode(X86ISD::FAND, dl, LogicVT, Mag, MagMask);\n  }\n\n  // OR the magnitude value with the sign bit.\n  SDValue Or = DAG.getNode(X86ISD::FOR, dl, LogicVT, MagBits, SignBit);\n  return !IsFakeVector ? Or : DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, Or,\n                                          DAG.getIntPtrConstant(0, dl));\n}\n\nstatic SDValue LowerFGETSIGN(SDValue Op, SelectionDAG &DAG) {\n  SDValue N0 = Op.getOperand(0);\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  MVT OpVT = N0.getSimpleValueType();\n  assert((OpVT == MVT::f32 || OpVT == MVT::f64) &&\n         \"Unexpected type for FGETSIGN\");\n\n  // Lower ISD::FGETSIGN to (AND (X86ISD::MOVMSK ...) 1).\n  MVT VecVT = (OpVT == MVT::f32 ? MVT::v4f32 : MVT::v2f64);\n  SDValue Res = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VecVT, N0);\n  Res = DAG.getNode(X86ISD::MOVMSK, dl, MVT::i32, Res);\n  Res = DAG.getZExtOrTrunc(Res, dl, VT);\n  Res = DAG.getNode(ISD::AND, dl, VT, Res, DAG.getConstant(1, dl, VT));\n  return Res;\n}\n\n/// Helper for creating a X86ISD::SETCC node.\nstatic SDValue getSETCC(X86::CondCode Cond, SDValue EFLAGS, const SDLoc &dl,\n                        SelectionDAG &DAG) {\n  return DAG.getNode(X86ISD::SETCC, dl, MVT::i8,\n                     DAG.getTargetConstant(Cond, dl, MVT::i8), EFLAGS);\n}\n\n/// Helper for matching OR(EXTRACTELT(X,0),OR(EXTRACTELT(X,1),...))\n/// style scalarized (associative) reduction patterns. Partial reductions\n/// are supported when the pointer SrcMask is non-null.\n/// TODO - move this to SelectionDAG?\nstatic bool matchScalarReduction(SDValue Op, ISD::NodeType BinOp,\n                                 SmallVectorImpl<SDValue> &SrcOps,\n                                 SmallVectorImpl<APInt> *SrcMask = nullptr) {\n  SmallVector<SDValue, 8> Opnds;\n  DenseMap<SDValue, APInt> SrcOpMap;\n  EVT VT = MVT::Other;\n\n  // Recognize a special case where a vector is casted into wide integer to\n  // test all 0s.\n  assert(Op.getOpcode() == unsigned(BinOp) &&\n         \"Unexpected bit reduction opcode\");\n  Opnds.push_back(Op.getOperand(0));\n  Opnds.push_back(Op.getOperand(1));\n\n  for (unsigned Slot = 0, e = Opnds.size(); Slot < e; ++Slot) {\n    SmallVectorImpl<SDValue>::const_iterator I = Opnds.begin() + Slot;\n    // BFS traverse all BinOp operands.\n    if (I->getOpcode() == unsigned(BinOp)) {\n      Opnds.push_back(I->getOperand(0));\n      Opnds.push_back(I->getOperand(1));\n      // Re-evaluate the number of nodes to be traversed.\n      e += 2; // 2 more nodes (LHS and RHS) are pushed.\n      continue;\n    }\n\n    // Quit if a non-EXTRACT_VECTOR_ELT\n    if (I->getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return false;\n\n    // Quit if without a constant index.\n    auto *Idx = dyn_cast<ConstantSDNode>(I->getOperand(1));\n    if (!Idx)\n      return false;\n\n    SDValue Src = I->getOperand(0);\n    DenseMap<SDValue, APInt>::iterator M = SrcOpMap.find(Src);\n    if (M == SrcOpMap.end()) {\n      VT = Src.getValueType();\n      // Quit if not the same type.\n      if (!SrcOpMap.empty() && VT != SrcOpMap.begin()->first.getValueType())\n        return false;\n      unsigned NumElts = VT.getVectorNumElements();\n      APInt EltCount = APInt::getNullValue(NumElts);\n      M = SrcOpMap.insert(std::make_pair(Src, EltCount)).first;\n      SrcOps.push_back(Src);\n    }\n\n    // Quit if element already used.\n    unsigned CIdx = Idx->getZExtValue();\n    if (M->second[CIdx])\n      return false;\n    M->second.setBit(CIdx);\n  }\n\n  if (SrcMask) {\n    // Collect the source partial masks.\n    for (SDValue &SrcOp : SrcOps)\n      SrcMask->push_back(SrcOpMap[SrcOp]);\n  } else {\n    // Quit if not all elements are used.\n    for (DenseMap<SDValue, APInt>::const_iterator I = SrcOpMap.begin(),\n                                                  E = SrcOpMap.end();\n         I != E; ++I) {\n      if (!I->second.isAllOnesValue())\n        return false;\n    }\n  }\n\n  return true;\n}\n\n// Helper function for comparing all bits of a vector against zero.\nstatic SDValue LowerVectorAllZero(const SDLoc &DL, SDValue V, ISD::CondCode CC,\n                                  const APInt &Mask,\n                                  const X86Subtarget &Subtarget,\n                                  SelectionDAG &DAG, X86::CondCode &X86CC) {\n  EVT VT = V.getValueType();\n  unsigned ScalarSize = VT.getScalarSizeInBits();\n  if (Mask.getBitWidth() != ScalarSize) {\n    assert(ScalarSize == 1 && \"Element Mask vs Vector bitwidth mismatch\");\n    return SDValue();\n  }\n\n  assert((CC == ISD::SETEQ || CC == ISD::SETNE) && \"Unsupported ISD::CondCode\");\n  X86CC = (CC == ISD::SETEQ ? X86::COND_E : X86::COND_NE);\n\n  auto MaskBits = [&](SDValue Src) {\n    if (Mask.isAllOnesValue())\n      return Src;\n    EVT SrcVT = Src.getValueType();\n    SDValue MaskValue = DAG.getConstant(Mask, DL, SrcVT);\n    return DAG.getNode(ISD::AND, DL, SrcVT, Src, MaskValue);\n  };\n\n  // For sub-128-bit vector, cast to (legal) integer and compare with zero.\n  if (VT.getSizeInBits() < 128) {\n    EVT IntVT = EVT::getIntegerVT(*DAG.getContext(), VT.getSizeInBits());\n    if (!DAG.getTargetLoweringInfo().isTypeLegal(IntVT))\n      return SDValue();\n    return DAG.getNode(X86ISD::CMP, DL, MVT::i32,\n                       DAG.getBitcast(IntVT, MaskBits(V)),\n                       DAG.getConstant(0, DL, IntVT));\n  }\n\n  // Quit if not splittable to 128/256-bit vector.\n  if (!isPowerOf2_32(VT.getSizeInBits()))\n    return SDValue();\n\n  // Split down to 128/256-bit vector.\n  unsigned TestSize = Subtarget.hasAVX() ? 256 : 128;\n  while (VT.getSizeInBits() > TestSize) {\n    auto Split = DAG.SplitVector(V, DL);\n    VT = Split.first.getValueType();\n    V = DAG.getNode(ISD::OR, DL, VT, Split.first, Split.second);\n  }\n\n  bool UsePTEST = Subtarget.hasSSE41();\n  if (UsePTEST) {\n    MVT TestVT = VT.is128BitVector() ? MVT::v2i64 : MVT::v4i64;\n    V = DAG.getBitcast(TestVT, MaskBits(V));\n    return DAG.getNode(X86ISD::PTEST, DL, MVT::i32, V, V);\n  }\n\n  // Without PTEST, a masked v2i64 or-reduction is not faster than\n  // scalarization.\n  if (!Mask.isAllOnesValue() && VT.getScalarSizeInBits() > 32)\n      return SDValue();\n\n  V = DAG.getBitcast(MVT::v16i8, MaskBits(V));\n  V = DAG.getNode(X86ISD::PCMPEQ, DL, MVT::v16i8, V,\n                  getZeroVector(MVT::v16i8, Subtarget, DAG, DL));\n  V = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, V);\n  return DAG.getNode(X86ISD::CMP, DL, MVT::i32, V,\n                     DAG.getConstant(0xFFFF, DL, MVT::i32));\n}\n\n// Check whether an OR'd reduction tree is PTEST-able, or if we can fallback to\n// CMP(MOVMSK(PCMPEQB(X,0))).\nstatic SDValue MatchVectorAllZeroTest(SDValue Op, ISD::CondCode CC,\n                                      const SDLoc &DL,\n                                      const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG, SDValue &X86CC) {\n  assert((CC == ISD::SETEQ || CC == ISD::SETNE) && \"Unsupported ISD::CondCode\");\n\n  if (!Subtarget.hasSSE2() || !Op->hasOneUse())\n    return SDValue();\n\n  // Check whether we're masking/truncating an OR-reduction result, in which\n  // case track the masked bits.\n  APInt Mask = APInt::getAllOnesValue(Op.getScalarValueSizeInBits());\n  switch (Op.getOpcode()) {\n  case ISD::TRUNCATE: {\n    SDValue Src = Op.getOperand(0);\n    Mask = APInt::getLowBitsSet(Src.getScalarValueSizeInBits(),\n                                Op.getScalarValueSizeInBits());\n    Op = Src;\n    break;\n  }\n  case ISD::AND: {\n    if (auto *Cst = dyn_cast<ConstantSDNode>(Op.getOperand(1))) {\n      Mask = Cst->getAPIntValue();\n      Op = Op.getOperand(0);\n    }\n    break;\n  }\n  }\n\n  SmallVector<SDValue, 8> VecIns;\n  if (Op.getOpcode() == ISD::OR && matchScalarReduction(Op, ISD::OR, VecIns)) {\n    EVT VT = VecIns[0].getValueType();\n    assert(llvm::all_of(VecIns,\n                        [VT](SDValue V) { return VT == V.getValueType(); }) &&\n           \"Reduction source vector mismatch\");\n\n    // Quit if less than 128-bits or not splittable to 128/256-bit vector.\n    if (VT.getSizeInBits() < 128 || !isPowerOf2_32(VT.getSizeInBits()))\n      return SDValue();\n\n    // If more than one full vector is evaluated, OR them first before PTEST.\n    for (unsigned Slot = 0, e = VecIns.size(); e - Slot > 1;\n         Slot += 2, e += 1) {\n      // Each iteration will OR 2 nodes and append the result until there is\n      // only 1 node left, i.e. the final OR'd value of all vectors.\n      SDValue LHS = VecIns[Slot];\n      SDValue RHS = VecIns[Slot + 1];\n      VecIns.push_back(DAG.getNode(ISD::OR, DL, VT, LHS, RHS));\n    }\n\n    X86::CondCode CCode;\n    if (SDValue V = LowerVectorAllZero(DL, VecIns.back(), CC, Mask, Subtarget,\n                                       DAG, CCode)) {\n      X86CC = DAG.getTargetConstant(CCode, DL, MVT::i8);\n      return V;\n    }\n  }\n\n  if (Op.getOpcode() == ISD::EXTRACT_VECTOR_ELT) {\n    ISD::NodeType BinOp;\n    if (SDValue Match =\n            DAG.matchBinOpReduction(Op.getNode(), BinOp, {ISD::OR})) {\n      X86::CondCode CCode;\n      if (SDValue V =\n              LowerVectorAllZero(DL, Match, CC, Mask, Subtarget, DAG, CCode)) {\n        X86CC = DAG.getTargetConstant(CCode, DL, MVT::i8);\n        return V;\n      }\n    }\n  }\n\n  return SDValue();\n}\n\n/// return true if \\c Op has a use that doesn't just read flags.\nstatic bool hasNonFlagsUse(SDValue Op) {\n  for (SDNode::use_iterator UI = Op->use_begin(), UE = Op->use_end(); UI != UE;\n       ++UI) {\n    SDNode *User = *UI;\n    unsigned UOpNo = UI.getOperandNo();\n    if (User->getOpcode() == ISD::TRUNCATE && User->hasOneUse()) {\n      // Look pass truncate.\n      UOpNo = User->use_begin().getOperandNo();\n      User = *User->use_begin();\n    }\n\n    if (User->getOpcode() != ISD::BRCOND && User->getOpcode() != ISD::SETCC &&\n        !(User->getOpcode() == ISD::SELECT && UOpNo == 0))\n      return true;\n  }\n  return false;\n}\n\n// Transform to an x86-specific ALU node with flags if there is a chance of\n// using an RMW op or only the flags are used. Otherwise, leave\n// the node alone and emit a 'cmp' or 'test' instruction.\nstatic bool isProfitableToUseFlagOp(SDValue Op) {\n  for (SDNode *U : Op->uses())\n    if (U->getOpcode() != ISD::CopyToReg &&\n        U->getOpcode() != ISD::SETCC &&\n        U->getOpcode() != ISD::STORE)\n      return false;\n\n  return true;\n}\n\n/// Emit nodes that will be selected as \"test Op0,Op0\", or something\n/// equivalent.\nstatic SDValue EmitTest(SDValue Op, unsigned X86CC, const SDLoc &dl,\n                        SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  // CF and OF aren't always set the way we want. Determine which\n  // of these we need.\n  bool NeedCF = false;\n  bool NeedOF = false;\n  switch (X86CC) {\n  default: break;\n  case X86::COND_A: case X86::COND_AE:\n  case X86::COND_B: case X86::COND_BE:\n    NeedCF = true;\n    break;\n  case X86::COND_G: case X86::COND_GE:\n  case X86::COND_L: case X86::COND_LE:\n  case X86::COND_O: case X86::COND_NO: {\n    // Check if we really need to set the\n    // Overflow flag. If NoSignedWrap is present\n    // that is not actually needed.\n    switch (Op->getOpcode()) {\n    case ISD::ADD:\n    case ISD::SUB:\n    case ISD::MUL:\n    case ISD::SHL:\n      if (Op.getNode()->getFlags().hasNoSignedWrap())\n        break;\n      LLVM_FALLTHROUGH;\n    default:\n      NeedOF = true;\n      break;\n    }\n    break;\n  }\n  }\n  // See if we can use the EFLAGS value from the operand instead of\n  // doing a separate TEST. TEST always sets OF and CF to 0, so unless\n  // we prove that the arithmetic won't overflow, we can't use OF or CF.\n  if (Op.getResNo() != 0 || NeedOF || NeedCF) {\n    // Emit a CMP with 0, which is the TEST pattern.\n    return DAG.getNode(X86ISD::CMP, dl, MVT::i32, Op,\n                       DAG.getConstant(0, dl, Op.getValueType()));\n  }\n  unsigned Opcode = 0;\n  unsigned NumOperands = 0;\n\n  SDValue ArithOp = Op;\n\n  // NOTICE: In the code below we use ArithOp to hold the arithmetic operation\n  // which may be the result of a CAST.  We use the variable 'Op', which is the\n  // non-casted variable when we check for possible users.\n  switch (ArithOp.getOpcode()) {\n  case ISD::AND:\n    // If the primary 'and' result isn't used, don't bother using X86ISD::AND,\n    // because a TEST instruction will be better.\n    if (!hasNonFlagsUse(Op))\n      break;\n\n    LLVM_FALLTHROUGH;\n  case ISD::ADD:\n  case ISD::SUB:\n  case ISD::OR:\n  case ISD::XOR:\n    if (!isProfitableToUseFlagOp(Op))\n      break;\n\n    // Otherwise use a regular EFLAGS-setting instruction.\n    switch (ArithOp.getOpcode()) {\n    default: llvm_unreachable(\"unexpected operator!\");\n    case ISD::ADD: Opcode = X86ISD::ADD; break;\n    case ISD::SUB: Opcode = X86ISD::SUB; break;\n    case ISD::XOR: Opcode = X86ISD::XOR; break;\n    case ISD::AND: Opcode = X86ISD::AND; break;\n    case ISD::OR:  Opcode = X86ISD::OR;  break;\n    }\n\n    NumOperands = 2;\n    break;\n  case X86ISD::ADD:\n  case X86ISD::SUB:\n  case X86ISD::OR:\n  case X86ISD::XOR:\n  case X86ISD::AND:\n    return SDValue(Op.getNode(), 1);\n  case ISD::SSUBO:\n  case ISD::USUBO: {\n    // /USUBO/SSUBO will become a X86ISD::SUB and we can use its Z flag.\n    SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::i32);\n    return DAG.getNode(X86ISD::SUB, dl, VTs, Op->getOperand(0),\n                       Op->getOperand(1)).getValue(1);\n  }\n  default:\n    break;\n  }\n\n  if (Opcode == 0) {\n    // Emit a CMP with 0, which is the TEST pattern.\n    return DAG.getNode(X86ISD::CMP, dl, MVT::i32, Op,\n                       DAG.getConstant(0, dl, Op.getValueType()));\n  }\n  SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::i32);\n  SmallVector<SDValue, 4> Ops(Op->op_begin(), Op->op_begin() + NumOperands);\n\n  SDValue New = DAG.getNode(Opcode, dl, VTs, Ops);\n  DAG.ReplaceAllUsesOfValueWith(SDValue(Op.getNode(), 0), New);\n  return SDValue(New.getNode(), 1);\n}\n\n/// Emit nodes that will be selected as \"cmp Op0,Op1\", or something\n/// equivalent.\nstatic SDValue EmitCmp(SDValue Op0, SDValue Op1, unsigned X86CC,\n                       const SDLoc &dl, SelectionDAG &DAG,\n                       const X86Subtarget &Subtarget) {\n  if (isNullConstant(Op1))\n    return EmitTest(Op0, X86CC, dl, DAG, Subtarget);\n\n  EVT CmpVT = Op0.getValueType();\n\n  assert((CmpVT == MVT::i8 || CmpVT == MVT::i16 ||\n          CmpVT == MVT::i32 || CmpVT == MVT::i64) && \"Unexpected VT!\");\n\n  // Only promote the compare up to I32 if it is a 16 bit operation\n  // with an immediate.  16 bit immediates are to be avoided.\n  if (CmpVT == MVT::i16 && !Subtarget.isAtom() &&\n      !DAG.getMachineFunction().getFunction().hasMinSize()) {\n    ConstantSDNode *COp0 = dyn_cast<ConstantSDNode>(Op0);\n    ConstantSDNode *COp1 = dyn_cast<ConstantSDNode>(Op1);\n    // Don't do this if the immediate can fit in 8-bits.\n    if ((COp0 && !COp0->getAPIntValue().isSignedIntN(8)) ||\n        (COp1 && !COp1->getAPIntValue().isSignedIntN(8))) {\n      unsigned ExtendOp =\n          isX86CCSigned(X86CC) ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;\n      if (X86CC == X86::COND_E || X86CC == X86::COND_NE) {\n        // For equality comparisons try to use SIGN_EXTEND if the input was\n        // truncate from something with enough sign bits.\n        if (Op0.getOpcode() == ISD::TRUNCATE) {\n          SDValue In = Op0.getOperand(0);\n          unsigned EffBits =\n              In.getScalarValueSizeInBits() - DAG.ComputeNumSignBits(In) + 1;\n          if (EffBits <= 16)\n            ExtendOp = ISD::SIGN_EXTEND;\n        } else if (Op1.getOpcode() == ISD::TRUNCATE) {\n          SDValue In = Op1.getOperand(0);\n          unsigned EffBits =\n              In.getScalarValueSizeInBits() - DAG.ComputeNumSignBits(In) + 1;\n          if (EffBits <= 16)\n            ExtendOp = ISD::SIGN_EXTEND;\n        }\n      }\n\n      CmpVT = MVT::i32;\n      Op0 = DAG.getNode(ExtendOp, dl, CmpVT, Op0);\n      Op1 = DAG.getNode(ExtendOp, dl, CmpVT, Op1);\n    }\n  }\n\n  // Try to shrink i64 compares if the input has enough zero bits.\n  // FIXME: Do this for non-constant compares for constant on LHS?\n  if (CmpVT == MVT::i64 && isa<ConstantSDNode>(Op1) && !isX86CCSigned(X86CC) &&\n      Op0.hasOneUse() && // Hacky way to not break CSE opportunities with sub.\n      cast<ConstantSDNode>(Op1)->getAPIntValue().getActiveBits() <= 32 &&\n      DAG.MaskedValueIsZero(Op0, APInt::getHighBitsSet(64, 32))) {\n    CmpVT = MVT::i32;\n    Op0 = DAG.getNode(ISD::TRUNCATE, dl, CmpVT, Op0);\n    Op1 = DAG.getNode(ISD::TRUNCATE, dl, CmpVT, Op1);\n  }\n\n  // 0-x == y --> x+y == 0\n  // 0-x != y --> x+y != 0\n  if (Op0.getOpcode() == ISD::SUB && isNullConstant(Op0.getOperand(0)) &&\n      Op0.hasOneUse() && (X86CC == X86::COND_E || X86CC == X86::COND_NE)) {\n    SDVTList VTs = DAG.getVTList(CmpVT, MVT::i32);\n    SDValue Add = DAG.getNode(X86ISD::ADD, dl, VTs, Op0.getOperand(1), Op1);\n    return Add.getValue(1);\n  }\n\n  // x == 0-y --> x+y == 0\n  // x != 0-y --> x+y != 0\n  if (Op1.getOpcode() == ISD::SUB && isNullConstant(Op1.getOperand(0)) &&\n      Op1.hasOneUse() && (X86CC == X86::COND_E || X86CC == X86::COND_NE)) {\n    SDVTList VTs = DAG.getVTList(CmpVT, MVT::i32);\n    SDValue Add = DAG.getNode(X86ISD::ADD, dl, VTs, Op0, Op1.getOperand(1));\n    return Add.getValue(1);\n  }\n\n  // Use SUB instead of CMP to enable CSE between SUB and CMP.\n  SDVTList VTs = DAG.getVTList(CmpVT, MVT::i32);\n  SDValue Sub = DAG.getNode(X86ISD::SUB, dl, VTs, Op0, Op1);\n  return Sub.getValue(1);\n}\n\n/// Check if replacement of SQRT with RSQRT should be disabled.\nbool X86TargetLowering::isFsqrtCheap(SDValue Op, SelectionDAG &DAG) const {\n  EVT VT = Op.getValueType();\n\n  // We never want to use both SQRT and RSQRT instructions for the same input.\n  if (DAG.getNodeIfExists(X86ISD::FRSQRT, DAG.getVTList(VT), Op))\n    return false;\n\n  if (VT.isVector())\n    return Subtarget.hasFastVectorFSQRT();\n  return Subtarget.hasFastScalarFSQRT();\n}\n\n/// The minimum architected relative accuracy is 2^-12. We need one\n/// Newton-Raphson step to have a good float result (24 bits of precision).\nSDValue X86TargetLowering::getSqrtEstimate(SDValue Op,\n                                           SelectionDAG &DAG, int Enabled,\n                                           int &RefinementSteps,\n                                           bool &UseOneConstNR,\n                                           bool Reciprocal) const {\n  EVT VT = Op.getValueType();\n\n  // SSE1 has rsqrtss and rsqrtps. AVX adds a 256-bit variant for rsqrtps.\n  // It is likely not profitable to do this for f64 because a double-precision\n  // rsqrt estimate with refinement on x86 prior to FMA requires at least 16\n  // instructions: convert to single, rsqrtss, convert back to double, refine\n  // (3 steps = at least 13 insts). If an 'rsqrtsd' variant was added to the ISA\n  // along with FMA, this could be a throughput win.\n  // TODO: SQRT requires SSE2 to prevent the introduction of an illegal v4i32\n  // after legalize types.\n  if ((VT == MVT::f32 && Subtarget.hasSSE1()) ||\n      (VT == MVT::v4f32 && Subtarget.hasSSE1() && Reciprocal) ||\n      (VT == MVT::v4f32 && Subtarget.hasSSE2() && !Reciprocal) ||\n      (VT == MVT::v8f32 && Subtarget.hasAVX()) ||\n      (VT == MVT::v16f32 && Subtarget.useAVX512Regs())) {\n    if (RefinementSteps == ReciprocalEstimate::Unspecified)\n      RefinementSteps = 1;\n\n    UseOneConstNR = false;\n    // There is no FSQRT for 512-bits, but there is RSQRT14.\n    unsigned Opcode = VT == MVT::v16f32 ? X86ISD::RSQRT14 : X86ISD::FRSQRT;\n    return DAG.getNode(Opcode, SDLoc(Op), VT, Op);\n  }\n  return SDValue();\n}\n\n/// The minimum architected relative accuracy is 2^-12. We need one\n/// Newton-Raphson step to have a good float result (24 bits of precision).\nSDValue X86TargetLowering::getRecipEstimate(SDValue Op, SelectionDAG &DAG,\n                                            int Enabled,\n                                            int &RefinementSteps) const {\n  EVT VT = Op.getValueType();\n\n  // SSE1 has rcpss and rcpps. AVX adds a 256-bit variant for rcpps.\n  // It is likely not profitable to do this for f64 because a double-precision\n  // reciprocal estimate with refinement on x86 prior to FMA requires\n  // 15 instructions: convert to single, rcpss, convert back to double, refine\n  // (3 steps = 12 insts). If an 'rcpsd' variant was added to the ISA\n  // along with FMA, this could be a throughput win.\n\n  if ((VT == MVT::f32 && Subtarget.hasSSE1()) ||\n      (VT == MVT::v4f32 && Subtarget.hasSSE1()) ||\n      (VT == MVT::v8f32 && Subtarget.hasAVX()) ||\n      (VT == MVT::v16f32 && Subtarget.useAVX512Regs())) {\n    // Enable estimate codegen with 1 refinement step for vector division.\n    // Scalar division estimates are disabled because they break too much\n    // real-world code. These defaults are intended to match GCC behavior.\n    if (VT == MVT::f32 && Enabled == ReciprocalEstimate::Unspecified)\n      return SDValue();\n\n    if (RefinementSteps == ReciprocalEstimate::Unspecified)\n      RefinementSteps = 1;\n\n    // There is no FSQRT for 512-bits, but there is RCP14.\n    unsigned Opcode = VT == MVT::v16f32 ? X86ISD::RCP14 : X86ISD::FRCP;\n    return DAG.getNode(Opcode, SDLoc(Op), VT, Op);\n  }\n  return SDValue();\n}\n\n/// If we have at least two divisions that use the same divisor, convert to\n/// multiplication by a reciprocal. This may need to be adjusted for a given\n/// CPU if a division's cost is not at least twice the cost of a multiplication.\n/// This is because we still need one division to calculate the reciprocal and\n/// then we need two multiplies by that reciprocal as replacements for the\n/// original divisions.\nunsigned X86TargetLowering::combineRepeatedFPDivisors() const {\n  return 2;\n}\n\nSDValue\nX86TargetLowering::BuildSDIVPow2(SDNode *N, const APInt &Divisor,\n                                 SelectionDAG &DAG,\n                                 SmallVectorImpl<SDNode *> &Created) const {\n  AttributeList Attr = DAG.getMachineFunction().getFunction().getAttributes();\n  if (isIntDivCheap(N->getValueType(0), Attr))\n    return SDValue(N,0); // Lower SDIV as SDIV\n\n  assert((Divisor.isPowerOf2() || (-Divisor).isPowerOf2()) &&\n         \"Unexpected divisor!\");\n\n  // Only perform this transform if CMOV is supported otherwise the select\n  // below will become a branch.\n  if (!Subtarget.hasCMov())\n    return SDValue();\n\n  // fold (sdiv X, pow2)\n  EVT VT = N->getValueType(0);\n  // FIXME: Support i8.\n  if (VT != MVT::i16 && VT != MVT::i32 &&\n      !(Subtarget.is64Bit() && VT == MVT::i64))\n    return SDValue();\n\n  unsigned Lg2 = Divisor.countTrailingZeros();\n\n  // If the divisor is 2 or -2, the default expansion is better.\n  if (Lg2 == 1)\n    return SDValue();\n\n  SDLoc DL(N);\n  SDValue N0 = N->getOperand(0);\n  SDValue Zero = DAG.getConstant(0, DL, VT);\n  APInt Lg2Mask = APInt::getLowBitsSet(VT.getSizeInBits(), Lg2);\n  SDValue Pow2MinusOne = DAG.getConstant(Lg2Mask, DL, VT);\n\n  // If N0 is negative, we need to add (Pow2 - 1) to it before shifting right.\n  SDValue Cmp = DAG.getSetCC(DL, MVT::i8, N0, Zero, ISD::SETLT);\n  SDValue Add = DAG.getNode(ISD::ADD, DL, VT, N0, Pow2MinusOne);\n  SDValue CMov = DAG.getNode(ISD::SELECT, DL, VT, Cmp, Add, N0);\n\n  Created.push_back(Cmp.getNode());\n  Created.push_back(Add.getNode());\n  Created.push_back(CMov.getNode());\n\n  // Divide by pow2.\n  SDValue SRA =\n      DAG.getNode(ISD::SRA, DL, VT, CMov, DAG.getConstant(Lg2, DL, MVT::i8));\n\n  // If we're dividing by a positive value, we're done.  Otherwise, we must\n  // negate the result.\n  if (Divisor.isNonNegative())\n    return SRA;\n\n  Created.push_back(SRA.getNode());\n  return DAG.getNode(ISD::SUB, DL, VT, Zero, SRA);\n}\n\n/// Result of 'and' is compared against zero. Change to a BT node if possible.\n/// Returns the BT node and the condition code needed to use it.\nstatic SDValue LowerAndToBT(SDValue And, ISD::CondCode CC,\n                            const SDLoc &dl, SelectionDAG &DAG,\n                            SDValue &X86CC) {\n  assert(And.getOpcode() == ISD::AND && \"Expected AND node!\");\n  SDValue Op0 = And.getOperand(0);\n  SDValue Op1 = And.getOperand(1);\n  if (Op0.getOpcode() == ISD::TRUNCATE)\n    Op0 = Op0.getOperand(0);\n  if (Op1.getOpcode() == ISD::TRUNCATE)\n    Op1 = Op1.getOperand(0);\n\n  SDValue Src, BitNo;\n  if (Op1.getOpcode() == ISD::SHL)\n    std::swap(Op0, Op1);\n  if (Op0.getOpcode() == ISD::SHL) {\n    if (isOneConstant(Op0.getOperand(0))) {\n      // If we looked past a truncate, check that it's only truncating away\n      // known zeros.\n      unsigned BitWidth = Op0.getValueSizeInBits();\n      unsigned AndBitWidth = And.getValueSizeInBits();\n      if (BitWidth > AndBitWidth) {\n        KnownBits Known = DAG.computeKnownBits(Op0);\n        if (Known.countMinLeadingZeros() < BitWidth - AndBitWidth)\n          return SDValue();\n      }\n      Src = Op1;\n      BitNo = Op0.getOperand(1);\n    }\n  } else if (Op1.getOpcode() == ISD::Constant) {\n    ConstantSDNode *AndRHS = cast<ConstantSDNode>(Op1);\n    uint64_t AndRHSVal = AndRHS->getZExtValue();\n    SDValue AndLHS = Op0;\n\n    if (AndRHSVal == 1 && AndLHS.getOpcode() == ISD::SRL) {\n      Src = AndLHS.getOperand(0);\n      BitNo = AndLHS.getOperand(1);\n    } else {\n      // Use BT if the immediate can't be encoded in a TEST instruction or we\n      // are optimizing for size and the immedaite won't fit in a byte.\n      bool OptForSize = DAG.shouldOptForSize();\n      if ((!isUInt<32>(AndRHSVal) || (OptForSize && !isUInt<8>(AndRHSVal))) &&\n          isPowerOf2_64(AndRHSVal)) {\n        Src = AndLHS;\n        BitNo = DAG.getConstant(Log2_64_Ceil(AndRHSVal), dl,\n                                Src.getValueType());\n      }\n    }\n  }\n\n  // No patterns found, give up.\n  if (!Src.getNode())\n    return SDValue();\n\n  // If Src is i8, promote it to i32 with any_extend.  There is no i8 BT\n  // instruction.  Since the shift amount is in-range-or-undefined, we know\n  // that doing a bittest on the i32 value is ok.  We extend to i32 because\n  // the encoding for the i16 version is larger than the i32 version.\n  // Also promote i16 to i32 for performance / code size reason.\n  if (Src.getValueType() == MVT::i8 || Src.getValueType() == MVT::i16)\n    Src = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i32, Src);\n\n  // See if we can use the 32-bit instruction instead of the 64-bit one for a\n  // shorter encoding. Since the former takes the modulo 32 of BitNo and the\n  // latter takes the modulo 64, this is only valid if the 5th bit of BitNo is\n  // known to be zero.\n  if (Src.getValueType() == MVT::i64 &&\n      DAG.MaskedValueIsZero(BitNo, APInt(BitNo.getValueSizeInBits(), 32)))\n    Src = DAG.getNode(ISD::TRUNCATE, dl, MVT::i32, Src);\n\n  // If the operand types disagree, extend the shift amount to match.  Since\n  // BT ignores high bits (like shifts) we can use anyextend.\n  if (Src.getValueType() != BitNo.getValueType())\n    BitNo = DAG.getNode(ISD::ANY_EXTEND, dl, Src.getValueType(), BitNo);\n\n  X86CC = DAG.getTargetConstant(CC == ISD::SETEQ ? X86::COND_AE : X86::COND_B,\n                                dl, MVT::i8);\n  return DAG.getNode(X86ISD::BT, dl, MVT::i32, Src, BitNo);\n}\n\n/// Turns an ISD::CondCode into a value suitable for SSE floating-point mask\n/// CMPs.\nstatic unsigned translateX86FSETCC(ISD::CondCode SetCCOpcode, SDValue &Op0,\n                                   SDValue &Op1, bool &IsAlwaysSignaling) {\n  unsigned SSECC;\n  bool Swap = false;\n\n  // SSE Condition code mapping:\n  //  0 - EQ\n  //  1 - LT\n  //  2 - LE\n  //  3 - UNORD\n  //  4 - NEQ\n  //  5 - NLT\n  //  6 - NLE\n  //  7 - ORD\n  switch (SetCCOpcode) {\n  default: llvm_unreachable(\"Unexpected SETCC condition\");\n  case ISD::SETOEQ:\n  case ISD::SETEQ:  SSECC = 0; break;\n  case ISD::SETOGT:\n  case ISD::SETGT:  Swap = true; LLVM_FALLTHROUGH;\n  case ISD::SETLT:\n  case ISD::SETOLT: SSECC = 1; break;\n  case ISD::SETOGE:\n  case ISD::SETGE:  Swap = true; LLVM_FALLTHROUGH;\n  case ISD::SETLE:\n  case ISD::SETOLE: SSECC = 2; break;\n  case ISD::SETUO:  SSECC = 3; break;\n  case ISD::SETUNE:\n  case ISD::SETNE:  SSECC = 4; break;\n  case ISD::SETULE: Swap = true; LLVM_FALLTHROUGH;\n  case ISD::SETUGE: SSECC = 5; break;\n  case ISD::SETULT: Swap = true; LLVM_FALLTHROUGH;\n  case ISD::SETUGT: SSECC = 6; break;\n  case ISD::SETO:   SSECC = 7; break;\n  case ISD::SETUEQ: SSECC = 8; break;\n  case ISD::SETONE: SSECC = 12; break;\n  }\n  if (Swap)\n    std::swap(Op0, Op1);\n\n  switch (SetCCOpcode) {\n  default:\n    IsAlwaysSignaling = true;\n    break;\n  case ISD::SETEQ:\n  case ISD::SETOEQ:\n  case ISD::SETUEQ:\n  case ISD::SETNE:\n  case ISD::SETONE:\n  case ISD::SETUNE:\n  case ISD::SETO:\n  case ISD::SETUO:\n    IsAlwaysSignaling = false;\n    break;\n  }\n\n  return SSECC;\n}\n\n/// Break a VSETCC 256-bit integer VSETCC into two new 128 ones and then\n/// concatenate the result back.\nstatic SDValue splitIntVSETCC(SDValue Op, SelectionDAG &DAG) {\n  EVT VT = Op.getValueType();\n\n  assert(Op.getOpcode() == ISD::SETCC && \"Unsupported operation\");\n  assert(Op.getOperand(0).getValueType().isInteger() &&\n         VT == Op.getOperand(0).getValueType() && \"Unsupported VTs!\");\n\n  SDLoc dl(Op);\n  SDValue CC = Op.getOperand(2);\n\n  // Extract the LHS Lo/Hi vectors\n  SDValue LHS1, LHS2;\n  std::tie(LHS1, LHS2) = splitVector(Op.getOperand(0), DAG, dl);\n\n  // Extract the RHS Lo/Hi vectors\n  SDValue RHS1, RHS2;\n  std::tie(RHS1, RHS2) = splitVector(Op.getOperand(1), DAG, dl);\n\n  // Issue the operation on the smaller types and concatenate the result back\n  EVT LoVT, HiVT;\n  std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT,\n                     DAG.getNode(ISD::SETCC, dl, LoVT, LHS1, RHS1, CC),\n                     DAG.getNode(ISD::SETCC, dl, HiVT, LHS2, RHS2, CC));\n}\n\nstatic SDValue LowerIntVSETCC_AVX512(SDValue Op, SelectionDAG &DAG) {\n\n  SDValue Op0 = Op.getOperand(0);\n  SDValue Op1 = Op.getOperand(1);\n  SDValue CC = Op.getOperand(2);\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n\n  assert(VT.getVectorElementType() == MVT::i1 &&\n         \"Cannot set masked compare for this operation\");\n\n  ISD::CondCode SetCCOpcode = cast<CondCodeSDNode>(CC)->get();\n\n  // Prefer SETGT over SETLT.\n  if (SetCCOpcode == ISD::SETLT) {\n    SetCCOpcode = ISD::getSetCCSwappedOperands(SetCCOpcode);\n    std::swap(Op0, Op1);\n  }\n\n  return DAG.getSetCC(dl, VT, Op0, Op1, SetCCOpcode);\n}\n\n/// Given a buildvector constant, return a new vector constant with each element\n/// incremented or decremented. If incrementing or decrementing would result in\n/// unsigned overflow or underflow or this is not a simple vector constant,\n/// return an empty value.\nstatic SDValue incDecVectorConstant(SDValue V, SelectionDAG &DAG, bool IsInc) {\n  auto *BV = dyn_cast<BuildVectorSDNode>(V.getNode());\n  if (!BV)\n    return SDValue();\n\n  MVT VT = V.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  unsigned NumElts = VT.getVectorNumElements();\n  SmallVector<SDValue, 8> NewVecC;\n  SDLoc DL(V);\n  for (unsigned i = 0; i < NumElts; ++i) {\n    auto *Elt = dyn_cast<ConstantSDNode>(BV->getOperand(i));\n    if (!Elt || Elt->isOpaque() || Elt->getSimpleValueType(0) != EltVT)\n      return SDValue();\n\n    // Avoid overflow/underflow.\n    const APInt &EltC = Elt->getAPIntValue();\n    if ((IsInc && EltC.isMaxValue()) || (!IsInc && EltC.isNullValue()))\n      return SDValue();\n\n    NewVecC.push_back(DAG.getConstant(EltC + (IsInc ? 1 : -1), DL, EltVT));\n  }\n\n  return DAG.getBuildVector(VT, DL, NewVecC);\n}\n\n/// As another special case, use PSUBUS[BW] when it's profitable. E.g. for\n/// Op0 u<= Op1:\n///   t = psubus Op0, Op1\n///   pcmpeq t, <0..0>\nstatic SDValue LowerVSETCCWithSUBUS(SDValue Op0, SDValue Op1, MVT VT,\n                                    ISD::CondCode Cond, const SDLoc &dl,\n                                    const X86Subtarget &Subtarget,\n                                    SelectionDAG &DAG) {\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  MVT VET = VT.getVectorElementType();\n  if (VET != MVT::i8 && VET != MVT::i16)\n    return SDValue();\n\n  switch (Cond) {\n  default:\n    return SDValue();\n  case ISD::SETULT: {\n    // If the comparison is against a constant we can turn this into a\n    // setule.  With psubus, setule does not require a swap.  This is\n    // beneficial because the constant in the register is no longer\n    // destructed as the destination so it can be hoisted out of a loop.\n    // Only do this pre-AVX since vpcmp* is no longer destructive.\n    if (Subtarget.hasAVX())\n      return SDValue();\n    SDValue ULEOp1 = incDecVectorConstant(Op1, DAG, /*IsInc*/false);\n    if (!ULEOp1)\n      return SDValue();\n    Op1 = ULEOp1;\n    break;\n  }\n  case ISD::SETUGT: {\n    // If the comparison is against a constant, we can turn this into a setuge.\n    // This is beneficial because materializing a constant 0 for the PCMPEQ is\n    // probably cheaper than XOR+PCMPGT using 2 different vector constants:\n    // cmpgt (xor X, SignMaskC) CmpC --> cmpeq (usubsat (CmpC+1), X), 0\n    SDValue UGEOp1 = incDecVectorConstant(Op1, DAG, /*IsInc*/true);\n    if (!UGEOp1)\n      return SDValue();\n    Op1 = Op0;\n    Op0 = UGEOp1;\n    break;\n  }\n  // Psubus is better than flip-sign because it requires no inversion.\n  case ISD::SETUGE:\n    std::swap(Op0, Op1);\n    break;\n  case ISD::SETULE:\n    break;\n  }\n\n  SDValue Result = DAG.getNode(ISD::USUBSAT, dl, VT, Op0, Op1);\n  return DAG.getNode(X86ISD::PCMPEQ, dl, VT, Result,\n                     DAG.getConstant(0, dl, VT));\n}\n\nstatic SDValue LowerVSETCC(SDValue Op, const X86Subtarget &Subtarget,\n                           SelectionDAG &DAG) {\n  bool IsStrict = Op.getOpcode() == ISD::STRICT_FSETCC ||\n                  Op.getOpcode() == ISD::STRICT_FSETCCS;\n  SDValue Op0 = Op.getOperand(IsStrict ? 1 : 0);\n  SDValue Op1 = Op.getOperand(IsStrict ? 2 : 1);\n  SDValue CC = Op.getOperand(IsStrict ? 3 : 2);\n  MVT VT = Op->getSimpleValueType(0);\n  ISD::CondCode Cond = cast<CondCodeSDNode>(CC)->get();\n  bool isFP = Op1.getSimpleValueType().isFloatingPoint();\n  SDLoc dl(Op);\n\n  if (isFP) {\n#ifndef NDEBUG\n    MVT EltVT = Op0.getSimpleValueType().getVectorElementType();\n    assert(EltVT == MVT::f32 || EltVT == MVT::f64);\n#endif\n\n    bool IsSignaling = Op.getOpcode() == ISD::STRICT_FSETCCS;\n    SDValue Chain = IsStrict ? Op.getOperand(0) : SDValue();\n\n    // If we have a strict compare with a vXi1 result and the input is 128/256\n    // bits we can't use a masked compare unless we have VLX. If we use a wider\n    // compare like we do for non-strict, we might trigger spurious exceptions\n    // from the upper elements. Instead emit a AVX compare and convert to mask.\n    unsigned Opc;\n    if (Subtarget.hasAVX512() && VT.getVectorElementType() == MVT::i1 &&\n        (!IsStrict || Subtarget.hasVLX() ||\n         Op0.getSimpleValueType().is512BitVector())) {\n      assert(VT.getVectorNumElements() <= 16);\n      Opc = IsStrict ? X86ISD::STRICT_CMPM : X86ISD::CMPM;\n    } else {\n      Opc = IsStrict ? X86ISD::STRICT_CMPP : X86ISD::CMPP;\n      // The SSE/AVX packed FP comparison nodes are defined with a\n      // floating-point vector result that matches the operand type. This allows\n      // them to work with an SSE1 target (integer vector types are not legal).\n      VT = Op0.getSimpleValueType();\n    }\n\n    SDValue Cmp;\n    bool IsAlwaysSignaling;\n    unsigned SSECC = translateX86FSETCC(Cond, Op0, Op1, IsAlwaysSignaling);\n    if (!Subtarget.hasAVX()) {\n      // TODO: We could use following steps to handle a quiet compare with\n      // signaling encodings.\n      // 1. Get ordered masks from a quiet ISD::SETO\n      // 2. Use the masks to mask potential unordered elements in operand A, B\n      // 3. Get the compare results of masked A, B\n      // 4. Calculating final result using the mask and result from 3\n      // But currently, we just fall back to scalar operations.\n      if (IsStrict && IsAlwaysSignaling && !IsSignaling)\n        return SDValue();\n\n      // Insert an extra signaling instruction to raise exception.\n      if (IsStrict && !IsAlwaysSignaling && IsSignaling) {\n        SDValue SignalCmp = DAG.getNode(\n            Opc, dl, {VT, MVT::Other},\n            {Chain, Op0, Op1, DAG.getTargetConstant(1, dl, MVT::i8)}); // LT_OS\n        // FIXME: It seems we need to update the flags of all new strict nodes.\n        // Otherwise, mayRaiseFPException in MI will return false due to\n        // NoFPExcept = false by default. However, I didn't find it in other\n        // patches.\n        SignalCmp->setFlags(Op->getFlags());\n        Chain = SignalCmp.getValue(1);\n      }\n\n      // In the two cases not handled by SSE compare predicates (SETUEQ/SETONE),\n      // emit two comparisons and a logic op to tie them together.\n      if (SSECC >= 8) {\n        // LLVM predicate is SETUEQ or SETONE.\n        unsigned CC0, CC1;\n        unsigned CombineOpc;\n        if (Cond == ISD::SETUEQ) {\n          CC0 = 3; // UNORD\n          CC1 = 0; // EQ\n          CombineOpc = X86ISD::FOR;\n        } else {\n          assert(Cond == ISD::SETONE);\n          CC0 = 7; // ORD\n          CC1 = 4; // NEQ\n          CombineOpc = X86ISD::FAND;\n        }\n\n        SDValue Cmp0, Cmp1;\n        if (IsStrict) {\n          Cmp0 = DAG.getNode(\n              Opc, dl, {VT, MVT::Other},\n              {Chain, Op0, Op1, DAG.getTargetConstant(CC0, dl, MVT::i8)});\n          Cmp1 = DAG.getNode(\n              Opc, dl, {VT, MVT::Other},\n              {Chain, Op0, Op1, DAG.getTargetConstant(CC1, dl, MVT::i8)});\n          Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other, Cmp0.getValue(1),\n                              Cmp1.getValue(1));\n        } else {\n          Cmp0 = DAG.getNode(\n              Opc, dl, VT, Op0, Op1, DAG.getTargetConstant(CC0, dl, MVT::i8));\n          Cmp1 = DAG.getNode(\n              Opc, dl, VT, Op0, Op1, DAG.getTargetConstant(CC1, dl, MVT::i8));\n        }\n        Cmp = DAG.getNode(CombineOpc, dl, VT, Cmp0, Cmp1);\n      } else {\n        if (IsStrict) {\n          Cmp = DAG.getNode(\n              Opc, dl, {VT, MVT::Other},\n              {Chain, Op0, Op1, DAG.getTargetConstant(SSECC, dl, MVT::i8)});\n          Chain = Cmp.getValue(1);\n        } else\n          Cmp = DAG.getNode(\n              Opc, dl, VT, Op0, Op1, DAG.getTargetConstant(SSECC, dl, MVT::i8));\n      }\n    } else {\n      // Handle all other FP comparisons here.\n      if (IsStrict) {\n        // Make a flip on already signaling CCs before setting bit 4 of AVX CC.\n        SSECC |= (IsAlwaysSignaling ^ IsSignaling) << 4;\n        Cmp = DAG.getNode(\n            Opc, dl, {VT, MVT::Other},\n            {Chain, Op0, Op1, DAG.getTargetConstant(SSECC, dl, MVT::i8)});\n        Chain = Cmp.getValue(1);\n      } else\n        Cmp = DAG.getNode(\n            Opc, dl, VT, Op0, Op1, DAG.getTargetConstant(SSECC, dl, MVT::i8));\n    }\n\n    if (VT.getFixedSizeInBits() >\n        Op.getSimpleValueType().getFixedSizeInBits()) {\n      // We emitted a compare with an XMM/YMM result. Finish converting to a\n      // mask register using a vptestm.\n      EVT CastVT = EVT(VT).changeVectorElementTypeToInteger();\n      Cmp = DAG.getBitcast(CastVT, Cmp);\n      Cmp = DAG.getSetCC(dl, Op.getSimpleValueType(), Cmp,\n                         DAG.getConstant(0, dl, CastVT), ISD::SETNE);\n    } else {\n      // If this is SSE/AVX CMPP, bitcast the result back to integer to match\n      // the result type of SETCC. The bitcast is expected to be optimized\n      // away during combining/isel.\n      Cmp = DAG.getBitcast(Op.getSimpleValueType(), Cmp);\n    }\n\n    if (IsStrict)\n      return DAG.getMergeValues({Cmp, Chain}, dl);\n\n    return Cmp;\n  }\n\n  assert(!IsStrict && \"Strict SETCC only handles FP operands.\");\n\n  MVT VTOp0 = Op0.getSimpleValueType();\n  (void)VTOp0;\n  assert(VTOp0 == Op1.getSimpleValueType() &&\n         \"Expected operands with same type!\");\n  assert(VT.getVectorNumElements() == VTOp0.getVectorNumElements() &&\n         \"Invalid number of packed elements for source and destination!\");\n\n  // The non-AVX512 code below works under the assumption that source and\n  // destination types are the same.\n  assert((Subtarget.hasAVX512() || (VT == VTOp0)) &&\n         \"Value types for source and destination must be the same!\");\n\n  // The result is boolean, but operands are int/float\n  if (VT.getVectorElementType() == MVT::i1) {\n    // In AVX-512 architecture setcc returns mask with i1 elements,\n    // But there is no compare instruction for i8 and i16 elements in KNL.\n    assert((VTOp0.getScalarSizeInBits() >= 32 || Subtarget.hasBWI()) &&\n           \"Unexpected operand type\");\n    return LowerIntVSETCC_AVX512(Op, DAG);\n  }\n\n  // Lower using XOP integer comparisons.\n  if (VT.is128BitVector() && Subtarget.hasXOP()) {\n    // Translate compare code to XOP PCOM compare mode.\n    unsigned CmpMode = 0;\n    switch (Cond) {\n    default: llvm_unreachable(\"Unexpected SETCC condition\");\n    case ISD::SETULT:\n    case ISD::SETLT: CmpMode = 0x00; break;\n    case ISD::SETULE:\n    case ISD::SETLE: CmpMode = 0x01; break;\n    case ISD::SETUGT:\n    case ISD::SETGT: CmpMode = 0x02; break;\n    case ISD::SETUGE:\n    case ISD::SETGE: CmpMode = 0x03; break;\n    case ISD::SETEQ: CmpMode = 0x04; break;\n    case ISD::SETNE: CmpMode = 0x05; break;\n    }\n\n    // Are we comparing unsigned or signed integers?\n    unsigned Opc =\n        ISD::isUnsignedIntSetCC(Cond) ? X86ISD::VPCOMU : X86ISD::VPCOM;\n\n    return DAG.getNode(Opc, dl, VT, Op0, Op1,\n                       DAG.getTargetConstant(CmpMode, dl, MVT::i8));\n  }\n\n  // (X & Y) != 0 --> (X & Y) == Y iff Y is power-of-2.\n  // Revert part of the simplifySetCCWithAnd combine, to avoid an invert.\n  if (Cond == ISD::SETNE && ISD::isBuildVectorAllZeros(Op1.getNode())) {\n    SDValue BC0 = peekThroughBitcasts(Op0);\n    if (BC0.getOpcode() == ISD::AND) {\n      APInt UndefElts;\n      SmallVector<APInt, 64> EltBits;\n      if (getTargetConstantBitsFromNode(BC0.getOperand(1),\n                                        VT.getScalarSizeInBits(), UndefElts,\n                                        EltBits, false, false)) {\n        if (llvm::all_of(EltBits, [](APInt &V) { return V.isPowerOf2(); })) {\n          Cond = ISD::SETEQ;\n          Op1 = DAG.getBitcast(VT, BC0.getOperand(1));\n        }\n      }\n    }\n  }\n\n  // ICMP_EQ(AND(X,C),C) -> SRA(SHL(X,LOG2(C)),BW-1) iff C is power-of-2.\n  if (Cond == ISD::SETEQ && Op0.getOpcode() == ISD::AND &&\n      Op0.getOperand(1) == Op1 && Op0.hasOneUse()) {\n    ConstantSDNode *C1 = isConstOrConstSplat(Op1);\n    if (C1 && C1->getAPIntValue().isPowerOf2()) {\n      unsigned BitWidth = VT.getScalarSizeInBits();\n      unsigned ShiftAmt = BitWidth - C1->getAPIntValue().logBase2() - 1;\n\n      SDValue Result = Op0.getOperand(0);\n      Result = DAG.getNode(ISD::SHL, dl, VT, Result,\n                           DAG.getConstant(ShiftAmt, dl, VT));\n      Result = DAG.getNode(ISD::SRA, dl, VT, Result,\n                           DAG.getConstant(BitWidth - 1, dl, VT));\n      return Result;\n    }\n  }\n\n  // Break 256-bit integer vector compare into smaller ones.\n  if (VT.is256BitVector() && !Subtarget.hasInt256())\n    return splitIntVSETCC(Op, DAG);\n\n  if (VT == MVT::v32i16 || VT == MVT::v64i8) {\n    assert(!Subtarget.hasBWI() && \"Unexpected VT with AVX512BW!\");\n    return splitIntVSETCC(Op, DAG);\n  }\n\n  // If this is a SETNE against the signed minimum value, change it to SETGT.\n  // If this is a SETNE against the signed maximum value, change it to SETLT.\n  // which will be swapped to SETGT.\n  // Otherwise we use PCMPEQ+invert.\n  APInt ConstValue;\n  if (Cond == ISD::SETNE &&\n      ISD::isConstantSplatVector(Op1.getNode(), ConstValue)) {\n    if (ConstValue.isMinSignedValue())\n      Cond = ISD::SETGT;\n    else if (ConstValue.isMaxSignedValue())\n      Cond = ISD::SETLT;\n  }\n\n  // If both operands are known non-negative, then an unsigned compare is the\n  // same as a signed compare and there's no need to flip signbits.\n  // TODO: We could check for more general simplifications here since we're\n  // computing known bits.\n  bool FlipSigns = ISD::isUnsignedIntSetCC(Cond) &&\n                   !(DAG.SignBitIsZero(Op0) && DAG.SignBitIsZero(Op1));\n\n  // Special case: Use min/max operations for unsigned compares.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (ISD::isUnsignedIntSetCC(Cond) &&\n      (FlipSigns || ISD::isTrueWhenEqual(Cond)) &&\n      TLI.isOperationLegal(ISD::UMIN, VT)) {\n    // If we have a constant operand, increment/decrement it and change the\n    // condition to avoid an invert.\n    if (Cond == ISD::SETUGT) {\n      // X > C --> X >= (C+1) --> X == umax(X, C+1)\n      if (SDValue UGTOp1 = incDecVectorConstant(Op1, DAG, /*IsInc*/true)) {\n        Op1 = UGTOp1;\n        Cond = ISD::SETUGE;\n      }\n    }\n    if (Cond == ISD::SETULT) {\n      // X < C --> X <= (C-1) --> X == umin(X, C-1)\n      if (SDValue ULTOp1 = incDecVectorConstant(Op1, DAG, /*IsInc*/false)) {\n        Op1 = ULTOp1;\n        Cond = ISD::SETULE;\n      }\n    }\n    bool Invert = false;\n    unsigned Opc;\n    switch (Cond) {\n    default: llvm_unreachable(\"Unexpected condition code\");\n    case ISD::SETUGT: Invert = true; LLVM_FALLTHROUGH;\n    case ISD::SETULE: Opc = ISD::UMIN; break;\n    case ISD::SETULT: Invert = true; LLVM_FALLTHROUGH;\n    case ISD::SETUGE: Opc = ISD::UMAX; break;\n    }\n\n    SDValue Result = DAG.getNode(Opc, dl, VT, Op0, Op1);\n    Result = DAG.getNode(X86ISD::PCMPEQ, dl, VT, Op0, Result);\n\n    // If the logical-not of the result is required, perform that now.\n    if (Invert)\n      Result = DAG.getNOT(dl, Result, VT);\n\n    return Result;\n  }\n\n  // Try to use SUBUS and PCMPEQ.\n  if (FlipSigns)\n    if (SDValue V =\n            LowerVSETCCWithSUBUS(Op0, Op1, VT, Cond, dl, Subtarget, DAG))\n      return V;\n\n  // We are handling one of the integer comparisons here. Since SSE only has\n  // GT and EQ comparisons for integer, swapping operands and multiple\n  // operations may be required for some comparisons.\n  unsigned Opc = (Cond == ISD::SETEQ || Cond == ISD::SETNE) ? X86ISD::PCMPEQ\n                                                            : X86ISD::PCMPGT;\n  bool Swap = Cond == ISD::SETLT || Cond == ISD::SETULT ||\n              Cond == ISD::SETGE || Cond == ISD::SETUGE;\n  bool Invert = Cond == ISD::SETNE ||\n                (Cond != ISD::SETEQ && ISD::isTrueWhenEqual(Cond));\n\n  if (Swap)\n    std::swap(Op0, Op1);\n\n  // Check that the operation in question is available (most are plain SSE2,\n  // but PCMPGTQ and PCMPEQQ have different requirements).\n  if (VT == MVT::v2i64) {\n    if (Opc == X86ISD::PCMPGT && !Subtarget.hasSSE42()) {\n      assert(Subtarget.hasSSE2() && \"Don't know how to lower!\");\n\n      // Special case for sign bit test. We can use a v4i32 PCMPGT and shuffle\n      // the odd elements over the even elements.\n      if (!FlipSigns && !Invert && ISD::isBuildVectorAllZeros(Op0.getNode())) {\n        Op0 = DAG.getConstant(0, dl, MVT::v4i32);\n        Op1 = DAG.getBitcast(MVT::v4i32, Op1);\n\n        SDValue GT = DAG.getNode(X86ISD::PCMPGT, dl, MVT::v4i32, Op0, Op1);\n        static const int MaskHi[] = { 1, 1, 3, 3 };\n        SDValue Result = DAG.getVectorShuffle(MVT::v4i32, dl, GT, GT, MaskHi);\n\n        return DAG.getBitcast(VT, Result);\n      }\n\n      if (!FlipSigns && !Invert && ISD::isBuildVectorAllOnes(Op1.getNode())) {\n        Op0 = DAG.getBitcast(MVT::v4i32, Op0);\n        Op1 = DAG.getConstant(-1, dl, MVT::v4i32);\n\n        SDValue GT = DAG.getNode(X86ISD::PCMPGT, dl, MVT::v4i32, Op0, Op1);\n        static const int MaskHi[] = { 1, 1, 3, 3 };\n        SDValue Result = DAG.getVectorShuffle(MVT::v4i32, dl, GT, GT, MaskHi);\n\n        return DAG.getBitcast(VT, Result);\n      }\n\n      // Since SSE has no unsigned integer comparisons, we need to flip the sign\n      // bits of the inputs before performing those operations. The lower\n      // compare is always unsigned.\n      SDValue SB;\n      if (FlipSigns) {\n        SB = DAG.getConstant(0x8000000080000000ULL, dl, MVT::v2i64);\n      } else {\n        SB = DAG.getConstant(0x0000000080000000ULL, dl, MVT::v2i64);\n      }\n      Op0 = DAG.getNode(ISD::XOR, dl, MVT::v2i64, Op0, SB);\n      Op1 = DAG.getNode(ISD::XOR, dl, MVT::v2i64, Op1, SB);\n\n      // Cast everything to the right type.\n      Op0 = DAG.getBitcast(MVT::v4i32, Op0);\n      Op1 = DAG.getBitcast(MVT::v4i32, Op1);\n\n      // Emulate PCMPGTQ with (hi1 > hi2) | ((hi1 == hi2) & (lo1 > lo2))\n      SDValue GT = DAG.getNode(X86ISD::PCMPGT, dl, MVT::v4i32, Op0, Op1);\n      SDValue EQ = DAG.getNode(X86ISD::PCMPEQ, dl, MVT::v4i32, Op0, Op1);\n\n      // Create masks for only the low parts/high parts of the 64 bit integers.\n      static const int MaskHi[] = { 1, 1, 3, 3 };\n      static const int MaskLo[] = { 0, 0, 2, 2 };\n      SDValue EQHi = DAG.getVectorShuffle(MVT::v4i32, dl, EQ, EQ, MaskHi);\n      SDValue GTLo = DAG.getVectorShuffle(MVT::v4i32, dl, GT, GT, MaskLo);\n      SDValue GTHi = DAG.getVectorShuffle(MVT::v4i32, dl, GT, GT, MaskHi);\n\n      SDValue Result = DAG.getNode(ISD::AND, dl, MVT::v4i32, EQHi, GTLo);\n      Result = DAG.getNode(ISD::OR, dl, MVT::v4i32, Result, GTHi);\n\n      if (Invert)\n        Result = DAG.getNOT(dl, Result, MVT::v4i32);\n\n      return DAG.getBitcast(VT, Result);\n    }\n\n    if (Opc == X86ISD::PCMPEQ && !Subtarget.hasSSE41()) {\n      // If pcmpeqq is missing but pcmpeqd is available synthesize pcmpeqq with\n      // pcmpeqd + pshufd + pand.\n      assert(Subtarget.hasSSE2() && !FlipSigns && \"Don't know how to lower!\");\n\n      // First cast everything to the right type.\n      Op0 = DAG.getBitcast(MVT::v4i32, Op0);\n      Op1 = DAG.getBitcast(MVT::v4i32, Op1);\n\n      // Do the compare.\n      SDValue Result = DAG.getNode(Opc, dl, MVT::v4i32, Op0, Op1);\n\n      // Make sure the lower and upper halves are both all-ones.\n      static const int Mask[] = { 1, 0, 3, 2 };\n      SDValue Shuf = DAG.getVectorShuffle(MVT::v4i32, dl, Result, Result, Mask);\n      Result = DAG.getNode(ISD::AND, dl, MVT::v4i32, Result, Shuf);\n\n      if (Invert)\n        Result = DAG.getNOT(dl, Result, MVT::v4i32);\n\n      return DAG.getBitcast(VT, Result);\n    }\n  }\n\n  // Since SSE has no unsigned integer comparisons, we need to flip the sign\n  // bits of the inputs before performing those operations.\n  if (FlipSigns) {\n    MVT EltVT = VT.getVectorElementType();\n    SDValue SM = DAG.getConstant(APInt::getSignMask(EltVT.getSizeInBits()), dl,\n                                 VT);\n    Op0 = DAG.getNode(ISD::XOR, dl, VT, Op0, SM);\n    Op1 = DAG.getNode(ISD::XOR, dl, VT, Op1, SM);\n  }\n\n  SDValue Result = DAG.getNode(Opc, dl, VT, Op0, Op1);\n\n  // If the logical-not of the result is required, perform that now.\n  if (Invert)\n    Result = DAG.getNOT(dl, Result, VT);\n\n  return Result;\n}\n\n// Try to select this as a KORTEST+SETCC or KTEST+SETCC if possible.\nstatic SDValue EmitAVX512Test(SDValue Op0, SDValue Op1, ISD::CondCode CC,\n                              const SDLoc &dl, SelectionDAG &DAG,\n                              const X86Subtarget &Subtarget,\n                              SDValue &X86CC) {\n  // Only support equality comparisons.\n  if (CC != ISD::SETEQ && CC != ISD::SETNE)\n    return SDValue();\n\n  // Must be a bitcast from vXi1.\n  if (Op0.getOpcode() != ISD::BITCAST)\n    return SDValue();\n\n  Op0 = Op0.getOperand(0);\n  MVT VT = Op0.getSimpleValueType();\n  if (!(Subtarget.hasAVX512() && VT == MVT::v16i1) &&\n      !(Subtarget.hasDQI() && VT == MVT::v8i1) &&\n      !(Subtarget.hasBWI() && (VT == MVT::v32i1 || VT == MVT::v64i1)))\n    return SDValue();\n\n  X86::CondCode X86Cond;\n  if (isNullConstant(Op1)) {\n    X86Cond = CC == ISD::SETEQ ? X86::COND_E : X86::COND_NE;\n  } else if (isAllOnesConstant(Op1)) {\n    // C flag is set for all ones.\n    X86Cond = CC == ISD::SETEQ ? X86::COND_B : X86::COND_AE;\n  } else\n    return SDValue();\n\n  // If the input is an AND, we can combine it's operands into the KTEST.\n  bool KTestable = false;\n  if (Subtarget.hasDQI() && (VT == MVT::v8i1 || VT == MVT::v16i1))\n    KTestable = true;\n  if (Subtarget.hasBWI() && (VT == MVT::v32i1 || VT == MVT::v64i1))\n    KTestable = true;\n  if (!isNullConstant(Op1))\n    KTestable = false;\n  if (KTestable && Op0.getOpcode() == ISD::AND && Op0.hasOneUse()) {\n    SDValue LHS = Op0.getOperand(0);\n    SDValue RHS = Op0.getOperand(1);\n    X86CC = DAG.getTargetConstant(X86Cond, dl, MVT::i8);\n    return DAG.getNode(X86ISD::KTEST, dl, MVT::i32, LHS, RHS);\n  }\n\n  // If the input is an OR, we can combine it's operands into the KORTEST.\n  SDValue LHS = Op0;\n  SDValue RHS = Op0;\n  if (Op0.getOpcode() == ISD::OR && Op0.hasOneUse()) {\n    LHS = Op0.getOperand(0);\n    RHS = Op0.getOperand(1);\n  }\n\n  X86CC = DAG.getTargetConstant(X86Cond, dl, MVT::i8);\n  return DAG.getNode(X86ISD::KORTEST, dl, MVT::i32, LHS, RHS);\n}\n\n/// Emit flags for the given setcc condition and operands. Also returns the\n/// corresponding X86 condition code constant in X86CC.\nSDValue X86TargetLowering::emitFlagsForSetcc(SDValue Op0, SDValue Op1,\n                                             ISD::CondCode CC, const SDLoc &dl,\n                                             SelectionDAG &DAG,\n                                             SDValue &X86CC) const {\n  // Optimize to BT if possible.\n  // Lower (X & (1 << N)) == 0 to BT(X, N).\n  // Lower ((X >>u N) & 1) != 0 to BT(X, N).\n  // Lower ((X >>s N) & 1) != 0 to BT(X, N).\n  if (Op0.getOpcode() == ISD::AND && Op0.hasOneUse() && isNullConstant(Op1) &&\n      (CC == ISD::SETEQ || CC == ISD::SETNE)) {\n    if (SDValue BT = LowerAndToBT(Op0, CC, dl, DAG, X86CC))\n      return BT;\n  }\n\n  // Try to use PTEST/PMOVMSKB for a tree ORs equality compared with 0.\n  // TODO: We could do AND tree with all 1s as well by using the C flag.\n  if (isNullConstant(Op1) && (CC == ISD::SETEQ || CC == ISD::SETNE))\n    if (SDValue CmpZ =\n            MatchVectorAllZeroTest(Op0, CC, dl, Subtarget, DAG, X86CC))\n      return CmpZ;\n\n  // Try to lower using KORTEST or KTEST.\n  if (SDValue Test = EmitAVX512Test(Op0, Op1, CC, dl, DAG, Subtarget, X86CC))\n    return Test;\n\n  // Look for X == 0, X == 1, X != 0, or X != 1.  We can simplify some forms of\n  // these.\n  if ((isOneConstant(Op1) || isNullConstant(Op1)) &&\n      (CC == ISD::SETEQ || CC == ISD::SETNE)) {\n    // If the input is a setcc, then reuse the input setcc or use a new one with\n    // the inverted condition.\n    if (Op0.getOpcode() == X86ISD::SETCC) {\n      bool Invert = (CC == ISD::SETNE) ^ isNullConstant(Op1);\n\n      X86CC = Op0.getOperand(0);\n      if (Invert) {\n        X86::CondCode CCode = (X86::CondCode)Op0.getConstantOperandVal(0);\n        CCode = X86::GetOppositeBranchCondition(CCode);\n        X86CC = DAG.getTargetConstant(CCode, dl, MVT::i8);\n      }\n\n      return Op0.getOperand(1);\n    }\n  }\n\n  // Try to use the carry flag from the add in place of an separate CMP for:\n  // (seteq (add X, -1), -1). Similar for setne.\n  if (isAllOnesConstant(Op1) && Op0.getOpcode() == ISD::ADD &&\n      Op0.getOperand(1) == Op1 && (CC == ISD::SETEQ || CC == ISD::SETNE)) {\n    if (isProfitableToUseFlagOp(Op0)) {\n      SDVTList VTs = DAG.getVTList(Op0.getValueType(), MVT::i32);\n\n      SDValue New = DAG.getNode(X86ISD::ADD, dl, VTs, Op0.getOperand(0),\n                                Op0.getOperand(1));\n      DAG.ReplaceAllUsesOfValueWith(SDValue(Op0.getNode(), 0), New);\n      X86::CondCode CCode = CC == ISD::SETEQ ? X86::COND_AE : X86::COND_B;\n      X86CC = DAG.getTargetConstant(CCode, dl, MVT::i8);\n      return SDValue(New.getNode(), 1);\n    }\n  }\n\n  X86::CondCode CondCode =\n      TranslateX86CC(CC, dl, /*IsFP*/ false, Op0, Op1, DAG);\n  assert(CondCode != X86::COND_INVALID && \"Unexpected condition code!\");\n\n  SDValue EFLAGS = EmitCmp(Op0, Op1, CondCode, dl, DAG, Subtarget);\n  X86CC = DAG.getTargetConstant(CondCode, dl, MVT::i8);\n  return EFLAGS;\n}\n\nSDValue X86TargetLowering::LowerSETCC(SDValue Op, SelectionDAG &DAG) const {\n\n  bool IsStrict = Op.getOpcode() == ISD::STRICT_FSETCC ||\n                  Op.getOpcode() == ISD::STRICT_FSETCCS;\n  MVT VT = Op->getSimpleValueType(0);\n\n  if (VT.isVector()) return LowerVSETCC(Op, Subtarget, DAG);\n\n  assert(VT == MVT::i8 && \"SetCC type must be 8-bit integer\");\n  SDValue Chain = IsStrict ? Op.getOperand(0) : SDValue();\n  SDValue Op0 = Op.getOperand(IsStrict ? 1 : 0);\n  SDValue Op1 = Op.getOperand(IsStrict ? 2 : 1);\n  SDLoc dl(Op);\n  ISD::CondCode CC =\n      cast<CondCodeSDNode>(Op.getOperand(IsStrict ? 3 : 2))->get();\n\n  // Handle f128 first, since one possible outcome is a normal integer\n  // comparison which gets handled by emitFlagsForSetcc.\n  if (Op0.getValueType() == MVT::f128) {\n    softenSetCCOperands(DAG, MVT::f128, Op0, Op1, CC, dl, Op0, Op1, Chain,\n                        Op.getOpcode() == ISD::STRICT_FSETCCS);\n\n    // If softenSetCCOperands returned a scalar, use it.\n    if (!Op1.getNode()) {\n      assert(Op0.getValueType() == Op.getValueType() &&\n             \"Unexpected setcc expansion!\");\n      if (IsStrict)\n        return DAG.getMergeValues({Op0, Chain}, dl);\n      return Op0;\n    }\n  }\n\n  if (Op0.getSimpleValueType().isInteger()) {\n    SDValue X86CC;\n    SDValue EFLAGS = emitFlagsForSetcc(Op0, Op1, CC, dl, DAG, X86CC);\n    SDValue Res = DAG.getNode(X86ISD::SETCC, dl, MVT::i8, X86CC, EFLAGS);\n    return IsStrict ? DAG.getMergeValues({Res, Chain}, dl) : Res;\n  }\n\n  // Handle floating point.\n  X86::CondCode CondCode = TranslateX86CC(CC, dl, /*IsFP*/ true, Op0, Op1, DAG);\n  if (CondCode == X86::COND_INVALID)\n    return SDValue();\n\n  SDValue EFLAGS;\n  if (IsStrict) {\n    bool IsSignaling = Op.getOpcode() == ISD::STRICT_FSETCCS;\n    EFLAGS =\n        DAG.getNode(IsSignaling ? X86ISD::STRICT_FCMPS : X86ISD::STRICT_FCMP,\n                    dl, {MVT::i32, MVT::Other}, {Chain, Op0, Op1});\n    Chain = EFLAGS.getValue(1);\n  } else {\n    EFLAGS = DAG.getNode(X86ISD::FCMP, dl, MVT::i32, Op0, Op1);\n  }\n\n  SDValue X86CC = DAG.getTargetConstant(CondCode, dl, MVT::i8);\n  SDValue Res = DAG.getNode(X86ISD::SETCC, dl, MVT::i8, X86CC, EFLAGS);\n  return IsStrict ? DAG.getMergeValues({Res, Chain}, dl) : Res;\n}\n\nSDValue X86TargetLowering::LowerSETCCCARRY(SDValue Op, SelectionDAG &DAG) const {\n  SDValue LHS = Op.getOperand(0);\n  SDValue RHS = Op.getOperand(1);\n  SDValue Carry = Op.getOperand(2);\n  SDValue Cond = Op.getOperand(3);\n  SDLoc DL(Op);\n\n  assert(LHS.getSimpleValueType().isInteger() && \"SETCCCARRY is integer only.\");\n  X86::CondCode CC = TranslateIntegerX86CC(cast<CondCodeSDNode>(Cond)->get());\n\n  // Recreate the carry if needed.\n  EVT CarryVT = Carry.getValueType();\n  Carry = DAG.getNode(X86ISD::ADD, DL, DAG.getVTList(CarryVT, MVT::i32),\n                      Carry, DAG.getAllOnesConstant(DL, CarryVT));\n\n  SDVTList VTs = DAG.getVTList(LHS.getValueType(), MVT::i32);\n  SDValue Cmp = DAG.getNode(X86ISD::SBB, DL, VTs, LHS, RHS, Carry.getValue(1));\n  return getSETCC(CC, Cmp.getValue(1), DL, DAG);\n}\n\n// This function returns three things: the arithmetic computation itself\n// (Value), an EFLAGS result (Overflow), and a condition code (Cond).  The\n// flag and the condition code define the case in which the arithmetic\n// computation overflows.\nstatic std::pair<SDValue, SDValue>\ngetX86XALUOOp(X86::CondCode &Cond, SDValue Op, SelectionDAG &DAG) {\n  assert(Op.getResNo() == 0 && \"Unexpected result number!\");\n  SDValue Value, Overflow;\n  SDValue LHS = Op.getOperand(0);\n  SDValue RHS = Op.getOperand(1);\n  unsigned BaseOp = 0;\n  SDLoc DL(Op);\n  switch (Op.getOpcode()) {\n  default: llvm_unreachable(\"Unknown ovf instruction!\");\n  case ISD::SADDO:\n    BaseOp = X86ISD::ADD;\n    Cond = X86::COND_O;\n    break;\n  case ISD::UADDO:\n    BaseOp = X86ISD::ADD;\n    Cond = isOneConstant(RHS) ? X86::COND_E : X86::COND_B;\n    break;\n  case ISD::SSUBO:\n    BaseOp = X86ISD::SUB;\n    Cond = X86::COND_O;\n    break;\n  case ISD::USUBO:\n    BaseOp = X86ISD::SUB;\n    Cond = X86::COND_B;\n    break;\n  case ISD::SMULO:\n    BaseOp = X86ISD::SMUL;\n    Cond = X86::COND_O;\n    break;\n  case ISD::UMULO:\n    BaseOp = X86ISD::UMUL;\n    Cond = X86::COND_O;\n    break;\n  }\n\n  if (BaseOp) {\n    // Also sets EFLAGS.\n    SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::i32);\n    Value = DAG.getNode(BaseOp, DL, VTs, LHS, RHS);\n    Overflow = Value.getValue(1);\n  }\n\n  return std::make_pair(Value, Overflow);\n}\n\nstatic SDValue LowerXALUO(SDValue Op, SelectionDAG &DAG) {\n  // Lower the \"add/sub/mul with overflow\" instruction into a regular ins plus\n  // a \"setcc\" instruction that checks the overflow flag. The \"brcond\" lowering\n  // looks for this combo and may remove the \"setcc\" instruction if the \"setcc\"\n  // has only one use.\n  SDLoc DL(Op);\n  X86::CondCode Cond;\n  SDValue Value, Overflow;\n  std::tie(Value, Overflow) = getX86XALUOOp(Cond, Op, DAG);\n\n  SDValue SetCC = getSETCC(Cond, Overflow, DL, DAG);\n  assert(Op->getValueType(1) == MVT::i8 && \"Unexpected VT!\");\n  return DAG.getNode(ISD::MERGE_VALUES, DL, Op->getVTList(), Value, SetCC);\n}\n\n/// Return true if opcode is a X86 logical comparison.\nstatic bool isX86LogicalCmp(SDValue Op) {\n  unsigned Opc = Op.getOpcode();\n  if (Opc == X86ISD::CMP || Opc == X86ISD::COMI || Opc == X86ISD::UCOMI ||\n      Opc == X86ISD::FCMP)\n    return true;\n  if (Op.getResNo() == 1 &&\n      (Opc == X86ISD::ADD || Opc == X86ISD::SUB || Opc == X86ISD::ADC ||\n       Opc == X86ISD::SBB || Opc == X86ISD::SMUL || Opc == X86ISD::UMUL ||\n       Opc == X86ISD::OR || Opc == X86ISD::XOR || Opc == X86ISD::AND))\n    return true;\n\n  return false;\n}\n\nstatic bool isTruncWithZeroHighBitsInput(SDValue V, SelectionDAG &DAG) {\n  if (V.getOpcode() != ISD::TRUNCATE)\n    return false;\n\n  SDValue VOp0 = V.getOperand(0);\n  unsigned InBits = VOp0.getValueSizeInBits();\n  unsigned Bits = V.getValueSizeInBits();\n  return DAG.MaskedValueIsZero(VOp0, APInt::getHighBitsSet(InBits,InBits-Bits));\n}\n\nSDValue X86TargetLowering::LowerSELECT(SDValue Op, SelectionDAG &DAG) const {\n  bool AddTest = true;\n  SDValue Cond  = Op.getOperand(0);\n  SDValue Op1 = Op.getOperand(1);\n  SDValue Op2 = Op.getOperand(2);\n  SDLoc DL(Op);\n  MVT VT = Op1.getSimpleValueType();\n  SDValue CC;\n\n  // Lower FP selects into a CMP/AND/ANDN/OR sequence when the necessary SSE ops\n  // are available or VBLENDV if AVX is available.\n  // Otherwise FP cmovs get lowered into a less efficient branch sequence later.\n  if (Cond.getOpcode() == ISD::SETCC && isScalarFPTypeInSSEReg(VT) &&\n      VT == Cond.getOperand(0).getSimpleValueType() && Cond->hasOneUse()) {\n    SDValue CondOp0 = Cond.getOperand(0), CondOp1 = Cond.getOperand(1);\n    bool IsAlwaysSignaling;\n    unsigned SSECC =\n        translateX86FSETCC(cast<CondCodeSDNode>(Cond.getOperand(2))->get(),\n                           CondOp0, CondOp1, IsAlwaysSignaling);\n\n    if (Subtarget.hasAVX512()) {\n      SDValue Cmp =\n          DAG.getNode(X86ISD::FSETCCM, DL, MVT::v1i1, CondOp0, CondOp1,\n                      DAG.getTargetConstant(SSECC, DL, MVT::i8));\n      assert(!VT.isVector() && \"Not a scalar type?\");\n      return DAG.getNode(X86ISD::SELECTS, DL, VT, Cmp, Op1, Op2);\n    }\n\n    if (SSECC < 8 || Subtarget.hasAVX()) {\n      SDValue Cmp = DAG.getNode(X86ISD::FSETCC, DL, VT, CondOp0, CondOp1,\n                                DAG.getTargetConstant(SSECC, DL, MVT::i8));\n\n      // If we have AVX, we can use a variable vector select (VBLENDV) instead\n      // of 3 logic instructions for size savings and potentially speed.\n      // Unfortunately, there is no scalar form of VBLENDV.\n\n      // If either operand is a +0.0 constant, don't try this. We can expect to\n      // optimize away at least one of the logic instructions later in that\n      // case, so that sequence would be faster than a variable blend.\n\n      // BLENDV was introduced with SSE 4.1, but the 2 register form implicitly\n      // uses XMM0 as the selection register. That may need just as many\n      // instructions as the AND/ANDN/OR sequence due to register moves, so\n      // don't bother.\n      if (Subtarget.hasAVX() && !isNullFPConstant(Op1) &&\n          !isNullFPConstant(Op2)) {\n        // Convert to vectors, do a VSELECT, and convert back to scalar.\n        // All of the conversions should be optimized away.\n        MVT VecVT = VT == MVT::f32 ? MVT::v4f32 : MVT::v2f64;\n        SDValue VOp1 = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecVT, Op1);\n        SDValue VOp2 = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecVT, Op2);\n        SDValue VCmp = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecVT, Cmp);\n\n        MVT VCmpVT = VT == MVT::f32 ? MVT::v4i32 : MVT::v2i64;\n        VCmp = DAG.getBitcast(VCmpVT, VCmp);\n\n        SDValue VSel = DAG.getSelect(DL, VecVT, VCmp, VOp1, VOp2);\n\n        return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT,\n                           VSel, DAG.getIntPtrConstant(0, DL));\n      }\n      SDValue AndN = DAG.getNode(X86ISD::FANDN, DL, VT, Cmp, Op2);\n      SDValue And = DAG.getNode(X86ISD::FAND, DL, VT, Cmp, Op1);\n      return DAG.getNode(X86ISD::FOR, DL, VT, AndN, And);\n    }\n  }\n\n  // AVX512 fallback is to lower selects of scalar floats to masked moves.\n  if (isScalarFPTypeInSSEReg(VT) && Subtarget.hasAVX512()) {\n    SDValue Cmp = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v1i1, Cond);\n    return DAG.getNode(X86ISD::SELECTS, DL, VT, Cmp, Op1, Op2);\n  }\n\n  if (Cond.getOpcode() == ISD::SETCC) {\n    if (SDValue NewCond = LowerSETCC(Cond, DAG)) {\n      Cond = NewCond;\n      // If the condition was updated, it's possible that the operands of the\n      // select were also updated (for example, EmitTest has a RAUW). Refresh\n      // the local references to the select operands in case they got stale.\n      Op1 = Op.getOperand(1);\n      Op2 = Op.getOperand(2);\n    }\n  }\n\n  // (select (x == 0), -1, y) -> (sign_bit (x - 1)) | y\n  // (select (x == 0), y, -1) -> ~(sign_bit (x - 1)) | y\n  // (select (x != 0), y, -1) -> (sign_bit (x - 1)) | y\n  // (select (x != 0), -1, y) -> ~(sign_bit (x - 1)) | y\n  // (select (and (x , 0x1) == 0), y, (z ^ y) ) -> (-(and (x , 0x1)) & z ) ^ y\n  // (select (and (x , 0x1) == 0), y, (z | y) ) -> (-(and (x , 0x1)) & z ) | y\n  if (Cond.getOpcode() == X86ISD::SETCC &&\n      Cond.getOperand(1).getOpcode() == X86ISD::CMP &&\n      isNullConstant(Cond.getOperand(1).getOperand(1))) {\n    SDValue Cmp = Cond.getOperand(1);\n    SDValue CmpOp0 = Cmp.getOperand(0);\n    unsigned CondCode = Cond.getConstantOperandVal(0);\n\n    // Special handling for __builtin_ffs(X) - 1 pattern which looks like\n    // (select (seteq X, 0), -1, (cttz_zero_undef X)). Disable the special\n    // handle to keep the CMP with 0. This should be removed by\n    // optimizeCompareInst by using the flags from the BSR/TZCNT used for the\n    // cttz_zero_undef.\n    auto MatchFFSMinus1 = [&](SDValue Op1, SDValue Op2) {\n      return (Op1.getOpcode() == ISD::CTTZ_ZERO_UNDEF && Op1.hasOneUse() &&\n              Op1.getOperand(0) == CmpOp0 && isAllOnesConstant(Op2));\n    };\n    if (Subtarget.hasCMov() && (VT == MVT::i32 || VT == MVT::i64) &&\n        ((CondCode == X86::COND_NE && MatchFFSMinus1(Op1, Op2)) ||\n         (CondCode == X86::COND_E && MatchFFSMinus1(Op2, Op1)))) {\n      // Keep Cmp.\n    } else if ((isAllOnesConstant(Op1) || isAllOnesConstant(Op2)) &&\n        (CondCode == X86::COND_E || CondCode == X86::COND_NE)) {\n      SDValue Y = isAllOnesConstant(Op2) ? Op1 : Op2;\n\n      SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::i32);\n      SDVTList CmpVTs = DAG.getVTList(CmpOp0.getValueType(), MVT::i32);\n\n      // Apply further optimizations for special cases\n      // (select (x != 0), -1, 0) -> neg & sbb\n      // (select (x == 0), 0, -1) -> neg & sbb\n      if (isNullConstant(Y) &&\n          (isAllOnesConstant(Op1) == (CondCode == X86::COND_NE))) {\n        SDValue Zero = DAG.getConstant(0, DL, CmpOp0.getValueType());\n        SDValue Neg = DAG.getNode(X86ISD::SUB, DL, CmpVTs, Zero, CmpOp0);\n        Zero = DAG.getConstant(0, DL, Op.getValueType());\n        return DAG.getNode(X86ISD::SBB, DL, VTs, Zero, Zero, Neg.getValue(1));\n      }\n\n      Cmp = DAG.getNode(X86ISD::SUB, DL, CmpVTs,\n                        CmpOp0, DAG.getConstant(1, DL, CmpOp0.getValueType()));\n\n      SDValue Zero = DAG.getConstant(0, DL, Op.getValueType());\n      SDValue Res =   // Res = 0 or -1.\n        DAG.getNode(X86ISD::SBB, DL, VTs, Zero, Zero, Cmp.getValue(1));\n\n      if (isAllOnesConstant(Op1) != (CondCode == X86::COND_E))\n        Res = DAG.getNOT(DL, Res, Res.getValueType());\n\n      return DAG.getNode(ISD::OR, DL, Res.getValueType(), Res, Y);\n    } else if (!Subtarget.hasCMov() && CondCode == X86::COND_E &&\n               Cmp.getOperand(0).getOpcode() == ISD::AND &&\n               isOneConstant(Cmp.getOperand(0).getOperand(1))) {\n      SDValue Src1, Src2;\n      // true if Op2 is XOR or OR operator and one of its operands\n      // is equal to Op1\n      // ( a , a op b) || ( b , a op b)\n      auto isOrXorPattern = [&]() {\n        if ((Op2.getOpcode() == ISD::XOR || Op2.getOpcode() == ISD::OR) &&\n            (Op2.getOperand(0) == Op1 || Op2.getOperand(1) == Op1)) {\n          Src1 =\n              Op2.getOperand(0) == Op1 ? Op2.getOperand(1) : Op2.getOperand(0);\n          Src2 = Op1;\n          return true;\n        }\n        return false;\n      };\n\n      if (isOrXorPattern()) {\n        SDValue Neg;\n        unsigned int CmpSz = CmpOp0.getSimpleValueType().getSizeInBits();\n        // we need mask of all zeros or ones with same size of the other\n        // operands.\n        if (CmpSz > VT.getSizeInBits())\n          Neg = DAG.getNode(ISD::TRUNCATE, DL, VT, CmpOp0);\n        else if (CmpSz < VT.getSizeInBits())\n          Neg = DAG.getNode(ISD::AND, DL, VT,\n              DAG.getNode(ISD::ANY_EXTEND, DL, VT, CmpOp0.getOperand(0)),\n              DAG.getConstant(1, DL, VT));\n        else\n          Neg = CmpOp0;\n        SDValue Mask = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT),\n                                   Neg); // -(and (x, 0x1))\n        SDValue And = DAG.getNode(ISD::AND, DL, VT, Mask, Src1); // Mask & z\n        return DAG.getNode(Op2.getOpcode(), DL, VT, And, Src2);  // And Op y\n      }\n    }\n  }\n\n  // Look past (and (setcc_carry (cmp ...)), 1).\n  if (Cond.getOpcode() == ISD::AND &&\n      Cond.getOperand(0).getOpcode() == X86ISD::SETCC_CARRY &&\n      isOneConstant(Cond.getOperand(1)))\n    Cond = Cond.getOperand(0);\n\n  // If condition flag is set by a X86ISD::CMP, then use it as the condition\n  // setting operand in place of the X86ISD::SETCC.\n  unsigned CondOpcode = Cond.getOpcode();\n  if (CondOpcode == X86ISD::SETCC ||\n      CondOpcode == X86ISD::SETCC_CARRY) {\n    CC = Cond.getOperand(0);\n\n    SDValue Cmp = Cond.getOperand(1);\n    bool IllegalFPCMov = false;\n    if (VT.isFloatingPoint() && !VT.isVector() &&\n        !isScalarFPTypeInSSEReg(VT) && Subtarget.hasCMov())  // FPStack?\n      IllegalFPCMov = !hasFPCMov(cast<ConstantSDNode>(CC)->getSExtValue());\n\n    if ((isX86LogicalCmp(Cmp) && !IllegalFPCMov) ||\n        Cmp.getOpcode() == X86ISD::BT) { // FIXME\n      Cond = Cmp;\n      AddTest = false;\n    }\n  } else if (CondOpcode == ISD::USUBO || CondOpcode == ISD::SSUBO ||\n             CondOpcode == ISD::UADDO || CondOpcode == ISD::SADDO ||\n             CondOpcode == ISD::UMULO || CondOpcode == ISD::SMULO) {\n    SDValue Value;\n    X86::CondCode X86Cond;\n    std::tie(Value, Cond) = getX86XALUOOp(X86Cond, Cond.getValue(0), DAG);\n\n    CC = DAG.getTargetConstant(X86Cond, DL, MVT::i8);\n    AddTest = false;\n  }\n\n  if (AddTest) {\n    // Look past the truncate if the high bits are known zero.\n    if (isTruncWithZeroHighBitsInput(Cond, DAG))\n      Cond = Cond.getOperand(0);\n\n    // We know the result of AND is compared against zero. Try to match\n    // it to BT.\n    if (Cond.getOpcode() == ISD::AND && Cond.hasOneUse()) {\n      SDValue BTCC;\n      if (SDValue BT = LowerAndToBT(Cond, ISD::SETNE, DL, DAG, BTCC)) {\n        CC = BTCC;\n        Cond = BT;\n        AddTest = false;\n      }\n    }\n  }\n\n  if (AddTest) {\n    CC = DAG.getTargetConstant(X86::COND_NE, DL, MVT::i8);\n    Cond = EmitTest(Cond, X86::COND_NE, DL, DAG, Subtarget);\n  }\n\n  // a <  b ? -1 :  0 -> RES = ~setcc_carry\n  // a <  b ?  0 : -1 -> RES = setcc_carry\n  // a >= b ? -1 :  0 -> RES = setcc_carry\n  // a >= b ?  0 : -1 -> RES = ~setcc_carry\n  if (Cond.getOpcode() == X86ISD::SUB) {\n    unsigned CondCode = cast<ConstantSDNode>(CC)->getZExtValue();\n\n    if ((CondCode == X86::COND_AE || CondCode == X86::COND_B) &&\n        (isAllOnesConstant(Op1) || isAllOnesConstant(Op2)) &&\n        (isNullConstant(Op1) || isNullConstant(Op2))) {\n      SDValue Res =\n          DAG.getNode(X86ISD::SETCC_CARRY, DL, Op.getValueType(),\n                      DAG.getTargetConstant(X86::COND_B, DL, MVT::i8), Cond);\n      if (isAllOnesConstant(Op1) != (CondCode == X86::COND_B))\n        return DAG.getNOT(DL, Res, Res.getValueType());\n      return Res;\n    }\n  }\n\n  // X86 doesn't have an i8 cmov. If both operands are the result of a truncate\n  // widen the cmov and push the truncate through. This avoids introducing a new\n  // branch during isel and doesn't add any extensions.\n  if (Op.getValueType() == MVT::i8 &&\n      Op1.getOpcode() == ISD::TRUNCATE && Op2.getOpcode() == ISD::TRUNCATE) {\n    SDValue T1 = Op1.getOperand(0), T2 = Op2.getOperand(0);\n    if (T1.getValueType() == T2.getValueType() &&\n        // Exclude CopyFromReg to avoid partial register stalls.\n        T1.getOpcode() != ISD::CopyFromReg && T2.getOpcode()!=ISD::CopyFromReg){\n      SDValue Cmov = DAG.getNode(X86ISD::CMOV, DL, T1.getValueType(), T2, T1,\n                                 CC, Cond);\n      return DAG.getNode(ISD::TRUNCATE, DL, Op.getValueType(), Cmov);\n    }\n  }\n\n  // Or finally, promote i8 cmovs if we have CMOV,\n  //                 or i16 cmovs if it won't prevent folding a load.\n  // FIXME: we should not limit promotion of i8 case to only when the CMOV is\n  //        legal, but EmitLoweredSelect() can not deal with these extensions\n  //        being inserted between two CMOV's. (in i16 case too TBN)\n  //        https://bugs.llvm.org/show_bug.cgi?id=40974\n  if ((Op.getValueType() == MVT::i8 && Subtarget.hasCMov()) ||\n      (Op.getValueType() == MVT::i16 && !MayFoldLoad(Op1) &&\n       !MayFoldLoad(Op2))) {\n    Op1 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, Op1);\n    Op2 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, Op2);\n    SDValue Ops[] = { Op2, Op1, CC, Cond };\n    SDValue Cmov = DAG.getNode(X86ISD::CMOV, DL, MVT::i32, Ops);\n    return DAG.getNode(ISD::TRUNCATE, DL, Op.getValueType(), Cmov);\n  }\n\n  // X86ISD::CMOV means set the result (which is operand 1) to the RHS if\n  // condition is true.\n  SDValue Ops[] = { Op2, Op1, CC, Cond };\n  return DAG.getNode(X86ISD::CMOV, DL, Op.getValueType(), Ops);\n}\n\nstatic SDValue LowerSIGN_EXTEND_Mask(SDValue Op,\n                                     const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  MVT VT = Op->getSimpleValueType(0);\n  SDValue In = Op->getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  assert(InVT.getVectorElementType() == MVT::i1 && \"Unexpected input type!\");\n  MVT VTElt = VT.getVectorElementType();\n  SDLoc dl(Op);\n\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // Extend VT if the scalar type is i8/i16 and BWI is not supported.\n  MVT ExtVT = VT;\n  if (!Subtarget.hasBWI() && VTElt.getSizeInBits() <= 16) {\n    // If v16i32 is to be avoided, we'll need to split and concatenate.\n    if (NumElts == 16 && !Subtarget.canExtendTo512DQ())\n      return SplitAndExtendv16i1(Op.getOpcode(), VT, In, dl, DAG);\n\n    ExtVT = MVT::getVectorVT(MVT::i32, NumElts);\n  }\n\n  // Widen to 512-bits if VLX is not supported.\n  MVT WideVT = ExtVT;\n  if (!ExtVT.is512BitVector() && !Subtarget.hasVLX()) {\n    NumElts *= 512 / ExtVT.getSizeInBits();\n    InVT = MVT::getVectorVT(MVT::i1, NumElts);\n    In = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, InVT, DAG.getUNDEF(InVT),\n                     In, DAG.getIntPtrConstant(0, dl));\n    WideVT = MVT::getVectorVT(ExtVT.getVectorElementType(), NumElts);\n  }\n\n  SDValue V;\n  MVT WideEltVT = WideVT.getVectorElementType();\n  if ((Subtarget.hasDQI() && WideEltVT.getSizeInBits() >= 32) ||\n      (Subtarget.hasBWI() && WideEltVT.getSizeInBits() <= 16)) {\n    V = DAG.getNode(Op.getOpcode(), dl, WideVT, In);\n  } else {\n    SDValue NegOne = DAG.getConstant(-1, dl, WideVT);\n    SDValue Zero = DAG.getConstant(0, dl, WideVT);\n    V = DAG.getSelect(dl, WideVT, In, NegOne, Zero);\n  }\n\n  // Truncate if we had to extend i16/i8 above.\n  if (VT != ExtVT) {\n    WideVT = MVT::getVectorVT(VTElt, NumElts);\n    V = DAG.getNode(ISD::TRUNCATE, dl, WideVT, V);\n  }\n\n  // Extract back to 128/256-bit if we widened.\n  if (WideVT != VT)\n    V = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, V,\n                    DAG.getIntPtrConstant(0, dl));\n\n  return V;\n}\n\nstatic SDValue LowerANY_EXTEND(SDValue Op, const X86Subtarget &Subtarget,\n                               SelectionDAG &DAG) {\n  SDValue In = Op->getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n\n  if (InVT.getVectorElementType() == MVT::i1)\n    return LowerSIGN_EXTEND_Mask(Op, Subtarget, DAG);\n\n  assert(Subtarget.hasAVX() && \"Expected AVX support\");\n  return LowerAVXExtend(Op, DAG, Subtarget);\n}\n\n// Lowering for SIGN_EXTEND_VECTOR_INREG and ZERO_EXTEND_VECTOR_INREG.\n// For sign extend this needs to handle all vector sizes and SSE4.1 and\n// non-SSE4.1 targets. For zero extend this should only handle inputs of\n// MVT::v64i8 when BWI is not supported, but AVX512 is.\nstatic SDValue LowerEXTEND_VECTOR_INREG(SDValue Op,\n                                        const X86Subtarget &Subtarget,\n                                        SelectionDAG &DAG) {\n  SDValue In = Op->getOperand(0);\n  MVT VT = Op->getSimpleValueType(0);\n  MVT InVT = In.getSimpleValueType();\n\n  MVT SVT = VT.getVectorElementType();\n  MVT InSVT = InVT.getVectorElementType();\n  assert(SVT.getFixedSizeInBits() > InSVT.getFixedSizeInBits());\n\n  if (SVT != MVT::i64 && SVT != MVT::i32 && SVT != MVT::i16)\n    return SDValue();\n  if (InSVT != MVT::i32 && InSVT != MVT::i16 && InSVT != MVT::i8)\n    return SDValue();\n  if (!(VT.is128BitVector() && Subtarget.hasSSE2()) &&\n      !(VT.is256BitVector() && Subtarget.hasAVX()) &&\n      !(VT.is512BitVector() && Subtarget.hasAVX512()))\n    return SDValue();\n\n  SDLoc dl(Op);\n  unsigned Opc = Op.getOpcode();\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // For 256-bit vectors, we only need the lower (128-bit) half of the input.\n  // For 512-bit vectors, we need 128-bits or 256-bits.\n  if (InVT.getSizeInBits() > 128) {\n    // Input needs to be at least the same number of elements as output, and\n    // at least 128-bits.\n    int InSize = InSVT.getSizeInBits() * NumElts;\n    In = extractSubVector(In, 0, DAG, dl, std::max(InSize, 128));\n    InVT = In.getSimpleValueType();\n  }\n\n  // SSE41 targets can use the pmov[sz]x* instructions directly for 128-bit results,\n  // so are legal and shouldn't occur here. AVX2/AVX512 pmovsx* instructions still\n  // need to be handled here for 256/512-bit results.\n  if (Subtarget.hasInt256()) {\n    assert(VT.getSizeInBits() > 128 && \"Unexpected 128-bit vector extension\");\n\n    if (InVT.getVectorNumElements() != NumElts)\n      return DAG.getNode(Op.getOpcode(), dl, VT, In);\n\n    // FIXME: Apparently we create inreg operations that could be regular\n    // extends.\n    unsigned ExtOpc =\n        Opc == ISD::SIGN_EXTEND_VECTOR_INREG ? ISD::SIGN_EXTEND\n                                             : ISD::ZERO_EXTEND;\n    return DAG.getNode(ExtOpc, dl, VT, In);\n  }\n\n  // pre-AVX2 256-bit extensions need to be split into 128-bit instructions.\n  if (Subtarget.hasAVX()) {\n    assert(VT.is256BitVector() && \"256-bit vector expected\");\n    MVT HalfVT = VT.getHalfNumVectorElementsVT();\n    int HalfNumElts = HalfVT.getVectorNumElements();\n\n    unsigned NumSrcElts = InVT.getVectorNumElements();\n    SmallVector<int, 16> HiMask(NumSrcElts, SM_SentinelUndef);\n    for (int i = 0; i != HalfNumElts; ++i)\n      HiMask[i] = HalfNumElts + i;\n\n    SDValue Lo = DAG.getNode(Opc, dl, HalfVT, In);\n    SDValue Hi = DAG.getVectorShuffle(InVT, dl, In, DAG.getUNDEF(InVT), HiMask);\n    Hi = DAG.getNode(Opc, dl, HalfVT, Hi);\n    return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n  }\n\n  // We should only get here for sign extend.\n  assert(Opc == ISD::SIGN_EXTEND_VECTOR_INREG && \"Unexpected opcode!\");\n  assert(VT.is128BitVector() && InVT.is128BitVector() && \"Unexpected VTs\");\n\n  // pre-SSE41 targets unpack lower lanes and then sign-extend using SRAI.\n  SDValue Curr = In;\n  SDValue SignExt = Curr;\n\n  // As SRAI is only available on i16/i32 types, we expand only up to i32\n  // and handle i64 separately.\n  if (InVT != MVT::v4i32) {\n    MVT DestVT = VT == MVT::v2i64 ? MVT::v4i32 : VT;\n\n    unsigned DestWidth = DestVT.getScalarSizeInBits();\n    unsigned Scale = DestWidth / InSVT.getSizeInBits();\n\n    unsigned InNumElts = InVT.getVectorNumElements();\n    unsigned DestElts = DestVT.getVectorNumElements();\n\n    // Build a shuffle mask that takes each input element and places it in the\n    // MSBs of the new element size.\n    SmallVector<int, 16> Mask(InNumElts, SM_SentinelUndef);\n    for (unsigned i = 0; i != DestElts; ++i)\n      Mask[i * Scale + (Scale - 1)] = i;\n\n    Curr = DAG.getVectorShuffle(InVT, dl, In, In, Mask);\n    Curr = DAG.getBitcast(DestVT, Curr);\n\n    unsigned SignExtShift = DestWidth - InSVT.getSizeInBits();\n    SignExt = DAG.getNode(X86ISD::VSRAI, dl, DestVT, Curr,\n                          DAG.getTargetConstant(SignExtShift, dl, MVT::i8));\n  }\n\n  if (VT == MVT::v2i64) {\n    assert(Curr.getValueType() == MVT::v4i32 && \"Unexpected input VT\");\n    SDValue Zero = DAG.getConstant(0, dl, MVT::v4i32);\n    SDValue Sign = DAG.getSetCC(dl, MVT::v4i32, Zero, Curr, ISD::SETGT);\n    SignExt = DAG.getVectorShuffle(MVT::v4i32, dl, SignExt, Sign, {0, 4, 1, 5});\n    SignExt = DAG.getBitcast(VT, SignExt);\n  }\n\n  return SignExt;\n}\n\nstatic SDValue LowerSIGN_EXTEND(SDValue Op, const X86Subtarget &Subtarget,\n                                SelectionDAG &DAG) {\n  MVT VT = Op->getSimpleValueType(0);\n  SDValue In = Op->getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  SDLoc dl(Op);\n\n  if (InVT.getVectorElementType() == MVT::i1)\n    return LowerSIGN_EXTEND_Mask(Op, Subtarget, DAG);\n\n  assert(VT.isVector() && InVT.isVector() && \"Expected vector type\");\n  assert(VT.getVectorNumElements() == InVT.getVectorNumElements() &&\n         \"Expected same number of elements\");\n  assert((VT.getVectorElementType() == MVT::i16 ||\n          VT.getVectorElementType() == MVT::i32 ||\n          VT.getVectorElementType() == MVT::i64) &&\n         \"Unexpected element type\");\n  assert((InVT.getVectorElementType() == MVT::i8 ||\n          InVT.getVectorElementType() == MVT::i16 ||\n          InVT.getVectorElementType() == MVT::i32) &&\n         \"Unexpected element type\");\n\n  if (VT == MVT::v32i16 && !Subtarget.hasBWI()) {\n    assert(InVT == MVT::v32i8 && \"Unexpected VT!\");\n    return splitVectorIntUnary(Op, DAG);\n  }\n\n  if (Subtarget.hasInt256())\n    return Op;\n\n  // Optimize vectors in AVX mode\n  // Sign extend  v8i16 to v8i32 and\n  //              v4i32 to v4i64\n  //\n  // Divide input vector into two parts\n  // for v4i32 the high shuffle mask will be {2, 3, -1, -1}\n  // use vpmovsx instruction to extend v4i32 -> v2i64; v8i16 -> v4i32\n  // concat the vectors to original VT\n  MVT HalfVT = VT.getHalfNumVectorElementsVT();\n  SDValue OpLo = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, HalfVT, In);\n\n  unsigned NumElems = InVT.getVectorNumElements();\n  SmallVector<int,8> ShufMask(NumElems, -1);\n  for (unsigned i = 0; i != NumElems/2; ++i)\n    ShufMask[i] = i + NumElems/2;\n\n  SDValue OpHi = DAG.getVectorShuffle(InVT, dl, In, In, ShufMask);\n  OpHi = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, HalfVT, OpHi);\n\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, OpLo, OpHi);\n}\n\n/// Change a vector store into a pair of half-size vector stores.\nstatic SDValue splitVectorStore(StoreSDNode *Store, SelectionDAG &DAG) {\n  SDValue StoredVal = Store->getValue();\n  assert((StoredVal.getValueType().is256BitVector() ||\n          StoredVal.getValueType().is512BitVector()) &&\n         \"Expecting 256/512-bit op\");\n\n  // Splitting volatile memory ops is not allowed unless the operation was not\n  // legal to begin with. Assume the input store is legal (this transform is\n  // only used for targets with AVX). Note: It is possible that we have an\n  // illegal type like v2i128, and so we could allow splitting a volatile store\n  // in that case if that is important.\n  if (!Store->isSimple())\n    return SDValue();\n\n  SDLoc DL(Store);\n  SDValue Value0, Value1;\n  std::tie(Value0, Value1) = splitVector(StoredVal, DAG, DL);\n  unsigned HalfOffset = Value0.getValueType().getStoreSize();\n  SDValue Ptr0 = Store->getBasePtr();\n  SDValue Ptr1 =\n      DAG.getMemBasePlusOffset(Ptr0, TypeSize::Fixed(HalfOffset), DL);\n  SDValue Ch0 =\n      DAG.getStore(Store->getChain(), DL, Value0, Ptr0, Store->getPointerInfo(),\n                   Store->getOriginalAlign(),\n                   Store->getMemOperand()->getFlags());\n  SDValue Ch1 = DAG.getStore(Store->getChain(), DL, Value1, Ptr1,\n                             Store->getPointerInfo().getWithOffset(HalfOffset),\n                             Store->getOriginalAlign(),\n                             Store->getMemOperand()->getFlags());\n  return DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Ch0, Ch1);\n}\n\n/// Scalarize a vector store, bitcasting to TargetVT to determine the scalar\n/// type.\nstatic SDValue scalarizeVectorStore(StoreSDNode *Store, MVT StoreVT,\n                                    SelectionDAG &DAG) {\n  SDValue StoredVal = Store->getValue();\n  assert(StoreVT.is128BitVector() &&\n         StoredVal.getValueType().is128BitVector() && \"Expecting 128-bit op\");\n  StoredVal = DAG.getBitcast(StoreVT, StoredVal);\n\n  // Splitting volatile memory ops is not allowed unless the operation was not\n  // legal to begin with. We are assuming the input op is legal (this transform\n  // is only used for targets with AVX).\n  if (!Store->isSimple())\n    return SDValue();\n\n  MVT StoreSVT = StoreVT.getScalarType();\n  unsigned NumElems = StoreVT.getVectorNumElements();\n  unsigned ScalarSize = StoreSVT.getStoreSize();\n\n  SDLoc DL(Store);\n  SmallVector<SDValue, 4> Stores;\n  for (unsigned i = 0; i != NumElems; ++i) {\n    unsigned Offset = i * ScalarSize;\n    SDValue Ptr = DAG.getMemBasePlusOffset(Store->getBasePtr(),\n                                           TypeSize::Fixed(Offset), DL);\n    SDValue Scl = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, StoreSVT, StoredVal,\n                              DAG.getIntPtrConstant(i, DL));\n    SDValue Ch = DAG.getStore(Store->getChain(), DL, Scl, Ptr,\n                              Store->getPointerInfo().getWithOffset(Offset),\n                              Store->getOriginalAlign(),\n                              Store->getMemOperand()->getFlags());\n    Stores.push_back(Ch);\n  }\n  return DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Stores);\n}\n\nstatic SDValue LowerStore(SDValue Op, const X86Subtarget &Subtarget,\n                          SelectionDAG &DAG) {\n  StoreSDNode *St = cast<StoreSDNode>(Op.getNode());\n  SDLoc dl(St);\n  SDValue StoredVal = St->getValue();\n\n  // Without AVX512DQ, we need to use a scalar type for v2i1/v4i1/v8i1 stores.\n  if (StoredVal.getValueType().isVector() &&\n      StoredVal.getValueType().getVectorElementType() == MVT::i1) {\n    unsigned NumElts = StoredVal.getValueType().getVectorNumElements();\n    assert(NumElts <= 8 && \"Unexpected VT\");\n    assert(!St->isTruncatingStore() && \"Expected non-truncating store\");\n    assert(Subtarget.hasAVX512() && !Subtarget.hasDQI() &&\n           \"Expected AVX512F without AVX512DQI\");\n\n    // We must pad with zeros to ensure we store zeroes to any unused bits.\n    StoredVal = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, MVT::v16i1,\n                            DAG.getUNDEF(MVT::v16i1), StoredVal,\n                            DAG.getIntPtrConstant(0, dl));\n    StoredVal = DAG.getBitcast(MVT::i16, StoredVal);\n    StoredVal = DAG.getNode(ISD::TRUNCATE, dl, MVT::i8, StoredVal);\n    // Make sure we store zeros in the extra bits.\n    if (NumElts < 8)\n      StoredVal = DAG.getZeroExtendInReg(\n          StoredVal, dl, EVT::getIntegerVT(*DAG.getContext(), NumElts));\n\n    return DAG.getStore(St->getChain(), dl, StoredVal, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  if (St->isTruncatingStore())\n    return SDValue();\n\n  // If this is a 256-bit store of concatenated ops, we are better off splitting\n  // that store into two 128-bit stores. This avoids spurious use of 256-bit ops\n  // and each half can execute independently. Some cores would split the op into\n  // halves anyway, so the concat (vinsertf128) is purely an extra op.\n  MVT StoreVT = StoredVal.getSimpleValueType();\n  if (StoreVT.is256BitVector() ||\n      ((StoreVT == MVT::v32i16 || StoreVT == MVT::v64i8) &&\n       !Subtarget.hasBWI())) {\n    SmallVector<SDValue, 4> CatOps;\n    if (StoredVal.hasOneUse() && collectConcatOps(StoredVal.getNode(), CatOps))\n      return splitVectorStore(St, DAG);\n    return SDValue();\n  }\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  assert(StoreVT.isVector() && StoreVT.getSizeInBits() == 64 &&\n         \"Unexpected VT\");\n  assert(TLI.getTypeAction(*DAG.getContext(), StoreVT) ==\n             TargetLowering::TypeWidenVector && \"Unexpected type action!\");\n\n  EVT WideVT = TLI.getTypeToTransformTo(*DAG.getContext(), StoreVT);\n  StoredVal = DAG.getNode(ISD::CONCAT_VECTORS, dl, WideVT, StoredVal,\n                          DAG.getUNDEF(StoreVT));\n\n  if (Subtarget.hasSSE2()) {\n    // Widen the vector, cast to a v2x64 type, extract the single 64-bit element\n    // and store it.\n    MVT StVT = Subtarget.is64Bit() && StoreVT.isInteger() ? MVT::i64 : MVT::f64;\n    MVT CastVT = MVT::getVectorVT(StVT, 2);\n    StoredVal = DAG.getBitcast(CastVT, StoredVal);\n    StoredVal = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, StVT, StoredVal,\n                            DAG.getIntPtrConstant(0, dl));\n\n    return DAG.getStore(St->getChain(), dl, StoredVal, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n  assert(Subtarget.hasSSE1() && \"Expected SSE\");\n  SDVTList Tys = DAG.getVTList(MVT::Other);\n  SDValue Ops[] = {St->getChain(), StoredVal, St->getBasePtr()};\n  return DAG.getMemIntrinsicNode(X86ISD::VEXTRACT_STORE, dl, Tys, Ops, MVT::i64,\n                                 St->getMemOperand());\n}\n\n// Lower vector extended loads using a shuffle. If SSSE3 is not available we\n// may emit an illegal shuffle but the expansion is still better than scalar\n// code. We generate sext/sext_invec for SEXTLOADs if it's available, otherwise\n// we'll emit a shuffle and a arithmetic shift.\n// FIXME: Is the expansion actually better than scalar code? It doesn't seem so.\n// TODO: It is possible to support ZExt by zeroing the undef values during\n// the shuffle phase or after the shuffle.\nstatic SDValue LowerLoad(SDValue Op, const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  MVT RegVT = Op.getSimpleValueType();\n  assert(RegVT.isVector() && \"We only custom lower vector loads.\");\n  assert(RegVT.isInteger() &&\n         \"We only custom lower integer vector loads.\");\n\n  LoadSDNode *Ld = cast<LoadSDNode>(Op.getNode());\n  SDLoc dl(Ld);\n\n  // Without AVX512DQ, we need to use a scalar type for v2i1/v4i1/v8i1 loads.\n  if (RegVT.getVectorElementType() == MVT::i1) {\n    assert(EVT(RegVT) == Ld->getMemoryVT() && \"Expected non-extending load\");\n    assert(RegVT.getVectorNumElements() <= 8 && \"Unexpected VT\");\n    assert(Subtarget.hasAVX512() && !Subtarget.hasDQI() &&\n           \"Expected AVX512F without AVX512DQI\");\n\n    SDValue NewLd = DAG.getLoad(MVT::i8, dl, Ld->getChain(), Ld->getBasePtr(),\n                                Ld->getPointerInfo(), Ld->getOriginalAlign(),\n                                Ld->getMemOperand()->getFlags());\n\n    // Replace chain users with the new chain.\n    assert(NewLd->getNumValues() == 2 && \"Loads must carry a chain!\");\n\n    SDValue Val = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i16, NewLd);\n    Val = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, RegVT,\n                      DAG.getBitcast(MVT::v16i1, Val),\n                      DAG.getIntPtrConstant(0, dl));\n    return DAG.getMergeValues({Val, NewLd.getValue(1)}, dl);\n  }\n\n  return SDValue();\n}\n\n/// Return true if node is an ISD::AND or ISD::OR of two X86ISD::SETCC nodes\n/// each of which has no other use apart from the AND / OR.\nstatic bool isAndOrOfSetCCs(SDValue Op, unsigned &Opc) {\n  Opc = Op.getOpcode();\n  if (Opc != ISD::OR && Opc != ISD::AND)\n    return false;\n  return (Op.getOperand(0).getOpcode() == X86ISD::SETCC &&\n          Op.getOperand(0).hasOneUse() &&\n          Op.getOperand(1).getOpcode() == X86ISD::SETCC &&\n          Op.getOperand(1).hasOneUse());\n}\n\nSDValue X86TargetLowering::LowerBRCOND(SDValue Op, SelectionDAG &DAG) const {\n  SDValue Chain = Op.getOperand(0);\n  SDValue Cond  = Op.getOperand(1);\n  SDValue Dest  = Op.getOperand(2);\n  SDLoc dl(Op);\n\n  if (Cond.getOpcode() == ISD::SETCC &&\n      Cond.getOperand(0).getValueType() != MVT::f128) {\n    SDValue LHS = Cond.getOperand(0);\n    SDValue RHS = Cond.getOperand(1);\n    ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n\n    // Special case for\n    // setcc([su]{add,sub,mul}o == 0)\n    // setcc([su]{add,sub,mul}o != 1)\n    if (ISD::isOverflowIntrOpRes(LHS) &&\n        (CC == ISD::SETEQ || CC == ISD::SETNE) &&\n        (isNullConstant(RHS) || isOneConstant(RHS))) {\n      SDValue Value, Overflow;\n      X86::CondCode X86Cond;\n      std::tie(Value, Overflow) = getX86XALUOOp(X86Cond, LHS.getValue(0), DAG);\n\n      if ((CC == ISD::SETEQ) == isNullConstant(RHS))\n        X86Cond = X86::GetOppositeBranchCondition(X86Cond);\n\n      SDValue CCVal = DAG.getTargetConstant(X86Cond, dl, MVT::i8);\n      return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                         Overflow);\n    }\n\n    if (LHS.getSimpleValueType().isInteger()) {\n      SDValue CCVal;\n      SDValue EFLAGS = emitFlagsForSetcc(LHS, RHS, CC, SDLoc(Cond), DAG, CCVal);\n      return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                         EFLAGS);\n    }\n\n    if (CC == ISD::SETOEQ) {\n      // For FCMP_OEQ, we can emit\n      // two branches instead of an explicit AND instruction with a\n      // separate test. However, we only do this if this block doesn't\n      // have a fall-through edge, because this requires an explicit\n      // jmp when the condition is false.\n      if (Op.getNode()->hasOneUse()) {\n        SDNode *User = *Op.getNode()->use_begin();\n        // Look for an unconditional branch following this conditional branch.\n        // We need this because we need to reverse the successors in order\n        // to implement FCMP_OEQ.\n        if (User->getOpcode() == ISD::BR) {\n          SDValue FalseBB = User->getOperand(1);\n          SDNode *NewBR =\n            DAG.UpdateNodeOperands(User, User->getOperand(0), Dest);\n          assert(NewBR == User);\n          (void)NewBR;\n          Dest = FalseBB;\n\n          SDValue Cmp =\n              DAG.getNode(X86ISD::FCMP, SDLoc(Cond), MVT::i32, LHS, RHS);\n          SDValue CCVal = DAG.getTargetConstant(X86::COND_NE, dl, MVT::i8);\n          Chain = DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest,\n                              CCVal, Cmp);\n          CCVal = DAG.getTargetConstant(X86::COND_P, dl, MVT::i8);\n          return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                             Cmp);\n        }\n      }\n    } else if (CC == ISD::SETUNE) {\n      // For FCMP_UNE, we can emit\n      // two branches instead of an explicit OR instruction with a\n      // separate test.\n      SDValue Cmp = DAG.getNode(X86ISD::FCMP, SDLoc(Cond), MVT::i32, LHS, RHS);\n      SDValue CCVal = DAG.getTargetConstant(X86::COND_NE, dl, MVT::i8);\n      Chain =\n          DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal, Cmp);\n      CCVal = DAG.getTargetConstant(X86::COND_P, dl, MVT::i8);\n      return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                         Cmp);\n    } else {\n      X86::CondCode X86Cond =\n          TranslateX86CC(CC, dl, /*IsFP*/ true, LHS, RHS, DAG);\n      SDValue Cmp = DAG.getNode(X86ISD::FCMP, SDLoc(Cond), MVT::i32, LHS, RHS);\n      SDValue CCVal = DAG.getTargetConstant(X86Cond, dl, MVT::i8);\n      return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                         Cmp);\n    }\n  }\n\n  if (ISD::isOverflowIntrOpRes(Cond)) {\n    SDValue Value, Overflow;\n    X86::CondCode X86Cond;\n    std::tie(Value, Overflow) = getX86XALUOOp(X86Cond, Cond.getValue(0), DAG);\n\n    SDValue CCVal = DAG.getTargetConstant(X86Cond, dl, MVT::i8);\n    return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                       Overflow);\n  }\n\n  // Look past the truncate if the high bits are known zero.\n  if (isTruncWithZeroHighBitsInput(Cond, DAG))\n    Cond = Cond.getOperand(0);\n\n  EVT CondVT = Cond.getValueType();\n\n  // Add an AND with 1 if we don't already have one.\n  if (!(Cond.getOpcode() == ISD::AND && isOneConstant(Cond.getOperand(1))))\n    Cond =\n        DAG.getNode(ISD::AND, dl, CondVT, Cond, DAG.getConstant(1, dl, CondVT));\n\n  SDValue LHS = Cond;\n  SDValue RHS = DAG.getConstant(0, dl, CondVT);\n\n  SDValue CCVal;\n  SDValue EFLAGS = emitFlagsForSetcc(LHS, RHS, ISD::SETNE, dl, DAG, CCVal);\n  return DAG.getNode(X86ISD::BRCOND, dl, MVT::Other, Chain, Dest, CCVal,\n                     EFLAGS);\n}\n\n// Lower dynamic stack allocation to _alloca call for Cygwin/Mingw targets.\n// Calls to _alloca are needed to probe the stack when allocating more than 4k\n// bytes in one go. Touching the stack at 4K increments is necessary to ensure\n// that the guard pages used by the OS virtual memory manager are allocated in\n// correct sequence.\nSDValue\nX86TargetLowering::LowerDYNAMIC_STACKALLOC(SDValue Op,\n                                           SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  bool SplitStack = MF.shouldSplitStack();\n  bool EmitStackProbeCall = hasStackProbeSymbol(MF);\n  bool Lower = (Subtarget.isOSWindows() && !Subtarget.isTargetMachO()) ||\n               SplitStack || EmitStackProbeCall;\n  SDLoc dl(Op);\n\n  // Get the inputs.\n  SDNode *Node = Op.getNode();\n  SDValue Chain = Op.getOperand(0);\n  SDValue Size  = Op.getOperand(1);\n  MaybeAlign Alignment(Op.getConstantOperandVal(2));\n  EVT VT = Node->getValueType(0);\n\n  // Chain the dynamic stack allocation so that it doesn't modify the stack\n  // pointer when other instructions are using the stack.\n  Chain = DAG.getCALLSEQ_START(Chain, 0, 0, dl);\n\n  bool Is64Bit = Subtarget.is64Bit();\n  MVT SPTy = getPointerTy(DAG.getDataLayout());\n\n  SDValue Result;\n  if (!Lower) {\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    Register SPReg = TLI.getStackPointerRegisterToSaveRestore();\n    assert(SPReg && \"Target cannot require DYNAMIC_STACKALLOC expansion and\"\n                    \" not tell us which reg is the stack pointer!\");\n\n    const TargetFrameLowering &TFI = *Subtarget.getFrameLowering();\n    const Align StackAlign = TFI.getStackAlign();\n    if (hasInlineStackProbe(MF)) {\n      MachineRegisterInfo &MRI = MF.getRegInfo();\n\n      const TargetRegisterClass *AddrRegClass = getRegClassFor(SPTy);\n      Register Vreg = MRI.createVirtualRegister(AddrRegClass);\n      Chain = DAG.getCopyToReg(Chain, dl, Vreg, Size);\n      Result = DAG.getNode(X86ISD::PROBED_ALLOCA, dl, SPTy, Chain,\n                           DAG.getRegister(Vreg, SPTy));\n    } else {\n      SDValue SP = DAG.getCopyFromReg(Chain, dl, SPReg, VT);\n      Chain = SP.getValue(1);\n      Result = DAG.getNode(ISD::SUB, dl, VT, SP, Size); // Value\n    }\n    if (Alignment && *Alignment > StackAlign)\n      Result =\n          DAG.getNode(ISD::AND, dl, VT, Result,\n                      DAG.getConstant(~(Alignment->value() - 1ULL), dl, VT));\n    Chain = DAG.getCopyToReg(Chain, dl, SPReg, Result); // Output chain\n  } else if (SplitStack) {\n    MachineRegisterInfo &MRI = MF.getRegInfo();\n\n    if (Is64Bit) {\n      // The 64 bit implementation of segmented stacks needs to clobber both r10\n      // r11. This makes it impossible to use it along with nested parameters.\n      const Function &F = MF.getFunction();\n      for (const auto &A : F.args()) {\n        if (A.hasNestAttr())\n          report_fatal_error(\"Cannot use segmented stacks with functions that \"\n                             \"have nested arguments.\");\n      }\n    }\n\n    const TargetRegisterClass *AddrRegClass = getRegClassFor(SPTy);\n    Register Vreg = MRI.createVirtualRegister(AddrRegClass);\n    Chain = DAG.getCopyToReg(Chain, dl, Vreg, Size);\n    Result = DAG.getNode(X86ISD::SEG_ALLOCA, dl, SPTy, Chain,\n                                DAG.getRegister(Vreg, SPTy));\n  } else {\n    SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);\n    Chain = DAG.getNode(X86ISD::WIN_ALLOCA, dl, NodeTys, Chain, Size);\n    MF.getInfo<X86MachineFunctionInfo>()->setHasWinAlloca(true);\n\n    const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n    Register SPReg = RegInfo->getStackRegister();\n    SDValue SP = DAG.getCopyFromReg(Chain, dl, SPReg, SPTy);\n    Chain = SP.getValue(1);\n\n    if (Alignment) {\n      SP = DAG.getNode(ISD::AND, dl, VT, SP.getValue(0),\n                       DAG.getConstant(~(Alignment->value() - 1ULL), dl, VT));\n      Chain = DAG.getCopyToReg(Chain, dl, SPReg, SP);\n    }\n\n    Result = SP;\n  }\n\n  Chain = DAG.getCALLSEQ_END(Chain, DAG.getIntPtrConstant(0, dl, true),\n                             DAG.getIntPtrConstant(0, dl, true), SDValue(), dl);\n\n  SDValue Ops[2] = {Result, Chain};\n  return DAG.getMergeValues(Ops, dl);\n}\n\nSDValue X86TargetLowering::LowerVASTART(SDValue Op, SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  auto PtrVT = getPointerTy(MF.getDataLayout());\n  X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();\n\n  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();\n  SDLoc DL(Op);\n\n  if (!Subtarget.is64Bit() ||\n      Subtarget.isCallingConvWin64(MF.getFunction().getCallingConv())) {\n    // vastart just stores the address of the VarArgsFrameIndex slot into the\n    // memory location argument.\n    SDValue FR = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(), PtrVT);\n    return DAG.getStore(Op.getOperand(0), DL, FR, Op.getOperand(1),\n                        MachinePointerInfo(SV));\n  }\n\n  // __va_list_tag:\n  //   gp_offset         (0 - 6 * 8)\n  //   fp_offset         (48 - 48 + 8 * 16)\n  //   overflow_arg_area (point to parameters coming in memory).\n  //   reg_save_area\n  SmallVector<SDValue, 8> MemOps;\n  SDValue FIN = Op.getOperand(1);\n  // Store gp_offset\n  SDValue Store = DAG.getStore(\n      Op.getOperand(0), DL,\n      DAG.getConstant(FuncInfo->getVarArgsGPOffset(), DL, MVT::i32), FIN,\n      MachinePointerInfo(SV));\n  MemOps.push_back(Store);\n\n  // Store fp_offset\n  FIN = DAG.getMemBasePlusOffset(FIN, TypeSize::Fixed(4), DL);\n  Store = DAG.getStore(\n      Op.getOperand(0), DL,\n      DAG.getConstant(FuncInfo->getVarArgsFPOffset(), DL, MVT::i32), FIN,\n      MachinePointerInfo(SV, 4));\n  MemOps.push_back(Store);\n\n  // Store ptr to overflow_arg_area\n  FIN = DAG.getNode(ISD::ADD, DL, PtrVT, FIN, DAG.getIntPtrConstant(4, DL));\n  SDValue OVFIN = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(), PtrVT);\n  Store =\n      DAG.getStore(Op.getOperand(0), DL, OVFIN, FIN, MachinePointerInfo(SV, 8));\n  MemOps.push_back(Store);\n\n  // Store ptr to reg_save_area.\n  FIN = DAG.getNode(ISD::ADD, DL, PtrVT, FIN, DAG.getIntPtrConstant(\n      Subtarget.isTarget64BitLP64() ? 8 : 4, DL));\n  SDValue RSFIN = DAG.getFrameIndex(FuncInfo->getRegSaveFrameIndex(), PtrVT);\n  Store = DAG.getStore(\n      Op.getOperand(0), DL, RSFIN, FIN,\n      MachinePointerInfo(SV, Subtarget.isTarget64BitLP64() ? 16 : 12));\n  MemOps.push_back(Store);\n  return DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOps);\n}\n\nSDValue X86TargetLowering::LowerVAARG(SDValue Op, SelectionDAG &DAG) const {\n  assert(Subtarget.is64Bit() &&\n         \"LowerVAARG only handles 64-bit va_arg!\");\n  assert(Op.getNumOperands() == 4);\n\n  MachineFunction &MF = DAG.getMachineFunction();\n  if (Subtarget.isCallingConvWin64(MF.getFunction().getCallingConv()))\n    // The Win64 ABI uses char* instead of a structure.\n    return DAG.expandVAArg(Op.getNode());\n\n  SDValue Chain = Op.getOperand(0);\n  SDValue SrcPtr = Op.getOperand(1);\n  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();\n  unsigned Align = Op.getConstantOperandVal(3);\n  SDLoc dl(Op);\n\n  EVT ArgVT = Op.getNode()->getValueType(0);\n  Type *ArgTy = ArgVT.getTypeForEVT(*DAG.getContext());\n  uint32_t ArgSize = DAG.getDataLayout().getTypeAllocSize(ArgTy);\n  uint8_t ArgMode;\n\n  // Decide which area this value should be read from.\n  // TODO: Implement the AMD64 ABI in its entirety. This simple\n  // selection mechanism works only for the basic types.\n  assert(ArgVT != MVT::f80 && \"va_arg for f80 not yet implemented\");\n  if (ArgVT.isFloatingPoint() && ArgSize <= 16 /*bytes*/) {\n    ArgMode = 2;  // Argument passed in XMM register. Use fp_offset.\n  } else {\n    assert(ArgVT.isInteger() && ArgSize <= 32 /*bytes*/ &&\n           \"Unhandled argument type in LowerVAARG\");\n    ArgMode = 1;  // Argument passed in GPR64 register(s). Use gp_offset.\n  }\n\n  if (ArgMode == 2) {\n    // Sanity Check: Make sure using fp_offset makes sense.\n    assert(!Subtarget.useSoftFloat() &&\n           !(MF.getFunction().hasFnAttribute(Attribute::NoImplicitFloat)) &&\n           Subtarget.hasSSE1());\n  }\n\n  // Insert VAARG node into the DAG\n  // VAARG returns two values: Variable Argument Address, Chain\n  SDValue InstOps[] = {Chain, SrcPtr,\n                       DAG.getTargetConstant(ArgSize, dl, MVT::i32),\n                       DAG.getTargetConstant(ArgMode, dl, MVT::i8),\n                       DAG.getTargetConstant(Align, dl, MVT::i32)};\n  SDVTList VTs = DAG.getVTList(getPointerTy(DAG.getDataLayout()), MVT::Other);\n  SDValue VAARG = DAG.getMemIntrinsicNode(\n      Subtarget.isTarget64BitLP64() ? X86ISD::VAARG_64 : X86ISD::VAARG_X32, dl,\n      VTs, InstOps, MVT::i64, MachinePointerInfo(SV),\n      /*Alignment=*/None,\n      MachineMemOperand::MOLoad | MachineMemOperand::MOStore);\n  Chain = VAARG.getValue(1);\n\n  // Load the next argument and return it\n  return DAG.getLoad(ArgVT, dl, Chain, VAARG, MachinePointerInfo());\n}\n\nstatic SDValue LowerVACOPY(SDValue Op, const X86Subtarget &Subtarget,\n                           SelectionDAG &DAG) {\n  // X86-64 va_list is a struct { i32, i32, i8*, i8* }, except on Windows,\n  // where a va_list is still an i8*.\n  assert(Subtarget.is64Bit() && \"This code only handles 64-bit va_copy!\");\n  if (Subtarget.isCallingConvWin64(\n        DAG.getMachineFunction().getFunction().getCallingConv()))\n    // Probably a Win64 va_copy.\n    return DAG.expandVACopy(Op.getNode());\n\n  SDValue Chain = Op.getOperand(0);\n  SDValue DstPtr = Op.getOperand(1);\n  SDValue SrcPtr = Op.getOperand(2);\n  const Value *DstSV = cast<SrcValueSDNode>(Op.getOperand(3))->getValue();\n  const Value *SrcSV = cast<SrcValueSDNode>(Op.getOperand(4))->getValue();\n  SDLoc DL(Op);\n\n  return DAG.getMemcpy(\n      Chain, DL, DstPtr, SrcPtr,\n      DAG.getIntPtrConstant(Subtarget.isTarget64BitLP64() ? 24 : 16, DL),\n      Align(Subtarget.isTarget64BitLP64() ? 8 : 4), /*isVolatile*/ false, false,\n      false, MachinePointerInfo(DstSV), MachinePointerInfo(SrcSV));\n}\n\n// Helper to get immediate/variable SSE shift opcode from other shift opcodes.\nstatic unsigned getTargetVShiftUniformOpcode(unsigned Opc, bool IsVariable) {\n  switch (Opc) {\n  case ISD::SHL:\n  case X86ISD::VSHL:\n  case X86ISD::VSHLI:\n    return IsVariable ? X86ISD::VSHL : X86ISD::VSHLI;\n  case ISD::SRL:\n  case X86ISD::VSRL:\n  case X86ISD::VSRLI:\n    return IsVariable ? X86ISD::VSRL : X86ISD::VSRLI;\n  case ISD::SRA:\n  case X86ISD::VSRA:\n  case X86ISD::VSRAI:\n    return IsVariable ? X86ISD::VSRA : X86ISD::VSRAI;\n  }\n  llvm_unreachable(\"Unknown target vector shift node\");\n}\n\n/// Handle vector element shifts where the shift amount is a constant.\n/// Takes immediate version of shift as input.\nstatic SDValue getTargetVShiftByConstNode(unsigned Opc, const SDLoc &dl, MVT VT,\n                                          SDValue SrcOp, uint64_t ShiftAmt,\n                                          SelectionDAG &DAG) {\n  MVT ElementType = VT.getVectorElementType();\n\n  // Bitcast the source vector to the output type, this is mainly necessary for\n  // vXi8/vXi64 shifts.\n  if (VT != SrcOp.getSimpleValueType())\n    SrcOp = DAG.getBitcast(VT, SrcOp);\n\n  // Fold this packed shift into its first operand if ShiftAmt is 0.\n  if (ShiftAmt == 0)\n    return SrcOp;\n\n  // Check for ShiftAmt >= element width\n  if (ShiftAmt >= ElementType.getSizeInBits()) {\n    if (Opc == X86ISD::VSRAI)\n      ShiftAmt = ElementType.getSizeInBits() - 1;\n    else\n      return DAG.getConstant(0, dl, VT);\n  }\n\n  assert((Opc == X86ISD::VSHLI || Opc == X86ISD::VSRLI || Opc == X86ISD::VSRAI)\n         && \"Unknown target vector shift-by-constant node\");\n\n  // Fold this packed vector shift into a build vector if SrcOp is a\n  // vector of Constants or UNDEFs.\n  if (ISD::isBuildVectorOfConstantSDNodes(SrcOp.getNode())) {\n    SmallVector<SDValue, 8> Elts;\n    unsigned NumElts = SrcOp->getNumOperands();\n\n    switch (Opc) {\n    default: llvm_unreachable(\"Unknown opcode!\");\n    case X86ISD::VSHLI:\n      for (unsigned i = 0; i != NumElts; ++i) {\n        SDValue CurrentOp = SrcOp->getOperand(i);\n        if (CurrentOp->isUndef()) {\n          // Must produce 0s in the correct bits.\n          Elts.push_back(DAG.getConstant(0, dl, ElementType));\n          continue;\n        }\n        auto *ND = cast<ConstantSDNode>(CurrentOp);\n        const APInt &C = ND->getAPIntValue();\n        Elts.push_back(DAG.getConstant(C.shl(ShiftAmt), dl, ElementType));\n      }\n      break;\n    case X86ISD::VSRLI:\n      for (unsigned i = 0; i != NumElts; ++i) {\n        SDValue CurrentOp = SrcOp->getOperand(i);\n        if (CurrentOp->isUndef()) {\n          // Must produce 0s in the correct bits.\n          Elts.push_back(DAG.getConstant(0, dl, ElementType));\n          continue;\n        }\n        auto *ND = cast<ConstantSDNode>(CurrentOp);\n        const APInt &C = ND->getAPIntValue();\n        Elts.push_back(DAG.getConstant(C.lshr(ShiftAmt), dl, ElementType));\n      }\n      break;\n    case X86ISD::VSRAI:\n      for (unsigned i = 0; i != NumElts; ++i) {\n        SDValue CurrentOp = SrcOp->getOperand(i);\n        if (CurrentOp->isUndef()) {\n          // All shifted in bits must be the same so use 0.\n          Elts.push_back(DAG.getConstant(0, dl, ElementType));\n          continue;\n        }\n        auto *ND = cast<ConstantSDNode>(CurrentOp);\n        const APInt &C = ND->getAPIntValue();\n        Elts.push_back(DAG.getConstant(C.ashr(ShiftAmt), dl, ElementType));\n      }\n      break;\n    }\n\n    return DAG.getBuildVector(VT, dl, Elts);\n  }\n\n  return DAG.getNode(Opc, dl, VT, SrcOp,\n                     DAG.getTargetConstant(ShiftAmt, dl, MVT::i8));\n}\n\n/// Handle vector element shifts where the shift amount may or may not be a\n/// constant. Takes immediate version of shift as input.\nstatic SDValue getTargetVShiftNode(unsigned Opc, const SDLoc &dl, MVT VT,\n                                   SDValue SrcOp, SDValue ShAmt,\n                                   const X86Subtarget &Subtarget,\n                                   SelectionDAG &DAG) {\n  MVT SVT = ShAmt.getSimpleValueType();\n  assert((SVT == MVT::i32 || SVT == MVT::i64) && \"Unexpected value type!\");\n\n  // Catch shift-by-constant.\n  if (ConstantSDNode *CShAmt = dyn_cast<ConstantSDNode>(ShAmt))\n    return getTargetVShiftByConstNode(Opc, dl, VT, SrcOp,\n                                      CShAmt->getZExtValue(), DAG);\n\n  // Change opcode to non-immediate version.\n  Opc = getTargetVShiftUniformOpcode(Opc, true);\n\n  // Need to build a vector containing shift amount.\n  // SSE/AVX packed shifts only use the lower 64-bit of the shift count.\n  // +====================+============+=======================================+\n  // | ShAmt is           | HasSSE4.1? | Construct ShAmt vector as             |\n  // +====================+============+=======================================+\n  // | i64                | Yes, No    | Use ShAmt as lowest elt               |\n  // | i32                | Yes        | zero-extend in-reg                    |\n  // | (i32 zext(i16/i8)) | Yes        | zero-extend in-reg                    |\n  // | (i32 zext(i16/i8)) | No         | byte-shift-in-reg                     |\n  // | i16/i32            | No         | v4i32 build_vector(ShAmt, 0, ud, ud)) |\n  // +====================+============+=======================================+\n\n  if (SVT == MVT::i64)\n    ShAmt = DAG.getNode(ISD::SCALAR_TO_VECTOR, SDLoc(ShAmt), MVT::v2i64, ShAmt);\n  else if (ShAmt.getOpcode() == ISD::ZERO_EXTEND &&\n           ShAmt.getOperand(0).getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n           (ShAmt.getOperand(0).getSimpleValueType() == MVT::i16 ||\n            ShAmt.getOperand(0).getSimpleValueType() == MVT::i8)) {\n    ShAmt = ShAmt.getOperand(0);\n    MVT AmtTy = ShAmt.getSimpleValueType() == MVT::i8 ? MVT::v16i8 : MVT::v8i16;\n    ShAmt = DAG.getNode(ISD::SCALAR_TO_VECTOR, SDLoc(ShAmt), AmtTy, ShAmt);\n    if (Subtarget.hasSSE41())\n      ShAmt = DAG.getNode(ISD::ZERO_EXTEND_VECTOR_INREG, SDLoc(ShAmt),\n                          MVT::v2i64, ShAmt);\n    else {\n      SDValue ByteShift = DAG.getTargetConstant(\n          (128 - AmtTy.getScalarSizeInBits()) / 8, SDLoc(ShAmt), MVT::i8);\n      ShAmt = DAG.getBitcast(MVT::v16i8, ShAmt);\n      ShAmt = DAG.getNode(X86ISD::VSHLDQ, SDLoc(ShAmt), MVT::v16i8, ShAmt,\n                          ByteShift);\n      ShAmt = DAG.getNode(X86ISD::VSRLDQ, SDLoc(ShAmt), MVT::v16i8, ShAmt,\n                          ByteShift);\n    }\n  } else if (Subtarget.hasSSE41() &&\n             ShAmt.getOpcode() == ISD::EXTRACT_VECTOR_ELT) {\n    ShAmt = DAG.getNode(ISD::SCALAR_TO_VECTOR, SDLoc(ShAmt), MVT::v4i32, ShAmt);\n    ShAmt = DAG.getNode(ISD::ZERO_EXTEND_VECTOR_INREG, SDLoc(ShAmt),\n                        MVT::v2i64, ShAmt);\n  } else {\n    SDValue ShOps[4] = {ShAmt, DAG.getConstant(0, dl, SVT), DAG.getUNDEF(SVT),\n                        DAG.getUNDEF(SVT)};\n    ShAmt = DAG.getBuildVector(MVT::v4i32, dl, ShOps);\n  }\n\n  // The return type has to be a 128-bit type with the same element\n  // type as the input type.\n  MVT EltVT = VT.getVectorElementType();\n  MVT ShVT = MVT::getVectorVT(EltVT, 128 / EltVT.getSizeInBits());\n\n  ShAmt = DAG.getBitcast(ShVT, ShAmt);\n  return DAG.getNode(Opc, dl, VT, SrcOp, ShAmt);\n}\n\n/// Return Mask with the necessary casting or extending\n/// for \\p Mask according to \\p MaskVT when lowering masking intrinsics\nstatic SDValue getMaskNode(SDValue Mask, MVT MaskVT,\n                           const X86Subtarget &Subtarget, SelectionDAG &DAG,\n                           const SDLoc &dl) {\n\n  if (isAllOnesConstant(Mask))\n    return DAG.getConstant(1, dl, MaskVT);\n  if (X86::isZeroNode(Mask))\n    return DAG.getConstant(0, dl, MaskVT);\n\n  assert(MaskVT.bitsLE(Mask.getSimpleValueType()) && \"Unexpected mask size!\");\n\n  if (Mask.getSimpleValueType() == MVT::i64 && Subtarget.is32Bit()) {\n    assert(MaskVT == MVT::v64i1 && \"Expected v64i1 mask!\");\n    assert(Subtarget.hasBWI() && \"Expected AVX512BW target!\");\n    // In case 32bit mode, bitcast i64 is illegal, extend/split it.\n    SDValue Lo, Hi;\n    Lo = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, MVT::i32, Mask,\n                        DAG.getConstant(0, dl, MVT::i32));\n    Hi = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, MVT::i32, Mask,\n                        DAG.getConstant(1, dl, MVT::i32));\n\n    Lo = DAG.getBitcast(MVT::v32i1, Lo);\n    Hi = DAG.getBitcast(MVT::v32i1, Hi);\n\n    return DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v64i1, Lo, Hi);\n  } else {\n    MVT BitcastVT = MVT::getVectorVT(MVT::i1,\n                                     Mask.getSimpleValueType().getSizeInBits());\n    // In case when MaskVT equals v2i1 or v4i1, low 2 or 4 elements\n    // are extracted by EXTRACT_SUBVECTOR.\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MaskVT,\n                       DAG.getBitcast(BitcastVT, Mask),\n                       DAG.getIntPtrConstant(0, dl));\n  }\n}\n\n/// Return (and \\p Op, \\p Mask) for compare instructions or\n/// (vselect \\p Mask, \\p Op, \\p PreservedSrc) for others along with the\n/// necessary casting or extending for \\p Mask when lowering masking intrinsics\nstatic SDValue getVectorMaskingNode(SDValue Op, SDValue Mask,\n                  SDValue PreservedSrc,\n                  const X86Subtarget &Subtarget,\n                  SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  MVT MaskVT = MVT::getVectorVT(MVT::i1, VT.getVectorNumElements());\n  unsigned OpcodeSelect = ISD::VSELECT;\n  SDLoc dl(Op);\n\n  if (isAllOnesConstant(Mask))\n    return Op;\n\n  SDValue VMask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n\n  if (PreservedSrc.isUndef())\n    PreservedSrc = getZeroVector(VT, Subtarget, DAG, dl);\n  return DAG.getNode(OpcodeSelect, dl, VT, VMask, Op, PreservedSrc);\n}\n\n/// Creates an SDNode for a predicated scalar operation.\n/// \\returns (X86vselect \\p Mask, \\p Op, \\p PreservedSrc).\n/// The mask is coming as MVT::i8 and it should be transformed\n/// to MVT::v1i1 while lowering masking intrinsics.\n/// The main difference between ScalarMaskingNode and VectorMaskingNode is using\n/// \"X86select\" instead of \"vselect\". We just can't create the \"vselect\" node\n/// for a scalar instruction.\nstatic SDValue getScalarMaskingNode(SDValue Op, SDValue Mask,\n                                    SDValue PreservedSrc,\n                                    const X86Subtarget &Subtarget,\n                                    SelectionDAG &DAG) {\n\n  if (auto *MaskConst = dyn_cast<ConstantSDNode>(Mask))\n    if (MaskConst->getZExtValue() & 0x1)\n      return Op;\n\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n\n  assert(Mask.getValueType() == MVT::i8 && \"Unexpect type\");\n  SDValue IMask = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v1i1,\n                              DAG.getBitcast(MVT::v8i1, Mask),\n                              DAG.getIntPtrConstant(0, dl));\n  if (Op.getOpcode() == X86ISD::FSETCCM ||\n      Op.getOpcode() == X86ISD::FSETCCM_SAE ||\n      Op.getOpcode() == X86ISD::VFPCLASSS)\n    return DAG.getNode(ISD::AND, dl, VT, Op, IMask);\n\n  if (PreservedSrc.isUndef())\n    PreservedSrc = getZeroVector(VT, Subtarget, DAG, dl);\n  return DAG.getNode(X86ISD::SELECTS, dl, VT, IMask, Op, PreservedSrc);\n}\n\nstatic int getSEHRegistrationNodeSize(const Function *Fn) {\n  if (!Fn->hasPersonalityFn())\n    report_fatal_error(\n        \"querying registration node size for function without personality\");\n  // The RegNodeSize is 6 32-bit words for SEH and 4 for C++ EH. See\n  // WinEHStatePass for the full struct definition.\n  switch (classifyEHPersonality(Fn->getPersonalityFn())) {\n  case EHPersonality::MSVC_X86SEH: return 24;\n  case EHPersonality::MSVC_CXX: return 16;\n  default: break;\n  }\n  report_fatal_error(\n      \"can only recover FP for 32-bit MSVC EH personality functions\");\n}\n\n/// When the MSVC runtime transfers control to us, either to an outlined\n/// function or when returning to a parent frame after catching an exception, we\n/// recover the parent frame pointer by doing arithmetic on the incoming EBP.\n/// Here's the math:\n///   RegNodeBase = EntryEBP - RegNodeSize\n///   ParentFP = RegNodeBase - ParentFrameOffset\n/// Subtracting RegNodeSize takes us to the offset of the registration node, and\n/// subtracting the offset (negative on x86) takes us back to the parent FP.\nstatic SDValue recoverFramePointer(SelectionDAG &DAG, const Function *Fn,\n                                   SDValue EntryEBP) {\n  MachineFunction &MF = DAG.getMachineFunction();\n  SDLoc dl;\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  MVT PtrVT = TLI.getPointerTy(DAG.getDataLayout());\n\n  // It's possible that the parent function no longer has a personality function\n  // if the exceptional code was optimized away, in which case we just return\n  // the incoming EBP.\n  if (!Fn->hasPersonalityFn())\n    return EntryEBP;\n\n  // Get an MCSymbol that will ultimately resolve to the frame offset of the EH\n  // registration, or the .set_setframe offset.\n  MCSymbol *OffsetSym =\n      MF.getMMI().getContext().getOrCreateParentFrameOffsetSymbol(\n          GlobalValue::dropLLVMManglingEscape(Fn->getName()));\n  SDValue OffsetSymVal = DAG.getMCSymbol(OffsetSym, PtrVT);\n  SDValue ParentFrameOffset =\n      DAG.getNode(ISD::LOCAL_RECOVER, dl, PtrVT, OffsetSymVal);\n\n  // Return EntryEBP + ParentFrameOffset for x64. This adjusts from RSP after\n  // prologue to RBP in the parent function.\n  const X86Subtarget &Subtarget =\n      static_cast<const X86Subtarget &>(DAG.getSubtarget());\n  if (Subtarget.is64Bit())\n    return DAG.getNode(ISD::ADD, dl, PtrVT, EntryEBP, ParentFrameOffset);\n\n  int RegNodeSize = getSEHRegistrationNodeSize(Fn);\n  // RegNodeBase = EntryEBP - RegNodeSize\n  // ParentFP = RegNodeBase - ParentFrameOffset\n  SDValue RegNodeBase = DAG.getNode(ISD::SUB, dl, PtrVT, EntryEBP,\n                                    DAG.getConstant(RegNodeSize, dl, PtrVT));\n  return DAG.getNode(ISD::SUB, dl, PtrVT, RegNodeBase, ParentFrameOffset);\n}\n\nSDValue X86TargetLowering::LowerINTRINSIC_WO_CHAIN(SDValue Op,\n                                                   SelectionDAG &DAG) const {\n  // Helper to detect if the operand is CUR_DIRECTION rounding mode.\n  auto isRoundModeCurDirection = [](SDValue Rnd) {\n    if (auto *C = dyn_cast<ConstantSDNode>(Rnd))\n      return C->getAPIntValue() == X86::STATIC_ROUNDING::CUR_DIRECTION;\n\n    return false;\n  };\n  auto isRoundModeSAE = [](SDValue Rnd) {\n    if (auto *C = dyn_cast<ConstantSDNode>(Rnd)) {\n      unsigned RC = C->getZExtValue();\n      if (RC & X86::STATIC_ROUNDING::NO_EXC) {\n        // Clear the NO_EXC bit and check remaining bits.\n        RC ^= X86::STATIC_ROUNDING::NO_EXC;\n        // As a convenience we allow no other bits or explicitly\n        // current direction.\n        return RC == 0 || RC == X86::STATIC_ROUNDING::CUR_DIRECTION;\n      }\n    }\n\n    return false;\n  };\n  auto isRoundModeSAEToX = [](SDValue Rnd, unsigned &RC) {\n    if (auto *C = dyn_cast<ConstantSDNode>(Rnd)) {\n      RC = C->getZExtValue();\n      if (RC & X86::STATIC_ROUNDING::NO_EXC) {\n        // Clear the NO_EXC bit and check remaining bits.\n        RC ^= X86::STATIC_ROUNDING::NO_EXC;\n        return RC == X86::STATIC_ROUNDING::TO_NEAREST_INT ||\n               RC == X86::STATIC_ROUNDING::TO_NEG_INF ||\n               RC == X86::STATIC_ROUNDING::TO_POS_INF ||\n               RC == X86::STATIC_ROUNDING::TO_ZERO;\n      }\n    }\n\n    return false;\n  };\n\n  SDLoc dl(Op);\n  unsigned IntNo = Op.getConstantOperandVal(0);\n  MVT VT = Op.getSimpleValueType();\n  const IntrinsicData* IntrData = getIntrinsicWithoutChain(IntNo);\n\n  if (IntrData) {\n    switch(IntrData->Type) {\n    case INTR_TYPE_1OP: {\n      // We specify 2 possible opcodes for intrinsics with rounding modes.\n      // First, we check if the intrinsic may have non-default rounding mode,\n      // (IntrData->Opc1 != 0), then we check the rounding mode operand.\n      unsigned IntrWithRoundingModeOpcode = IntrData->Opc1;\n      if (IntrWithRoundingModeOpcode != 0) {\n        SDValue Rnd = Op.getOperand(2);\n        unsigned RC = 0;\n        if (isRoundModeSAEToX(Rnd, RC))\n          return DAG.getNode(IntrWithRoundingModeOpcode, dl, Op.getValueType(),\n                             Op.getOperand(1),\n                             DAG.getTargetConstant(RC, dl, MVT::i32));\n        if (!isRoundModeCurDirection(Rnd))\n          return SDValue();\n      }\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1));\n    }\n    case INTR_TYPE_1OP_SAE: {\n      SDValue Sae = Op.getOperand(2);\n\n      unsigned Opc;\n      if (isRoundModeCurDirection(Sae))\n        Opc = IntrData->Opc0;\n      else if (isRoundModeSAE(Sae))\n        Opc = IntrData->Opc1;\n      else\n        return SDValue();\n\n      return DAG.getNode(Opc, dl, Op.getValueType(), Op.getOperand(1));\n    }\n    case INTR_TYPE_2OP: {\n      SDValue Src2 = Op.getOperand(2);\n\n      // We specify 2 possible opcodes for intrinsics with rounding modes.\n      // First, we check if the intrinsic may have non-default rounding mode,\n      // (IntrData->Opc1 != 0), then we check the rounding mode operand.\n      unsigned IntrWithRoundingModeOpcode = IntrData->Opc1;\n      if (IntrWithRoundingModeOpcode != 0) {\n        SDValue Rnd = Op.getOperand(3);\n        unsigned RC = 0;\n        if (isRoundModeSAEToX(Rnd, RC))\n          return DAG.getNode(IntrWithRoundingModeOpcode, dl, Op.getValueType(),\n                             Op.getOperand(1), Src2,\n                             DAG.getTargetConstant(RC, dl, MVT::i32));\n        if (!isRoundModeCurDirection(Rnd))\n          return SDValue();\n      }\n\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1), Src2);\n    }\n    case INTR_TYPE_2OP_SAE: {\n      SDValue Sae = Op.getOperand(3);\n\n      unsigned Opc;\n      if (isRoundModeCurDirection(Sae))\n        Opc = IntrData->Opc0;\n      else if (isRoundModeSAE(Sae))\n        Opc = IntrData->Opc1;\n      else\n        return SDValue();\n\n      return DAG.getNode(Opc, dl, Op.getValueType(), Op.getOperand(1),\n                         Op.getOperand(2));\n    }\n    case INTR_TYPE_3OP:\n    case INTR_TYPE_3OP_IMM8: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue Src3 = Op.getOperand(3);\n\n      if (IntrData->Type == INTR_TYPE_3OP_IMM8 &&\n          Src3.getValueType() != MVT::i8) {\n        Src3 = DAG.getTargetConstant(\n            cast<ConstantSDNode>(Src3)->getZExtValue() & 0xff, dl, MVT::i8);\n      }\n\n      // We specify 2 possible opcodes for intrinsics with rounding modes.\n      // First, we check if the intrinsic may have non-default rounding mode,\n      // (IntrData->Opc1 != 0), then we check the rounding mode operand.\n      unsigned IntrWithRoundingModeOpcode = IntrData->Opc1;\n      if (IntrWithRoundingModeOpcode != 0) {\n        SDValue Rnd = Op.getOperand(4);\n        unsigned RC = 0;\n        if (isRoundModeSAEToX(Rnd, RC))\n          return DAG.getNode(IntrWithRoundingModeOpcode, dl, Op.getValueType(),\n                             Src1, Src2, Src3,\n                             DAG.getTargetConstant(RC, dl, MVT::i32));\n        if (!isRoundModeCurDirection(Rnd))\n          return SDValue();\n      }\n\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         {Src1, Src2, Src3});\n    }\n    case INTR_TYPE_4OP_IMM8: {\n      assert(Op.getOperand(4)->getOpcode() == ISD::TargetConstant);\n      SDValue Src4 = Op.getOperand(4);\n      if (Src4.getValueType() != MVT::i8) {\n        Src4 = DAG.getTargetConstant(\n            cast<ConstantSDNode>(Src4)->getZExtValue() & 0xff, dl, MVT::i8);\n      }\n\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1), Op.getOperand(2), Op.getOperand(3),\n                         Src4);\n    }\n    case INTR_TYPE_1OP_MASK: {\n      SDValue Src = Op.getOperand(1);\n      SDValue PassThru = Op.getOperand(2);\n      SDValue Mask = Op.getOperand(3);\n      // We add rounding mode to the Node when\n      //   - RC Opcode is specified and\n      //   - RC is not \"current direction\".\n      unsigned IntrWithRoundingModeOpcode = IntrData->Opc1;\n      if (IntrWithRoundingModeOpcode != 0) {\n        SDValue Rnd = Op.getOperand(4);\n        unsigned RC = 0;\n        if (isRoundModeSAEToX(Rnd, RC))\n          return getVectorMaskingNode(\n              DAG.getNode(IntrWithRoundingModeOpcode, dl, Op.getValueType(),\n                          Src, DAG.getTargetConstant(RC, dl, MVT::i32)),\n              Mask, PassThru, Subtarget, DAG);\n        if (!isRoundModeCurDirection(Rnd))\n          return SDValue();\n      }\n      return getVectorMaskingNode(\n          DAG.getNode(IntrData->Opc0, dl, VT, Src), Mask, PassThru,\n          Subtarget, DAG);\n    }\n    case INTR_TYPE_1OP_MASK_SAE: {\n      SDValue Src = Op.getOperand(1);\n      SDValue PassThru = Op.getOperand(2);\n      SDValue Mask = Op.getOperand(3);\n      SDValue Rnd = Op.getOperand(4);\n\n      unsigned Opc;\n      if (isRoundModeCurDirection(Rnd))\n        Opc = IntrData->Opc0;\n      else if (isRoundModeSAE(Rnd))\n        Opc = IntrData->Opc1;\n      else\n        return SDValue();\n\n      return getVectorMaskingNode(DAG.getNode(Opc, dl, VT, Src), Mask, PassThru,\n                                  Subtarget, DAG);\n    }\n    case INTR_TYPE_SCALAR_MASK: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue passThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n      unsigned IntrWithRoundingModeOpcode = IntrData->Opc1;\n      // There are 2 kinds of intrinsics in this group:\n      // (1) With suppress-all-exceptions (sae) or rounding mode- 6 operands\n      // (2) With rounding mode and sae - 7 operands.\n      bool HasRounding = IntrWithRoundingModeOpcode != 0;\n      if (Op.getNumOperands() == (5U + HasRounding)) {\n        if (HasRounding) {\n          SDValue Rnd = Op.getOperand(5);\n          unsigned RC = 0;\n          if (isRoundModeSAEToX(Rnd, RC))\n            return getScalarMaskingNode(\n                DAG.getNode(IntrWithRoundingModeOpcode, dl, VT, Src1, Src2,\n                            DAG.getTargetConstant(RC, dl, MVT::i32)),\n                Mask, passThru, Subtarget, DAG);\n          if (!isRoundModeCurDirection(Rnd))\n            return SDValue();\n        }\n        return getScalarMaskingNode(DAG.getNode(IntrData->Opc0, dl, VT, Src1,\n                                                Src2),\n                                    Mask, passThru, Subtarget, DAG);\n      }\n\n      assert(Op.getNumOperands() == (6U + HasRounding) &&\n             \"Unexpected intrinsic form\");\n      SDValue RoundingMode = Op.getOperand(5);\n      unsigned Opc = IntrData->Opc0;\n      if (HasRounding) {\n        SDValue Sae = Op.getOperand(6);\n        if (isRoundModeSAE(Sae))\n          Opc = IntrWithRoundingModeOpcode;\n        else if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n      return getScalarMaskingNode(DAG.getNode(Opc, dl, VT, Src1,\n                                              Src2, RoundingMode),\n                                  Mask, passThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_SCALAR_MASK_RND: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue passThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n      SDValue Rnd = Op.getOperand(5);\n\n      SDValue NewOp;\n      unsigned RC = 0;\n      if (isRoundModeCurDirection(Rnd))\n        NewOp = DAG.getNode(IntrData->Opc0, dl, VT, Src1, Src2);\n      else if (isRoundModeSAEToX(Rnd, RC))\n        NewOp = DAG.getNode(IntrData->Opc1, dl, VT, Src1, Src2,\n                            DAG.getTargetConstant(RC, dl, MVT::i32));\n      else\n        return SDValue();\n\n      return getScalarMaskingNode(NewOp, Mask, passThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_SCALAR_MASK_SAE: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue passThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n      SDValue Sae = Op.getOperand(5);\n      unsigned Opc;\n      if (isRoundModeCurDirection(Sae))\n        Opc = IntrData->Opc0;\n      else if (isRoundModeSAE(Sae))\n        Opc = IntrData->Opc1;\n      else\n        return SDValue();\n\n      return getScalarMaskingNode(DAG.getNode(Opc, dl, VT, Src1, Src2),\n                                  Mask, passThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_2OP_MASK: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue PassThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n      SDValue NewOp;\n      if (IntrData->Opc1 != 0) {\n        SDValue Rnd = Op.getOperand(5);\n        unsigned RC = 0;\n        if (isRoundModeSAEToX(Rnd, RC))\n          NewOp = DAG.getNode(IntrData->Opc1, dl, VT, Src1, Src2,\n                              DAG.getTargetConstant(RC, dl, MVT::i32));\n        else if (!isRoundModeCurDirection(Rnd))\n          return SDValue();\n      }\n      if (!NewOp)\n        NewOp = DAG.getNode(IntrData->Opc0, dl, VT, Src1, Src2);\n      return getVectorMaskingNode(NewOp, Mask, PassThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_2OP_MASK_SAE: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue PassThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n\n      unsigned Opc = IntrData->Opc0;\n      if (IntrData->Opc1 != 0) {\n        SDValue Sae = Op.getOperand(5);\n        if (isRoundModeSAE(Sae))\n          Opc = IntrData->Opc1;\n        else if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n\n      return getVectorMaskingNode(DAG.getNode(Opc, dl, VT, Src1, Src2),\n                                  Mask, PassThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_3OP_SCALAR_MASK_SAE: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue Src3 = Op.getOperand(3);\n      SDValue PassThru = Op.getOperand(4);\n      SDValue Mask = Op.getOperand(5);\n      SDValue Sae = Op.getOperand(6);\n      unsigned Opc;\n      if (isRoundModeCurDirection(Sae))\n        Opc = IntrData->Opc0;\n      else if (isRoundModeSAE(Sae))\n        Opc = IntrData->Opc1;\n      else\n        return SDValue();\n\n      return getScalarMaskingNode(DAG.getNode(Opc, dl, VT, Src1, Src2, Src3),\n                                  Mask, PassThru, Subtarget, DAG);\n    }\n    case INTR_TYPE_3OP_MASK_SAE: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue Src3 = Op.getOperand(3);\n      SDValue PassThru = Op.getOperand(4);\n      SDValue Mask = Op.getOperand(5);\n\n      unsigned Opc = IntrData->Opc0;\n      if (IntrData->Opc1 != 0) {\n        SDValue Sae = Op.getOperand(6);\n        if (isRoundModeSAE(Sae))\n          Opc = IntrData->Opc1;\n        else if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n      return getVectorMaskingNode(DAG.getNode(Opc, dl, VT, Src1, Src2, Src3),\n                                  Mask, PassThru, Subtarget, DAG);\n    }\n    case BLENDV: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue Src3 = Op.getOperand(3);\n\n      EVT MaskVT = Src3.getValueType().changeVectorElementTypeToInteger();\n      Src3 = DAG.getBitcast(MaskVT, Src3);\n\n      // Reverse the operands to match VSELECT order.\n      return DAG.getNode(IntrData->Opc0, dl, VT, Src3, Src2, Src1);\n    }\n    case VPERM_2OP : {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n\n      // Swap Src1 and Src2 in the node creation\n      return DAG.getNode(IntrData->Opc0, dl, VT,Src2, Src1);\n    }\n    case IFMA_OP:\n      // NOTE: We need to swizzle the operands to pass the multiply operands\n      // first.\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(2), Op.getOperand(3), Op.getOperand(1));\n    case FPCLASSS: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Imm = Op.getOperand(2);\n      SDValue Mask = Op.getOperand(3);\n      SDValue FPclass = DAG.getNode(IntrData->Opc0, dl, MVT::v1i1, Src1, Imm);\n      SDValue FPclassMask = getScalarMaskingNode(FPclass, Mask, SDValue(),\n                                                 Subtarget, DAG);\n      // Need to fill with zeros to ensure the bitcast will produce zeroes\n      // for the upper bits. An EXTRACT_ELEMENT here wouldn't guarantee that.\n      SDValue Ins = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, MVT::v8i1,\n                                DAG.getConstant(0, dl, MVT::v8i1),\n                                FPclassMask, DAG.getIntPtrConstant(0, dl));\n      return DAG.getBitcast(MVT::i8, Ins);\n    }\n\n    case CMP_MASK_CC: {\n      MVT MaskVT = Op.getSimpleValueType();\n      SDValue CC = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n      // We specify 2 possible opcodes for intrinsics with rounding modes.\n      // First, we check if the intrinsic may have non-default rounding mode,\n      // (IntrData->Opc1 != 0), then we check the rounding mode operand.\n      if (IntrData->Opc1 != 0) {\n        SDValue Sae = Op.getOperand(5);\n        if (isRoundModeSAE(Sae))\n          return DAG.getNode(IntrData->Opc1, dl, MaskVT, Op.getOperand(1),\n                             Op.getOperand(2), CC, Mask, Sae);\n        if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n      //default rounding mode\n      return DAG.getNode(IntrData->Opc0, dl, MaskVT,\n                         {Op.getOperand(1), Op.getOperand(2), CC, Mask});\n    }\n    case CMP_MASK_SCALAR_CC: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue CC = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n\n      SDValue Cmp;\n      if (IntrData->Opc1 != 0) {\n        SDValue Sae = Op.getOperand(5);\n        if (isRoundModeSAE(Sae))\n          Cmp = DAG.getNode(IntrData->Opc1, dl, MVT::v1i1, Src1, Src2, CC, Sae);\n        else if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n      //default rounding mode\n      if (!Cmp.getNode())\n        Cmp = DAG.getNode(IntrData->Opc0, dl, MVT::v1i1, Src1, Src2, CC);\n\n      SDValue CmpMask = getScalarMaskingNode(Cmp, Mask, SDValue(),\n                                             Subtarget, DAG);\n      // Need to fill with zeros to ensure the bitcast will produce zeroes\n      // for the upper bits. An EXTRACT_ELEMENT here wouldn't guarantee that.\n      SDValue Ins = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, MVT::v8i1,\n                                DAG.getConstant(0, dl, MVT::v8i1),\n                                CmpMask, DAG.getIntPtrConstant(0, dl));\n      return DAG.getBitcast(MVT::i8, Ins);\n    }\n    case COMI: { // Comparison intrinsics\n      ISD::CondCode CC = (ISD::CondCode)IntrData->Opc1;\n      SDValue LHS = Op.getOperand(1);\n      SDValue RHS = Op.getOperand(2);\n      // Some conditions require the operands to be swapped.\n      if (CC == ISD::SETLT || CC == ISD::SETLE)\n        std::swap(LHS, RHS);\n\n      SDValue Comi = DAG.getNode(IntrData->Opc0, dl, MVT::i32, LHS, RHS);\n      SDValue SetCC;\n      switch (CC) {\n      case ISD::SETEQ: { // (ZF = 0 and PF = 0)\n        SetCC = getSETCC(X86::COND_E, Comi, dl, DAG);\n        SDValue SetNP = getSETCC(X86::COND_NP, Comi, dl, DAG);\n        SetCC = DAG.getNode(ISD::AND, dl, MVT::i8, SetCC, SetNP);\n        break;\n      }\n      case ISD::SETNE: { // (ZF = 1 or PF = 1)\n        SetCC = getSETCC(X86::COND_NE, Comi, dl, DAG);\n        SDValue SetP = getSETCC(X86::COND_P, Comi, dl, DAG);\n        SetCC = DAG.getNode(ISD::OR, dl, MVT::i8, SetCC, SetP);\n        break;\n      }\n      case ISD::SETGT: // (CF = 0 and ZF = 0)\n      case ISD::SETLT: { // Condition opposite to GT. Operands swapped above.\n        SetCC = getSETCC(X86::COND_A, Comi, dl, DAG);\n        break;\n      }\n      case ISD::SETGE: // CF = 0\n      case ISD::SETLE: // Condition opposite to GE. Operands swapped above.\n        SetCC = getSETCC(X86::COND_AE, Comi, dl, DAG);\n        break;\n      default:\n        llvm_unreachable(\"Unexpected illegal condition!\");\n      }\n      return DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, SetCC);\n    }\n    case COMI_RM: { // Comparison intrinsics with Sae\n      SDValue LHS = Op.getOperand(1);\n      SDValue RHS = Op.getOperand(2);\n      unsigned CondVal = Op.getConstantOperandVal(3);\n      SDValue Sae = Op.getOperand(4);\n\n      SDValue FCmp;\n      if (isRoundModeCurDirection(Sae))\n        FCmp = DAG.getNode(X86ISD::FSETCCM, dl, MVT::v1i1, LHS, RHS,\n                           DAG.getTargetConstant(CondVal, dl, MVT::i8));\n      else if (isRoundModeSAE(Sae))\n        FCmp = DAG.getNode(X86ISD::FSETCCM_SAE, dl, MVT::v1i1, LHS, RHS,\n                           DAG.getTargetConstant(CondVal, dl, MVT::i8), Sae);\n      else\n        return SDValue();\n      // Need to fill with zeros to ensure the bitcast will produce zeroes\n      // for the upper bits. An EXTRACT_ELEMENT here wouldn't guarantee that.\n      SDValue Ins = DAG.getNode(ISD::INSERT_SUBVECTOR, dl, MVT::v16i1,\n                                DAG.getConstant(0, dl, MVT::v16i1),\n                                FCmp, DAG.getIntPtrConstant(0, dl));\n      return DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32,\n                         DAG.getBitcast(MVT::i16, Ins));\n    }\n    case VSHIFT:\n      return getTargetVShiftNode(IntrData->Opc0, dl, Op.getSimpleValueType(),\n                                 Op.getOperand(1), Op.getOperand(2), Subtarget,\n                                 DAG);\n    case COMPRESS_EXPAND_IN_REG: {\n      SDValue Mask = Op.getOperand(3);\n      SDValue DataToCompress = Op.getOperand(1);\n      SDValue PassThru = Op.getOperand(2);\n      if (ISD::isBuildVectorAllOnes(Mask.getNode())) // return data as is\n        return Op.getOperand(1);\n\n      // Avoid false dependency.\n      if (PassThru.isUndef())\n        PassThru = DAG.getConstant(0, dl, VT);\n\n      return DAG.getNode(IntrData->Opc0, dl, VT, DataToCompress, PassThru,\n                         Mask);\n    }\n    case FIXUPIMM:\n    case FIXUPIMM_MASKZ: {\n      SDValue Src1 = Op.getOperand(1);\n      SDValue Src2 = Op.getOperand(2);\n      SDValue Src3 = Op.getOperand(3);\n      SDValue Imm = Op.getOperand(4);\n      SDValue Mask = Op.getOperand(5);\n      SDValue Passthru = (IntrData->Type == FIXUPIMM)\n                             ? Src1\n                             : getZeroVector(VT, Subtarget, DAG, dl);\n\n      unsigned Opc = IntrData->Opc0;\n      if (IntrData->Opc1 != 0) {\n        SDValue Sae = Op.getOperand(6);\n        if (isRoundModeSAE(Sae))\n          Opc = IntrData->Opc1;\n        else if (!isRoundModeCurDirection(Sae))\n          return SDValue();\n      }\n\n      SDValue FixupImm = DAG.getNode(Opc, dl, VT, Src1, Src2, Src3, Imm);\n\n      if (Opc == X86ISD::VFIXUPIMM || Opc == X86ISD::VFIXUPIMM_SAE)\n        return getVectorMaskingNode(FixupImm, Mask, Passthru, Subtarget, DAG);\n\n      return getScalarMaskingNode(FixupImm, Mask, Passthru, Subtarget, DAG);\n    }\n    case ROUNDP: {\n      assert(IntrData->Opc0 == X86ISD::VRNDSCALE && \"Unexpected opcode\");\n      // Clear the upper bits of the rounding immediate so that the legacy\n      // intrinsic can't trigger the scaling behavior of VRNDSCALE.\n      auto Round = cast<ConstantSDNode>(Op.getOperand(2));\n      SDValue RoundingMode =\n          DAG.getTargetConstant(Round->getZExtValue() & 0xf, dl, MVT::i32);\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1), RoundingMode);\n    }\n    case ROUNDS: {\n      assert(IntrData->Opc0 == X86ISD::VRNDSCALES && \"Unexpected opcode\");\n      // Clear the upper bits of the rounding immediate so that the legacy\n      // intrinsic can't trigger the scaling behavior of VRNDSCALE.\n      auto Round = cast<ConstantSDNode>(Op.getOperand(3));\n      SDValue RoundingMode =\n          DAG.getTargetConstant(Round->getZExtValue() & 0xf, dl, MVT::i32);\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1), Op.getOperand(2), RoundingMode);\n    }\n    case BEXTRI: {\n      assert(IntrData->Opc0 == X86ISD::BEXTRI && \"Unexpected opcode\");\n\n      uint64_t Imm = Op.getConstantOperandVal(2);\n      SDValue Control = DAG.getTargetConstant(Imm & 0xffff, dl,\n                                              Op.getValueType());\n      return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(),\n                         Op.getOperand(1), Control);\n    }\n    // ADC/ADCX/SBB\n    case ADX: {\n      SDVTList CFVTs = DAG.getVTList(Op->getValueType(0), MVT::i32);\n      SDVTList VTs = DAG.getVTList(Op.getOperand(2).getValueType(), MVT::i32);\n\n      SDValue Res;\n      // If the carry in is zero, then we should just use ADD/SUB instead of\n      // ADC/SBB.\n      if (isNullConstant(Op.getOperand(1))) {\n        Res = DAG.getNode(IntrData->Opc1, dl, VTs, Op.getOperand(2),\n                          Op.getOperand(3));\n      } else {\n        SDValue GenCF = DAG.getNode(X86ISD::ADD, dl, CFVTs, Op.getOperand(1),\n                                    DAG.getConstant(-1, dl, MVT::i8));\n        Res = DAG.getNode(IntrData->Opc0, dl, VTs, Op.getOperand(2),\n                          Op.getOperand(3), GenCF.getValue(1));\n      }\n      SDValue SetCC = getSETCC(X86::COND_B, Res.getValue(1), dl, DAG);\n      SDValue Results[] = { SetCC, Res };\n      return DAG.getMergeValues(Results, dl);\n    }\n    case CVTPD2PS_MASK:\n    case CVTPD2DQ_MASK:\n    case CVTQQ2PS_MASK:\n    case TRUNCATE_TO_REG: {\n      SDValue Src = Op.getOperand(1);\n      SDValue PassThru = Op.getOperand(2);\n      SDValue Mask = Op.getOperand(3);\n\n      if (isAllOnesConstant(Mask))\n        return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(), Src);\n\n      MVT SrcVT = Src.getSimpleValueType();\n      MVT MaskVT = MVT::getVectorVT(MVT::i1, SrcVT.getVectorNumElements());\n      Mask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n      return DAG.getNode(IntrData->Opc1, dl, Op.getValueType(),\n                         {Src, PassThru, Mask});\n    }\n    case CVTPS2PH_MASK: {\n      SDValue Src = Op.getOperand(1);\n      SDValue Rnd = Op.getOperand(2);\n      SDValue PassThru = Op.getOperand(3);\n      SDValue Mask = Op.getOperand(4);\n\n      if (isAllOnesConstant(Mask))\n        return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(), Src, Rnd);\n\n      MVT SrcVT = Src.getSimpleValueType();\n      MVT MaskVT = MVT::getVectorVT(MVT::i1, SrcVT.getVectorNumElements());\n      Mask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n      return DAG.getNode(IntrData->Opc1, dl, Op.getValueType(), Src, Rnd,\n                         PassThru, Mask);\n\n    }\n    case CVTNEPS2BF16_MASK: {\n      SDValue Src = Op.getOperand(1);\n      SDValue PassThru = Op.getOperand(2);\n      SDValue Mask = Op.getOperand(3);\n\n      if (ISD::isBuildVectorAllOnes(Mask.getNode()))\n        return DAG.getNode(IntrData->Opc0, dl, Op.getValueType(), Src);\n\n      // Break false dependency.\n      if (PassThru.isUndef())\n        PassThru = DAG.getConstant(0, dl, PassThru.getValueType());\n\n      return DAG.getNode(IntrData->Opc1, dl, Op.getValueType(), Src, PassThru,\n                         Mask);\n    }\n    default:\n      break;\n    }\n  }\n\n  switch (IntNo) {\n  default: return SDValue();    // Don't custom lower most intrinsics.\n\n  // ptest and testp intrinsics. The intrinsic these come from are designed to\n  // return an integer value, not just an instruction so lower it to the ptest\n  // or testp pattern and a setcc for the result.\n  case Intrinsic::x86_avx512_ktestc_b:\n  case Intrinsic::x86_avx512_ktestc_w:\n  case Intrinsic::x86_avx512_ktestc_d:\n  case Intrinsic::x86_avx512_ktestc_q:\n  case Intrinsic::x86_avx512_ktestz_b:\n  case Intrinsic::x86_avx512_ktestz_w:\n  case Intrinsic::x86_avx512_ktestz_d:\n  case Intrinsic::x86_avx512_ktestz_q:\n  case Intrinsic::x86_sse41_ptestz:\n  case Intrinsic::x86_sse41_ptestc:\n  case Intrinsic::x86_sse41_ptestnzc:\n  case Intrinsic::x86_avx_ptestz_256:\n  case Intrinsic::x86_avx_ptestc_256:\n  case Intrinsic::x86_avx_ptestnzc_256:\n  case Intrinsic::x86_avx_vtestz_ps:\n  case Intrinsic::x86_avx_vtestc_ps:\n  case Intrinsic::x86_avx_vtestnzc_ps:\n  case Intrinsic::x86_avx_vtestz_pd:\n  case Intrinsic::x86_avx_vtestc_pd:\n  case Intrinsic::x86_avx_vtestnzc_pd:\n  case Intrinsic::x86_avx_vtestz_ps_256:\n  case Intrinsic::x86_avx_vtestc_ps_256:\n  case Intrinsic::x86_avx_vtestnzc_ps_256:\n  case Intrinsic::x86_avx_vtestz_pd_256:\n  case Intrinsic::x86_avx_vtestc_pd_256:\n  case Intrinsic::x86_avx_vtestnzc_pd_256: {\n    unsigned TestOpc = X86ISD::PTEST;\n    X86::CondCode X86CC;\n    switch (IntNo) {\n    default: llvm_unreachable(\"Bad fallthrough in Intrinsic lowering.\");\n    case Intrinsic::x86_avx512_ktestc_b:\n    case Intrinsic::x86_avx512_ktestc_w:\n    case Intrinsic::x86_avx512_ktestc_d:\n    case Intrinsic::x86_avx512_ktestc_q:\n      // CF = 1\n      TestOpc = X86ISD::KTEST;\n      X86CC = X86::COND_B;\n      break;\n    case Intrinsic::x86_avx512_ktestz_b:\n    case Intrinsic::x86_avx512_ktestz_w:\n    case Intrinsic::x86_avx512_ktestz_d:\n    case Intrinsic::x86_avx512_ktestz_q:\n      TestOpc = X86ISD::KTEST;\n      X86CC = X86::COND_E;\n      break;\n    case Intrinsic::x86_avx_vtestz_ps:\n    case Intrinsic::x86_avx_vtestz_pd:\n    case Intrinsic::x86_avx_vtestz_ps_256:\n    case Intrinsic::x86_avx_vtestz_pd_256:\n      TestOpc = X86ISD::TESTP;\n      LLVM_FALLTHROUGH;\n    case Intrinsic::x86_sse41_ptestz:\n    case Intrinsic::x86_avx_ptestz_256:\n      // ZF = 1\n      X86CC = X86::COND_E;\n      break;\n    case Intrinsic::x86_avx_vtestc_ps:\n    case Intrinsic::x86_avx_vtestc_pd:\n    case Intrinsic::x86_avx_vtestc_ps_256:\n    case Intrinsic::x86_avx_vtestc_pd_256:\n      TestOpc = X86ISD::TESTP;\n      LLVM_FALLTHROUGH;\n    case Intrinsic::x86_sse41_ptestc:\n    case Intrinsic::x86_avx_ptestc_256:\n      // CF = 1\n      X86CC = X86::COND_B;\n      break;\n    case Intrinsic::x86_avx_vtestnzc_ps:\n    case Intrinsic::x86_avx_vtestnzc_pd:\n    case Intrinsic::x86_avx_vtestnzc_ps_256:\n    case Intrinsic::x86_avx_vtestnzc_pd_256:\n      TestOpc = X86ISD::TESTP;\n      LLVM_FALLTHROUGH;\n    case Intrinsic::x86_sse41_ptestnzc:\n    case Intrinsic::x86_avx_ptestnzc_256:\n      // ZF and CF = 0\n      X86CC = X86::COND_A;\n      break;\n    }\n\n    SDValue LHS = Op.getOperand(1);\n    SDValue RHS = Op.getOperand(2);\n    SDValue Test = DAG.getNode(TestOpc, dl, MVT::i32, LHS, RHS);\n    SDValue SetCC = getSETCC(X86CC, Test, dl, DAG);\n    return DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, SetCC);\n  }\n\n  case Intrinsic::x86_sse42_pcmpistria128:\n  case Intrinsic::x86_sse42_pcmpestria128:\n  case Intrinsic::x86_sse42_pcmpistric128:\n  case Intrinsic::x86_sse42_pcmpestric128:\n  case Intrinsic::x86_sse42_pcmpistrio128:\n  case Intrinsic::x86_sse42_pcmpestrio128:\n  case Intrinsic::x86_sse42_pcmpistris128:\n  case Intrinsic::x86_sse42_pcmpestris128:\n  case Intrinsic::x86_sse42_pcmpistriz128:\n  case Intrinsic::x86_sse42_pcmpestriz128: {\n    unsigned Opcode;\n    X86::CondCode X86CC;\n    switch (IntNo) {\n    default: llvm_unreachable(\"Impossible intrinsic\");  // Can't reach here.\n    case Intrinsic::x86_sse42_pcmpistria128:\n      Opcode = X86ISD::PCMPISTR;\n      X86CC = X86::COND_A;\n      break;\n    case Intrinsic::x86_sse42_pcmpestria128:\n      Opcode = X86ISD::PCMPESTR;\n      X86CC = X86::COND_A;\n      break;\n    case Intrinsic::x86_sse42_pcmpistric128:\n      Opcode = X86ISD::PCMPISTR;\n      X86CC = X86::COND_B;\n      break;\n    case Intrinsic::x86_sse42_pcmpestric128:\n      Opcode = X86ISD::PCMPESTR;\n      X86CC = X86::COND_B;\n      break;\n    case Intrinsic::x86_sse42_pcmpistrio128:\n      Opcode = X86ISD::PCMPISTR;\n      X86CC = X86::COND_O;\n      break;\n    case Intrinsic::x86_sse42_pcmpestrio128:\n      Opcode = X86ISD::PCMPESTR;\n      X86CC = X86::COND_O;\n      break;\n    case Intrinsic::x86_sse42_pcmpistris128:\n      Opcode = X86ISD::PCMPISTR;\n      X86CC = X86::COND_S;\n      break;\n    case Intrinsic::x86_sse42_pcmpestris128:\n      Opcode = X86ISD::PCMPESTR;\n      X86CC = X86::COND_S;\n      break;\n    case Intrinsic::x86_sse42_pcmpistriz128:\n      Opcode = X86ISD::PCMPISTR;\n      X86CC = X86::COND_E;\n      break;\n    case Intrinsic::x86_sse42_pcmpestriz128:\n      Opcode = X86ISD::PCMPESTR;\n      X86CC = X86::COND_E;\n      break;\n    }\n    SmallVector<SDValue, 5> NewOps(Op->op_begin()+1, Op->op_end());\n    SDVTList VTs = DAG.getVTList(MVT::i32, MVT::v16i8, MVT::i32);\n    SDValue PCMP = DAG.getNode(Opcode, dl, VTs, NewOps).getValue(2);\n    SDValue SetCC = getSETCC(X86CC, PCMP, dl, DAG);\n    return DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, SetCC);\n  }\n\n  case Intrinsic::x86_sse42_pcmpistri128:\n  case Intrinsic::x86_sse42_pcmpestri128: {\n    unsigned Opcode;\n    if (IntNo == Intrinsic::x86_sse42_pcmpistri128)\n      Opcode = X86ISD::PCMPISTR;\n    else\n      Opcode = X86ISD::PCMPESTR;\n\n    SmallVector<SDValue, 5> NewOps(Op->op_begin()+1, Op->op_end());\n    SDVTList VTs = DAG.getVTList(MVT::i32, MVT::v16i8, MVT::i32);\n    return DAG.getNode(Opcode, dl, VTs, NewOps);\n  }\n\n  case Intrinsic::x86_sse42_pcmpistrm128:\n  case Intrinsic::x86_sse42_pcmpestrm128: {\n    unsigned Opcode;\n    if (IntNo == Intrinsic::x86_sse42_pcmpistrm128)\n      Opcode = X86ISD::PCMPISTR;\n    else\n      Opcode = X86ISD::PCMPESTR;\n\n    SmallVector<SDValue, 5> NewOps(Op->op_begin()+1, Op->op_end());\n    SDVTList VTs = DAG.getVTList(MVT::i32, MVT::v16i8, MVT::i32);\n    return DAG.getNode(Opcode, dl, VTs, NewOps).getValue(1);\n  }\n\n  case Intrinsic::eh_sjlj_lsda: {\n    MachineFunction &MF = DAG.getMachineFunction();\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    MVT PtrVT = TLI.getPointerTy(DAG.getDataLayout());\n    auto &Context = MF.getMMI().getContext();\n    MCSymbol *S = Context.getOrCreateSymbol(Twine(\"GCC_except_table\") +\n                                            Twine(MF.getFunctionNumber()));\n    return DAG.getNode(getGlobalWrapperKind(), dl, VT,\n                       DAG.getMCSymbol(S, PtrVT));\n  }\n\n  case Intrinsic::x86_seh_lsda: {\n    // Compute the symbol for the LSDA. We know it'll get emitted later.\n    MachineFunction &MF = DAG.getMachineFunction();\n    SDValue Op1 = Op.getOperand(1);\n    auto *Fn = cast<Function>(cast<GlobalAddressSDNode>(Op1)->getGlobal());\n    MCSymbol *LSDASym = MF.getMMI().getContext().getOrCreateLSDASymbol(\n        GlobalValue::dropLLVMManglingEscape(Fn->getName()));\n\n    // Generate a simple absolute symbol reference. This intrinsic is only\n    // supported on 32-bit Windows, which isn't PIC.\n    SDValue Result = DAG.getMCSymbol(LSDASym, VT);\n    return DAG.getNode(X86ISD::Wrapper, dl, VT, Result);\n  }\n\n  case Intrinsic::eh_recoverfp: {\n    SDValue FnOp = Op.getOperand(1);\n    SDValue IncomingFPOp = Op.getOperand(2);\n    GlobalAddressSDNode *GSD = dyn_cast<GlobalAddressSDNode>(FnOp);\n    auto *Fn = dyn_cast_or_null<Function>(GSD ? GSD->getGlobal() : nullptr);\n    if (!Fn)\n      report_fatal_error(\n          \"llvm.eh.recoverfp must take a function as the first argument\");\n    return recoverFramePointer(DAG, Fn, IncomingFPOp);\n  }\n\n  case Intrinsic::localaddress: {\n    // Returns one of the stack, base, or frame pointer registers, depending on\n    // which is used to reference local variables.\n    MachineFunction &MF = DAG.getMachineFunction();\n    const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n    unsigned Reg;\n    if (RegInfo->hasBasePointer(MF))\n      Reg = RegInfo->getBaseRegister();\n    else { // Handles the SP or FP case.\n      bool CantUseFP = RegInfo->needsStackRealignment(MF);\n      if (CantUseFP)\n        Reg = RegInfo->getPtrSizedStackRegister(MF);\n      else\n        Reg = RegInfo->getPtrSizedFrameRegister(MF);\n    }\n    return DAG.getCopyFromReg(DAG.getEntryNode(), dl, Reg, VT);\n  }\n\n  case Intrinsic::x86_avx512_vp2intersect_q_512:\n  case Intrinsic::x86_avx512_vp2intersect_q_256:\n  case Intrinsic::x86_avx512_vp2intersect_q_128:\n  case Intrinsic::x86_avx512_vp2intersect_d_512:\n  case Intrinsic::x86_avx512_vp2intersect_d_256:\n  case Intrinsic::x86_avx512_vp2intersect_d_128: {\n    MVT MaskVT = Op.getSimpleValueType();\n\n    SDVTList VTs = DAG.getVTList(MVT::Untyped, MVT::Other);\n    SDLoc DL(Op);\n\n    SDValue Operation =\n        DAG.getNode(X86ISD::VP2INTERSECT, DL, VTs,\n                    Op->getOperand(1), Op->getOperand(2));\n\n    SDValue Result0 = DAG.getTargetExtractSubreg(X86::sub_mask_0, DL,\n                                                 MaskVT, Operation);\n    SDValue Result1 = DAG.getTargetExtractSubreg(X86::sub_mask_1, DL,\n                                                 MaskVT, Operation);\n    return DAG.getMergeValues({Result0, Result1}, DL);\n  }\n  case Intrinsic::x86_mmx_pslli_w:\n  case Intrinsic::x86_mmx_pslli_d:\n  case Intrinsic::x86_mmx_pslli_q:\n  case Intrinsic::x86_mmx_psrli_w:\n  case Intrinsic::x86_mmx_psrli_d:\n  case Intrinsic::x86_mmx_psrli_q:\n  case Intrinsic::x86_mmx_psrai_w:\n  case Intrinsic::x86_mmx_psrai_d: {\n    SDLoc DL(Op);\n    SDValue ShAmt = Op.getOperand(2);\n    // If the argument is a constant, convert it to a target constant.\n    if (auto *C = dyn_cast<ConstantSDNode>(ShAmt)) {\n      // Clamp out of bounds shift amounts since they will otherwise be masked\n      // to 8-bits which may make it no longer out of bounds.\n      unsigned ShiftAmount = C->getAPIntValue().getLimitedValue(255);\n      if (ShiftAmount == 0)\n        return Op.getOperand(1);\n\n      return DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, Op.getValueType(),\n                         Op.getOperand(0), Op.getOperand(1),\n                         DAG.getTargetConstant(ShiftAmount, DL, MVT::i32));\n    }\n\n    unsigned NewIntrinsic;\n    switch (IntNo) {\n    default: llvm_unreachable(\"Impossible intrinsic\");  // Can't reach here.\n    case Intrinsic::x86_mmx_pslli_w:\n      NewIntrinsic = Intrinsic::x86_mmx_psll_w;\n      break;\n    case Intrinsic::x86_mmx_pslli_d:\n      NewIntrinsic = Intrinsic::x86_mmx_psll_d;\n      break;\n    case Intrinsic::x86_mmx_pslli_q:\n      NewIntrinsic = Intrinsic::x86_mmx_psll_q;\n      break;\n    case Intrinsic::x86_mmx_psrli_w:\n      NewIntrinsic = Intrinsic::x86_mmx_psrl_w;\n      break;\n    case Intrinsic::x86_mmx_psrli_d:\n      NewIntrinsic = Intrinsic::x86_mmx_psrl_d;\n      break;\n    case Intrinsic::x86_mmx_psrli_q:\n      NewIntrinsic = Intrinsic::x86_mmx_psrl_q;\n      break;\n    case Intrinsic::x86_mmx_psrai_w:\n      NewIntrinsic = Intrinsic::x86_mmx_psra_w;\n      break;\n    case Intrinsic::x86_mmx_psrai_d:\n      NewIntrinsic = Intrinsic::x86_mmx_psra_d;\n      break;\n    }\n\n    // The vector shift intrinsics with scalars uses 32b shift amounts but\n    // the sse2/mmx shift instructions reads 64 bits. Copy the 32 bits to an\n    // MMX register.\n    ShAmt = DAG.getNode(X86ISD::MMX_MOVW2D, DL, MVT::x86mmx, ShAmt);\n    return DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, Op.getValueType(),\n                       DAG.getTargetConstant(NewIntrinsic, DL,\n                                             getPointerTy(DAG.getDataLayout())),\n                       Op.getOperand(1), ShAmt);\n  }\n  }\n}\n\nstatic SDValue getAVX2GatherNode(unsigned Opc, SDValue Op, SelectionDAG &DAG,\n                                 SDValue Src, SDValue Mask, SDValue Base,\n                                 SDValue Index, SDValue ScaleOp, SDValue Chain,\n                                 const X86Subtarget &Subtarget) {\n  SDLoc dl(Op);\n  auto *C = dyn_cast<ConstantSDNode>(ScaleOp);\n  // Scale must be constant.\n  if (!C)\n    return SDValue();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  SDValue Scale = DAG.getTargetConstant(C->getZExtValue(), dl,\n                                        TLI.getPointerTy(DAG.getDataLayout()));\n  EVT MaskVT = Mask.getValueType().changeVectorElementTypeToInteger();\n  SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Other);\n  // If source is undef or we know it won't be used, use a zero vector\n  // to break register dependency.\n  // TODO: use undef instead and let BreakFalseDeps deal with it?\n  if (Src.isUndef() || ISD::isBuildVectorAllOnes(Mask.getNode()))\n    Src = getZeroVector(Op.getSimpleValueType(), Subtarget, DAG, dl);\n\n  // Cast mask to an integer type.\n  Mask = DAG.getBitcast(MaskVT, Mask);\n\n  MemIntrinsicSDNode *MemIntr = cast<MemIntrinsicSDNode>(Op);\n\n  SDValue Ops[] = {Chain, Src, Mask, Base, Index, Scale };\n  SDValue Res =\n      DAG.getMemIntrinsicNode(X86ISD::MGATHER, dl, VTs, Ops,\n                              MemIntr->getMemoryVT(), MemIntr->getMemOperand());\n  return DAG.getMergeValues({Res, Res.getValue(1)}, dl);\n}\n\nstatic SDValue getGatherNode(SDValue Op, SelectionDAG &DAG,\n                             SDValue Src, SDValue Mask, SDValue Base,\n                             SDValue Index, SDValue ScaleOp, SDValue Chain,\n                             const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n  auto *C = dyn_cast<ConstantSDNode>(ScaleOp);\n  // Scale must be constant.\n  if (!C)\n    return SDValue();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  SDValue Scale = DAG.getTargetConstant(C->getZExtValue(), dl,\n                                        TLI.getPointerTy(DAG.getDataLayout()));\n  unsigned MinElts = std::min(Index.getSimpleValueType().getVectorNumElements(),\n                              VT.getVectorNumElements());\n  MVT MaskVT = MVT::getVectorVT(MVT::i1, MinElts);\n\n  // We support two versions of the gather intrinsics. One with scalar mask and\n  // one with vXi1 mask. Convert scalar to vXi1 if necessary.\n  if (Mask.getValueType() != MaskVT)\n    Mask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n\n  SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Other);\n  // If source is undef or we know it won't be used, use a zero vector\n  // to break register dependency.\n  // TODO: use undef instead and let BreakFalseDeps deal with it?\n  if (Src.isUndef() || ISD::isBuildVectorAllOnes(Mask.getNode()))\n    Src = getZeroVector(Op.getSimpleValueType(), Subtarget, DAG, dl);\n\n  MemIntrinsicSDNode *MemIntr = cast<MemIntrinsicSDNode>(Op);\n\n  SDValue Ops[] = {Chain, Src, Mask, Base, Index, Scale };\n  SDValue Res =\n      DAG.getMemIntrinsicNode(X86ISD::MGATHER, dl, VTs, Ops,\n                              MemIntr->getMemoryVT(), MemIntr->getMemOperand());\n  return DAG.getMergeValues({Res, Res.getValue(1)}, dl);\n}\n\nstatic SDValue getScatterNode(unsigned Opc, SDValue Op, SelectionDAG &DAG,\n                               SDValue Src, SDValue Mask, SDValue Base,\n                               SDValue Index, SDValue ScaleOp, SDValue Chain,\n                               const X86Subtarget &Subtarget) {\n  SDLoc dl(Op);\n  auto *C = dyn_cast<ConstantSDNode>(ScaleOp);\n  // Scale must be constant.\n  if (!C)\n    return SDValue();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  SDValue Scale = DAG.getTargetConstant(C->getZExtValue(), dl,\n                                        TLI.getPointerTy(DAG.getDataLayout()));\n  unsigned MinElts = std::min(Index.getSimpleValueType().getVectorNumElements(),\n                              Src.getSimpleValueType().getVectorNumElements());\n  MVT MaskVT = MVT::getVectorVT(MVT::i1, MinElts);\n\n  // We support two versions of the scatter intrinsics. One with scalar mask and\n  // one with vXi1 mask. Convert scalar to vXi1 if necessary.\n  if (Mask.getValueType() != MaskVT)\n    Mask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n\n  MemIntrinsicSDNode *MemIntr = cast<MemIntrinsicSDNode>(Op);\n\n  SDVTList VTs = DAG.getVTList(MVT::Other);\n  SDValue Ops[] = {Chain, Src, Mask, Base, Index, Scale};\n  SDValue Res =\n      DAG.getMemIntrinsicNode(X86ISD::MSCATTER, dl, VTs, Ops,\n                              MemIntr->getMemoryVT(), MemIntr->getMemOperand());\n  return Res;\n}\n\nstatic SDValue getPrefetchNode(unsigned Opc, SDValue Op, SelectionDAG &DAG,\n                               SDValue Mask, SDValue Base, SDValue Index,\n                               SDValue ScaleOp, SDValue Chain,\n                               const X86Subtarget &Subtarget) {\n  SDLoc dl(Op);\n  auto *C = dyn_cast<ConstantSDNode>(ScaleOp);\n  // Scale must be constant.\n  if (!C)\n    return SDValue();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  SDValue Scale = DAG.getTargetConstant(C->getZExtValue(), dl,\n                                        TLI.getPointerTy(DAG.getDataLayout()));\n  SDValue Disp = DAG.getTargetConstant(0, dl, MVT::i32);\n  SDValue Segment = DAG.getRegister(0, MVT::i32);\n  MVT MaskVT =\n    MVT::getVectorVT(MVT::i1, Index.getSimpleValueType().getVectorNumElements());\n  SDValue VMask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n  SDValue Ops[] = {VMask, Base, Scale, Index, Disp, Segment, Chain};\n  SDNode *Res = DAG.getMachineNode(Opc, dl, MVT::Other, Ops);\n  return SDValue(Res, 0);\n}\n\n/// Handles the lowering of builtin intrinsics with chain that return their\n/// value into registers EDX:EAX.\n/// If operand ScrReg is a valid register identifier, then operand 2 of N is\n/// copied to SrcReg. The assumption is that SrcReg is an implicit input to\n/// TargetOpcode.\n/// Returns a Glue value which can be used to add extra copy-from-reg if the\n/// expanded intrinsics implicitly defines extra registers (i.e. not just\n/// EDX:EAX).\nstatic SDValue expandIntrinsicWChainHelper(SDNode *N, const SDLoc &DL,\n                                        SelectionDAG &DAG,\n                                        unsigned TargetOpcode,\n                                        unsigned SrcReg,\n                                        const X86Subtarget &Subtarget,\n                                        SmallVectorImpl<SDValue> &Results) {\n  SDValue Chain = N->getOperand(0);\n  SDValue Glue;\n\n  if (SrcReg) {\n    assert(N->getNumOperands() == 3 && \"Unexpected number of operands!\");\n    Chain = DAG.getCopyToReg(Chain, DL, SrcReg, N->getOperand(2), Glue);\n    Glue = Chain.getValue(1);\n  }\n\n  SDVTList Tys = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue N1Ops[] = {Chain, Glue};\n  SDNode *N1 = DAG.getMachineNode(\n      TargetOpcode, DL, Tys, ArrayRef<SDValue>(N1Ops, Glue.getNode() ? 2 : 1));\n  Chain = SDValue(N1, 0);\n\n  // Reads the content of XCR and returns it in registers EDX:EAX.\n  SDValue LO, HI;\n  if (Subtarget.is64Bit()) {\n    LO = DAG.getCopyFromReg(Chain, DL, X86::RAX, MVT::i64, SDValue(N1, 1));\n    HI = DAG.getCopyFromReg(LO.getValue(1), DL, X86::RDX, MVT::i64,\n                            LO.getValue(2));\n  } else {\n    LO = DAG.getCopyFromReg(Chain, DL, X86::EAX, MVT::i32, SDValue(N1, 1));\n    HI = DAG.getCopyFromReg(LO.getValue(1), DL, X86::EDX, MVT::i32,\n                            LO.getValue(2));\n  }\n  Chain = HI.getValue(1);\n  Glue = HI.getValue(2);\n\n  if (Subtarget.is64Bit()) {\n    // Merge the two 32-bit values into a 64-bit one.\n    SDValue Tmp = DAG.getNode(ISD::SHL, DL, MVT::i64, HI,\n                              DAG.getConstant(32, DL, MVT::i8));\n    Results.push_back(DAG.getNode(ISD::OR, DL, MVT::i64, LO, Tmp));\n    Results.push_back(Chain);\n    return Glue;\n  }\n\n  // Use a buildpair to merge the two 32-bit values into a 64-bit one.\n  SDValue Ops[] = { LO, HI };\n  SDValue Pair = DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, Ops);\n  Results.push_back(Pair);\n  Results.push_back(Chain);\n  return Glue;\n}\n\n/// Handles the lowering of builtin intrinsics that read the time stamp counter\n/// (x86_rdtsc and x86_rdtscp). This function is also used to custom lower\n/// READCYCLECOUNTER nodes.\nstatic void getReadTimeStampCounter(SDNode *N, const SDLoc &DL, unsigned Opcode,\n                                    SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget,\n                                    SmallVectorImpl<SDValue> &Results) {\n  // The processor's time-stamp counter (a 64-bit MSR) is stored into the\n  // EDX:EAX registers. EDX is loaded with the high-order 32 bits of the MSR\n  // and the EAX register is loaded with the low-order 32 bits.\n  SDValue Glue = expandIntrinsicWChainHelper(N, DL, DAG, Opcode,\n                                             /* NoRegister */0, Subtarget,\n                                             Results);\n  if (Opcode != X86::RDTSCP)\n    return;\n\n  SDValue Chain = Results[1];\n  // Instruction RDTSCP loads the IA32:TSC_AUX_MSR (address C000_0103H) into\n  // the ECX register. Add 'ecx' explicitly to the chain.\n  SDValue ecx = DAG.getCopyFromReg(Chain, DL, X86::ECX, MVT::i32, Glue);\n  Results[1] = ecx;\n  Results.push_back(ecx.getValue(1));\n}\n\nstatic SDValue LowerREADCYCLECOUNTER(SDValue Op, const X86Subtarget &Subtarget,\n                                     SelectionDAG &DAG) {\n  SmallVector<SDValue, 3> Results;\n  SDLoc DL(Op);\n  getReadTimeStampCounter(Op.getNode(), DL, X86::RDTSC, DAG, Subtarget,\n                          Results);\n  return DAG.getMergeValues(Results, DL);\n}\n\nstatic SDValue MarkEHRegistrationNode(SDValue Op, SelectionDAG &DAG) {\n  MachineFunction &MF = DAG.getMachineFunction();\n  SDValue Chain = Op.getOperand(0);\n  SDValue RegNode = Op.getOperand(2);\n  WinEHFuncInfo *EHInfo = MF.getWinEHFuncInfo();\n  if (!EHInfo)\n    report_fatal_error(\"EH registrations only live in functions using WinEH\");\n\n  // Cast the operand to an alloca, and remember the frame index.\n  auto *FINode = dyn_cast<FrameIndexSDNode>(RegNode);\n  if (!FINode)\n    report_fatal_error(\"llvm.x86.seh.ehregnode expects a static alloca\");\n  EHInfo->EHRegNodeFrameIndex = FINode->getIndex();\n\n  // Return the chain operand without making any DAG nodes.\n  return Chain;\n}\n\nstatic SDValue MarkEHGuard(SDValue Op, SelectionDAG &DAG) {\n  MachineFunction &MF = DAG.getMachineFunction();\n  SDValue Chain = Op.getOperand(0);\n  SDValue EHGuard = Op.getOperand(2);\n  WinEHFuncInfo *EHInfo = MF.getWinEHFuncInfo();\n  if (!EHInfo)\n    report_fatal_error(\"EHGuard only live in functions using WinEH\");\n\n  // Cast the operand to an alloca, and remember the frame index.\n  auto *FINode = dyn_cast<FrameIndexSDNode>(EHGuard);\n  if (!FINode)\n    report_fatal_error(\"llvm.x86.seh.ehguard expects a static alloca\");\n  EHInfo->EHGuardFrameIndex = FINode->getIndex();\n\n  // Return the chain operand without making any DAG nodes.\n  return Chain;\n}\n\n/// Emit Truncating Store with signed or unsigned saturation.\nstatic SDValue\nEmitTruncSStore(bool SignedSat, SDValue Chain, const SDLoc &Dl, SDValue Val,\n                SDValue Ptr, EVT MemVT, MachineMemOperand *MMO,\n                SelectionDAG &DAG) {\n  SDVTList VTs = DAG.getVTList(MVT::Other);\n  SDValue Undef = DAG.getUNDEF(Ptr.getValueType());\n  SDValue Ops[] = { Chain, Val, Ptr, Undef };\n  unsigned Opc = SignedSat ? X86ISD::VTRUNCSTORES : X86ISD::VTRUNCSTOREUS;\n  return DAG.getMemIntrinsicNode(Opc, Dl, VTs, Ops, MemVT, MMO);\n}\n\n/// Emit Masked Truncating Store with signed or unsigned saturation.\nstatic SDValue\nEmitMaskedTruncSStore(bool SignedSat, SDValue Chain, const SDLoc &Dl,\n                      SDValue Val, SDValue Ptr, SDValue Mask, EVT MemVT,\n                      MachineMemOperand *MMO, SelectionDAG &DAG) {\n  SDVTList VTs = DAG.getVTList(MVT::Other);\n  SDValue Ops[] = { Chain, Val, Ptr, Mask };\n  unsigned Opc = SignedSat ? X86ISD::VMTRUNCSTORES : X86ISD::VMTRUNCSTOREUS;\n  return DAG.getMemIntrinsicNode(Opc, Dl, VTs, Ops, MemVT, MMO);\n}\n\nstatic SDValue LowerINTRINSIC_W_CHAIN(SDValue Op, const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  unsigned IntNo = Op.getConstantOperandVal(1);\n  const IntrinsicData *IntrData = getIntrinsicWithChain(IntNo);\n  if (!IntrData) {\n    switch (IntNo) {\n    case llvm::Intrinsic::x86_seh_ehregnode:\n      return MarkEHRegistrationNode(Op, DAG);\n    case llvm::Intrinsic::x86_seh_ehguard:\n      return MarkEHGuard(Op, DAG);\n    case llvm::Intrinsic::x86_rdpkru: {\n      SDLoc dl(Op);\n      SDVTList VTs = DAG.getVTList(MVT::i32, MVT::Other);\n      // Create a RDPKRU node and pass 0 to the ECX parameter.\n      return DAG.getNode(X86ISD::RDPKRU, dl, VTs, Op.getOperand(0),\n                         DAG.getConstant(0, dl, MVT::i32));\n    }\n    case llvm::Intrinsic::x86_wrpkru: {\n      SDLoc dl(Op);\n      // Create a WRPKRU node, pass the input to the EAX parameter,  and pass 0\n      // to the EDX and ECX parameters.\n      return DAG.getNode(X86ISD::WRPKRU, dl, MVT::Other,\n                         Op.getOperand(0), Op.getOperand(2),\n                         DAG.getConstant(0, dl, MVT::i32),\n                         DAG.getConstant(0, dl, MVT::i32));\n    }\n    case llvm::Intrinsic::x86_flags_read_u32:\n    case llvm::Intrinsic::x86_flags_read_u64:\n    case llvm::Intrinsic::x86_flags_write_u32:\n    case llvm::Intrinsic::x86_flags_write_u64: {\n      // We need a frame pointer because this will get lowered to a PUSH/POP\n      // sequence.\n      MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();\n      MFI.setHasCopyImplyingStackAdjustment(true);\n      // Don't do anything here, we will expand these intrinsics out later\n      // during FinalizeISel in EmitInstrWithCustomInserter.\n      return Op;\n    }\n    case Intrinsic::x86_lwpins32:\n    case Intrinsic::x86_lwpins64:\n    case Intrinsic::x86_umwait:\n    case Intrinsic::x86_tpause: {\n      SDLoc dl(Op);\n      SDValue Chain = Op->getOperand(0);\n      SDVTList VTs = DAG.getVTList(MVT::i32, MVT::Other);\n      unsigned Opcode;\n\n      switch (IntNo) {\n      default: llvm_unreachable(\"Impossible intrinsic\");\n      case Intrinsic::x86_umwait:\n        Opcode = X86ISD::UMWAIT;\n        break;\n      case Intrinsic::x86_tpause:\n        Opcode = X86ISD::TPAUSE;\n        break;\n      case Intrinsic::x86_lwpins32:\n      case Intrinsic::x86_lwpins64:\n        Opcode = X86ISD::LWPINS;\n        break;\n      }\n\n      SDValue Operation =\n          DAG.getNode(Opcode, dl, VTs, Chain, Op->getOperand(2),\n                      Op->getOperand(3), Op->getOperand(4));\n      SDValue SetCC = getSETCC(X86::COND_B, Operation.getValue(0), dl, DAG);\n      return DAG.getNode(ISD::MERGE_VALUES, dl, Op->getVTList(), SetCC,\n                         Operation.getValue(1));\n    }\n    case Intrinsic::x86_enqcmd:\n    case Intrinsic::x86_enqcmds: {\n      SDLoc dl(Op);\n      SDValue Chain = Op.getOperand(0);\n      SDVTList VTs = DAG.getVTList(MVT::i32, MVT::Other);\n      unsigned Opcode;\n      switch (IntNo) {\n      default: llvm_unreachable(\"Impossible intrinsic!\");\n      case Intrinsic::x86_enqcmd:\n        Opcode = X86ISD::ENQCMD;\n        break;\n      case Intrinsic::x86_enqcmds:\n        Opcode = X86ISD::ENQCMDS;\n        break;\n      }\n      SDValue Operation = DAG.getNode(Opcode, dl, VTs, Chain, Op.getOperand(2),\n                                      Op.getOperand(3));\n      SDValue SetCC = getSETCC(X86::COND_E, Operation.getValue(0), dl, DAG);\n      return DAG.getNode(ISD::MERGE_VALUES, dl, Op->getVTList(), SetCC,\n                         Operation.getValue(1));\n    }\n    case Intrinsic::x86_aesenc128kl:\n    case Intrinsic::x86_aesdec128kl:\n    case Intrinsic::x86_aesenc256kl:\n    case Intrinsic::x86_aesdec256kl: {\n      SDLoc DL(Op);\n      SDVTList VTs = DAG.getVTList(MVT::v2i64, MVT::i32, MVT::Other);\n      SDValue Chain = Op.getOperand(0);\n      unsigned Opcode;\n\n      switch (IntNo) {\n      default: llvm_unreachable(\"Impossible intrinsic\");\n      case Intrinsic::x86_aesenc128kl:\n        Opcode = X86ISD::AESENC128KL;\n        break;\n      case Intrinsic::x86_aesdec128kl:\n        Opcode = X86ISD::AESDEC128KL;\n        break;\n      case Intrinsic::x86_aesenc256kl:\n        Opcode = X86ISD::AESENC256KL;\n        break;\n      case Intrinsic::x86_aesdec256kl:\n        Opcode = X86ISD::AESDEC256KL;\n        break;\n      }\n\n      MemIntrinsicSDNode *MemIntr = cast<MemIntrinsicSDNode>(Op);\n      MachineMemOperand *MMO = MemIntr->getMemOperand();\n      EVT MemVT = MemIntr->getMemoryVT();\n      SDValue Operation = DAG.getMemIntrinsicNode(\n          Opcode, DL, VTs, {Chain, Op.getOperand(2), Op.getOperand(3)}, MemVT,\n          MMO);\n      SDValue ZF = getSETCC(X86::COND_E, Operation.getValue(1), DL, DAG);\n\n      return DAG.getNode(ISD::MERGE_VALUES, DL, Op->getVTList(),\n                         {ZF, Operation.getValue(0), Operation.getValue(2)});\n    }\n    case Intrinsic::x86_aesencwide128kl:\n    case Intrinsic::x86_aesdecwide128kl:\n    case Intrinsic::x86_aesencwide256kl:\n    case Intrinsic::x86_aesdecwide256kl: {\n      SDLoc DL(Op);\n      SDVTList VTs = DAG.getVTList(\n          {MVT::i32, MVT::v2i64, MVT::v2i64, MVT::v2i64, MVT::v2i64, MVT::v2i64,\n           MVT::v2i64, MVT::v2i64, MVT::v2i64, MVT::Other});\n      SDValue Chain = Op.getOperand(0);\n      unsigned Opcode;\n\n      switch (IntNo) {\n      default: llvm_unreachable(\"Impossible intrinsic\");\n      case Intrinsic::x86_aesencwide128kl:\n        Opcode = X86ISD::AESENCWIDE128KL;\n        break;\n      case Intrinsic::x86_aesdecwide128kl:\n        Opcode = X86ISD::AESDECWIDE128KL;\n        break;\n      case Intrinsic::x86_aesencwide256kl:\n        Opcode = X86ISD::AESENCWIDE256KL;\n        break;\n      case Intrinsic::x86_aesdecwide256kl:\n        Opcode = X86ISD::AESDECWIDE256KL;\n        break;\n      }\n\n      MemIntrinsicSDNode *MemIntr = cast<MemIntrinsicSDNode>(Op);\n      MachineMemOperand *MMO = MemIntr->getMemOperand();\n      EVT MemVT = MemIntr->getMemoryVT();\n      SDValue Operation = DAG.getMemIntrinsicNode(\n          Opcode, DL, VTs,\n          {Chain, Op.getOperand(2), Op.getOperand(3), Op.getOperand(4),\n           Op.getOperand(5), Op.getOperand(6), Op.getOperand(7),\n           Op.getOperand(8), Op.getOperand(9), Op.getOperand(10)},\n          MemVT, MMO);\n      SDValue ZF = getSETCC(X86::COND_E, Operation.getValue(0), DL, DAG);\n\n      return DAG.getNode(ISD::MERGE_VALUES, DL, Op->getVTList(),\n                         {ZF, Operation.getValue(1), Operation.getValue(2),\n                          Operation.getValue(3), Operation.getValue(4),\n                          Operation.getValue(5), Operation.getValue(6),\n                          Operation.getValue(7), Operation.getValue(8),\n                          Operation.getValue(9)});\n    }\n    case Intrinsic::x86_testui: {\n      SDLoc dl(Op);\n      SDValue Chain = Op.getOperand(0);\n      SDVTList VTs = DAG.getVTList(MVT::i32, MVT::Other);\n      SDValue Operation = DAG.getNode(X86ISD::TESTUI, dl, VTs, Chain);\n      SDValue SetCC = getSETCC(X86::COND_B, Operation.getValue(0), dl, DAG);\n      return DAG.getNode(ISD::MERGE_VALUES, dl, Op->getVTList(), SetCC,\n                         Operation.getValue(1));\n    }\n    }\n    return SDValue();\n  }\n\n  SDLoc dl(Op);\n  switch(IntrData->Type) {\n  default: llvm_unreachable(\"Unknown Intrinsic Type\");\n  case RDSEED:\n  case RDRAND: {\n    // Emit the node with the right value type.\n    SDVTList VTs = DAG.getVTList(Op->getValueType(0), MVT::i32, MVT::Other);\n    SDValue Result = DAG.getNode(IntrData->Opc0, dl, VTs, Op.getOperand(0));\n\n    // If the value returned by RDRAND/RDSEED was valid (CF=1), return 1.\n    // Otherwise return the value from Rand, which is always 0, casted to i32.\n    SDValue Ops[] = {DAG.getZExtOrTrunc(Result, dl, Op->getValueType(1)),\n                     DAG.getConstant(1, dl, Op->getValueType(1)),\n                     DAG.getTargetConstant(X86::COND_B, dl, MVT::i8),\n                     SDValue(Result.getNode(), 1)};\n    SDValue isValid = DAG.getNode(X86ISD::CMOV, dl, Op->getValueType(1), Ops);\n\n    // Return { result, isValid, chain }.\n    return DAG.getNode(ISD::MERGE_VALUES, dl, Op->getVTList(), Result, isValid,\n                       SDValue(Result.getNode(), 2));\n  }\n  case GATHER_AVX2: {\n    SDValue Chain = Op.getOperand(0);\n    SDValue Src   = Op.getOperand(2);\n    SDValue Base  = Op.getOperand(3);\n    SDValue Index = Op.getOperand(4);\n    SDValue Mask  = Op.getOperand(5);\n    SDValue Scale = Op.getOperand(6);\n    return getAVX2GatherNode(IntrData->Opc0, Op, DAG, Src, Mask, Base, Index,\n                             Scale, Chain, Subtarget);\n  }\n  case GATHER: {\n  //gather(v1, mask, index, base, scale);\n    SDValue Chain = Op.getOperand(0);\n    SDValue Src   = Op.getOperand(2);\n    SDValue Base  = Op.getOperand(3);\n    SDValue Index = Op.getOperand(4);\n    SDValue Mask  = Op.getOperand(5);\n    SDValue Scale = Op.getOperand(6);\n    return getGatherNode(Op, DAG, Src, Mask, Base, Index, Scale,\n                         Chain, Subtarget);\n  }\n  case SCATTER: {\n  //scatter(base, mask, index, v1, scale);\n    SDValue Chain = Op.getOperand(0);\n    SDValue Base  = Op.getOperand(2);\n    SDValue Mask  = Op.getOperand(3);\n    SDValue Index = Op.getOperand(4);\n    SDValue Src   = Op.getOperand(5);\n    SDValue Scale = Op.getOperand(6);\n    return getScatterNode(IntrData->Opc0, Op, DAG, Src, Mask, Base, Index,\n                          Scale, Chain, Subtarget);\n  }\n  case PREFETCH: {\n    const APInt &HintVal = Op.getConstantOperandAPInt(6);\n    assert((HintVal == 2 || HintVal == 3) &&\n           \"Wrong prefetch hint in intrinsic: should be 2 or 3\");\n    unsigned Opcode = (HintVal == 2 ? IntrData->Opc1 : IntrData->Opc0);\n    SDValue Chain = Op.getOperand(0);\n    SDValue Mask  = Op.getOperand(2);\n    SDValue Index = Op.getOperand(3);\n    SDValue Base  = Op.getOperand(4);\n    SDValue Scale = Op.getOperand(5);\n    return getPrefetchNode(Opcode, Op, DAG, Mask, Base, Index, Scale, Chain,\n                           Subtarget);\n  }\n  // Read Time Stamp Counter (RDTSC) and Processor ID (RDTSCP).\n  case RDTSC: {\n    SmallVector<SDValue, 2> Results;\n    getReadTimeStampCounter(Op.getNode(), dl, IntrData->Opc0, DAG, Subtarget,\n                            Results);\n    return DAG.getMergeValues(Results, dl);\n  }\n  // Read Performance Monitoring Counters.\n  case RDPMC:\n  // GetExtended Control Register.\n  case XGETBV: {\n    SmallVector<SDValue, 2> Results;\n\n    // RDPMC uses ECX to select the index of the performance counter to read.\n    // XGETBV uses ECX to select the index of the XCR register to return.\n    // The result is stored into registers EDX:EAX.\n    expandIntrinsicWChainHelper(Op.getNode(), dl, DAG, IntrData->Opc0, X86::ECX,\n                                Subtarget, Results);\n    return DAG.getMergeValues(Results, dl);\n  }\n  // XTEST intrinsics.\n  case XTEST: {\n    SDVTList VTs = DAG.getVTList(Op->getValueType(0), MVT::Other);\n    SDValue InTrans = DAG.getNode(IntrData->Opc0, dl, VTs, Op.getOperand(0));\n\n    SDValue SetCC = getSETCC(X86::COND_NE, InTrans, dl, DAG);\n    SDValue Ret = DAG.getNode(ISD::ZERO_EXTEND, dl, Op->getValueType(0), SetCC);\n    return DAG.getNode(ISD::MERGE_VALUES, dl, Op->getVTList(),\n                       Ret, SDValue(InTrans.getNode(), 1));\n  }\n  case TRUNCATE_TO_MEM_VI8:\n  case TRUNCATE_TO_MEM_VI16:\n  case TRUNCATE_TO_MEM_VI32: {\n    SDValue Mask = Op.getOperand(4);\n    SDValue DataToTruncate = Op.getOperand(3);\n    SDValue Addr = Op.getOperand(2);\n    SDValue Chain = Op.getOperand(0);\n\n    MemIntrinsicSDNode *MemIntr = dyn_cast<MemIntrinsicSDNode>(Op);\n    assert(MemIntr && \"Expected MemIntrinsicSDNode!\");\n\n    EVT MemVT  = MemIntr->getMemoryVT();\n\n    uint16_t TruncationOp = IntrData->Opc0;\n    switch (TruncationOp) {\n    case X86ISD::VTRUNC: {\n      if (isAllOnesConstant(Mask)) // return just a truncate store\n        return DAG.getTruncStore(Chain, dl, DataToTruncate, Addr, MemVT,\n                                 MemIntr->getMemOperand());\n\n      MVT MaskVT = MVT::getVectorVT(MVT::i1, MemVT.getVectorNumElements());\n      SDValue VMask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n      SDValue Offset = DAG.getUNDEF(VMask.getValueType());\n\n      return DAG.getMaskedStore(Chain, dl, DataToTruncate, Addr, Offset, VMask,\n                                MemVT, MemIntr->getMemOperand(), ISD::UNINDEXED,\n                                true /* truncating */);\n    }\n    case X86ISD::VTRUNCUS:\n    case X86ISD::VTRUNCS: {\n      bool IsSigned = (TruncationOp == X86ISD::VTRUNCS);\n      if (isAllOnesConstant(Mask))\n        return EmitTruncSStore(IsSigned, Chain, dl, DataToTruncate, Addr, MemVT,\n                               MemIntr->getMemOperand(), DAG);\n\n      MVT MaskVT = MVT::getVectorVT(MVT::i1, MemVT.getVectorNumElements());\n      SDValue VMask = getMaskNode(Mask, MaskVT, Subtarget, DAG, dl);\n\n      return EmitMaskedTruncSStore(IsSigned, Chain, dl, DataToTruncate, Addr,\n                                   VMask, MemVT, MemIntr->getMemOperand(), DAG);\n    }\n    default:\n      llvm_unreachable(\"Unsupported truncstore intrinsic\");\n    }\n  }\n  }\n}\n\nSDValue X86TargetLowering::LowerRETURNADDR(SDValue Op,\n                                           SelectionDAG &DAG) const {\n  MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();\n  MFI.setReturnAddressIsTaken(true);\n\n  if (verifyReturnAddressArgumentIsConstant(Op, DAG))\n    return SDValue();\n\n  unsigned Depth = Op.getConstantOperandVal(0);\n  SDLoc dl(Op);\n  EVT PtrVT = getPointerTy(DAG.getDataLayout());\n\n  if (Depth > 0) {\n    SDValue FrameAddr = LowerFRAMEADDR(Op, DAG);\n    const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n    SDValue Offset = DAG.getConstant(RegInfo->getSlotSize(), dl, PtrVT);\n    return DAG.getLoad(PtrVT, dl, DAG.getEntryNode(),\n                       DAG.getNode(ISD::ADD, dl, PtrVT, FrameAddr, Offset),\n                       MachinePointerInfo());\n  }\n\n  // Just load the return address.\n  SDValue RetAddrFI = getReturnAddressFrameIndex(DAG);\n  return DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), RetAddrFI,\n                     MachinePointerInfo());\n}\n\nSDValue X86TargetLowering::LowerADDROFRETURNADDR(SDValue Op,\n                                                 SelectionDAG &DAG) const {\n  DAG.getMachineFunction().getFrameInfo().setReturnAddressIsTaken(true);\n  return getReturnAddressFrameIndex(DAG);\n}\n\nSDValue X86TargetLowering::LowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  MachineFrameInfo &MFI = MF.getFrameInfo();\n  X86MachineFunctionInfo *FuncInfo = MF.getInfo<X86MachineFunctionInfo>();\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  EVT VT = Op.getValueType();\n\n  MFI.setFrameAddressIsTaken(true);\n\n  if (MF.getTarget().getMCAsmInfo()->usesWindowsCFI()) {\n    // Depth > 0 makes no sense on targets which use Windows unwind codes.  It\n    // is not possible to crawl up the stack without looking at the unwind codes\n    // simultaneously.\n    int FrameAddrIndex = FuncInfo->getFAIndex();\n    if (!FrameAddrIndex) {\n      // Set up a frame object for the return address.\n      unsigned SlotSize = RegInfo->getSlotSize();\n      FrameAddrIndex = MF.getFrameInfo().CreateFixedObject(\n          SlotSize, /*SPOffset=*/0, /*IsImmutable=*/false);\n      FuncInfo->setFAIndex(FrameAddrIndex);\n    }\n    return DAG.getFrameIndex(FrameAddrIndex, VT);\n  }\n\n  unsigned FrameReg =\n      RegInfo->getPtrSizedFrameRegister(DAG.getMachineFunction());\n  SDLoc dl(Op);  // FIXME probably not meaningful\n  unsigned Depth = Op.getConstantOperandVal(0);\n  assert(((FrameReg == X86::RBP && VT == MVT::i64) ||\n          (FrameReg == X86::EBP && VT == MVT::i32)) &&\n         \"Invalid Frame Register!\");\n  SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), dl, FrameReg, VT);\n  while (Depth--)\n    FrameAddr = DAG.getLoad(VT, dl, DAG.getEntryNode(), FrameAddr,\n                            MachinePointerInfo());\n  return FrameAddr;\n}\n\n// FIXME? Maybe this could be a TableGen attribute on some registers and\n// this table could be generated automatically from RegInfo.\nRegister X86TargetLowering::getRegisterByName(const char* RegName, LLT VT,\n                                              const MachineFunction &MF) const {\n  const TargetFrameLowering &TFI = *Subtarget.getFrameLowering();\n\n  Register Reg = StringSwitch<unsigned>(RegName)\n                       .Case(\"esp\", X86::ESP)\n                       .Case(\"rsp\", X86::RSP)\n                       .Case(\"ebp\", X86::EBP)\n                       .Case(\"rbp\", X86::RBP)\n                       .Default(0);\n\n  if (Reg == X86::EBP || Reg == X86::RBP) {\n    if (!TFI.hasFP(MF))\n      report_fatal_error(\"register \" + StringRef(RegName) +\n                         \" is allocatable: function has no frame pointer\");\n#ifndef NDEBUG\n    else {\n      const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n      Register FrameReg = RegInfo->getPtrSizedFrameRegister(MF);\n      assert((FrameReg == X86::EBP || FrameReg == X86::RBP) &&\n             \"Invalid Frame Register!\");\n    }\n#endif\n  }\n\n  if (Reg)\n    return Reg;\n\n  report_fatal_error(\"Invalid register name global variable\");\n}\n\nSDValue X86TargetLowering::LowerFRAME_TO_ARGS_OFFSET(SDValue Op,\n                                                     SelectionDAG &DAG) const {\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  return DAG.getIntPtrConstant(2 * RegInfo->getSlotSize(), SDLoc(Op));\n}\n\nRegister X86TargetLowering::getExceptionPointerRegister(\n    const Constant *PersonalityFn) const {\n  if (classifyEHPersonality(PersonalityFn) == EHPersonality::CoreCLR)\n    return Subtarget.isTarget64BitLP64() ? X86::RDX : X86::EDX;\n\n  return Subtarget.isTarget64BitLP64() ? X86::RAX : X86::EAX;\n}\n\nRegister X86TargetLowering::getExceptionSelectorRegister(\n    const Constant *PersonalityFn) const {\n  // Funclet personalities don't use selectors (the runtime does the selection).\n  assert(!isFuncletEHPersonality(classifyEHPersonality(PersonalityFn)));\n  return Subtarget.isTarget64BitLP64() ? X86::RDX : X86::EDX;\n}\n\nbool X86TargetLowering::needsFixedCatchObjects() const {\n  return Subtarget.isTargetWin64();\n}\n\nSDValue X86TargetLowering::LowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const {\n  SDValue Chain     = Op.getOperand(0);\n  SDValue Offset    = Op.getOperand(1);\n  SDValue Handler   = Op.getOperand(2);\n  SDLoc dl      (Op);\n\n  EVT PtrVT = getPointerTy(DAG.getDataLayout());\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  Register FrameReg = RegInfo->getFrameRegister(DAG.getMachineFunction());\n  assert(((FrameReg == X86::RBP && PtrVT == MVT::i64) ||\n          (FrameReg == X86::EBP && PtrVT == MVT::i32)) &&\n         \"Invalid Frame Register!\");\n  SDValue Frame = DAG.getCopyFromReg(DAG.getEntryNode(), dl, FrameReg, PtrVT);\n  Register StoreAddrReg = (PtrVT == MVT::i64) ? X86::RCX : X86::ECX;\n\n  SDValue StoreAddr = DAG.getNode(ISD::ADD, dl, PtrVT, Frame,\n                                 DAG.getIntPtrConstant(RegInfo->getSlotSize(),\n                                                       dl));\n  StoreAddr = DAG.getNode(ISD::ADD, dl, PtrVT, StoreAddr, Offset);\n  Chain = DAG.getStore(Chain, dl, Handler, StoreAddr, MachinePointerInfo());\n  Chain = DAG.getCopyToReg(Chain, dl, StoreAddrReg, StoreAddr);\n\n  return DAG.getNode(X86ISD::EH_RETURN, dl, MVT::Other, Chain,\n                     DAG.getRegister(StoreAddrReg, PtrVT));\n}\n\nSDValue X86TargetLowering::lowerEH_SJLJ_SETJMP(SDValue Op,\n                                               SelectionDAG &DAG) const {\n  SDLoc DL(Op);\n  // If the subtarget is not 64bit, we may need the global base reg\n  // after isel expand pseudo, i.e., after CGBR pass ran.\n  // Therefore, ask for the GlobalBaseReg now, so that the pass\n  // inserts the code for us in case we need it.\n  // Otherwise, we will end up in a situation where we will\n  // reference a virtual register that is not defined!\n  if (!Subtarget.is64Bit()) {\n    const X86InstrInfo *TII = Subtarget.getInstrInfo();\n    (void)TII->getGlobalBaseReg(&DAG.getMachineFunction());\n  }\n  return DAG.getNode(X86ISD::EH_SJLJ_SETJMP, DL,\n                     DAG.getVTList(MVT::i32, MVT::Other),\n                     Op.getOperand(0), Op.getOperand(1));\n}\n\nSDValue X86TargetLowering::lowerEH_SJLJ_LONGJMP(SDValue Op,\n                                                SelectionDAG &DAG) const {\n  SDLoc DL(Op);\n  return DAG.getNode(X86ISD::EH_SJLJ_LONGJMP, DL, MVT::Other,\n                     Op.getOperand(0), Op.getOperand(1));\n}\n\nSDValue X86TargetLowering::lowerEH_SJLJ_SETUP_DISPATCH(SDValue Op,\n                                                       SelectionDAG &DAG) const {\n  SDLoc DL(Op);\n  return DAG.getNode(X86ISD::EH_SJLJ_SETUP_DISPATCH, DL, MVT::Other,\n                     Op.getOperand(0));\n}\n\nstatic SDValue LowerADJUST_TRAMPOLINE(SDValue Op, SelectionDAG &DAG) {\n  return Op.getOperand(0);\n}\n\nSDValue X86TargetLowering::LowerINIT_TRAMPOLINE(SDValue Op,\n                                                SelectionDAG &DAG) const {\n  SDValue Root = Op.getOperand(0);\n  SDValue Trmp = Op.getOperand(1); // trampoline\n  SDValue FPtr = Op.getOperand(2); // nested function\n  SDValue Nest = Op.getOperand(3); // 'nest' parameter value\n  SDLoc dl (Op);\n\n  const Value *TrmpAddr = cast<SrcValueSDNode>(Op.getOperand(4))->getValue();\n  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n\n  if (Subtarget.is64Bit()) {\n    SDValue OutChains[6];\n\n    // Large code-model.\n    const unsigned char JMP64r  = 0xFF; // 64-bit jmp through register opcode.\n    const unsigned char MOV64ri = 0xB8; // X86::MOV64ri opcode.\n\n    const unsigned char N86R10 = TRI->getEncodingValue(X86::R10) & 0x7;\n    const unsigned char N86R11 = TRI->getEncodingValue(X86::R11) & 0x7;\n\n    const unsigned char REX_WB = 0x40 | 0x08 | 0x01; // REX prefix\n\n    // Load the pointer to the nested function into R11.\n    unsigned OpCode = ((MOV64ri | N86R11) << 8) | REX_WB; // movabsq r11\n    SDValue Addr = Trmp;\n    OutChains[0] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, dl, MVT::i16),\n                                Addr, MachinePointerInfo(TrmpAddr));\n\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,\n                       DAG.getConstant(2, dl, MVT::i64));\n    OutChains[1] = DAG.getStore(Root, dl, FPtr, Addr,\n                                MachinePointerInfo(TrmpAddr, 2), Align(2));\n\n    // Load the 'nest' parameter value into R10.\n    // R10 is specified in X86CallingConv.td\n    OpCode = ((MOV64ri | N86R10) << 8) | REX_WB; // movabsq r10\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,\n                       DAG.getConstant(10, dl, MVT::i64));\n    OutChains[2] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, dl, MVT::i16),\n                                Addr, MachinePointerInfo(TrmpAddr, 10));\n\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,\n                       DAG.getConstant(12, dl, MVT::i64));\n    OutChains[3] = DAG.getStore(Root, dl, Nest, Addr,\n                                MachinePointerInfo(TrmpAddr, 12), Align(2));\n\n    // Jump to the nested function.\n    OpCode = (JMP64r << 8) | REX_WB; // jmpq *...\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,\n                       DAG.getConstant(20, dl, MVT::i64));\n    OutChains[4] = DAG.getStore(Root, dl, DAG.getConstant(OpCode, dl, MVT::i16),\n                                Addr, MachinePointerInfo(TrmpAddr, 20));\n\n    unsigned char ModRM = N86R11 | (4 << 3) | (3 << 6); // ...r11\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i64, Trmp,\n                       DAG.getConstant(22, dl, MVT::i64));\n    OutChains[5] = DAG.getStore(Root, dl, DAG.getConstant(ModRM, dl, MVT::i8),\n                                Addr, MachinePointerInfo(TrmpAddr, 22));\n\n    return DAG.getNode(ISD::TokenFactor, dl, MVT::Other, OutChains);\n  } else {\n    const Function *Func =\n      cast<Function>(cast<SrcValueSDNode>(Op.getOperand(5))->getValue());\n    CallingConv::ID CC = Func->getCallingConv();\n    unsigned NestReg;\n\n    switch (CC) {\n    default:\n      llvm_unreachable(\"Unsupported calling convention\");\n    case CallingConv::C:\n    case CallingConv::X86_StdCall: {\n      // Pass 'nest' parameter in ECX.\n      // Must be kept in sync with X86CallingConv.td\n      NestReg = X86::ECX;\n\n      // Check that ECX wasn't needed by an 'inreg' parameter.\n      FunctionType *FTy = Func->getFunctionType();\n      const AttributeList &Attrs = Func->getAttributes();\n\n      if (!Attrs.isEmpty() && !Func->isVarArg()) {\n        unsigned InRegCount = 0;\n        unsigned Idx = 1;\n\n        for (FunctionType::param_iterator I = FTy->param_begin(),\n             E = FTy->param_end(); I != E; ++I, ++Idx)\n          if (Attrs.hasAttribute(Idx, Attribute::InReg)) {\n            const DataLayout &DL = DAG.getDataLayout();\n            // FIXME: should only count parameters that are lowered to integers.\n            InRegCount += (DL.getTypeSizeInBits(*I) + 31) / 32;\n          }\n\n        if (InRegCount > 2) {\n          report_fatal_error(\"Nest register in use - reduce number of inreg\"\n                             \" parameters!\");\n        }\n      }\n      break;\n    }\n    case CallingConv::X86_FastCall:\n    case CallingConv::X86_ThisCall:\n    case CallingConv::Fast:\n    case CallingConv::Tail:\n      // Pass 'nest' parameter in EAX.\n      // Must be kept in sync with X86CallingConv.td\n      NestReg = X86::EAX;\n      break;\n    }\n\n    SDValue OutChains[4];\n    SDValue Addr, Disp;\n\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i32, Trmp,\n                       DAG.getConstant(10, dl, MVT::i32));\n    Disp = DAG.getNode(ISD::SUB, dl, MVT::i32, FPtr, Addr);\n\n    // This is storing the opcode for MOV32ri.\n    const unsigned char MOV32ri = 0xB8; // X86::MOV32ri's opcode byte.\n    const unsigned char N86Reg = TRI->getEncodingValue(NestReg) & 0x7;\n    OutChains[0] =\n        DAG.getStore(Root, dl, DAG.getConstant(MOV32ri | N86Reg, dl, MVT::i8),\n                     Trmp, MachinePointerInfo(TrmpAddr));\n\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i32, Trmp,\n                       DAG.getConstant(1, dl, MVT::i32));\n    OutChains[1] = DAG.getStore(Root, dl, Nest, Addr,\n                                MachinePointerInfo(TrmpAddr, 1), Align(1));\n\n    const unsigned char JMP = 0xE9; // jmp <32bit dst> opcode.\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i32, Trmp,\n                       DAG.getConstant(5, dl, MVT::i32));\n    OutChains[2] =\n        DAG.getStore(Root, dl, DAG.getConstant(JMP, dl, MVT::i8), Addr,\n                     MachinePointerInfo(TrmpAddr, 5), Align(1));\n\n    Addr = DAG.getNode(ISD::ADD, dl, MVT::i32, Trmp,\n                       DAG.getConstant(6, dl, MVT::i32));\n    OutChains[3] = DAG.getStore(Root, dl, Disp, Addr,\n                                MachinePointerInfo(TrmpAddr, 6), Align(1));\n\n    return DAG.getNode(ISD::TokenFactor, dl, MVT::Other, OutChains);\n  }\n}\n\nSDValue X86TargetLowering::LowerFLT_ROUNDS_(SDValue Op,\n                                            SelectionDAG &DAG) const {\n  /*\n   The rounding mode is in bits 11:10 of FPSR, and has the following\n   settings:\n     00 Round to nearest\n     01 Round to -inf\n     10 Round to +inf\n     11 Round to 0\n\n  FLT_ROUNDS, on the other hand, expects the following:\n    -1 Undefined\n     0 Round to 0\n     1 Round to nearest\n     2 Round to +inf\n     3 Round to -inf\n\n  To perform the conversion, we use a packed lookup table of the four 2-bit\n  values that we can index by FPSP[11:10]\n    0x2d --> (0b00,10,11,01) --> (0,2,3,1) >> FPSR[11:10]\n\n    (0x2d >> ((FPSR & 0xc00) >> 9)) & 3\n  */\n\n  MachineFunction &MF = DAG.getMachineFunction();\n  MVT VT = Op.getSimpleValueType();\n  SDLoc DL(Op);\n\n  // Save FP Control Word to stack slot\n  int SSFI = MF.getFrameInfo().CreateStackObject(2, Align(2), false);\n  SDValue StackSlot =\n      DAG.getFrameIndex(SSFI, getPointerTy(DAG.getDataLayout()));\n\n  MachinePointerInfo MPI = MachinePointerInfo::getFixedStack(MF, SSFI);\n\n  SDValue Chain = Op.getOperand(0);\n  SDValue Ops[] = {Chain, StackSlot};\n  Chain = DAG.getMemIntrinsicNode(X86ISD::FNSTCW16m, DL,\n                                  DAG.getVTList(MVT::Other), Ops, MVT::i16, MPI,\n                                  Align(2), MachineMemOperand::MOStore);\n\n  // Load FP Control Word from stack slot\n  SDValue CWD = DAG.getLoad(MVT::i16, DL, Chain, StackSlot, MPI, Align(2));\n  Chain = CWD.getValue(1);\n\n  // Mask and turn the control bits into a shift for the lookup table.\n  SDValue Shift =\n    DAG.getNode(ISD::SRL, DL, MVT::i16,\n                DAG.getNode(ISD::AND, DL, MVT::i16,\n                            CWD, DAG.getConstant(0xc00, DL, MVT::i16)),\n                DAG.getConstant(9, DL, MVT::i8));\n  Shift = DAG.getNode(ISD::TRUNCATE, DL, MVT::i8, Shift);\n\n  SDValue LUT = DAG.getConstant(0x2d, DL, MVT::i32);\n  SDValue RetVal =\n    DAG.getNode(ISD::AND, DL, MVT::i32,\n                DAG.getNode(ISD::SRL, DL, MVT::i32, LUT, Shift),\n                DAG.getConstant(3, DL, MVT::i32));\n\n  RetVal = DAG.getZExtOrTrunc(RetVal, DL, VT);\n\n  return DAG.getMergeValues({RetVal, Chain}, DL);\n}\n\n/// Lower a vector CTLZ using native supported vector CTLZ instruction.\n//\n// i8/i16 vector implemented using dword LZCNT vector instruction\n// ( sub(trunc(lzcnt(zext32(x)))) ). In case zext32(x) is illegal,\n// split the vector, perform operation on it's Lo a Hi part and\n// concatenate the results.\nstatic SDValue LowerVectorCTLZ_AVX512CDI(SDValue Op, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  assert(Op.getOpcode() == ISD::CTLZ);\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  unsigned NumElems = VT.getVectorNumElements();\n\n  assert((EltVT == MVT::i8 || EltVT == MVT::i16) &&\n          \"Unsupported element type\");\n\n  // Split vector, it's Lo and Hi parts will be handled in next iteration.\n  if (NumElems > 16 ||\n      (NumElems == 16 && !Subtarget.canExtendTo512DQ()))\n    return splitVectorIntUnary(Op, DAG);\n\n  MVT NewVT = MVT::getVectorVT(MVT::i32, NumElems);\n  assert((NewVT.is256BitVector() || NewVT.is512BitVector()) &&\n          \"Unsupported value type for operation\");\n\n  // Use native supported vector instruction vplzcntd.\n  Op = DAG.getNode(ISD::ZERO_EXTEND, dl, NewVT, Op.getOperand(0));\n  SDValue CtlzNode = DAG.getNode(ISD::CTLZ, dl, NewVT, Op);\n  SDValue TruncNode = DAG.getNode(ISD::TRUNCATE, dl, VT, CtlzNode);\n  SDValue Delta = DAG.getConstant(32 - EltVT.getSizeInBits(), dl, VT);\n\n  return DAG.getNode(ISD::SUB, dl, VT, TruncNode, Delta);\n}\n\n// Lower CTLZ using a PSHUFB lookup table implementation.\nstatic SDValue LowerVectorCTLZInRegLUT(SDValue Op, const SDLoc &DL,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  int NumElts = VT.getVectorNumElements();\n  int NumBytes = NumElts * (VT.getScalarSizeInBits() / 8);\n  MVT CurrVT = MVT::getVectorVT(MVT::i8, NumBytes);\n\n  // Per-nibble leading zero PSHUFB lookup table.\n  const int LUT[16] = {/* 0 */ 4, /* 1 */ 3, /* 2 */ 2, /* 3 */ 2,\n                       /* 4 */ 1, /* 5 */ 1, /* 6 */ 1, /* 7 */ 1,\n                       /* 8 */ 0, /* 9 */ 0, /* a */ 0, /* b */ 0,\n                       /* c */ 0, /* d */ 0, /* e */ 0, /* f */ 0};\n\n  SmallVector<SDValue, 64> LUTVec;\n  for (int i = 0; i < NumBytes; ++i)\n    LUTVec.push_back(DAG.getConstant(LUT[i % 16], DL, MVT::i8));\n  SDValue InRegLUT = DAG.getBuildVector(CurrVT, DL, LUTVec);\n\n  // Begin by bitcasting the input to byte vector, then split those bytes\n  // into lo/hi nibbles and use the PSHUFB LUT to perform CLTZ on each of them.\n  // If the hi input nibble is zero then we add both results together, otherwise\n  // we just take the hi result (by masking the lo result to zero before the\n  // add).\n  SDValue Op0 = DAG.getBitcast(CurrVT, Op.getOperand(0));\n  SDValue Zero = DAG.getConstant(0, DL, CurrVT);\n\n  SDValue NibbleShift = DAG.getConstant(0x4, DL, CurrVT);\n  SDValue Lo = Op0;\n  SDValue Hi = DAG.getNode(ISD::SRL, DL, CurrVT, Op0, NibbleShift);\n  SDValue HiZ;\n  if (CurrVT.is512BitVector()) {\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, CurrVT.getVectorNumElements());\n    HiZ = DAG.getSetCC(DL, MaskVT, Hi, Zero, ISD::SETEQ);\n    HiZ = DAG.getNode(ISD::SIGN_EXTEND, DL, CurrVT, HiZ);\n  } else {\n    HiZ = DAG.getSetCC(DL, CurrVT, Hi, Zero, ISD::SETEQ);\n  }\n\n  Lo = DAG.getNode(X86ISD::PSHUFB, DL, CurrVT, InRegLUT, Lo);\n  Hi = DAG.getNode(X86ISD::PSHUFB, DL, CurrVT, InRegLUT, Hi);\n  Lo = DAG.getNode(ISD::AND, DL, CurrVT, Lo, HiZ);\n  SDValue Res = DAG.getNode(ISD::ADD, DL, CurrVT, Lo, Hi);\n\n  // Merge result back from vXi8 back to VT, working on the lo/hi halves\n  // of the current vector width in the same way we did for the nibbles.\n  // If the upper half of the input element is zero then add the halves'\n  // leading zero counts together, otherwise just use the upper half's.\n  // Double the width of the result until we are at target width.\n  while (CurrVT != VT) {\n    int CurrScalarSizeInBits = CurrVT.getScalarSizeInBits();\n    int CurrNumElts = CurrVT.getVectorNumElements();\n    MVT NextSVT = MVT::getIntegerVT(CurrScalarSizeInBits * 2);\n    MVT NextVT = MVT::getVectorVT(NextSVT, CurrNumElts / 2);\n    SDValue Shift = DAG.getConstant(CurrScalarSizeInBits, DL, NextVT);\n\n    // Check if the upper half of the input element is zero.\n    if (CurrVT.is512BitVector()) {\n      MVT MaskVT = MVT::getVectorVT(MVT::i1, CurrVT.getVectorNumElements());\n      HiZ = DAG.getSetCC(DL, MaskVT, DAG.getBitcast(CurrVT, Op0),\n                         DAG.getBitcast(CurrVT, Zero), ISD::SETEQ);\n      HiZ = DAG.getNode(ISD::SIGN_EXTEND, DL, CurrVT, HiZ);\n    } else {\n      HiZ = DAG.getSetCC(DL, CurrVT, DAG.getBitcast(CurrVT, Op0),\n                         DAG.getBitcast(CurrVT, Zero), ISD::SETEQ);\n    }\n    HiZ = DAG.getBitcast(NextVT, HiZ);\n\n    // Move the upper/lower halves to the lower bits as we'll be extending to\n    // NextVT. Mask the lower result to zero if HiZ is true and add the results\n    // together.\n    SDValue ResNext = Res = DAG.getBitcast(NextVT, Res);\n    SDValue R0 = DAG.getNode(ISD::SRL, DL, NextVT, ResNext, Shift);\n    SDValue R1 = DAG.getNode(ISD::SRL, DL, NextVT, HiZ, Shift);\n    R1 = DAG.getNode(ISD::AND, DL, NextVT, ResNext, R1);\n    Res = DAG.getNode(ISD::ADD, DL, NextVT, R0, R1);\n    CurrVT = NextVT;\n  }\n\n  return Res;\n}\n\nstatic SDValue LowerVectorCTLZ(SDValue Op, const SDLoc &DL,\n                               const X86Subtarget &Subtarget,\n                               SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n\n  if (Subtarget.hasCDI() &&\n      // vXi8 vectors need to be promoted to 512-bits for vXi32.\n      (Subtarget.canExtendTo512DQ() || VT.getVectorElementType() != MVT::i8))\n    return LowerVectorCTLZ_AVX512CDI(Op, DAG, Subtarget);\n\n  // Decompose 256-bit ops into smaller 128-bit ops.\n  if (VT.is256BitVector() && !Subtarget.hasInt256())\n    return splitVectorIntUnary(Op, DAG);\n\n  // Decompose 512-bit ops into smaller 256-bit ops.\n  if (VT.is512BitVector() && !Subtarget.hasBWI())\n    return splitVectorIntUnary(Op, DAG);\n\n  assert(Subtarget.hasSSSE3() && \"Expected SSSE3 support for PSHUFB\");\n  return LowerVectorCTLZInRegLUT(Op, DL, Subtarget, DAG);\n}\n\nstatic SDValue LowerCTLZ(SDValue Op, const X86Subtarget &Subtarget,\n                         SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  MVT OpVT = VT;\n  unsigned NumBits = VT.getSizeInBits();\n  SDLoc dl(Op);\n  unsigned Opc = Op.getOpcode();\n\n  if (VT.isVector())\n    return LowerVectorCTLZ(Op, dl, Subtarget, DAG);\n\n  Op = Op.getOperand(0);\n  if (VT == MVT::i8) {\n    // Zero extend to i32 since there is not an i8 bsr.\n    OpVT = MVT::i32;\n    Op = DAG.getNode(ISD::ZERO_EXTEND, dl, OpVT, Op);\n  }\n\n  // Issue a bsr (scan bits in reverse) which also sets EFLAGS.\n  SDVTList VTs = DAG.getVTList(OpVT, MVT::i32);\n  Op = DAG.getNode(X86ISD::BSR, dl, VTs, Op);\n\n  if (Opc == ISD::CTLZ) {\n    // If src is zero (i.e. bsr sets ZF), returns NumBits.\n    SDValue Ops[] = {Op, DAG.getConstant(NumBits + NumBits - 1, dl, OpVT),\n                     DAG.getTargetConstant(X86::COND_E, dl, MVT::i8),\n                     Op.getValue(1)};\n    Op = DAG.getNode(X86ISD::CMOV, dl, OpVT, Ops);\n  }\n\n  // Finally xor with NumBits-1.\n  Op = DAG.getNode(ISD::XOR, dl, OpVT, Op,\n                   DAG.getConstant(NumBits - 1, dl, OpVT));\n\n  if (VT == MVT::i8)\n    Op = DAG.getNode(ISD::TRUNCATE, dl, MVT::i8, Op);\n  return Op;\n}\n\nstatic SDValue LowerCTTZ(SDValue Op, const X86Subtarget &Subtarget,\n                         SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  unsigned NumBits = VT.getScalarSizeInBits();\n  SDValue N0 = Op.getOperand(0);\n  SDLoc dl(Op);\n\n  assert(!VT.isVector() && Op.getOpcode() == ISD::CTTZ &&\n         \"Only scalar CTTZ requires custom lowering\");\n\n  // Issue a bsf (scan bits forward) which also sets EFLAGS.\n  SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n  Op = DAG.getNode(X86ISD::BSF, dl, VTs, N0);\n\n  // If src is zero (i.e. bsf sets ZF), returns NumBits.\n  SDValue Ops[] = {Op, DAG.getConstant(NumBits, dl, VT),\n                   DAG.getTargetConstant(X86::COND_E, dl, MVT::i8),\n                   Op.getValue(1)};\n  return DAG.getNode(X86ISD::CMOV, dl, VT, Ops);\n}\n\nstatic SDValue lowerAddSub(SDValue Op, SelectionDAG &DAG,\n                           const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  if (VT == MVT::i16 || VT == MVT::i32)\n    return lowerAddSubToHorizontalOp(Op, DAG, Subtarget);\n\n  if (VT.getScalarType() == MVT::i1)\n    return DAG.getNode(ISD::XOR, SDLoc(Op), VT,\n                       Op.getOperand(0), Op.getOperand(1));\n\n  if (VT == MVT::v32i16 || VT == MVT::v64i8)\n    return splitVectorIntBinary(Op, DAG);\n\n  assert(Op.getSimpleValueType().is256BitVector() &&\n         Op.getSimpleValueType().isInteger() &&\n         \"Only handle AVX 256-bit vector integer operation\");\n  return splitVectorIntBinary(Op, DAG);\n}\n\nstatic SDValue LowerADDSAT_SUBSAT(SDValue Op, SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  SDValue X = Op.getOperand(0), Y = Op.getOperand(1);\n  unsigned Opcode = Op.getOpcode();\n  SDLoc DL(Op);\n\n  if (VT.getScalarType() == MVT::i1) {\n    switch (Opcode) {\n    default: llvm_unreachable(\"Expected saturated arithmetic opcode\");\n    case ISD::UADDSAT:\n    case ISD::SADDSAT:\n      // *addsat i1 X, Y --> X | Y\n      return DAG.getNode(ISD::OR, DL, VT, X, Y);\n    case ISD::USUBSAT:\n    case ISD::SSUBSAT:\n      // *subsat i1 X, Y --> X & ~Y\n      return DAG.getNode(ISD::AND, DL, VT, X, DAG.getNOT(DL, Y, VT));\n    }\n  }\n\n  if (VT == MVT::v32i16 || VT == MVT::v64i8 ||\n      (VT.is256BitVector() && !Subtarget.hasInt256())) {\n    assert(Op.getSimpleValueType().isInteger() &&\n           \"Only handle AVX vector integer operation\");\n    return splitVectorIntBinary(Op, DAG);\n  }\n\n  // Avoid the generic expansion with min/max if we don't have pminu*/pmaxu*.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  EVT SetCCResultType =\n      TLI.getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(), VT);\n\n  if (Opcode == ISD::USUBSAT && !TLI.isOperationLegal(ISD::UMAX, VT)) {\n    // usubsat X, Y --> (X >u Y) ? X - Y : 0\n    SDValue Sub = DAG.getNode(ISD::SUB, DL, VT, X, Y);\n    SDValue Cmp = DAG.getSetCC(DL, SetCCResultType, X, Y, ISD::SETUGT);\n    // TODO: Move this to DAGCombiner?\n    if (SetCCResultType == VT &&\n        DAG.ComputeNumSignBits(Cmp) == VT.getScalarSizeInBits())\n      return DAG.getNode(ISD::AND, DL, VT, Cmp, Sub);\n    return DAG.getSelect(DL, VT, Cmp, Sub, DAG.getConstant(0, DL, VT));\n  }\n\n  // Use default expansion.\n  return SDValue();\n}\n\nstatic SDValue LowerABS(SDValue Op, const X86Subtarget &Subtarget,\n                        SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  if (VT == MVT::i16 || VT == MVT::i32 || VT == MVT::i64) {\n    // Since X86 does not have CMOV for 8-bit integer, we don't convert\n    // 8-bit integer abs to NEG and CMOV.\n    SDLoc DL(Op);\n    SDValue N0 = Op.getOperand(0);\n    SDValue Neg = DAG.getNode(X86ISD::SUB, DL, DAG.getVTList(VT, MVT::i32),\n                              DAG.getConstant(0, DL, VT), N0);\n    SDValue Ops[] = {N0, Neg, DAG.getTargetConstant(X86::COND_GE, DL, MVT::i8),\n                     SDValue(Neg.getNode(), 1)};\n    return DAG.getNode(X86ISD::CMOV, DL, VT, Ops);\n  }\n\n  // ABS(vXi64 X) --> VPBLENDVPD(X, 0-X, X).\n  if ((VT == MVT::v2i64 || VT == MVT::v4i64) && Subtarget.hasSSE41()) {\n    SDLoc DL(Op);\n    SDValue Src = Op.getOperand(0);\n    SDValue Sub =\n        DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT), Src);\n    return DAG.getNode(X86ISD::BLENDV, DL, VT, Src, Sub, Src);\n  }\n\n  if (VT.is256BitVector() && !Subtarget.hasInt256()) {\n    assert(VT.isInteger() &&\n           \"Only handle AVX 256-bit vector integer operation\");\n    return splitVectorIntUnary(Op, DAG);\n  }\n\n  if ((VT == MVT::v32i16 || VT == MVT::v64i8) && !Subtarget.hasBWI())\n    return splitVectorIntUnary(Op, DAG);\n\n  // Default to expand.\n  return SDValue();\n}\n\nstatic SDValue LowerMINMAX(SDValue Op, SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n\n  // For AVX1 cases, split to use legal ops (everything but v4i64).\n  if (VT.getScalarType() != MVT::i64 && VT.is256BitVector())\n    return splitVectorIntBinary(Op, DAG);\n\n  if (VT == MVT::v32i16 || VT == MVT::v64i8)\n    return splitVectorIntBinary(Op, DAG);\n\n  // Default to expand.\n  return SDValue();\n}\n\nstatic SDValue LowerMUL(SDValue Op, const X86Subtarget &Subtarget,\n                        SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n\n  if (VT.getScalarType() == MVT::i1)\n    return DAG.getNode(ISD::AND, dl, VT, Op.getOperand(0), Op.getOperand(1));\n\n  // Decompose 256-bit ops into 128-bit ops.\n  if (VT.is256BitVector() && !Subtarget.hasInt256())\n    return splitVectorIntBinary(Op, DAG);\n\n  if ((VT == MVT::v32i16 || VT == MVT::v64i8) && !Subtarget.hasBWI())\n    return splitVectorIntBinary(Op, DAG);\n\n  SDValue A = Op.getOperand(0);\n  SDValue B = Op.getOperand(1);\n\n  // Lower v16i8/v32i8/v64i8 mul as sign-extension to v8i16/v16i16/v32i16\n  // vector pairs, multiply and truncate.\n  if (VT == MVT::v16i8 || VT == MVT::v32i8 || VT == MVT::v64i8) {\n    unsigned NumElts = VT.getVectorNumElements();\n\n    if ((VT == MVT::v16i8 && Subtarget.hasInt256()) ||\n        (VT == MVT::v32i8 && Subtarget.canExtendTo512BW())) {\n      MVT ExVT = MVT::getVectorVT(MVT::i16, VT.getVectorNumElements());\n      return DAG.getNode(\n          ISD::TRUNCATE, dl, VT,\n          DAG.getNode(ISD::MUL, dl, ExVT,\n                      DAG.getNode(ISD::ANY_EXTEND, dl, ExVT, A),\n                      DAG.getNode(ISD::ANY_EXTEND, dl, ExVT, B)));\n    }\n\n    MVT ExVT = MVT::getVectorVT(MVT::i16, NumElts / 2);\n\n    // Extract the lo/hi parts to any extend to i16.\n    // We're going to mask off the low byte of each result element of the\n    // pmullw, so it doesn't matter what's in the high byte of each 16-bit\n    // element.\n    SDValue Undef = DAG.getUNDEF(VT);\n    SDValue ALo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, A, Undef));\n    SDValue AHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, A, Undef));\n\n    SDValue BLo, BHi;\n    if (ISD::isBuildVectorOfConstantSDNodes(B.getNode())) {\n      // If the LHS is a constant, manually unpackl/unpackh.\n      SmallVector<SDValue, 16> LoOps, HiOps;\n      for (unsigned i = 0; i != NumElts; i += 16) {\n        for (unsigned j = 0; j != 8; ++j) {\n          LoOps.push_back(DAG.getAnyExtOrTrunc(B.getOperand(i + j), dl,\n                                               MVT::i16));\n          HiOps.push_back(DAG.getAnyExtOrTrunc(B.getOperand(i + j + 8), dl,\n                                               MVT::i16));\n        }\n      }\n\n      BLo = DAG.getBuildVector(ExVT, dl, LoOps);\n      BHi = DAG.getBuildVector(ExVT, dl, HiOps);\n    } else {\n      BLo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, B, Undef));\n      BHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, B, Undef));\n    }\n\n    // Multiply, mask the lower 8bits of the lo/hi results and pack.\n    SDValue RLo = DAG.getNode(ISD::MUL, dl, ExVT, ALo, BLo);\n    SDValue RHi = DAG.getNode(ISD::MUL, dl, ExVT, AHi, BHi);\n    RLo = DAG.getNode(ISD::AND, dl, ExVT, RLo, DAG.getConstant(255, dl, ExVT));\n    RHi = DAG.getNode(ISD::AND, dl, ExVT, RHi, DAG.getConstant(255, dl, ExVT));\n    return DAG.getNode(X86ISD::PACKUS, dl, VT, RLo, RHi);\n  }\n\n  // Lower v4i32 mul as 2x shuffle, 2x pmuludq, 2x shuffle.\n  if (VT == MVT::v4i32) {\n    assert(Subtarget.hasSSE2() && !Subtarget.hasSSE41() &&\n           \"Should not custom lower when pmulld is available!\");\n\n    // Extract the odd parts.\n    static const int UnpackMask[] = { 1, -1, 3, -1 };\n    SDValue Aodds = DAG.getVectorShuffle(VT, dl, A, A, UnpackMask);\n    SDValue Bodds = DAG.getVectorShuffle(VT, dl, B, B, UnpackMask);\n\n    // Multiply the even parts.\n    SDValue Evens = DAG.getNode(X86ISD::PMULUDQ, dl, MVT::v2i64,\n                                DAG.getBitcast(MVT::v2i64, A),\n                                DAG.getBitcast(MVT::v2i64, B));\n    // Now multiply odd parts.\n    SDValue Odds = DAG.getNode(X86ISD::PMULUDQ, dl, MVT::v2i64,\n                               DAG.getBitcast(MVT::v2i64, Aodds),\n                               DAG.getBitcast(MVT::v2i64, Bodds));\n\n    Evens = DAG.getBitcast(VT, Evens);\n    Odds = DAG.getBitcast(VT, Odds);\n\n    // Merge the two vectors back together with a shuffle. This expands into 2\n    // shuffles.\n    static const int ShufMask[] = { 0, 4, 2, 6 };\n    return DAG.getVectorShuffle(VT, dl, Evens, Odds, ShufMask);\n  }\n\n  assert((VT == MVT::v2i64 || VT == MVT::v4i64 || VT == MVT::v8i64) &&\n         \"Only know how to lower V2I64/V4I64/V8I64 multiply\");\n  assert(!Subtarget.hasDQI() && \"DQI should use MULLQ\");\n\n  //  Ahi = psrlqi(a, 32);\n  //  Bhi = psrlqi(b, 32);\n  //\n  //  AloBlo = pmuludq(a, b);\n  //  AloBhi = pmuludq(a, Bhi);\n  //  AhiBlo = pmuludq(Ahi, b);\n  //\n  //  Hi = psllqi(AloBhi + AhiBlo, 32);\n  //  return AloBlo + Hi;\n  KnownBits AKnown = DAG.computeKnownBits(A);\n  KnownBits BKnown = DAG.computeKnownBits(B);\n\n  APInt LowerBitsMask = APInt::getLowBitsSet(64, 32);\n  bool ALoIsZero = LowerBitsMask.isSubsetOf(AKnown.Zero);\n  bool BLoIsZero = LowerBitsMask.isSubsetOf(BKnown.Zero);\n\n  APInt UpperBitsMask = APInt::getHighBitsSet(64, 32);\n  bool AHiIsZero = UpperBitsMask.isSubsetOf(AKnown.Zero);\n  bool BHiIsZero = UpperBitsMask.isSubsetOf(BKnown.Zero);\n\n  SDValue Zero = DAG.getConstant(0, dl, VT);\n\n  // Only multiply lo/hi halves that aren't known to be zero.\n  SDValue AloBlo = Zero;\n  if (!ALoIsZero && !BLoIsZero)\n    AloBlo = DAG.getNode(X86ISD::PMULUDQ, dl, VT, A, B);\n\n  SDValue AloBhi = Zero;\n  if (!ALoIsZero && !BHiIsZero) {\n    SDValue Bhi = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, VT, B, 32, DAG);\n    AloBhi = DAG.getNode(X86ISD::PMULUDQ, dl, VT, A, Bhi);\n  }\n\n  SDValue AhiBlo = Zero;\n  if (!AHiIsZero && !BLoIsZero) {\n    SDValue Ahi = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, VT, A, 32, DAG);\n    AhiBlo = DAG.getNode(X86ISD::PMULUDQ, dl, VT, Ahi, B);\n  }\n\n  SDValue Hi = DAG.getNode(ISD::ADD, dl, VT, AloBhi, AhiBlo);\n  Hi = getTargetVShiftByConstNode(X86ISD::VSHLI, dl, VT, Hi, 32, DAG);\n\n  return DAG.getNode(ISD::ADD, dl, VT, AloBlo, Hi);\n}\n\nstatic SDValue LowerMULH(SDValue Op, const X86Subtarget &Subtarget,\n                         SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n  bool IsSigned = Op->getOpcode() == ISD::MULHS;\n  unsigned NumElts = VT.getVectorNumElements();\n  SDValue A = Op.getOperand(0);\n  SDValue B = Op.getOperand(1);\n\n  // Decompose 256-bit ops into 128-bit ops.\n  if (VT.is256BitVector() && !Subtarget.hasInt256())\n    return splitVectorIntBinary(Op, DAG);\n\n  if ((VT == MVT::v32i16 || VT == MVT::v64i8) && !Subtarget.hasBWI())\n    return splitVectorIntBinary(Op, DAG);\n\n  if (VT == MVT::v4i32 || VT == MVT::v8i32 || VT == MVT::v16i32) {\n    assert((VT == MVT::v4i32 && Subtarget.hasSSE2()) ||\n           (VT == MVT::v8i32 && Subtarget.hasInt256()) ||\n           (VT == MVT::v16i32 && Subtarget.hasAVX512()));\n\n    // PMULxD operations multiply each even value (starting at 0) of LHS with\n    // the related value of RHS and produce a widen result.\n    // E.g., PMULUDQ <4 x i32> <a|b|c|d>, <4 x i32> <e|f|g|h>\n    // => <2 x i64> <ae|cg>\n    //\n    // In other word, to have all the results, we need to perform two PMULxD:\n    // 1. one with the even values.\n    // 2. one with the odd values.\n    // To achieve #2, with need to place the odd values at an even position.\n    //\n    // Place the odd value at an even position (basically, shift all values 1\n    // step to the left):\n    const int Mask[] = {1, -1,  3, -1,  5, -1,  7, -1,\n                        9, -1, 11, -1, 13, -1, 15, -1};\n    // <a|b|c|d> => <b|undef|d|undef>\n    SDValue Odd0 = DAG.getVectorShuffle(VT, dl, A, A,\n                                        makeArrayRef(&Mask[0], NumElts));\n    // <e|f|g|h> => <f|undef|h|undef>\n    SDValue Odd1 = DAG.getVectorShuffle(VT, dl, B, B,\n                                        makeArrayRef(&Mask[0], NumElts));\n\n    // Emit two multiplies, one for the lower 2 ints and one for the higher 2\n    // ints.\n    MVT MulVT = MVT::getVectorVT(MVT::i64, NumElts / 2);\n    unsigned Opcode =\n        (IsSigned && Subtarget.hasSSE41()) ? X86ISD::PMULDQ : X86ISD::PMULUDQ;\n    // PMULUDQ <4 x i32> <a|b|c|d>, <4 x i32> <e|f|g|h>\n    // => <2 x i64> <ae|cg>\n    SDValue Mul1 = DAG.getBitcast(VT, DAG.getNode(Opcode, dl, MulVT,\n                                                  DAG.getBitcast(MulVT, A),\n                                                  DAG.getBitcast(MulVT, B)));\n    // PMULUDQ <4 x i32> <b|undef|d|undef>, <4 x i32> <f|undef|h|undef>\n    // => <2 x i64> <bf|dh>\n    SDValue Mul2 = DAG.getBitcast(VT, DAG.getNode(Opcode, dl, MulVT,\n                                                  DAG.getBitcast(MulVT, Odd0),\n                                                  DAG.getBitcast(MulVT, Odd1)));\n\n    // Shuffle it back into the right order.\n    SmallVector<int, 16> ShufMask(NumElts);\n    for (int i = 0; i != (int)NumElts; ++i)\n      ShufMask[i] = (i / 2) * 2 + ((i % 2) * NumElts) + 1;\n\n    SDValue Res = DAG.getVectorShuffle(VT, dl, Mul1, Mul2, ShufMask);\n\n    // If we have a signed multiply but no PMULDQ fix up the result of an\n    // unsigned multiply.\n    if (IsSigned && !Subtarget.hasSSE41()) {\n      SDValue Zero = DAG.getConstant(0, dl, VT);\n      SDValue T1 = DAG.getNode(ISD::AND, dl, VT,\n                               DAG.getSetCC(dl, VT, Zero, A, ISD::SETGT), B);\n      SDValue T2 = DAG.getNode(ISD::AND, dl, VT,\n                               DAG.getSetCC(dl, VT, Zero, B, ISD::SETGT), A);\n\n      SDValue Fixup = DAG.getNode(ISD::ADD, dl, VT, T1, T2);\n      Res = DAG.getNode(ISD::SUB, dl, VT, Res, Fixup);\n    }\n\n    return Res;\n  }\n\n  // Only i8 vectors should need custom lowering after this.\n  assert((VT == MVT::v16i8 || (VT == MVT::v32i8 && Subtarget.hasInt256()) ||\n         (VT == MVT::v64i8 && Subtarget.hasBWI())) &&\n         \"Unsupported vector type\");\n\n  // Lower v16i8/v32i8 as extension to v8i16/v16i16 vector pairs, multiply,\n  // logical shift down the upper half and pack back to i8.\n\n  // With SSE41 we can use sign/zero extend, but for pre-SSE41 we unpack\n  // and then ashr/lshr the upper bits down to the lower bits before multiply.\n  unsigned ExAVX = IsSigned ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;\n\n  if ((VT == MVT::v16i8 && Subtarget.hasInt256()) ||\n      (VT == MVT::v32i8 && Subtarget.canExtendTo512BW())) {\n    MVT ExVT = MVT::getVectorVT(MVT::i16, NumElts);\n    SDValue ExA = DAG.getNode(ExAVX, dl, ExVT, A);\n    SDValue ExB = DAG.getNode(ExAVX, dl, ExVT, B);\n    SDValue Mul = DAG.getNode(ISD::MUL, dl, ExVT, ExA, ExB);\n    Mul = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExVT, Mul, 8, DAG);\n    return DAG.getNode(ISD::TRUNCATE, dl, VT, Mul);\n  }\n\n  // For vXi8 we will unpack the low and high half of each 128 bit lane to widen\n  // to a vXi16 type. Do the multiplies, shift the results and pack the half\n  // lane results back together.\n\n  MVT ExVT = MVT::getVectorVT(MVT::i16, NumElts / 2);\n\n  static const int PSHUFDMask[] = { 8,  9, 10, 11, 12, 13, 14, 15,\n                                   -1, -1, -1, -1, -1, -1, -1, -1};\n\n  // Extract the lo parts and zero/sign extend to i16.\n  // Only use SSE4.1 instructions for signed v16i8 where using unpack requires\n  // shifts to sign extend. Using unpack for unsigned only requires an xor to\n  // create zeros and a copy due to tied registers contraints pre-avx. But using\n  // zero_extend_vector_inreg would require an additional pshufd for the high\n  // part.\n\n  SDValue ALo, AHi;\n  if (IsSigned && VT == MVT::v16i8 && Subtarget.hasSSE41()) {\n    ALo = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, ExVT, A);\n\n    AHi = DAG.getVectorShuffle(VT, dl, A, A, PSHUFDMask);\n    AHi = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, ExVT, AHi);\n  } else if (IsSigned) {\n    ALo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, DAG.getUNDEF(VT), A));\n    AHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, DAG.getUNDEF(VT), A));\n\n    ALo = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, ALo, 8, DAG);\n    AHi = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, AHi, 8, DAG);\n  } else {\n    ALo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, A,\n                                          DAG.getConstant(0, dl, VT)));\n    AHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, A,\n                                          DAG.getConstant(0, dl, VT)));\n  }\n\n  SDValue BLo, BHi;\n  if (ISD::isBuildVectorOfConstantSDNodes(B.getNode())) {\n    // If the LHS is a constant, manually unpackl/unpackh and extend.\n    SmallVector<SDValue, 16> LoOps, HiOps;\n    for (unsigned i = 0; i != NumElts; i += 16) {\n      for (unsigned j = 0; j != 8; ++j) {\n        SDValue LoOp = B.getOperand(i + j);\n        SDValue HiOp = B.getOperand(i + j + 8);\n\n        if (IsSigned) {\n          LoOp = DAG.getSExtOrTrunc(LoOp, dl, MVT::i16);\n          HiOp = DAG.getSExtOrTrunc(HiOp, dl, MVT::i16);\n        } else {\n          LoOp = DAG.getZExtOrTrunc(LoOp, dl, MVT::i16);\n          HiOp = DAG.getZExtOrTrunc(HiOp, dl, MVT::i16);\n        }\n\n        LoOps.push_back(LoOp);\n        HiOps.push_back(HiOp);\n      }\n    }\n\n    BLo = DAG.getBuildVector(ExVT, dl, LoOps);\n    BHi = DAG.getBuildVector(ExVT, dl, HiOps);\n  } else if (IsSigned && VT == MVT::v16i8 && Subtarget.hasSSE41()) {\n    BLo = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, ExVT, B);\n\n    BHi = DAG.getVectorShuffle(VT, dl, B, B, PSHUFDMask);\n    BHi = DAG.getNode(ISD::SIGN_EXTEND_VECTOR_INREG, dl, ExVT, BHi);\n  } else if (IsSigned) {\n    BLo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, DAG.getUNDEF(VT), B));\n    BHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, DAG.getUNDEF(VT), B));\n\n    BLo = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, BLo, 8, DAG);\n    BHi = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, BHi, 8, DAG);\n  } else {\n    BLo = DAG.getBitcast(ExVT, getUnpackl(DAG, dl, VT, B,\n                                          DAG.getConstant(0, dl, VT)));\n    BHi = DAG.getBitcast(ExVT, getUnpackh(DAG, dl, VT, B,\n                                          DAG.getConstant(0, dl, VT)));\n  }\n\n  // Multiply, lshr the upper 8bits to the lower 8bits of the lo/hi results and\n  // pack back to vXi8.\n  SDValue RLo = DAG.getNode(ISD::MUL, dl, ExVT, ALo, BLo);\n  SDValue RHi = DAG.getNode(ISD::MUL, dl, ExVT, AHi, BHi);\n  RLo = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExVT, RLo, 8, DAG);\n  RHi = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExVT, RHi, 8, DAG);\n\n  // Bitcast back to VT and then pack all the even elements from Lo and Hi.\n  return DAG.getNode(X86ISD::PACKUS, dl, VT, RLo, RHi);\n}\n\nSDValue X86TargetLowering::LowerWin64_i128OP(SDValue Op, SelectionDAG &DAG) const {\n  assert(Subtarget.isTargetWin64() && \"Unexpected target\");\n  EVT VT = Op.getValueType();\n  assert(VT.isInteger() && VT.getSizeInBits() == 128 &&\n         \"Unexpected return type for lowering\");\n\n  RTLIB::Libcall LC;\n  bool isSigned;\n  switch (Op->getOpcode()) {\n  default: llvm_unreachable(\"Unexpected request for libcall!\");\n  case ISD::SDIV:      isSigned = true;  LC = RTLIB::SDIV_I128;    break;\n  case ISD::UDIV:      isSigned = false; LC = RTLIB::UDIV_I128;    break;\n  case ISD::SREM:      isSigned = true;  LC = RTLIB::SREM_I128;    break;\n  case ISD::UREM:      isSigned = false; LC = RTLIB::UREM_I128;    break;\n  }\n\n  SDLoc dl(Op);\n  SDValue InChain = DAG.getEntryNode();\n\n  TargetLowering::ArgListTy Args;\n  TargetLowering::ArgListEntry Entry;\n  for (unsigned i = 0, e = Op->getNumOperands(); i != e; ++i) {\n    EVT ArgVT = Op->getOperand(i).getValueType();\n    assert(ArgVT.isInteger() && ArgVT.getSizeInBits() == 128 &&\n           \"Unexpected argument type for lowering\");\n    SDValue StackPtr = DAG.CreateStackTemporary(ArgVT, 16);\n    int SPFI = cast<FrameIndexSDNode>(StackPtr.getNode())->getIndex();\n    MachinePointerInfo MPI =\n        MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SPFI);\n    Entry.Node = StackPtr;\n    InChain =\n        DAG.getStore(InChain, dl, Op->getOperand(i), StackPtr, MPI, Align(16));\n    Type *ArgTy = ArgVT.getTypeForEVT(*DAG.getContext());\n    Entry.Ty = PointerType::get(ArgTy,0);\n    Entry.IsSExt = false;\n    Entry.IsZExt = false;\n    Args.push_back(Entry);\n  }\n\n  SDValue Callee = DAG.getExternalSymbol(getLibcallName(LC),\n                                         getPointerTy(DAG.getDataLayout()));\n\n  TargetLowering::CallLoweringInfo CLI(DAG);\n  CLI.setDebugLoc(dl)\n      .setChain(InChain)\n      .setLibCallee(\n          getLibcallCallingConv(LC),\n          static_cast<EVT>(MVT::v2i64).getTypeForEVT(*DAG.getContext()), Callee,\n          std::move(Args))\n      .setInRegister()\n      .setSExtResult(isSigned)\n      .setZExtResult(!isSigned);\n\n  std::pair<SDValue, SDValue> CallInfo = LowerCallTo(CLI);\n  return DAG.getBitcast(VT, CallInfo.first);\n}\n\n// Return true if the required (according to Opcode) shift-imm form is natively\n// supported by the Subtarget\nstatic bool SupportedVectorShiftWithImm(MVT VT, const X86Subtarget &Subtarget,\n                                        unsigned Opcode) {\n  if (VT.getScalarSizeInBits() < 16)\n    return false;\n\n  if (VT.is512BitVector() && Subtarget.hasAVX512() &&\n      (VT.getScalarSizeInBits() > 16 || Subtarget.hasBWI()))\n    return true;\n\n  bool LShift = (VT.is128BitVector() && Subtarget.hasSSE2()) ||\n                (VT.is256BitVector() && Subtarget.hasInt256());\n\n  bool AShift = LShift && (Subtarget.hasAVX512() ||\n                           (VT != MVT::v2i64 && VT != MVT::v4i64));\n  return (Opcode == ISD::SRA) ? AShift : LShift;\n}\n\n// The shift amount is a variable, but it is the same for all vector lanes.\n// These instructions are defined together with shift-immediate.\nstatic\nbool SupportedVectorShiftWithBaseAmnt(MVT VT, const X86Subtarget &Subtarget,\n                                      unsigned Opcode) {\n  return SupportedVectorShiftWithImm(VT, Subtarget, Opcode);\n}\n\n// Return true if the required (according to Opcode) variable-shift form is\n// natively supported by the Subtarget\nstatic bool SupportedVectorVarShift(MVT VT, const X86Subtarget &Subtarget,\n                                    unsigned Opcode) {\n\n  if (!Subtarget.hasInt256() || VT.getScalarSizeInBits() < 16)\n    return false;\n\n  // vXi16 supported only on AVX-512, BWI\n  if (VT.getScalarSizeInBits() == 16 && !Subtarget.hasBWI())\n    return false;\n\n  if (Subtarget.hasAVX512())\n    return true;\n\n  bool LShift = VT.is128BitVector() || VT.is256BitVector();\n  bool AShift = LShift &&  VT != MVT::v2i64 && VT != MVT::v4i64;\n  return (Opcode == ISD::SRA) ? AShift : LShift;\n}\n\nstatic SDValue LowerScalarImmediateShift(SDValue Op, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n  SDValue R = Op.getOperand(0);\n  SDValue Amt = Op.getOperand(1);\n  unsigned X86Opc = getTargetVShiftUniformOpcode(Op.getOpcode(), false);\n\n  auto ArithmeticShiftRight64 = [&](uint64_t ShiftAmt) {\n    assert((VT == MVT::v2i64 || VT == MVT::v4i64) && \"Unexpected SRA type\");\n    MVT ExVT = MVT::getVectorVT(MVT::i32, VT.getVectorNumElements() * 2);\n    SDValue Ex = DAG.getBitcast(ExVT, R);\n\n    // ashr(R, 63) === cmp_slt(R, 0)\n    if (ShiftAmt == 63 && Subtarget.hasSSE42()) {\n      assert((VT != MVT::v4i64 || Subtarget.hasInt256()) &&\n             \"Unsupported PCMPGT op\");\n      return DAG.getNode(X86ISD::PCMPGT, dl, VT, DAG.getConstant(0, dl, VT), R);\n    }\n\n    if (ShiftAmt >= 32) {\n      // Splat sign to upper i32 dst, and SRA upper i32 src to lower i32.\n      SDValue Upper =\n          getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, Ex, 31, DAG);\n      SDValue Lower = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, Ex,\n                                                 ShiftAmt - 32, DAG);\n      if (VT == MVT::v2i64)\n        Ex = DAG.getVectorShuffle(ExVT, dl, Upper, Lower, {5, 1, 7, 3});\n      if (VT == MVT::v4i64)\n        Ex = DAG.getVectorShuffle(ExVT, dl, Upper, Lower,\n                                  {9, 1, 11, 3, 13, 5, 15, 7});\n    } else {\n      // SRA upper i32, SRL whole i64 and select lower i32.\n      SDValue Upper = getTargetVShiftByConstNode(X86ISD::VSRAI, dl, ExVT, Ex,\n                                                 ShiftAmt, DAG);\n      SDValue Lower =\n          getTargetVShiftByConstNode(X86ISD::VSRLI, dl, VT, R, ShiftAmt, DAG);\n      Lower = DAG.getBitcast(ExVT, Lower);\n      if (VT == MVT::v2i64)\n        Ex = DAG.getVectorShuffle(ExVT, dl, Upper, Lower, {4, 1, 6, 3});\n      if (VT == MVT::v4i64)\n        Ex = DAG.getVectorShuffle(ExVT, dl, Upper, Lower,\n                                  {8, 1, 10, 3, 12, 5, 14, 7});\n    }\n    return DAG.getBitcast(VT, Ex);\n  };\n\n  // Optimize shl/srl/sra with constant shift amount.\n  APInt APIntShiftAmt;\n  if (!X86::isConstantSplat(Amt, APIntShiftAmt))\n    return SDValue();\n\n  // If the shift amount is out of range, return undef.\n  if (APIntShiftAmt.uge(VT.getScalarSizeInBits()))\n    return DAG.getUNDEF(VT);\n\n  uint64_t ShiftAmt = APIntShiftAmt.getZExtValue();\n\n  if (SupportedVectorShiftWithImm(VT, Subtarget, Op.getOpcode()))\n    return getTargetVShiftByConstNode(X86Opc, dl, VT, R, ShiftAmt, DAG);\n\n  // i64 SRA needs to be performed as partial shifts.\n  if (((!Subtarget.hasXOP() && VT == MVT::v2i64) ||\n       (Subtarget.hasInt256() && VT == MVT::v4i64)) &&\n      Op.getOpcode() == ISD::SRA)\n    return ArithmeticShiftRight64(ShiftAmt);\n\n  if (VT == MVT::v16i8 || (Subtarget.hasInt256() && VT == MVT::v32i8) ||\n      (Subtarget.hasBWI() && VT == MVT::v64i8)) {\n    unsigned NumElts = VT.getVectorNumElements();\n    MVT ShiftVT = MVT::getVectorVT(MVT::i16, NumElts / 2);\n\n    // Simple i8 add case\n    if (Op.getOpcode() == ISD::SHL && ShiftAmt == 1)\n      return DAG.getNode(ISD::ADD, dl, VT, R, R);\n\n    // ashr(R, 7)  === cmp_slt(R, 0)\n    if (Op.getOpcode() == ISD::SRA && ShiftAmt == 7) {\n      SDValue Zeros = DAG.getConstant(0, dl, VT);\n      if (VT.is512BitVector()) {\n        assert(VT == MVT::v64i8 && \"Unexpected element type!\");\n        SDValue CMP = DAG.getSetCC(dl, MVT::v64i1, Zeros, R, ISD::SETGT);\n        return DAG.getNode(ISD::SIGN_EXTEND, dl, VT, CMP);\n      }\n      return DAG.getNode(X86ISD::PCMPGT, dl, VT, Zeros, R);\n    }\n\n    // XOP can shift v16i8 directly instead of as shift v8i16 + mask.\n    if (VT == MVT::v16i8 && Subtarget.hasXOP())\n      return SDValue();\n\n    if (Op.getOpcode() == ISD::SHL) {\n      // Make a large shift.\n      SDValue SHL = getTargetVShiftByConstNode(X86ISD::VSHLI, dl, ShiftVT, R,\n                                               ShiftAmt, DAG);\n      SHL = DAG.getBitcast(VT, SHL);\n      // Zero out the rightmost bits.\n      APInt Mask = APInt::getHighBitsSet(8, 8 - ShiftAmt);\n      return DAG.getNode(ISD::AND, dl, VT, SHL, DAG.getConstant(Mask, dl, VT));\n    }\n    if (Op.getOpcode() == ISD::SRL) {\n      // Make a large shift.\n      SDValue SRL = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ShiftVT, R,\n                                               ShiftAmt, DAG);\n      SRL = DAG.getBitcast(VT, SRL);\n      // Zero out the leftmost bits.\n      return DAG.getNode(ISD::AND, dl, VT, SRL,\n                         DAG.getConstant(uint8_t(-1U) >> ShiftAmt, dl, VT));\n    }\n    if (Op.getOpcode() == ISD::SRA) {\n      // ashr(R, Amt) === sub(xor(lshr(R, Amt), Mask), Mask)\n      SDValue Res = DAG.getNode(ISD::SRL, dl, VT, R, Amt);\n\n      SDValue Mask = DAG.getConstant(128 >> ShiftAmt, dl, VT);\n      Res = DAG.getNode(ISD::XOR, dl, VT, Res, Mask);\n      Res = DAG.getNode(ISD::SUB, dl, VT, Res, Mask);\n      return Res;\n    }\n    llvm_unreachable(\"Unknown shift opcode.\");\n  }\n\n  return SDValue();\n}\n\nstatic SDValue LowerScalarVariableShift(SDValue Op, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n  SDValue R = Op.getOperand(0);\n  SDValue Amt = Op.getOperand(1);\n  unsigned Opcode = Op.getOpcode();\n  unsigned X86OpcI = getTargetVShiftUniformOpcode(Opcode, false);\n  unsigned X86OpcV = getTargetVShiftUniformOpcode(Opcode, true);\n\n  if (SDValue BaseShAmt = DAG.getSplatValue(Amt)) {\n    if (SupportedVectorShiftWithBaseAmnt(VT, Subtarget, Opcode)) {\n      MVT EltVT = VT.getVectorElementType();\n      assert(EltVT.bitsLE(MVT::i64) && \"Unexpected element type!\");\n      if (EltVT != MVT::i64 && EltVT.bitsGT(MVT::i32))\n        BaseShAmt = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i64, BaseShAmt);\n      else if (EltVT.bitsLT(MVT::i32))\n        BaseShAmt = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, BaseShAmt);\n\n      return getTargetVShiftNode(X86OpcI, dl, VT, R, BaseShAmt, Subtarget, DAG);\n    }\n\n    // vXi8 shifts - shift as v8i16 + mask result.\n    if (((VT == MVT::v16i8 && !Subtarget.canExtendTo512DQ()) ||\n         (VT == MVT::v32i8 && !Subtarget.canExtendTo512BW()) ||\n         VT == MVT::v64i8) &&\n        !Subtarget.hasXOP()) {\n      unsigned NumElts = VT.getVectorNumElements();\n      MVT ExtVT = MVT::getVectorVT(MVT::i16, NumElts / 2);\n      if (SupportedVectorShiftWithBaseAmnt(ExtVT, Subtarget, Opcode)) {\n        unsigned LogicalOp = (Opcode == ISD::SHL ? ISD::SHL : ISD::SRL);\n        unsigned LogicalX86Op = getTargetVShiftUniformOpcode(LogicalOp, false);\n        BaseShAmt = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, BaseShAmt);\n\n        // Create the mask using vXi16 shifts. For shift-rights we need to move\n        // the upper byte down before splatting the vXi8 mask.\n        SDValue BitMask = DAG.getConstant(-1, dl, ExtVT);\n        BitMask = getTargetVShiftNode(LogicalX86Op, dl, ExtVT, BitMask,\n                                      BaseShAmt, Subtarget, DAG);\n        if (Opcode != ISD::SHL)\n          BitMask = getTargetVShiftByConstNode(LogicalX86Op, dl, ExtVT, BitMask,\n                                               8, DAG);\n        BitMask = DAG.getBitcast(VT, BitMask);\n        BitMask = DAG.getVectorShuffle(VT, dl, BitMask, BitMask,\n                                       SmallVector<int, 64>(NumElts, 0));\n\n        SDValue Res = getTargetVShiftNode(LogicalX86Op, dl, ExtVT,\n                                          DAG.getBitcast(ExtVT, R), BaseShAmt,\n                                          Subtarget, DAG);\n        Res = DAG.getBitcast(VT, Res);\n        Res = DAG.getNode(ISD::AND, dl, VT, Res, BitMask);\n\n        if (Opcode == ISD::SRA) {\n          // ashr(R, Amt) === sub(xor(lshr(R, Amt), SignMask), SignMask)\n          // SignMask = lshr(SignBit, Amt) - safe to do this with PSRLW.\n          SDValue SignMask = DAG.getConstant(0x8080, dl, ExtVT);\n          SignMask = getTargetVShiftNode(LogicalX86Op, dl, ExtVT, SignMask,\n                                         BaseShAmt, Subtarget, DAG);\n          SignMask = DAG.getBitcast(VT, SignMask);\n          Res = DAG.getNode(ISD::XOR, dl, VT, Res, SignMask);\n          Res = DAG.getNode(ISD::SUB, dl, VT, Res, SignMask);\n        }\n        return Res;\n      }\n    }\n  }\n\n  // Check cases (mainly 32-bit) where i64 is expanded into high and low parts.\n  if (VT == MVT::v2i64 && Amt.getOpcode() == ISD::BITCAST &&\n      Amt.getOperand(0).getOpcode() == ISD::BUILD_VECTOR) {\n    Amt = Amt.getOperand(0);\n    unsigned Ratio = 64 / Amt.getScalarValueSizeInBits();\n    std::vector<SDValue> Vals(Ratio);\n    for (unsigned i = 0; i != Ratio; ++i)\n      Vals[i] = Amt.getOperand(i);\n    for (unsigned i = Ratio, e = Amt.getNumOperands(); i != e; i += Ratio) {\n      for (unsigned j = 0; j != Ratio; ++j)\n        if (Vals[j] != Amt.getOperand(i + j))\n          return SDValue();\n    }\n\n    if (SupportedVectorShiftWithBaseAmnt(VT, Subtarget, Op.getOpcode()))\n      return DAG.getNode(X86OpcV, dl, VT, R, Op.getOperand(1));\n  }\n  return SDValue();\n}\n\n// Convert a shift/rotate left amount to a multiplication scale factor.\nstatic SDValue convertShiftLeftToScale(SDValue Amt, const SDLoc &dl,\n                                       const X86Subtarget &Subtarget,\n                                       SelectionDAG &DAG) {\n  MVT VT = Amt.getSimpleValueType();\n  if (!(VT == MVT::v8i16 || VT == MVT::v4i32 ||\n        (Subtarget.hasInt256() && VT == MVT::v16i16) ||\n        (Subtarget.hasVBMI2() && VT == MVT::v32i16) ||\n        (!Subtarget.hasAVX512() && VT == MVT::v16i8)))\n    return SDValue();\n\n  if (ISD::isBuildVectorOfConstantSDNodes(Amt.getNode())) {\n    SmallVector<SDValue, 8> Elts;\n    MVT SVT = VT.getVectorElementType();\n    unsigned SVTBits = SVT.getSizeInBits();\n    APInt One(SVTBits, 1);\n    unsigned NumElems = VT.getVectorNumElements();\n\n    for (unsigned i = 0; i != NumElems; ++i) {\n      SDValue Op = Amt->getOperand(i);\n      if (Op->isUndef()) {\n        Elts.push_back(Op);\n        continue;\n      }\n\n      ConstantSDNode *ND = cast<ConstantSDNode>(Op);\n      APInt C(SVTBits, ND->getZExtValue());\n      uint64_t ShAmt = C.getZExtValue();\n      if (ShAmt >= SVTBits) {\n        Elts.push_back(DAG.getUNDEF(SVT));\n        continue;\n      }\n      Elts.push_back(DAG.getConstant(One.shl(ShAmt), dl, SVT));\n    }\n    return DAG.getBuildVector(VT, dl, Elts);\n  }\n\n  // If the target doesn't support variable shifts, use either FP conversion\n  // or integer multiplication to avoid shifting each element individually.\n  if (VT == MVT::v4i32) {\n    Amt = DAG.getNode(ISD::SHL, dl, VT, Amt, DAG.getConstant(23, dl, VT));\n    Amt = DAG.getNode(ISD::ADD, dl, VT, Amt,\n                      DAG.getConstant(0x3f800000U, dl, VT));\n    Amt = DAG.getBitcast(MVT::v4f32, Amt);\n    return DAG.getNode(ISD::FP_TO_SINT, dl, VT, Amt);\n  }\n\n  // AVX2 can more effectively perform this as a zext/trunc to/from v8i32.\n  if (VT == MVT::v8i16 && !Subtarget.hasAVX2()) {\n    SDValue Z = DAG.getConstant(0, dl, VT);\n    SDValue Lo = DAG.getBitcast(MVT::v4i32, getUnpackl(DAG, dl, VT, Amt, Z));\n    SDValue Hi = DAG.getBitcast(MVT::v4i32, getUnpackh(DAG, dl, VT, Amt, Z));\n    Lo = convertShiftLeftToScale(Lo, dl, Subtarget, DAG);\n    Hi = convertShiftLeftToScale(Hi, dl, Subtarget, DAG);\n    if (Subtarget.hasSSE41())\n      return DAG.getNode(X86ISD::PACKUS, dl, VT, Lo, Hi);\n\n    return DAG.getVectorShuffle(VT, dl, DAG.getBitcast(VT, Lo),\n                                        DAG.getBitcast(VT, Hi),\n                                        {0, 2, 4, 6, 8, 10, 12, 14});\n  }\n\n  return SDValue();\n}\n\nstatic SDValue LowerShift(SDValue Op, const X86Subtarget &Subtarget,\n                          SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  SDLoc dl(Op);\n  SDValue R = Op.getOperand(0);\n  SDValue Amt = Op.getOperand(1);\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  bool ConstantAmt = ISD::isBuildVectorOfConstantSDNodes(Amt.getNode());\n\n  unsigned Opc = Op.getOpcode();\n  unsigned X86OpcV = getTargetVShiftUniformOpcode(Opc, true);\n  unsigned X86OpcI = getTargetVShiftUniformOpcode(Opc, false);\n\n  assert(VT.isVector() && \"Custom lowering only for vector shifts!\");\n  assert(Subtarget.hasSSE2() && \"Only custom lower when we have SSE2!\");\n\n  if (SDValue V = LowerScalarImmediateShift(Op, DAG, Subtarget))\n    return V;\n\n  if (SDValue V = LowerScalarVariableShift(Op, DAG, Subtarget))\n    return V;\n\n  if (SupportedVectorVarShift(VT, Subtarget, Opc))\n    return Op;\n\n  // XOP has 128-bit variable logical/arithmetic shifts.\n  // +ve/-ve Amt = shift left/right.\n  if (Subtarget.hasXOP() && (VT == MVT::v2i64 || VT == MVT::v4i32 ||\n                             VT == MVT::v8i16 || VT == MVT::v16i8)) {\n    if (Opc == ISD::SRL || Opc == ISD::SRA) {\n      SDValue Zero = DAG.getConstant(0, dl, VT);\n      Amt = DAG.getNode(ISD::SUB, dl, VT, Zero, Amt);\n    }\n    if (Opc == ISD::SHL || Opc == ISD::SRL)\n      return DAG.getNode(X86ISD::VPSHL, dl, VT, R, Amt);\n    if (Opc == ISD::SRA)\n      return DAG.getNode(X86ISD::VPSHA, dl, VT, R, Amt);\n  }\n\n  // 2i64 vector logical shifts can efficiently avoid scalarization - do the\n  // shifts per-lane and then shuffle the partial results back together.\n  if (VT == MVT::v2i64 && Opc != ISD::SRA) {\n    // Splat the shift amounts so the scalar shifts above will catch it.\n    SDValue Amt0 = DAG.getVectorShuffle(VT, dl, Amt, Amt, {0, 0});\n    SDValue Amt1 = DAG.getVectorShuffle(VT, dl, Amt, Amt, {1, 1});\n    SDValue R0 = DAG.getNode(Opc, dl, VT, R, Amt0);\n    SDValue R1 = DAG.getNode(Opc, dl, VT, R, Amt1);\n    return DAG.getVectorShuffle(VT, dl, R0, R1, {0, 3});\n  }\n\n  // i64 vector arithmetic shift can be emulated with the transform:\n  // M = lshr(SIGN_MASK, Amt)\n  // ashr(R, Amt) === sub(xor(lshr(R, Amt), M), M)\n  if ((VT == MVT::v2i64 || (VT == MVT::v4i64 && Subtarget.hasInt256())) &&\n      Opc == ISD::SRA) {\n    SDValue S = DAG.getConstant(APInt::getSignMask(64), dl, VT);\n    SDValue M = DAG.getNode(ISD::SRL, dl, VT, S, Amt);\n    R = DAG.getNode(ISD::SRL, dl, VT, R, Amt);\n    R = DAG.getNode(ISD::XOR, dl, VT, R, M);\n    R = DAG.getNode(ISD::SUB, dl, VT, R, M);\n    return R;\n  }\n\n  // If possible, lower this shift as a sequence of two shifts by\n  // constant plus a BLENDing shuffle instead of scalarizing it.\n  // Example:\n  //   (v4i32 (srl A, (build_vector < X, Y, Y, Y>)))\n  //\n  // Could be rewritten as:\n  //   (v4i32 (MOVSS (srl A, <Y,Y,Y,Y>), (srl A, <X,X,X,X>)))\n  //\n  // The advantage is that the two shifts from the example would be\n  // lowered as X86ISD::VSRLI nodes in parallel before blending.\n  if (ConstantAmt && (VT == MVT::v8i16 || VT == MVT::v4i32 ||\n                      (VT == MVT::v16i16 && Subtarget.hasInt256()))) {\n    SDValue Amt1, Amt2;\n    unsigned NumElts = VT.getVectorNumElements();\n    SmallVector<int, 8> ShuffleMask;\n    for (unsigned i = 0; i != NumElts; ++i) {\n      SDValue A = Amt->getOperand(i);\n      if (A.isUndef()) {\n        ShuffleMask.push_back(SM_SentinelUndef);\n        continue;\n      }\n      if (!Amt1 || Amt1 == A) {\n        ShuffleMask.push_back(i);\n        Amt1 = A;\n        continue;\n      }\n      if (!Amt2 || Amt2 == A) {\n        ShuffleMask.push_back(i + NumElts);\n        Amt2 = A;\n        continue;\n      }\n      break;\n    }\n\n    // Only perform this blend if we can perform it without loading a mask.\n    if (ShuffleMask.size() == NumElts && Amt1 && Amt2 &&\n        (VT != MVT::v16i16 ||\n         is128BitLaneRepeatedShuffleMask(VT, ShuffleMask)) &&\n        (VT == MVT::v4i32 || Subtarget.hasSSE41() || Opc != ISD::SHL ||\n         canWidenShuffleElements(ShuffleMask))) {\n      auto *Cst1 = dyn_cast<ConstantSDNode>(Amt1);\n      auto *Cst2 = dyn_cast<ConstantSDNode>(Amt2);\n      if (Cst1 && Cst2 && Cst1->getAPIntValue().ult(EltSizeInBits) &&\n          Cst2->getAPIntValue().ult(EltSizeInBits)) {\n        SDValue Shift1 = getTargetVShiftByConstNode(X86OpcI, dl, VT, R,\n                                                    Cst1->getZExtValue(), DAG);\n        SDValue Shift2 = getTargetVShiftByConstNode(X86OpcI, dl, VT, R,\n                                                    Cst2->getZExtValue(), DAG);\n        return DAG.getVectorShuffle(VT, dl, Shift1, Shift2, ShuffleMask);\n      }\n    }\n  }\n\n  // If possible, lower this packed shift into a vector multiply instead of\n  // expanding it into a sequence of scalar shifts.\n  if (Opc == ISD::SHL)\n    if (SDValue Scale = convertShiftLeftToScale(Amt, dl, Subtarget, DAG))\n      return DAG.getNode(ISD::MUL, dl, VT, R, Scale);\n\n  // Constant ISD::SRL can be performed efficiently on vXi16 vectors as we\n  // can replace with ISD::MULHU, creating scale factor from (NumEltBits - Amt).\n  if (Opc == ISD::SRL && ConstantAmt &&\n      (VT == MVT::v8i16 || (VT == MVT::v16i16 && Subtarget.hasInt256()))) {\n    SDValue EltBits = DAG.getConstant(EltSizeInBits, dl, VT);\n    SDValue RAmt = DAG.getNode(ISD::SUB, dl, VT, EltBits, Amt);\n    if (SDValue Scale = convertShiftLeftToScale(RAmt, dl, Subtarget, DAG)) {\n      SDValue Zero = DAG.getConstant(0, dl, VT);\n      SDValue ZAmt = DAG.getSetCC(dl, VT, Amt, Zero, ISD::SETEQ);\n      SDValue Res = DAG.getNode(ISD::MULHU, dl, VT, R, Scale);\n      return DAG.getSelect(dl, VT, ZAmt, R, Res);\n    }\n  }\n\n  // Constant ISD::SRA can be performed efficiently on vXi16 vectors as we\n  // can replace with ISD::MULHS, creating scale factor from (NumEltBits - Amt).\n  // TODO: Special case handling for shift by 0/1, really we can afford either\n  // of these cases in pre-SSE41/XOP/AVX512 but not both.\n  if (Opc == ISD::SRA && ConstantAmt &&\n      (VT == MVT::v8i16 || (VT == MVT::v16i16 && Subtarget.hasInt256())) &&\n      ((Subtarget.hasSSE41() && !Subtarget.hasXOP() &&\n        !Subtarget.hasAVX512()) ||\n       DAG.isKnownNeverZero(Amt))) {\n    SDValue EltBits = DAG.getConstant(EltSizeInBits, dl, VT);\n    SDValue RAmt = DAG.getNode(ISD::SUB, dl, VT, EltBits, Amt);\n    if (SDValue Scale = convertShiftLeftToScale(RAmt, dl, Subtarget, DAG)) {\n      SDValue Amt0 =\n          DAG.getSetCC(dl, VT, Amt, DAG.getConstant(0, dl, VT), ISD::SETEQ);\n      SDValue Amt1 =\n          DAG.getSetCC(dl, VT, Amt, DAG.getConstant(1, dl, VT), ISD::SETEQ);\n      SDValue Sra1 =\n          getTargetVShiftByConstNode(X86ISD::VSRAI, dl, VT, R, 1, DAG);\n      SDValue Res = DAG.getNode(ISD::MULHS, dl, VT, R, Scale);\n      Res = DAG.getSelect(dl, VT, Amt0, R, Res);\n      return DAG.getSelect(dl, VT, Amt1, Sra1, Res);\n    }\n  }\n\n  // v4i32 Non Uniform Shifts.\n  // If the shift amount is constant we can shift each lane using the SSE2\n  // immediate shifts, else we need to zero-extend each lane to the lower i64\n  // and shift using the SSE2 variable shifts.\n  // The separate results can then be blended together.\n  if (VT == MVT::v4i32) {\n    SDValue Amt0, Amt1, Amt2, Amt3;\n    if (ConstantAmt) {\n      Amt0 = DAG.getVectorShuffle(VT, dl, Amt, DAG.getUNDEF(VT), {0, 0, 0, 0});\n      Amt1 = DAG.getVectorShuffle(VT, dl, Amt, DAG.getUNDEF(VT), {1, 1, 1, 1});\n      Amt2 = DAG.getVectorShuffle(VT, dl, Amt, DAG.getUNDEF(VT), {2, 2, 2, 2});\n      Amt3 = DAG.getVectorShuffle(VT, dl, Amt, DAG.getUNDEF(VT), {3, 3, 3, 3});\n    } else {\n      // The SSE2 shifts use the lower i64 as the same shift amount for\n      // all lanes and the upper i64 is ignored. On AVX we're better off\n      // just zero-extending, but for SSE just duplicating the top 16-bits is\n      // cheaper and has the same effect for out of range values.\n      if (Subtarget.hasAVX()) {\n        SDValue Z = DAG.getConstant(0, dl, VT);\n        Amt0 = DAG.getVectorShuffle(VT, dl, Amt, Z, {0, 4, -1, -1});\n        Amt1 = DAG.getVectorShuffle(VT, dl, Amt, Z, {1, 5, -1, -1});\n        Amt2 = DAG.getVectorShuffle(VT, dl, Amt, Z, {2, 6, -1, -1});\n        Amt3 = DAG.getVectorShuffle(VT, dl, Amt, Z, {3, 7, -1, -1});\n      } else {\n        SDValue Amt01 = DAG.getBitcast(MVT::v8i16, Amt);\n        SDValue Amt23 = DAG.getVectorShuffle(MVT::v8i16, dl, Amt01, Amt01,\n                                             {4, 5, 6, 7, -1, -1, -1, -1});\n        Amt0 = DAG.getVectorShuffle(MVT::v8i16, dl, Amt01, Amt01,\n                                    {0, 1, 1, 1, -1, -1, -1, -1});\n        Amt1 = DAG.getVectorShuffle(MVT::v8i16, dl, Amt01, Amt01,\n                                    {2, 3, 3, 3, -1, -1, -1, -1});\n        Amt2 = DAG.getVectorShuffle(MVT::v8i16, dl, Amt23, Amt23,\n                                    {0, 1, 1, 1, -1, -1, -1, -1});\n        Amt3 = DAG.getVectorShuffle(MVT::v8i16, dl, Amt23, Amt23,\n                                    {2, 3, 3, 3, -1, -1, -1, -1});\n      }\n    }\n\n    unsigned ShOpc = ConstantAmt ? Opc : X86OpcV;\n    SDValue R0 = DAG.getNode(ShOpc, dl, VT, R, DAG.getBitcast(VT, Amt0));\n    SDValue R1 = DAG.getNode(ShOpc, dl, VT, R, DAG.getBitcast(VT, Amt1));\n    SDValue R2 = DAG.getNode(ShOpc, dl, VT, R, DAG.getBitcast(VT, Amt2));\n    SDValue R3 = DAG.getNode(ShOpc, dl, VT, R, DAG.getBitcast(VT, Amt3));\n\n    // Merge the shifted lane results optimally with/without PBLENDW.\n    // TODO - ideally shuffle combining would handle this.\n    if (Subtarget.hasSSE41()) {\n      SDValue R02 = DAG.getVectorShuffle(VT, dl, R0, R2, {0, -1, 6, -1});\n      SDValue R13 = DAG.getVectorShuffle(VT, dl, R1, R3, {-1, 1, -1, 7});\n      return DAG.getVectorShuffle(VT, dl, R02, R13, {0, 5, 2, 7});\n    }\n    SDValue R01 = DAG.getVectorShuffle(VT, dl, R0, R1, {0, -1, -1, 5});\n    SDValue R23 = DAG.getVectorShuffle(VT, dl, R2, R3, {2, -1, -1, 7});\n    return DAG.getVectorShuffle(VT, dl, R01, R23, {0, 3, 4, 7});\n  }\n\n  // It's worth extending once and using the vXi16/vXi32 shifts for smaller\n  // types, but without AVX512 the extra overheads to get from vXi8 to vXi32\n  // make the existing SSE solution better.\n  // NOTE: We honor prefered vector width before promoting to 512-bits.\n  if ((Subtarget.hasInt256() && VT == MVT::v8i16) ||\n      (Subtarget.canExtendTo512DQ() && VT == MVT::v16i16) ||\n      (Subtarget.canExtendTo512DQ() && VT == MVT::v16i8) ||\n      (Subtarget.canExtendTo512BW() && VT == MVT::v32i8) ||\n      (Subtarget.hasBWI() && Subtarget.hasVLX() && VT == MVT::v16i8)) {\n    assert((!Subtarget.hasBWI() || VT == MVT::v32i8 || VT == MVT::v16i8) &&\n           \"Unexpected vector type\");\n    MVT EvtSVT = Subtarget.hasBWI() ? MVT::i16 : MVT::i32;\n    MVT ExtVT = MVT::getVectorVT(EvtSVT, VT.getVectorNumElements());\n    unsigned ExtOpc = Opc == ISD::SRA ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;\n    R = DAG.getNode(ExtOpc, dl, ExtVT, R);\n    Amt = DAG.getNode(ISD::ZERO_EXTEND, dl, ExtVT, Amt);\n    return DAG.getNode(ISD::TRUNCATE, dl, VT,\n                       DAG.getNode(Opc, dl, ExtVT, R, Amt));\n  }\n\n  // Constant ISD::SRA/SRL can be performed efficiently on vXi8 vectors as we\n  // extend to vXi16 to perform a MUL scale effectively as a MUL_LOHI.\n  if (ConstantAmt && (Opc == ISD::SRA || Opc == ISD::SRL) &&\n      (VT == MVT::v16i8 || (VT == MVT::v32i8 && Subtarget.hasInt256()) ||\n       (VT == MVT::v64i8 && Subtarget.hasBWI())) &&\n      !Subtarget.hasXOP()) {\n    int NumElts = VT.getVectorNumElements();\n    SDValue Cst8 = DAG.getTargetConstant(8, dl, MVT::i8);\n\n    // Extend constant shift amount to vXi16 (it doesn't matter if the type\n    // isn't legal).\n    MVT ExVT = MVT::getVectorVT(MVT::i16, NumElts);\n    Amt = DAG.getZExtOrTrunc(Amt, dl, ExVT);\n    Amt = DAG.getNode(ISD::SUB, dl, ExVT, DAG.getConstant(8, dl, ExVT), Amt);\n    Amt = DAG.getNode(ISD::SHL, dl, ExVT, DAG.getConstant(1, dl, ExVT), Amt);\n    assert(ISD::isBuildVectorOfConstantSDNodes(Amt.getNode()) &&\n           \"Constant build vector expected\");\n\n    if (VT == MVT::v16i8 && Subtarget.hasInt256()) {\n      R = Opc == ISD::SRA ? DAG.getSExtOrTrunc(R, dl, ExVT)\n                          : DAG.getZExtOrTrunc(R, dl, ExVT);\n      R = DAG.getNode(ISD::MUL, dl, ExVT, R, Amt);\n      R = DAG.getNode(X86ISD::VSRLI, dl, ExVT, R, Cst8);\n      return DAG.getZExtOrTrunc(R, dl, VT);\n    }\n\n    SmallVector<SDValue, 16> LoAmt, HiAmt;\n    for (int i = 0; i != NumElts; i += 16) {\n      for (int j = 0; j != 8; ++j) {\n        LoAmt.push_back(Amt.getOperand(i + j));\n        HiAmt.push_back(Amt.getOperand(i + j + 8));\n      }\n    }\n\n    MVT VT16 = MVT::getVectorVT(MVT::i16, NumElts / 2);\n    SDValue LoA = DAG.getBuildVector(VT16, dl, LoAmt);\n    SDValue HiA = DAG.getBuildVector(VT16, dl, HiAmt);\n\n    SDValue LoR = DAG.getBitcast(VT16, getUnpackl(DAG, dl, VT, R, R));\n    SDValue HiR = DAG.getBitcast(VT16, getUnpackh(DAG, dl, VT, R, R));\n    LoR = DAG.getNode(X86OpcI, dl, VT16, LoR, Cst8);\n    HiR = DAG.getNode(X86OpcI, dl, VT16, HiR, Cst8);\n    LoR = DAG.getNode(ISD::MUL, dl, VT16, LoR, LoA);\n    HiR = DAG.getNode(ISD::MUL, dl, VT16, HiR, HiA);\n    LoR = DAG.getNode(X86ISD::VSRLI, dl, VT16, LoR, Cst8);\n    HiR = DAG.getNode(X86ISD::VSRLI, dl, VT16, HiR, Cst8);\n    return DAG.getNode(X86ISD::PACKUS, dl, VT, LoR, HiR);\n  }\n\n  if (VT == MVT::v16i8 ||\n      (VT == MVT::v32i8 && Subtarget.hasInt256() && !Subtarget.hasXOP()) ||\n      (VT == MVT::v64i8 && Subtarget.hasBWI())) {\n    MVT ExtVT = MVT::getVectorVT(MVT::i16, VT.getVectorNumElements() / 2);\n\n    auto SignBitSelect = [&](MVT SelVT, SDValue Sel, SDValue V0, SDValue V1) {\n      if (VT.is512BitVector()) {\n        // On AVX512BW targets we make use of the fact that VSELECT lowers\n        // to a masked blend which selects bytes based just on the sign bit\n        // extracted to a mask.\n        MVT MaskVT = MVT::getVectorVT(MVT::i1, VT.getVectorNumElements());\n        V0 = DAG.getBitcast(VT, V0);\n        V1 = DAG.getBitcast(VT, V1);\n        Sel = DAG.getBitcast(VT, Sel);\n        Sel = DAG.getSetCC(dl, MaskVT, DAG.getConstant(0, dl, VT), Sel,\n                           ISD::SETGT);\n        return DAG.getBitcast(SelVT, DAG.getSelect(dl, VT, Sel, V0, V1));\n      } else if (Subtarget.hasSSE41()) {\n        // On SSE41 targets we can use PBLENDVB which selects bytes based just\n        // on the sign bit.\n        V0 = DAG.getBitcast(VT, V0);\n        V1 = DAG.getBitcast(VT, V1);\n        Sel = DAG.getBitcast(VT, Sel);\n        return DAG.getBitcast(SelVT,\n                              DAG.getNode(X86ISD::BLENDV, dl, VT, Sel, V0, V1));\n      }\n      // On pre-SSE41 targets we test for the sign bit by comparing to\n      // zero - a negative value will set all bits of the lanes to true\n      // and VSELECT uses that in its OR(AND(V0,C),AND(V1,~C)) lowering.\n      SDValue Z = DAG.getConstant(0, dl, SelVT);\n      SDValue C = DAG.getNode(X86ISD::PCMPGT, dl, SelVT, Z, Sel);\n      return DAG.getSelect(dl, SelVT, C, V0, V1);\n    };\n\n    // Turn 'a' into a mask suitable for VSELECT: a = a << 5;\n    // We can safely do this using i16 shifts as we're only interested in\n    // the 3 lower bits of each byte.\n    Amt = DAG.getBitcast(ExtVT, Amt);\n    Amt = getTargetVShiftByConstNode(X86ISD::VSHLI, dl, ExtVT, Amt, 5, DAG);\n    Amt = DAG.getBitcast(VT, Amt);\n\n    if (Opc == ISD::SHL || Opc == ISD::SRL) {\n      // r = VSELECT(r, shift(r, 4), a);\n      SDValue M = DAG.getNode(Opc, dl, VT, R, DAG.getConstant(4, dl, VT));\n      R = SignBitSelect(VT, Amt, M, R);\n\n      // a += a\n      Amt = DAG.getNode(ISD::ADD, dl, VT, Amt, Amt);\n\n      // r = VSELECT(r, shift(r, 2), a);\n      M = DAG.getNode(Opc, dl, VT, R, DAG.getConstant(2, dl, VT));\n      R = SignBitSelect(VT, Amt, M, R);\n\n      // a += a\n      Amt = DAG.getNode(ISD::ADD, dl, VT, Amt, Amt);\n\n      // return VSELECT(r, shift(r, 1), a);\n      M = DAG.getNode(Opc, dl, VT, R, DAG.getConstant(1, dl, VT));\n      R = SignBitSelect(VT, Amt, M, R);\n      return R;\n    }\n\n    if (Opc == ISD::SRA) {\n      // For SRA we need to unpack each byte to the higher byte of a i16 vector\n      // so we can correctly sign extend. We don't care what happens to the\n      // lower byte.\n      SDValue ALo = getUnpackl(DAG, dl, VT, DAG.getUNDEF(VT), Amt);\n      SDValue AHi = getUnpackh(DAG, dl, VT, DAG.getUNDEF(VT), Amt);\n      SDValue RLo = getUnpackl(DAG, dl, VT, DAG.getUNDEF(VT), R);\n      SDValue RHi = getUnpackh(DAG, dl, VT, DAG.getUNDEF(VT), R);\n      ALo = DAG.getBitcast(ExtVT, ALo);\n      AHi = DAG.getBitcast(ExtVT, AHi);\n      RLo = DAG.getBitcast(ExtVT, RLo);\n      RHi = DAG.getBitcast(ExtVT, RHi);\n\n      // r = VSELECT(r, shift(r, 4), a);\n      SDValue MLo = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RLo, 4, DAG);\n      SDValue MHi = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RHi, 4, DAG);\n      RLo = SignBitSelect(ExtVT, ALo, MLo, RLo);\n      RHi = SignBitSelect(ExtVT, AHi, MHi, RHi);\n\n      // a += a\n      ALo = DAG.getNode(ISD::ADD, dl, ExtVT, ALo, ALo);\n      AHi = DAG.getNode(ISD::ADD, dl, ExtVT, AHi, AHi);\n\n      // r = VSELECT(r, shift(r, 2), a);\n      MLo = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RLo, 2, DAG);\n      MHi = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RHi, 2, DAG);\n      RLo = SignBitSelect(ExtVT, ALo, MLo, RLo);\n      RHi = SignBitSelect(ExtVT, AHi, MHi, RHi);\n\n      // a += a\n      ALo = DAG.getNode(ISD::ADD, dl, ExtVT, ALo, ALo);\n      AHi = DAG.getNode(ISD::ADD, dl, ExtVT, AHi, AHi);\n\n      // r = VSELECT(r, shift(r, 1), a);\n      MLo = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RLo, 1, DAG);\n      MHi = getTargetVShiftByConstNode(X86OpcI, dl, ExtVT, RHi, 1, DAG);\n      RLo = SignBitSelect(ExtVT, ALo, MLo, RLo);\n      RHi = SignBitSelect(ExtVT, AHi, MHi, RHi);\n\n      // Logical shift the result back to the lower byte, leaving a zero upper\n      // byte meaning that we can safely pack with PACKUSWB.\n      RLo = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExtVT, RLo, 8, DAG);\n      RHi = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExtVT, RHi, 8, DAG);\n      return DAG.getNode(X86ISD::PACKUS, dl, VT, RLo, RHi);\n    }\n  }\n\n  if (Subtarget.hasInt256() && !Subtarget.hasXOP() && VT == MVT::v16i16) {\n    MVT ExtVT = MVT::v8i32;\n    SDValue Z = DAG.getConstant(0, dl, VT);\n    SDValue ALo = getUnpackl(DAG, dl, VT, Amt, Z);\n    SDValue AHi = getUnpackh(DAG, dl, VT, Amt, Z);\n    SDValue RLo = getUnpackl(DAG, dl, VT, Z, R);\n    SDValue RHi = getUnpackh(DAG, dl, VT, Z, R);\n    ALo = DAG.getBitcast(ExtVT, ALo);\n    AHi = DAG.getBitcast(ExtVT, AHi);\n    RLo = DAG.getBitcast(ExtVT, RLo);\n    RHi = DAG.getBitcast(ExtVT, RHi);\n    SDValue Lo = DAG.getNode(Opc, dl, ExtVT, RLo, ALo);\n    SDValue Hi = DAG.getNode(Opc, dl, ExtVT, RHi, AHi);\n    Lo = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExtVT, Lo, 16, DAG);\n    Hi = getTargetVShiftByConstNode(X86ISD::VSRLI, dl, ExtVT, Hi, 16, DAG);\n    return DAG.getNode(X86ISD::PACKUS, dl, VT, Lo, Hi);\n  }\n\n  if (VT == MVT::v8i16) {\n    // If we have a constant shift amount, the non-SSE41 path is best as\n    // avoiding bitcasts make it easier to constant fold and reduce to PBLENDW.\n    bool UseSSE41 = Subtarget.hasSSE41() &&\n                    !ISD::isBuildVectorOfConstantSDNodes(Amt.getNode());\n\n    auto SignBitSelect = [&](SDValue Sel, SDValue V0, SDValue V1) {\n      // On SSE41 targets we can use PBLENDVB which selects bytes based just on\n      // the sign bit.\n      if (UseSSE41) {\n        MVT ExtVT = MVT::getVectorVT(MVT::i8, VT.getVectorNumElements() * 2);\n        V0 = DAG.getBitcast(ExtVT, V0);\n        V1 = DAG.getBitcast(ExtVT, V1);\n        Sel = DAG.getBitcast(ExtVT, Sel);\n        return DAG.getBitcast(\n            VT, DAG.getNode(X86ISD::BLENDV, dl, ExtVT, Sel, V0, V1));\n      }\n      // On pre-SSE41 targets we splat the sign bit - a negative value will\n      // set all bits of the lanes to true and VSELECT uses that in\n      // its OR(AND(V0,C),AND(V1,~C)) lowering.\n      SDValue C =\n          getTargetVShiftByConstNode(X86ISD::VSRAI, dl, VT, Sel, 15, DAG);\n      return DAG.getSelect(dl, VT, C, V0, V1);\n    };\n\n    // Turn 'a' into a mask suitable for VSELECT: a = a << 12;\n    if (UseSSE41) {\n      // On SSE41 targets we need to replicate the shift mask in both\n      // bytes for PBLENDVB.\n      Amt = DAG.getNode(\n          ISD::OR, dl, VT,\n          getTargetVShiftByConstNode(X86ISD::VSHLI, dl, VT, Amt, 4, DAG),\n          getTargetVShiftByConstNode(X86ISD::VSHLI, dl, VT, Amt, 12, DAG));\n    } else {\n      Amt = getTargetVShiftByConstNode(X86ISD::VSHLI, dl, VT, Amt, 12, DAG);\n    }\n\n    // r = VSELECT(r, shift(r, 8), a);\n    SDValue M = getTargetVShiftByConstNode(X86OpcI, dl, VT, R, 8, DAG);\n    R = SignBitSelect(Amt, M, R);\n\n    // a += a\n    Amt = DAG.getNode(ISD::ADD, dl, VT, Amt, Amt);\n\n    // r = VSELECT(r, shift(r, 4), a);\n    M = getTargetVShiftByConstNode(X86OpcI, dl, VT, R, 4, DAG);\n    R = SignBitSelect(Amt, M, R);\n\n    // a += a\n    Amt = DAG.getNode(ISD::ADD, dl, VT, Amt, Amt);\n\n    // r = VSELECT(r, shift(r, 2), a);\n    M = getTargetVShiftByConstNode(X86OpcI, dl, VT, R, 2, DAG);\n    R = SignBitSelect(Amt, M, R);\n\n    // a += a\n    Amt = DAG.getNode(ISD::ADD, dl, VT, Amt, Amt);\n\n    // return VSELECT(r, shift(r, 1), a);\n    M = getTargetVShiftByConstNode(X86OpcI, dl, VT, R, 1, DAG);\n    R = SignBitSelect(Amt, M, R);\n    return R;\n  }\n\n  // Decompose 256-bit shifts into 128-bit shifts.\n  if (VT.is256BitVector())\n    return splitVectorIntBinary(Op, DAG);\n\n  if (VT == MVT::v32i16 || VT == MVT::v64i8)\n    return splitVectorIntBinary(Op, DAG);\n\n  return SDValue();\n}\n\nstatic SDValue LowerRotate(SDValue Op, const X86Subtarget &Subtarget,\n                           SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  assert(VT.isVector() && \"Custom lowering only for vector rotates!\");\n\n  SDLoc DL(Op);\n  SDValue R = Op.getOperand(0);\n  SDValue Amt = Op.getOperand(1);\n  unsigned Opcode = Op.getOpcode();\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n  int NumElts = VT.getVectorNumElements();\n\n  // Check for constant splat rotation amount.\n  APInt CstSplatValue;\n  bool IsCstSplat = X86::isConstantSplat(Amt, CstSplatValue);\n\n  // Check for splat rotate by zero.\n  if (IsCstSplat && CstSplatValue.urem(EltSizeInBits) == 0)\n    return R;\n\n  // AVX512 implicitly uses modulo rotation amounts.\n  if (Subtarget.hasAVX512() && 32 <= EltSizeInBits) {\n    // Attempt to rotate by immediate.\n    if (IsCstSplat) {\n      unsigned RotOpc = (Opcode == ISD::ROTL ? X86ISD::VROTLI : X86ISD::VROTRI);\n      uint64_t RotAmt = CstSplatValue.urem(EltSizeInBits);\n      return DAG.getNode(RotOpc, DL, VT, R,\n                         DAG.getTargetConstant(RotAmt, DL, MVT::i8));\n    }\n\n    // Else, fall-back on VPROLV/VPRORV.\n    return Op;\n  }\n\n  // AVX512 VBMI2 vXi16 - lower to funnel shifts.\n  if (Subtarget.hasVBMI2() && 16 == EltSizeInBits) {\n    unsigned FunnelOpc = (Opcode == ISD::ROTL ? ISD::FSHL : ISD::FSHR);\n    return DAG.getNode(FunnelOpc, DL, VT, R, R, Amt);\n  }\n\n  assert((Opcode == ISD::ROTL) && \"Only ROTL supported\");\n\n  // XOP has 128-bit vector variable + immediate rotates.\n  // +ve/-ve Amt = rotate left/right - just need to handle ISD::ROTL.\n  // XOP implicitly uses modulo rotation amounts.\n  if (Subtarget.hasXOP()) {\n    if (VT.is256BitVector())\n      return splitVectorIntBinary(Op, DAG);\n    assert(VT.is128BitVector() && \"Only rotate 128-bit vectors!\");\n\n    // Attempt to rotate by immediate.\n    if (IsCstSplat) {\n      uint64_t RotAmt = CstSplatValue.urem(EltSizeInBits);\n      return DAG.getNode(X86ISD::VROTLI, DL, VT, R,\n                         DAG.getTargetConstant(RotAmt, DL, MVT::i8));\n    }\n\n    // Use general rotate by variable (per-element).\n    return Op;\n  }\n\n  // Split 256-bit integers on pre-AVX2 targets.\n  if (VT.is256BitVector() && !Subtarget.hasAVX2())\n    return splitVectorIntBinary(Op, DAG);\n\n  assert((VT == MVT::v4i32 || VT == MVT::v8i16 || VT == MVT::v16i8 ||\n          ((VT == MVT::v8i32 || VT == MVT::v16i16 || VT == MVT::v32i8 ||\n            VT == MVT::v32i16) &&\n           Subtarget.hasAVX2())) &&\n         \"Only vXi32/vXi16/vXi8 vector rotates supported\");\n\n  // Rotate by an uniform constant - expand back to shifts.\n  if (IsCstSplat)\n    return SDValue();\n\n  bool IsSplatAmt = DAG.isSplatValue(Amt);\n\n  // v16i8/v32i8: Split rotation into rot4/rot2/rot1 stages and select by\n  // the amount bit.\n  if (EltSizeInBits == 8 && !IsSplatAmt) {\n    if (ISD::isBuildVectorOfConstantSDNodes(Amt.getNode()))\n      return SDValue();\n\n    // We don't need ModuloAmt here as we just peek at individual bits.\n    MVT ExtVT = MVT::getVectorVT(MVT::i16, NumElts / 2);\n\n    auto SignBitSelect = [&](MVT SelVT, SDValue Sel, SDValue V0, SDValue V1) {\n      if (Subtarget.hasSSE41()) {\n        // On SSE41 targets we can use PBLENDVB which selects bytes based just\n        // on the sign bit.\n        V0 = DAG.getBitcast(VT, V0);\n        V1 = DAG.getBitcast(VT, V1);\n        Sel = DAG.getBitcast(VT, Sel);\n        return DAG.getBitcast(SelVT,\n                              DAG.getNode(X86ISD::BLENDV, DL, VT, Sel, V0, V1));\n      }\n      // On pre-SSE41 targets we test for the sign bit by comparing to\n      // zero - a negative value will set all bits of the lanes to true\n      // and VSELECT uses that in its OR(AND(V0,C),AND(V1,~C)) lowering.\n      SDValue Z = DAG.getConstant(0, DL, SelVT);\n      SDValue C = DAG.getNode(X86ISD::PCMPGT, DL, SelVT, Z, Sel);\n      return DAG.getSelect(DL, SelVT, C, V0, V1);\n    };\n\n    // Turn 'a' into a mask suitable for VSELECT: a = a << 5;\n    // We can safely do this using i16 shifts as we're only interested in\n    // the 3 lower bits of each byte.\n    Amt = DAG.getBitcast(ExtVT, Amt);\n    Amt = DAG.getNode(ISD::SHL, DL, ExtVT, Amt, DAG.getConstant(5, DL, ExtVT));\n    Amt = DAG.getBitcast(VT, Amt);\n\n    // r = VSELECT(r, rot(r, 4), a);\n    SDValue M;\n    M = DAG.getNode(\n        ISD::OR, DL, VT,\n        DAG.getNode(ISD::SHL, DL, VT, R, DAG.getConstant(4, DL, VT)),\n        DAG.getNode(ISD::SRL, DL, VT, R, DAG.getConstant(4, DL, VT)));\n    R = SignBitSelect(VT, Amt, M, R);\n\n    // a += a\n    Amt = DAG.getNode(ISD::ADD, DL, VT, Amt, Amt);\n\n    // r = VSELECT(r, rot(r, 2), a);\n    M = DAG.getNode(\n        ISD::OR, DL, VT,\n        DAG.getNode(ISD::SHL, DL, VT, R, DAG.getConstant(2, DL, VT)),\n        DAG.getNode(ISD::SRL, DL, VT, R, DAG.getConstant(6, DL, VT)));\n    R = SignBitSelect(VT, Amt, M, R);\n\n    // a += a\n    Amt = DAG.getNode(ISD::ADD, DL, VT, Amt, Amt);\n\n    // return VSELECT(r, rot(r, 1), a);\n    M = DAG.getNode(\n        ISD::OR, DL, VT,\n        DAG.getNode(ISD::SHL, DL, VT, R, DAG.getConstant(1, DL, VT)),\n        DAG.getNode(ISD::SRL, DL, VT, R, DAG.getConstant(7, DL, VT)));\n    return SignBitSelect(VT, Amt, M, R);\n  }\n\n  // ISD::ROT* uses modulo rotate amounts.\n  Amt = DAG.getNode(ISD::AND, DL, VT, Amt,\n                    DAG.getConstant(EltSizeInBits - 1, DL, VT));\n\n  bool ConstantAmt = ISD::isBuildVectorOfConstantSDNodes(Amt.getNode());\n  bool LegalVarShifts = SupportedVectorVarShift(VT, Subtarget, ISD::SHL) &&\n                        SupportedVectorVarShift(VT, Subtarget, ISD::SRL);\n\n  // Fallback for splats + all supported variable shifts.\n  // Fallback for non-constants AVX2 vXi16 as well.\n  if (IsSplatAmt || LegalVarShifts || (Subtarget.hasAVX2() && !ConstantAmt)) {\n    SDValue AmtR = DAG.getConstant(EltSizeInBits, DL, VT);\n    AmtR = DAG.getNode(ISD::SUB, DL, VT, AmtR, Amt);\n    SDValue SHL = DAG.getNode(ISD::SHL, DL, VT, R, Amt);\n    SDValue SRL = DAG.getNode(ISD::SRL, DL, VT, R, AmtR);\n    return DAG.getNode(ISD::OR, DL, VT, SHL, SRL);\n  }\n\n  // As with shifts, convert the rotation amount to a multiplication factor.\n  SDValue Scale = convertShiftLeftToScale(Amt, DL, Subtarget, DAG);\n  assert(Scale && \"Failed to convert ROTL amount to scale\");\n\n  // v8i16/v16i16: perform unsigned multiply hi/lo and OR the results.\n  if (EltSizeInBits == 16) {\n    SDValue Lo = DAG.getNode(ISD::MUL, DL, VT, R, Scale);\n    SDValue Hi = DAG.getNode(ISD::MULHU, DL, VT, R, Scale);\n    return DAG.getNode(ISD::OR, DL, VT, Lo, Hi);\n  }\n\n  // v4i32: make use of the PMULUDQ instruction to multiply 2 lanes of v4i32\n  // to v2i64 results at a time. The upper 32-bits contain the wrapped bits\n  // that can then be OR'd with the lower 32-bits.\n  assert(VT == MVT::v4i32 && \"Only v4i32 vector rotate expected\");\n  static const int OddMask[] = {1, -1, 3, -1};\n  SDValue R13 = DAG.getVectorShuffle(VT, DL, R, R, OddMask);\n  SDValue Scale13 = DAG.getVectorShuffle(VT, DL, Scale, Scale, OddMask);\n\n  SDValue Res02 = DAG.getNode(X86ISD::PMULUDQ, DL, MVT::v2i64,\n                              DAG.getBitcast(MVT::v2i64, R),\n                              DAG.getBitcast(MVT::v2i64, Scale));\n  SDValue Res13 = DAG.getNode(X86ISD::PMULUDQ, DL, MVT::v2i64,\n                              DAG.getBitcast(MVT::v2i64, R13),\n                              DAG.getBitcast(MVT::v2i64, Scale13));\n  Res02 = DAG.getBitcast(VT, Res02);\n  Res13 = DAG.getBitcast(VT, Res13);\n\n  return DAG.getNode(ISD::OR, DL, VT,\n                     DAG.getVectorShuffle(VT, DL, Res02, Res13, {0, 4, 2, 6}),\n                     DAG.getVectorShuffle(VT, DL, Res02, Res13, {1, 5, 3, 7}));\n}\n\n/// Returns true if the operand type is exactly twice the native width, and\n/// the corresponding cmpxchg8b or cmpxchg16b instruction is available.\n/// Used to know whether to use cmpxchg8/16b when expanding atomic operations\n/// (otherwise we leave them alone to become __sync_fetch_and_... calls).\nbool X86TargetLowering::needsCmpXchgNb(Type *MemType) const {\n  unsigned OpWidth = MemType->getPrimitiveSizeInBits();\n\n  if (OpWidth == 64)\n    return Subtarget.hasCmpxchg8b() && !Subtarget.is64Bit();\n  if (OpWidth == 128)\n    return Subtarget.hasCmpxchg16b();\n\n  return false;\n}\n\nbool X86TargetLowering::shouldExpandAtomicStoreInIR(StoreInst *SI) const {\n  Type *MemType = SI->getValueOperand()->getType();\n\n  bool NoImplicitFloatOps =\n      SI->getFunction()->hasFnAttribute(Attribute::NoImplicitFloat);\n  if (MemType->getPrimitiveSizeInBits() == 64 && !Subtarget.is64Bit() &&\n      !Subtarget.useSoftFloat() && !NoImplicitFloatOps &&\n      (Subtarget.hasSSE1() || Subtarget.hasX87()))\n    return false;\n\n  return needsCmpXchgNb(MemType);\n}\n\n// Note: this turns large loads into lock cmpxchg8b/16b.\n// TODO: In 32-bit mode, use MOVLPS when SSE1 is available?\nTargetLowering::AtomicExpansionKind\nX86TargetLowering::shouldExpandAtomicLoadInIR(LoadInst *LI) const {\n  Type *MemType = LI->getType();\n\n  // If this a 64 bit atomic load on a 32-bit target and SSE2 is enabled, we\n  // can use movq to do the load. If we have X87 we can load into an 80-bit\n  // X87 register and store it to a stack temporary.\n  bool NoImplicitFloatOps =\n      LI->getFunction()->hasFnAttribute(Attribute::NoImplicitFloat);\n  if (MemType->getPrimitiveSizeInBits() == 64 && !Subtarget.is64Bit() &&\n      !Subtarget.useSoftFloat() && !NoImplicitFloatOps &&\n      (Subtarget.hasSSE1() || Subtarget.hasX87()))\n    return AtomicExpansionKind::None;\n\n  return needsCmpXchgNb(MemType) ? AtomicExpansionKind::CmpXChg\n                                 : AtomicExpansionKind::None;\n}\n\nTargetLowering::AtomicExpansionKind\nX86TargetLowering::shouldExpandAtomicRMWInIR(AtomicRMWInst *AI) const {\n  unsigned NativeWidth = Subtarget.is64Bit() ? 64 : 32;\n  Type *MemType = AI->getType();\n\n  // If the operand is too big, we must see if cmpxchg8/16b is available\n  // and default to library calls otherwise.\n  if (MemType->getPrimitiveSizeInBits() > NativeWidth) {\n    return needsCmpXchgNb(MemType) ? AtomicExpansionKind::CmpXChg\n                                   : AtomicExpansionKind::None;\n  }\n\n  AtomicRMWInst::BinOp Op = AI->getOperation();\n  switch (Op) {\n  default:\n    llvm_unreachable(\"Unknown atomic operation\");\n  case AtomicRMWInst::Xchg:\n  case AtomicRMWInst::Add:\n  case AtomicRMWInst::Sub:\n    // It's better to use xadd, xsub or xchg for these in all cases.\n    return AtomicExpansionKind::None;\n  case AtomicRMWInst::Or:\n  case AtomicRMWInst::And:\n  case AtomicRMWInst::Xor:\n    // If the atomicrmw's result isn't actually used, we can just add a \"lock\"\n    // prefix to a normal instruction for these operations.\n    return !AI->use_empty() ? AtomicExpansionKind::CmpXChg\n                            : AtomicExpansionKind::None;\n  case AtomicRMWInst::Nand:\n  case AtomicRMWInst::Max:\n  case AtomicRMWInst::Min:\n  case AtomicRMWInst::UMax:\n  case AtomicRMWInst::UMin:\n  case AtomicRMWInst::FAdd:\n  case AtomicRMWInst::FSub:\n    // These always require a non-trivial set of data operations on x86. We must\n    // use a cmpxchg loop.\n    return AtomicExpansionKind::CmpXChg;\n  }\n}\n\nLoadInst *\nX86TargetLowering::lowerIdempotentRMWIntoFencedLoad(AtomicRMWInst *AI) const {\n  unsigned NativeWidth = Subtarget.is64Bit() ? 64 : 32;\n  Type *MemType = AI->getType();\n  // Accesses larger than the native width are turned into cmpxchg/libcalls, so\n  // there is no benefit in turning such RMWs into loads, and it is actually\n  // harmful as it introduces a mfence.\n  if (MemType->getPrimitiveSizeInBits() > NativeWidth)\n    return nullptr;\n\n  // If this is a canonical idempotent atomicrmw w/no uses, we have a better\n  // lowering available in lowerAtomicArith.\n  // TODO: push more cases through this path.\n  if (auto *C = dyn_cast<ConstantInt>(AI->getValOperand()))\n    if (AI->getOperation() == AtomicRMWInst::Or && C->isZero() &&\n        AI->use_empty())\n      return nullptr;\n\n  IRBuilder<> Builder(AI);\n  Module *M = Builder.GetInsertBlock()->getParent()->getParent();\n  auto SSID = AI->getSyncScopeID();\n  // We must restrict the ordering to avoid generating loads with Release or\n  // ReleaseAcquire orderings.\n  auto Order = AtomicCmpXchgInst::getStrongestFailureOrdering(AI->getOrdering());\n\n  // Before the load we need a fence. Here is an example lifted from\n  // http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf showing why a fence\n  // is required:\n  // Thread 0:\n  //   x.store(1, relaxed);\n  //   r1 = y.fetch_add(0, release);\n  // Thread 1:\n  //   y.fetch_add(42, acquire);\n  //   r2 = x.load(relaxed);\n  // r1 = r2 = 0 is impossible, but becomes possible if the idempotent rmw is\n  // lowered to just a load without a fence. A mfence flushes the store buffer,\n  // making the optimization clearly correct.\n  // FIXME: it is required if isReleaseOrStronger(Order) but it is not clear\n  // otherwise, we might be able to be more aggressive on relaxed idempotent\n  // rmw. In practice, they do not look useful, so we don't try to be\n  // especially clever.\n  if (SSID == SyncScope::SingleThread)\n    // FIXME: we could just insert an X86ISD::MEMBARRIER here, except we are at\n    // the IR level, so we must wrap it in an intrinsic.\n    return nullptr;\n\n  if (!Subtarget.hasMFence())\n    // FIXME: it might make sense to use a locked operation here but on a\n    // different cache-line to prevent cache-line bouncing. In practice it\n    // is probably a small win, and x86 processors without mfence are rare\n    // enough that we do not bother.\n    return nullptr;\n\n  Function *MFence =\n      llvm::Intrinsic::getDeclaration(M, Intrinsic::x86_sse2_mfence);\n  Builder.CreateCall(MFence, {});\n\n  // Finally we can emit the atomic load.\n  LoadInst *Loaded =\n      Builder.CreateAlignedLoad(AI->getType(), AI->getPointerOperand(),\n                                Align(AI->getType()->getPrimitiveSizeInBits()));\n  Loaded->setAtomic(Order, SSID);\n  AI->replaceAllUsesWith(Loaded);\n  AI->eraseFromParent();\n  return Loaded;\n}\n\nbool X86TargetLowering::lowerAtomicStoreAsStoreSDNode(const StoreInst &SI) const {\n  if (!SI.isUnordered())\n    return false;\n  return ExperimentalUnorderedISEL;\n}\nbool X86TargetLowering::lowerAtomicLoadAsLoadSDNode(const LoadInst &LI) const {\n  if (!LI.isUnordered())\n    return false;\n  return ExperimentalUnorderedISEL;\n}\n\n\n/// Emit a locked operation on a stack location which does not change any\n/// memory location, but does involve a lock prefix.  Location is chosen to be\n/// a) very likely accessed only by a single thread to minimize cache traffic,\n/// and b) definitely dereferenceable.  Returns the new Chain result.\nstatic SDValue emitLockedStackOp(SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget, SDValue Chain,\n                                 const SDLoc &DL) {\n  // Implementation notes:\n  // 1) LOCK prefix creates a full read/write reordering barrier for memory\n  // operations issued by the current processor.  As such, the location\n  // referenced is not relevant for the ordering properties of the instruction.\n  // See: Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual,\n  // 8.2.3.9  Loads and Stores Are Not Reordered with Locked Instructions\n  // 2) Using an immediate operand appears to be the best encoding choice\n  // here since it doesn't require an extra register.\n  // 3) OR appears to be very slightly faster than ADD. (Though, the difference\n  // is small enough it might just be measurement noise.)\n  // 4) When choosing offsets, there are several contributing factors:\n  //   a) If there's no redzone, we default to TOS.  (We could allocate a cache\n  //      line aligned stack object to improve this case.)\n  //   b) To minimize our chances of introducing a false dependence, we prefer\n  //      to offset the stack usage from TOS slightly.\n  //   c) To minimize concerns about cross thread stack usage - in particular,\n  //      the idiomatic MyThreadPool.run([&StackVars]() {...}) pattern which\n  //      captures state in the TOS frame and accesses it from many threads -\n  //      we want to use an offset such that the offset is in a distinct cache\n  //      line from the TOS frame.\n  //\n  // For a general discussion of the tradeoffs and benchmark results, see:\n  // https://shipilev.net/blog/2014/on-the-fence-with-dependencies/\n\n  auto &MF = DAG.getMachineFunction();\n  auto &TFL = *Subtarget.getFrameLowering();\n  const unsigned SPOffset = TFL.has128ByteRedZone(MF) ? -64 : 0;\n\n  if (Subtarget.is64Bit()) {\n    SDValue Zero = DAG.getTargetConstant(0, DL, MVT::i32);\n    SDValue Ops[] = {\n      DAG.getRegister(X86::RSP, MVT::i64),                  // Base\n      DAG.getTargetConstant(1, DL, MVT::i8),                // Scale\n      DAG.getRegister(0, MVT::i64),                         // Index\n      DAG.getTargetConstant(SPOffset, DL, MVT::i32),        // Disp\n      DAG.getRegister(0, MVT::i16),                         // Segment.\n      Zero,\n      Chain};\n    SDNode *Res = DAG.getMachineNode(X86::OR32mi8Locked, DL, MVT::i32,\n                                     MVT::Other, Ops);\n    return SDValue(Res, 1);\n  }\n\n  SDValue Zero = DAG.getTargetConstant(0, DL, MVT::i32);\n  SDValue Ops[] = {\n    DAG.getRegister(X86::ESP, MVT::i32),            // Base\n    DAG.getTargetConstant(1, DL, MVT::i8),          // Scale\n    DAG.getRegister(0, MVT::i32),                   // Index\n    DAG.getTargetConstant(SPOffset, DL, MVT::i32),  // Disp\n    DAG.getRegister(0, MVT::i16),                   // Segment.\n    Zero,\n    Chain\n  };\n  SDNode *Res = DAG.getMachineNode(X86::OR32mi8Locked, DL, MVT::i32,\n                                   MVT::Other, Ops);\n  return SDValue(Res, 1);\n}\n\nstatic SDValue LowerATOMIC_FENCE(SDValue Op, const X86Subtarget &Subtarget,\n                                 SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  AtomicOrdering FenceOrdering =\n      static_cast<AtomicOrdering>(Op.getConstantOperandVal(1));\n  SyncScope::ID FenceSSID =\n      static_cast<SyncScope::ID>(Op.getConstantOperandVal(2));\n\n  // The only fence that needs an instruction is a sequentially-consistent\n  // cross-thread fence.\n  if (FenceOrdering == AtomicOrdering::SequentiallyConsistent &&\n      FenceSSID == SyncScope::System) {\n    if (Subtarget.hasMFence())\n      return DAG.getNode(X86ISD::MFENCE, dl, MVT::Other, Op.getOperand(0));\n\n    SDValue Chain = Op.getOperand(0);\n    return emitLockedStackOp(DAG, Subtarget, Chain, dl);\n  }\n\n  // MEMBARRIER is a compiler barrier; it codegens to a no-op.\n  return DAG.getNode(X86ISD::MEMBARRIER, dl, MVT::Other, Op.getOperand(0));\n}\n\nstatic SDValue LowerCMP_SWAP(SDValue Op, const X86Subtarget &Subtarget,\n                             SelectionDAG &DAG) {\n  MVT T = Op.getSimpleValueType();\n  SDLoc DL(Op);\n  unsigned Reg = 0;\n  unsigned size = 0;\n  switch(T.SimpleTy) {\n  default: llvm_unreachable(\"Invalid value type!\");\n  case MVT::i8:  Reg = X86::AL;  size = 1; break;\n  case MVT::i16: Reg = X86::AX;  size = 2; break;\n  case MVT::i32: Reg = X86::EAX; size = 4; break;\n  case MVT::i64:\n    assert(Subtarget.is64Bit() && \"Node not type legal!\");\n    Reg = X86::RAX; size = 8;\n    break;\n  }\n  SDValue cpIn = DAG.getCopyToReg(Op.getOperand(0), DL, Reg,\n                                  Op.getOperand(2), SDValue());\n  SDValue Ops[] = { cpIn.getValue(0),\n                    Op.getOperand(1),\n                    Op.getOperand(3),\n                    DAG.getTargetConstant(size, DL, MVT::i8),\n                    cpIn.getValue(1) };\n  SDVTList Tys = DAG.getVTList(MVT::Other, MVT::Glue);\n  MachineMemOperand *MMO = cast<AtomicSDNode>(Op)->getMemOperand();\n  SDValue Result = DAG.getMemIntrinsicNode(X86ISD::LCMPXCHG_DAG, DL, Tys,\n                                           Ops, T, MMO);\n\n  SDValue cpOut =\n    DAG.getCopyFromReg(Result.getValue(0), DL, Reg, T, Result.getValue(1));\n  SDValue EFLAGS = DAG.getCopyFromReg(cpOut.getValue(1), DL, X86::EFLAGS,\n                                      MVT::i32, cpOut.getValue(2));\n  SDValue Success = getSETCC(X86::COND_E, EFLAGS, DL, DAG);\n\n  return DAG.getNode(ISD::MERGE_VALUES, DL, Op->getVTList(),\n                     cpOut, Success, EFLAGS.getValue(1));\n}\n\n// Create MOVMSKB, taking into account whether we need to split for AVX1.\nstatic SDValue getPMOVMSKB(const SDLoc &DL, SDValue V, SelectionDAG &DAG,\n                           const X86Subtarget &Subtarget) {\n  MVT InVT = V.getSimpleValueType();\n\n  if (InVT == MVT::v64i8) {\n    SDValue Lo, Hi;\n    std::tie(Lo, Hi) = DAG.SplitVector(V, DL);\n    Lo = getPMOVMSKB(DL, Lo, DAG, Subtarget);\n    Hi = getPMOVMSKB(DL, Hi, DAG, Subtarget);\n    Lo = DAG.getNode(ISD::ZERO_EXTEND, DL, MVT::i64, Lo);\n    Hi = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i64, Hi);\n    Hi = DAG.getNode(ISD::SHL, DL, MVT::i64, Hi,\n                     DAG.getConstant(32, DL, MVT::i8));\n    return DAG.getNode(ISD::OR, DL, MVT::i64, Lo, Hi);\n  }\n  if (InVT == MVT::v32i8 && !Subtarget.hasInt256()) {\n    SDValue Lo, Hi;\n    std::tie(Lo, Hi) = DAG.SplitVector(V, DL);\n    Lo = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Lo);\n    Hi = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Hi);\n    Hi = DAG.getNode(ISD::SHL, DL, MVT::i32, Hi,\n                     DAG.getConstant(16, DL, MVT::i8));\n    return DAG.getNode(ISD::OR, DL, MVT::i32, Lo, Hi);\n  }\n\n  return DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, V);\n}\n\nstatic SDValue LowerBITCAST(SDValue Op, const X86Subtarget &Subtarget,\n                            SelectionDAG &DAG) {\n  SDValue Src = Op.getOperand(0);\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT DstVT = Op.getSimpleValueType();\n\n  // Legalize (v64i1 (bitcast i64 (X))) by splitting the i64, bitcasting each\n  // half to v32i1 and concatenating the result.\n  if (SrcVT == MVT::i64 && DstVT == MVT::v64i1) {\n    assert(!Subtarget.is64Bit() && \"Expected 32-bit mode\");\n    assert(Subtarget.hasBWI() && \"Expected BWI target\");\n    SDLoc dl(Op);\n    SDValue Lo = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, MVT::i32, Src,\n                             DAG.getIntPtrConstant(0, dl));\n    Lo = DAG.getBitcast(MVT::v32i1, Lo);\n    SDValue Hi = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, MVT::i32, Src,\n                             DAG.getIntPtrConstant(1, dl));\n    Hi = DAG.getBitcast(MVT::v32i1, Hi);\n    return DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v64i1, Lo, Hi);\n  }\n\n  // Use MOVMSK for vector to scalar conversion to prevent scalarization.\n  if ((SrcVT == MVT::v16i1 || SrcVT == MVT::v32i1) && DstVT.isScalarInteger()) {\n    assert(!Subtarget.hasAVX512() && \"Should use K-registers with AVX512\");\n    MVT SExtVT = SrcVT == MVT::v16i1 ? MVT::v16i8 : MVT::v32i8;\n    SDLoc DL(Op);\n    SDValue V = DAG.getSExtOrTrunc(Src, DL, SExtVT);\n    V = getPMOVMSKB(DL, V, DAG, Subtarget);\n    return DAG.getZExtOrTrunc(V, DL, DstVT);\n  }\n\n  assert((SrcVT == MVT::v2i32 || SrcVT == MVT::v4i16 || SrcVT == MVT::v8i8 ||\n          SrcVT == MVT::i64) && \"Unexpected VT!\");\n\n  assert(Subtarget.hasSSE2() && \"Requires at least SSE2!\");\n  if (!(DstVT == MVT::f64 && SrcVT == MVT::i64) &&\n      !(DstVT == MVT::x86mmx && SrcVT.isVector()))\n    // This conversion needs to be expanded.\n    return SDValue();\n\n  SDLoc dl(Op);\n  if (SrcVT.isVector()) {\n    // Widen the vector in input in the case of MVT::v2i32.\n    // Example: from MVT::v2i32 to MVT::v4i32.\n    MVT NewVT = MVT::getVectorVT(SrcVT.getVectorElementType(),\n                                 SrcVT.getVectorNumElements() * 2);\n    Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, NewVT, Src,\n                      DAG.getUNDEF(SrcVT));\n  } else {\n    assert(SrcVT == MVT::i64 && !Subtarget.is64Bit() &&\n           \"Unexpected source type in LowerBITCAST\");\n    Src = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64, Src);\n  }\n\n  MVT V2X64VT = DstVT == MVT::f64 ? MVT::v2f64 : MVT::v2i64;\n  Src = DAG.getNode(ISD::BITCAST, dl, V2X64VT, Src);\n\n  if (DstVT == MVT::x86mmx)\n    return DAG.getNode(X86ISD::MOVDQ2Q, dl, DstVT, Src);\n\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, DstVT, Src,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\n/// Compute the horizontal sum of bytes in V for the elements of VT.\n///\n/// Requires V to be a byte vector and VT to be an integer vector type with\n/// wider elements than V's type. The width of the elements of VT determines\n/// how many bytes of V are summed horizontally to produce each element of the\n/// result.\nstatic SDValue LowerHorizontalByteSum(SDValue V, MVT VT,\n                                      const X86Subtarget &Subtarget,\n                                      SelectionDAG &DAG) {\n  SDLoc DL(V);\n  MVT ByteVecVT = V.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  assert(ByteVecVT.getVectorElementType() == MVT::i8 &&\n         \"Expected value to have byte element type.\");\n  assert(EltVT != MVT::i8 &&\n         \"Horizontal byte sum only makes sense for wider elements!\");\n  unsigned VecSize = VT.getSizeInBits();\n  assert(ByteVecVT.getSizeInBits() == VecSize && \"Cannot change vector size!\");\n\n  // PSADBW instruction horizontally add all bytes and leave the result in i64\n  // chunks, thus directly computes the pop count for v2i64 and v4i64.\n  if (EltVT == MVT::i64) {\n    SDValue Zeros = DAG.getConstant(0, DL, ByteVecVT);\n    MVT SadVecVT = MVT::getVectorVT(MVT::i64, VecSize / 64);\n    V = DAG.getNode(X86ISD::PSADBW, DL, SadVecVT, V, Zeros);\n    return DAG.getBitcast(VT, V);\n  }\n\n  if (EltVT == MVT::i32) {\n    // We unpack the low half and high half into i32s interleaved with zeros so\n    // that we can use PSADBW to horizontally sum them. The most useful part of\n    // this is that it lines up the results of two PSADBW instructions to be\n    // two v2i64 vectors which concatenated are the 4 population counts. We can\n    // then use PACKUSWB to shrink and concatenate them into a v4i32 again.\n    SDValue Zeros = DAG.getConstant(0, DL, VT);\n    SDValue V32 = DAG.getBitcast(VT, V);\n    SDValue Low = getUnpackl(DAG, DL, VT, V32, Zeros);\n    SDValue High = getUnpackh(DAG, DL, VT, V32, Zeros);\n\n    // Do the horizontal sums into two v2i64s.\n    Zeros = DAG.getConstant(0, DL, ByteVecVT);\n    MVT SadVecVT = MVT::getVectorVT(MVT::i64, VecSize / 64);\n    Low = DAG.getNode(X86ISD::PSADBW, DL, SadVecVT,\n                      DAG.getBitcast(ByteVecVT, Low), Zeros);\n    High = DAG.getNode(X86ISD::PSADBW, DL, SadVecVT,\n                       DAG.getBitcast(ByteVecVT, High), Zeros);\n\n    // Merge them together.\n    MVT ShortVecVT = MVT::getVectorVT(MVT::i16, VecSize / 16);\n    V = DAG.getNode(X86ISD::PACKUS, DL, ByteVecVT,\n                    DAG.getBitcast(ShortVecVT, Low),\n                    DAG.getBitcast(ShortVecVT, High));\n\n    return DAG.getBitcast(VT, V);\n  }\n\n  // The only element type left is i16.\n  assert(EltVT == MVT::i16 && \"Unknown how to handle type\");\n\n  // To obtain pop count for each i16 element starting from the pop count for\n  // i8 elements, shift the i16s left by 8, sum as i8s, and then shift as i16s\n  // right by 8. It is important to shift as i16s as i8 vector shift isn't\n  // directly supported.\n  SDValue ShifterV = DAG.getConstant(8, DL, VT);\n  SDValue Shl = DAG.getNode(ISD::SHL, DL, VT, DAG.getBitcast(VT, V), ShifterV);\n  V = DAG.getNode(ISD::ADD, DL, ByteVecVT, DAG.getBitcast(ByteVecVT, Shl),\n                  DAG.getBitcast(ByteVecVT, V));\n  return DAG.getNode(ISD::SRL, DL, VT, DAG.getBitcast(VT, V), ShifterV);\n}\n\nstatic SDValue LowerVectorCTPOPInRegLUT(SDValue Op, const SDLoc &DL,\n                                        const X86Subtarget &Subtarget,\n                                        SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  MVT EltVT = VT.getVectorElementType();\n  int NumElts = VT.getVectorNumElements();\n  (void)EltVT;\n  assert(EltVT == MVT::i8 && \"Only vXi8 vector CTPOP lowering supported.\");\n\n  // Implement a lookup table in register by using an algorithm based on:\n  // http://wm.ite.pl/articles/sse-popcount.html\n  //\n  // The general idea is that every lower byte nibble in the input vector is an\n  // index into a in-register pre-computed pop count table. We then split up the\n  // input vector in two new ones: (1) a vector with only the shifted-right\n  // higher nibbles for each byte and (2) a vector with the lower nibbles (and\n  // masked out higher ones) for each byte. PSHUFB is used separately with both\n  // to index the in-register table. Next, both are added and the result is a\n  // i8 vector where each element contains the pop count for input byte.\n  const int LUT[16] = {/* 0 */ 0, /* 1 */ 1, /* 2 */ 1, /* 3 */ 2,\n                       /* 4 */ 1, /* 5 */ 2, /* 6 */ 2, /* 7 */ 3,\n                       /* 8 */ 1, /* 9 */ 2, /* a */ 2, /* b */ 3,\n                       /* c */ 2, /* d */ 3, /* e */ 3, /* f */ 4};\n\n  SmallVector<SDValue, 64> LUTVec;\n  for (int i = 0; i < NumElts; ++i)\n    LUTVec.push_back(DAG.getConstant(LUT[i % 16], DL, MVT::i8));\n  SDValue InRegLUT = DAG.getBuildVector(VT, DL, LUTVec);\n  SDValue M0F = DAG.getConstant(0x0F, DL, VT);\n\n  // High nibbles\n  SDValue FourV = DAG.getConstant(4, DL, VT);\n  SDValue HiNibbles = DAG.getNode(ISD::SRL, DL, VT, Op, FourV);\n\n  // Low nibbles\n  SDValue LoNibbles = DAG.getNode(ISD::AND, DL, VT, Op, M0F);\n\n  // The input vector is used as the shuffle mask that index elements into the\n  // LUT. After counting low and high nibbles, add the vector to obtain the\n  // final pop count per i8 element.\n  SDValue HiPopCnt = DAG.getNode(X86ISD::PSHUFB, DL, VT, InRegLUT, HiNibbles);\n  SDValue LoPopCnt = DAG.getNode(X86ISD::PSHUFB, DL, VT, InRegLUT, LoNibbles);\n  return DAG.getNode(ISD::ADD, DL, VT, HiPopCnt, LoPopCnt);\n}\n\n// Please ensure that any codegen change from LowerVectorCTPOP is reflected in\n// updated cost models in X86TTIImpl::getIntrinsicInstrCost.\nstatic SDValue LowerVectorCTPOP(SDValue Op, const X86Subtarget &Subtarget,\n                                SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  assert((VT.is512BitVector() || VT.is256BitVector() || VT.is128BitVector()) &&\n         \"Unknown CTPOP type to handle\");\n  SDLoc DL(Op.getNode());\n  SDValue Op0 = Op.getOperand(0);\n\n  // TRUNC(CTPOP(ZEXT(X))) to make use of vXi32/vXi64 VPOPCNT instructions.\n  if (Subtarget.hasVPOPCNTDQ()) {\n    unsigned NumElems = VT.getVectorNumElements();\n    assert((VT.getVectorElementType() == MVT::i8 ||\n            VT.getVectorElementType() == MVT::i16) && \"Unexpected type\");\n    if (NumElems < 16 || (NumElems == 16 && Subtarget.canExtendTo512DQ())) {\n      MVT NewVT = MVT::getVectorVT(MVT::i32, NumElems);\n      Op = DAG.getNode(ISD::ZERO_EXTEND, DL, NewVT, Op0);\n      Op = DAG.getNode(ISD::CTPOP, DL, NewVT, Op);\n      return DAG.getNode(ISD::TRUNCATE, DL, VT, Op);\n    }\n  }\n\n  // Decompose 256-bit ops into smaller 128-bit ops.\n  if (VT.is256BitVector() && !Subtarget.hasInt256())\n    return splitVectorIntUnary(Op, DAG);\n\n  // Decompose 512-bit ops into smaller 256-bit ops.\n  if (VT.is512BitVector() && !Subtarget.hasBWI())\n    return splitVectorIntUnary(Op, DAG);\n\n  // For element types greater than i8, do vXi8 pop counts and a bytesum.\n  if (VT.getScalarType() != MVT::i8) {\n    MVT ByteVT = MVT::getVectorVT(MVT::i8, VT.getSizeInBits() / 8);\n    SDValue ByteOp = DAG.getBitcast(ByteVT, Op0);\n    SDValue PopCnt8 = DAG.getNode(ISD::CTPOP, DL, ByteVT, ByteOp);\n    return LowerHorizontalByteSum(PopCnt8, VT, Subtarget, DAG);\n  }\n\n  // We can't use the fast LUT approach, so fall back on LegalizeDAG.\n  if (!Subtarget.hasSSSE3())\n    return SDValue();\n\n  return LowerVectorCTPOPInRegLUT(Op0, DL, Subtarget, DAG);\n}\n\nstatic SDValue LowerCTPOP(SDValue Op, const X86Subtarget &Subtarget,\n                          SelectionDAG &DAG) {\n  assert(Op.getSimpleValueType().isVector() &&\n         \"We only do custom lowering for vector population count.\");\n  return LowerVectorCTPOP(Op, Subtarget, DAG);\n}\n\nstatic SDValue LowerBITREVERSE_XOP(SDValue Op, SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n  SDValue In = Op.getOperand(0);\n  SDLoc DL(Op);\n\n  // For scalars, its still beneficial to transfer to/from the SIMD unit to\n  // perform the BITREVERSE.\n  if (!VT.isVector()) {\n    MVT VecVT = MVT::getVectorVT(VT, 128 / VT.getSizeInBits());\n    SDValue Res = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecVT, In);\n    Res = DAG.getNode(ISD::BITREVERSE, DL, VecVT, Res);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Res,\n                       DAG.getIntPtrConstant(0, DL));\n  }\n\n  int NumElts = VT.getVectorNumElements();\n  int ScalarSizeInBytes = VT.getScalarSizeInBits() / 8;\n\n  // Decompose 256-bit ops into smaller 128-bit ops.\n  if (VT.is256BitVector())\n    return splitVectorIntUnary(Op, DAG);\n\n  assert(VT.is128BitVector() &&\n         \"Only 128-bit vector bitreverse lowering supported.\");\n\n  // VPPERM reverses the bits of a byte with the permute Op (2 << 5), and we\n  // perform the BSWAP in the shuffle.\n  // Its best to shuffle using the second operand as this will implicitly allow\n  // memory folding for multiple vectors.\n  SmallVector<SDValue, 16> MaskElts;\n  for (int i = 0; i != NumElts; ++i) {\n    for (int j = ScalarSizeInBytes - 1; j >= 0; --j) {\n      int SourceByte = 16 + (i * ScalarSizeInBytes) + j;\n      int PermuteByte = SourceByte | (2 << 5);\n      MaskElts.push_back(DAG.getConstant(PermuteByte, DL, MVT::i8));\n    }\n  }\n\n  SDValue Mask = DAG.getBuildVector(MVT::v16i8, DL, MaskElts);\n  SDValue Res = DAG.getBitcast(MVT::v16i8, In);\n  Res = DAG.getNode(X86ISD::VPPERM, DL, MVT::v16i8, DAG.getUNDEF(MVT::v16i8),\n                    Res, Mask);\n  return DAG.getBitcast(VT, Res);\n}\n\nstatic SDValue LowerBITREVERSE(SDValue Op, const X86Subtarget &Subtarget,\n                               SelectionDAG &DAG) {\n  MVT VT = Op.getSimpleValueType();\n\n  if (Subtarget.hasXOP() && !VT.is512BitVector())\n    return LowerBITREVERSE_XOP(Op, DAG);\n\n  assert(Subtarget.hasSSSE3() && \"SSSE3 required for BITREVERSE\");\n\n  SDValue In = Op.getOperand(0);\n  SDLoc DL(Op);\n\n  assert(VT.getScalarType() == MVT::i8 &&\n         \"Only byte vector BITREVERSE supported\");\n\n  // Split v64i8 without BWI so that we can still use the PSHUFB lowering.\n  if (VT == MVT::v64i8 && !Subtarget.hasBWI())\n    return splitVectorIntUnary(Op, DAG);\n\n  // Decompose 256-bit ops into smaller 128-bit ops on pre-AVX2.\n  if (VT == MVT::v32i8 && !Subtarget.hasInt256())\n    return splitVectorIntUnary(Op, DAG);\n\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // If we have GFNI, we can use GF2P8AFFINEQB to reverse the bits.\n  if (Subtarget.hasGFNI()) {\n    MVT MatrixVT = MVT::getVectorVT(MVT::i64, NumElts / 8);\n    SDValue Matrix = DAG.getConstant(0x8040201008040201ULL, DL, MatrixVT);\n    Matrix = DAG.getBitcast(VT, Matrix);\n    return DAG.getNode(X86ISD::GF2P8AFFINEQB, DL, VT, In, Matrix,\n                       DAG.getTargetConstant(0, DL, MVT::i8));\n  }\n\n  // Perform BITREVERSE using PSHUFB lookups. Each byte is split into\n  // two nibbles and a PSHUFB lookup to find the bitreverse of each\n  // 0-15 value (moved to the other nibble).\n  SDValue NibbleMask = DAG.getConstant(0xF, DL, VT);\n  SDValue Lo = DAG.getNode(ISD::AND, DL, VT, In, NibbleMask);\n  SDValue Hi = DAG.getNode(ISD::SRL, DL, VT, In, DAG.getConstant(4, DL, VT));\n\n  const int LoLUT[16] = {\n      /* 0 */ 0x00, /* 1 */ 0x80, /* 2 */ 0x40, /* 3 */ 0xC0,\n      /* 4 */ 0x20, /* 5 */ 0xA0, /* 6 */ 0x60, /* 7 */ 0xE0,\n      /* 8 */ 0x10, /* 9 */ 0x90, /* a */ 0x50, /* b */ 0xD0,\n      /* c */ 0x30, /* d */ 0xB0, /* e */ 0x70, /* f */ 0xF0};\n  const int HiLUT[16] = {\n      /* 0 */ 0x00, /* 1 */ 0x08, /* 2 */ 0x04, /* 3 */ 0x0C,\n      /* 4 */ 0x02, /* 5 */ 0x0A, /* 6 */ 0x06, /* 7 */ 0x0E,\n      /* 8 */ 0x01, /* 9 */ 0x09, /* a */ 0x05, /* b */ 0x0D,\n      /* c */ 0x03, /* d */ 0x0B, /* e */ 0x07, /* f */ 0x0F};\n\n  SmallVector<SDValue, 16> LoMaskElts, HiMaskElts;\n  for (unsigned i = 0; i < NumElts; ++i) {\n    LoMaskElts.push_back(DAG.getConstant(LoLUT[i % 16], DL, MVT::i8));\n    HiMaskElts.push_back(DAG.getConstant(HiLUT[i % 16], DL, MVT::i8));\n  }\n\n  SDValue LoMask = DAG.getBuildVector(VT, DL, LoMaskElts);\n  SDValue HiMask = DAG.getBuildVector(VT, DL, HiMaskElts);\n  Lo = DAG.getNode(X86ISD::PSHUFB, DL, VT, LoMask, Lo);\n  Hi = DAG.getNode(X86ISD::PSHUFB, DL, VT, HiMask, Hi);\n  return DAG.getNode(ISD::OR, DL, VT, Lo, Hi);\n}\n\nstatic SDValue LowerPARITY(SDValue Op, const X86Subtarget &Subtarget,\n                           SelectionDAG &DAG) {\n  SDLoc DL(Op);\n  SDValue X = Op.getOperand(0);\n  MVT VT = Op.getSimpleValueType();\n\n  // Special case. If the input fits in 8-bits we can use a single 8-bit TEST.\n  if (VT == MVT::i8 ||\n      DAG.MaskedValueIsZero(X, APInt::getBitsSetFrom(VT.getSizeInBits(), 8))) {\n    X = DAG.getNode(ISD::TRUNCATE, DL, MVT::i8, X);\n    SDValue Flags = DAG.getNode(X86ISD::CMP, DL, MVT::i32, X,\n                                DAG.getConstant(0, DL, MVT::i8));\n    // Copy the inverse of the parity flag into a register with setcc.\n    SDValue Setnp = getSETCC(X86::COND_NP, Flags, DL, DAG);\n    // Extend to the original type.\n    return DAG.getNode(ISD::ZERO_EXTEND, DL, VT, Setnp);\n  }\n\n  if (VT == MVT::i64) {\n    // Xor the high and low 16-bits together using a 32-bit operation.\n    SDValue Hi = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32,\n                             DAG.getNode(ISD::SRL, DL, MVT::i64, X,\n                                         DAG.getConstant(32, DL, MVT::i8)));\n    SDValue Lo = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, X);\n    X = DAG.getNode(ISD::XOR, DL, MVT::i32, Lo, Hi);\n  }\n\n  if (VT != MVT::i16) {\n    // Xor the high and low 16-bits together using a 32-bit operation.\n    SDValue Hi16 = DAG.getNode(ISD::SRL, DL, MVT::i32, X,\n                               DAG.getConstant(16, DL, MVT::i8));\n    X = DAG.getNode(ISD::XOR, DL, MVT::i32, X, Hi16);\n  } else {\n    // If the input is 16-bits, we need to extend to use an i32 shift below.\n    X = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, X);\n  }\n\n  // Finally xor the low 2 bytes together and use a 8-bit flag setting xor.\n  // This should allow an h-reg to be used to save a shift.\n  SDValue Hi = DAG.getNode(\n      ISD::TRUNCATE, DL, MVT::i8,\n      DAG.getNode(ISD::SRL, DL, MVT::i32, X, DAG.getConstant(8, DL, MVT::i8)));\n  SDValue Lo = DAG.getNode(ISD::TRUNCATE, DL, MVT::i8, X);\n  SDVTList VTs = DAG.getVTList(MVT::i8, MVT::i32);\n  SDValue Flags = DAG.getNode(X86ISD::XOR, DL, VTs, Lo, Hi).getValue(1);\n\n  // Copy the inverse of the parity flag into a register with setcc.\n  SDValue Setnp = getSETCC(X86::COND_NP, Flags, DL, DAG);\n  // Extend to the original type.\n  return DAG.getNode(ISD::ZERO_EXTEND, DL, VT, Setnp);\n}\n\nstatic SDValue lowerAtomicArithWithLOCK(SDValue N, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  unsigned NewOpc = 0;\n  switch (N->getOpcode()) {\n  case ISD::ATOMIC_LOAD_ADD:\n    NewOpc = X86ISD::LADD;\n    break;\n  case ISD::ATOMIC_LOAD_SUB:\n    NewOpc = X86ISD::LSUB;\n    break;\n  case ISD::ATOMIC_LOAD_OR:\n    NewOpc = X86ISD::LOR;\n    break;\n  case ISD::ATOMIC_LOAD_XOR:\n    NewOpc = X86ISD::LXOR;\n    break;\n  case ISD::ATOMIC_LOAD_AND:\n    NewOpc = X86ISD::LAND;\n    break;\n  default:\n    llvm_unreachable(\"Unknown ATOMIC_LOAD_ opcode\");\n  }\n\n  MachineMemOperand *MMO = cast<MemSDNode>(N)->getMemOperand();\n\n  return DAG.getMemIntrinsicNode(\n      NewOpc, SDLoc(N), DAG.getVTList(MVT::i32, MVT::Other),\n      {N->getOperand(0), N->getOperand(1), N->getOperand(2)},\n      /*MemVT=*/N->getSimpleValueType(0), MMO);\n}\n\n/// Lower atomic_load_ops into LOCK-prefixed operations.\nstatic SDValue lowerAtomicArith(SDValue N, SelectionDAG &DAG,\n                                const X86Subtarget &Subtarget) {\n  AtomicSDNode *AN = cast<AtomicSDNode>(N.getNode());\n  SDValue Chain = N->getOperand(0);\n  SDValue LHS = N->getOperand(1);\n  SDValue RHS = N->getOperand(2);\n  unsigned Opc = N->getOpcode();\n  MVT VT = N->getSimpleValueType(0);\n  SDLoc DL(N);\n\n  // We can lower atomic_load_add into LXADD. However, any other atomicrmw op\n  // can only be lowered when the result is unused.  They should have already\n  // been transformed into a cmpxchg loop in AtomicExpand.\n  if (N->hasAnyUseOfValue(0)) {\n    // Handle (atomic_load_sub p, v) as (atomic_load_add p, -v), to be able to\n    // select LXADD if LOCK_SUB can't be selected.\n    if (Opc == ISD::ATOMIC_LOAD_SUB) {\n      RHS = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT), RHS);\n      return DAG.getAtomic(ISD::ATOMIC_LOAD_ADD, DL, VT, Chain, LHS,\n                           RHS, AN->getMemOperand());\n    }\n    assert(Opc == ISD::ATOMIC_LOAD_ADD &&\n           \"Used AtomicRMW ops other than Add should have been expanded!\");\n    return N;\n  }\n\n  // Specialized lowering for the canonical form of an idemptotent atomicrmw.\n  // The core idea here is that since the memory location isn't actually\n  // changing, all we need is a lowering for the *ordering* impacts of the\n  // atomicrmw.  As such, we can chose a different operation and memory\n  // location to minimize impact on other code.\n  if (Opc == ISD::ATOMIC_LOAD_OR && isNullConstant(RHS)) {\n    // On X86, the only ordering which actually requires an instruction is\n    // seq_cst which isn't SingleThread, everything just needs to be preserved\n    // during codegen and then dropped. Note that we expect (but don't assume),\n    // that orderings other than seq_cst and acq_rel have been canonicalized to\n    // a store or load.\n    if (AN->getOrdering() == AtomicOrdering::SequentiallyConsistent &&\n        AN->getSyncScopeID() == SyncScope::System) {\n      // Prefer a locked operation against a stack location to minimize cache\n      // traffic.  This assumes that stack locations are very likely to be\n      // accessed only by the owning thread.\n      SDValue NewChain = emitLockedStackOp(DAG, Subtarget, Chain, DL);\n      assert(!N->hasAnyUseOfValue(0));\n      // NOTE: The getUNDEF is needed to give something for the unused result 0.\n      return DAG.getNode(ISD::MERGE_VALUES, DL, N->getVTList(),\n                         DAG.getUNDEF(VT), NewChain);\n    }\n    // MEMBARRIER is a compiler barrier; it codegens to a no-op.\n    SDValue NewChain = DAG.getNode(X86ISD::MEMBARRIER, DL, MVT::Other, Chain);\n    assert(!N->hasAnyUseOfValue(0));\n    // NOTE: The getUNDEF is needed to give something for the unused result 0.\n    return DAG.getNode(ISD::MERGE_VALUES, DL, N->getVTList(),\n                       DAG.getUNDEF(VT), NewChain);\n  }\n\n  SDValue LockOp = lowerAtomicArithWithLOCK(N, DAG, Subtarget);\n  // RAUW the chain, but don't worry about the result, as it's unused.\n  assert(!N->hasAnyUseOfValue(0));\n  // NOTE: The getUNDEF is needed to give something for the unused result 0.\n  return DAG.getNode(ISD::MERGE_VALUES, DL, N->getVTList(),\n                     DAG.getUNDEF(VT), LockOp.getValue(1));\n}\n\nstatic SDValue LowerATOMIC_STORE(SDValue Op, SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget) {\n  auto *Node = cast<AtomicSDNode>(Op.getNode());\n  SDLoc dl(Node);\n  EVT VT = Node->getMemoryVT();\n\n  bool IsSeqCst = Node->getOrdering() == AtomicOrdering::SequentiallyConsistent;\n  bool IsTypeLegal = DAG.getTargetLoweringInfo().isTypeLegal(VT);\n\n  // If this store is not sequentially consistent and the type is legal\n  // we can just keep it.\n  if (!IsSeqCst && IsTypeLegal)\n    return Op;\n\n  if (VT == MVT::i64 && !IsTypeLegal) {\n    // For illegal i64 atomic_stores, we can try to use MOVQ or MOVLPS if SSE\n    // is enabled.\n    bool NoImplicitFloatOps =\n        DAG.getMachineFunction().getFunction().hasFnAttribute(\n            Attribute::NoImplicitFloat);\n    if (!Subtarget.useSoftFloat() && !NoImplicitFloatOps) {\n      SDValue Chain;\n      if (Subtarget.hasSSE1()) {\n        SDValue SclToVec = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64,\n                                       Node->getOperand(2));\n        MVT StVT = Subtarget.hasSSE2() ? MVT::v2i64 : MVT::v4f32;\n        SclToVec = DAG.getBitcast(StVT, SclToVec);\n        SDVTList Tys = DAG.getVTList(MVT::Other);\n        SDValue Ops[] = {Node->getChain(), SclToVec, Node->getBasePtr()};\n        Chain = DAG.getMemIntrinsicNode(X86ISD::VEXTRACT_STORE, dl, Tys, Ops,\n                                        MVT::i64, Node->getMemOperand());\n      } else if (Subtarget.hasX87()) {\n        // First load this into an 80-bit X87 register using a stack temporary.\n        // This will put the whole integer into the significand.\n        SDValue StackPtr = DAG.CreateStackTemporary(MVT::i64);\n        int SPFI = cast<FrameIndexSDNode>(StackPtr.getNode())->getIndex();\n        MachinePointerInfo MPI =\n            MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SPFI);\n        Chain =\n            DAG.getStore(Node->getChain(), dl, Node->getOperand(2), StackPtr,\n                         MPI, MaybeAlign(), MachineMemOperand::MOStore);\n        SDVTList Tys = DAG.getVTList(MVT::f80, MVT::Other);\n        SDValue LdOps[] = {Chain, StackPtr};\n        SDValue Value =\n            DAG.getMemIntrinsicNode(X86ISD::FILD, dl, Tys, LdOps, MVT::i64, MPI,\n                                    /*Align*/ None, MachineMemOperand::MOLoad);\n        Chain = Value.getValue(1);\n\n        // Now use an FIST to do the atomic store.\n        SDValue StoreOps[] = {Chain, Value, Node->getBasePtr()};\n        Chain =\n            DAG.getMemIntrinsicNode(X86ISD::FIST, dl, DAG.getVTList(MVT::Other),\n                                    StoreOps, MVT::i64, Node->getMemOperand());\n      }\n\n      if (Chain) {\n        // If this is a sequentially consistent store, also emit an appropriate\n        // barrier.\n        if (IsSeqCst)\n          Chain = emitLockedStackOp(DAG, Subtarget, Chain, dl);\n\n        return Chain;\n      }\n    }\n  }\n\n  // Convert seq_cst store -> xchg\n  // Convert wide store -> swap (-> cmpxchg8b/cmpxchg16b)\n  // FIXME: 16-byte ATOMIC_SWAP isn't actually hooked up at the moment.\n  SDValue Swap = DAG.getAtomic(ISD::ATOMIC_SWAP, dl,\n                               Node->getMemoryVT(),\n                               Node->getOperand(0),\n                               Node->getOperand(1), Node->getOperand(2),\n                               Node->getMemOperand());\n  return Swap.getValue(1);\n}\n\nstatic SDValue LowerADDSUBCARRY(SDValue Op, SelectionDAG &DAG) {\n  SDNode *N = Op.getNode();\n  MVT VT = N->getSimpleValueType(0);\n  unsigned Opc = Op.getOpcode();\n\n  // Let legalize expand this if it isn't a legal type yet.\n  if (!DAG.getTargetLoweringInfo().isTypeLegal(VT))\n    return SDValue();\n\n  SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n  SDLoc DL(N);\n\n  // Set the carry flag.\n  SDValue Carry = Op.getOperand(2);\n  EVT CarryVT = Carry.getValueType();\n  Carry = DAG.getNode(X86ISD::ADD, DL, DAG.getVTList(CarryVT, MVT::i32),\n                      Carry, DAG.getAllOnesConstant(DL, CarryVT));\n\n  bool IsAdd = Opc == ISD::ADDCARRY || Opc == ISD::SADDO_CARRY;\n  SDValue Sum = DAG.getNode(IsAdd ? X86ISD::ADC : X86ISD::SBB, DL, VTs,\n                            Op.getOperand(0), Op.getOperand(1),\n                            Carry.getValue(1));\n\n  bool IsSigned = Opc == ISD::SADDO_CARRY || Opc == ISD::SSUBO_CARRY;\n  SDValue SetCC = getSETCC(IsSigned ? X86::COND_O : X86::COND_B,\n                           Sum.getValue(1), DL, DAG);\n  if (N->getValueType(1) == MVT::i1)\n    SetCC = DAG.getNode(ISD::TRUNCATE, DL, MVT::i1, SetCC);\n\n  return DAG.getNode(ISD::MERGE_VALUES, DL, N->getVTList(), Sum, SetCC);\n}\n\nstatic SDValue LowerFSINCOS(SDValue Op, const X86Subtarget &Subtarget,\n                            SelectionDAG &DAG) {\n  assert(Subtarget.isTargetDarwin() && Subtarget.is64Bit());\n\n  // For MacOSX, we want to call an alternative entry point: __sincos_stret,\n  // which returns the values as { float, float } (in XMM0) or\n  // { double, double } (which is returned in XMM0, XMM1).\n  SDLoc dl(Op);\n  SDValue Arg = Op.getOperand(0);\n  EVT ArgVT = Arg.getValueType();\n  Type *ArgTy = ArgVT.getTypeForEVT(*DAG.getContext());\n\n  TargetLowering::ArgListTy Args;\n  TargetLowering::ArgListEntry Entry;\n\n  Entry.Node = Arg;\n  Entry.Ty = ArgTy;\n  Entry.IsSExt = false;\n  Entry.IsZExt = false;\n  Args.push_back(Entry);\n\n  bool isF64 = ArgVT == MVT::f64;\n  // Only optimize x86_64 for now. i386 is a bit messy. For f32,\n  // the small struct {f32, f32} is returned in (eax, edx). For f64,\n  // the results are returned via SRet in memory.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  RTLIB::Libcall LC = isF64 ? RTLIB::SINCOS_STRET_F64 : RTLIB::SINCOS_STRET_F32;\n  const char *LibcallName = TLI.getLibcallName(LC);\n  SDValue Callee =\n      DAG.getExternalSymbol(LibcallName, TLI.getPointerTy(DAG.getDataLayout()));\n\n  Type *RetTy = isF64 ? (Type *)StructType::get(ArgTy, ArgTy)\n                      : (Type *)FixedVectorType::get(ArgTy, 4);\n\n  TargetLowering::CallLoweringInfo CLI(DAG);\n  CLI.setDebugLoc(dl)\n      .setChain(DAG.getEntryNode())\n      .setLibCallee(CallingConv::C, RetTy, Callee, std::move(Args));\n\n  std::pair<SDValue, SDValue> CallResult = TLI.LowerCallTo(CLI);\n\n  if (isF64)\n    // Returned in xmm0 and xmm1.\n    return CallResult.first;\n\n  // Returned in bits 0:31 and 32:64 xmm0.\n  SDValue SinVal = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, ArgVT,\n                               CallResult.first, DAG.getIntPtrConstant(0, dl));\n  SDValue CosVal = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, ArgVT,\n                               CallResult.first, DAG.getIntPtrConstant(1, dl));\n  SDVTList Tys = DAG.getVTList(ArgVT, ArgVT);\n  return DAG.getNode(ISD::MERGE_VALUES, dl, Tys, SinVal, CosVal);\n}\n\n/// Widen a vector input to a vector of NVT.  The\n/// input vector must have the same element type as NVT.\nstatic SDValue ExtendToType(SDValue InOp, MVT NVT, SelectionDAG &DAG,\n                            bool FillWithZeroes = false) {\n  // Check if InOp already has the right width.\n  MVT InVT = InOp.getSimpleValueType();\n  if (InVT == NVT)\n    return InOp;\n\n  if (InOp.isUndef())\n    return DAG.getUNDEF(NVT);\n\n  assert(InVT.getVectorElementType() == NVT.getVectorElementType() &&\n         \"input and widen element type must match\");\n\n  unsigned InNumElts = InVT.getVectorNumElements();\n  unsigned WidenNumElts = NVT.getVectorNumElements();\n  assert(WidenNumElts > InNumElts && WidenNumElts % InNumElts == 0 &&\n         \"Unexpected request for vector widening\");\n\n  SDLoc dl(InOp);\n  if (InOp.getOpcode() == ISD::CONCAT_VECTORS &&\n      InOp.getNumOperands() == 2) {\n    SDValue N1 = InOp.getOperand(1);\n    if ((ISD::isBuildVectorAllZeros(N1.getNode()) && FillWithZeroes) ||\n        N1.isUndef()) {\n      InOp = InOp.getOperand(0);\n      InVT = InOp.getSimpleValueType();\n      InNumElts = InVT.getVectorNumElements();\n    }\n  }\n  if (ISD::isBuildVectorOfConstantSDNodes(InOp.getNode()) ||\n      ISD::isBuildVectorOfConstantFPSDNodes(InOp.getNode())) {\n    SmallVector<SDValue, 16> Ops;\n    for (unsigned i = 0; i < InNumElts; ++i)\n      Ops.push_back(InOp.getOperand(i));\n\n    EVT EltVT = InOp.getOperand(0).getValueType();\n\n    SDValue FillVal = FillWithZeroes ? DAG.getConstant(0, dl, EltVT) :\n      DAG.getUNDEF(EltVT);\n    for (unsigned i = 0; i < WidenNumElts - InNumElts; ++i)\n      Ops.push_back(FillVal);\n    return DAG.getBuildVector(NVT, dl, Ops);\n  }\n  SDValue FillVal = FillWithZeroes ? DAG.getConstant(0, dl, NVT) :\n    DAG.getUNDEF(NVT);\n  return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, NVT, FillVal,\n                     InOp, DAG.getIntPtrConstant(0, dl));\n}\n\nstatic SDValue LowerMSCATTER(SDValue Op, const X86Subtarget &Subtarget,\n                             SelectionDAG &DAG) {\n  assert(Subtarget.hasAVX512() &&\n         \"MGATHER/MSCATTER are supported on AVX-512 arch only\");\n\n  MaskedScatterSDNode *N = cast<MaskedScatterSDNode>(Op.getNode());\n  SDValue Src = N->getValue();\n  MVT VT = Src.getSimpleValueType();\n  assert(VT.getScalarSizeInBits() >= 32 && \"Unsupported scatter op\");\n  SDLoc dl(Op);\n\n  SDValue Scale = N->getScale();\n  SDValue Index = N->getIndex();\n  SDValue Mask = N->getMask();\n  SDValue Chain = N->getChain();\n  SDValue BasePtr = N->getBasePtr();\n\n  if (VT == MVT::v2f32 || VT == MVT::v2i32) {\n    assert(Mask.getValueType() == MVT::v2i1 && \"Unexpected mask type\");\n    // If the index is v2i64 and we have VLX we can use xmm for data and index.\n    if (Index.getValueType() == MVT::v2i64 && Subtarget.hasVLX()) {\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      EVT WideVT = TLI.getTypeToTransformTo(*DAG.getContext(), VT);\n      Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, WideVT, Src, DAG.getUNDEF(VT));\n      SDVTList VTs = DAG.getVTList(MVT::Other);\n      SDValue Ops[] = {Chain, Src, Mask, BasePtr, Index, Scale};\n      return DAG.getMemIntrinsicNode(X86ISD::MSCATTER, dl, VTs, Ops,\n                                     N->getMemoryVT(), N->getMemOperand());\n    }\n    return SDValue();\n  }\n\n  MVT IndexVT = Index.getSimpleValueType();\n\n  // If the index is v2i32, we're being called by type legalization and we\n  // should just let the default handling take care of it.\n  if (IndexVT == MVT::v2i32)\n    return SDValue();\n\n  // If we don't have VLX and neither the passthru or index is 512-bits, we\n  // need to widen until one is.\n  if (!Subtarget.hasVLX() && !VT.is512BitVector() &&\n      !Index.getSimpleValueType().is512BitVector()) {\n    // Determine how much we need to widen by to get a 512-bit type.\n    unsigned Factor = std::min(512/VT.getSizeInBits(),\n                               512/IndexVT.getSizeInBits());\n    unsigned NumElts = VT.getVectorNumElements() * Factor;\n\n    VT = MVT::getVectorVT(VT.getVectorElementType(), NumElts);\n    IndexVT = MVT::getVectorVT(IndexVT.getVectorElementType(), NumElts);\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, NumElts);\n\n    Src = ExtendToType(Src, VT, DAG);\n    Index = ExtendToType(Index, IndexVT, DAG);\n    Mask = ExtendToType(Mask, MaskVT, DAG, true);\n  }\n\n  SDVTList VTs = DAG.getVTList(MVT::Other);\n  SDValue Ops[] = {Chain, Src, Mask, BasePtr, Index, Scale};\n  return DAG.getMemIntrinsicNode(X86ISD::MSCATTER, dl, VTs, Ops,\n                                 N->getMemoryVT(), N->getMemOperand());\n}\n\nstatic SDValue LowerMLOAD(SDValue Op, const X86Subtarget &Subtarget,\n                          SelectionDAG &DAG) {\n\n  MaskedLoadSDNode *N = cast<MaskedLoadSDNode>(Op.getNode());\n  MVT VT = Op.getSimpleValueType();\n  MVT ScalarVT = VT.getScalarType();\n  SDValue Mask = N->getMask();\n  MVT MaskVT = Mask.getSimpleValueType();\n  SDValue PassThru = N->getPassThru();\n  SDLoc dl(Op);\n\n  // Handle AVX masked loads which don't support passthru other than 0.\n  if (MaskVT.getVectorElementType() != MVT::i1) {\n    // We also allow undef in the isel pattern.\n    if (PassThru.isUndef() || ISD::isBuildVectorAllZeros(PassThru.getNode()))\n      return Op;\n\n    SDValue NewLoad = DAG.getMaskedLoad(\n        VT, dl, N->getChain(), N->getBasePtr(), N->getOffset(), Mask,\n        getZeroVector(VT, Subtarget, DAG, dl), N->getMemoryVT(),\n        N->getMemOperand(), N->getAddressingMode(), N->getExtensionType(),\n        N->isExpandingLoad());\n    // Emit a blend.\n    SDValue Select = DAG.getNode(ISD::VSELECT, dl, VT, Mask, NewLoad, PassThru);\n    return DAG.getMergeValues({ Select, NewLoad.getValue(1) }, dl);\n  }\n\n  assert((!N->isExpandingLoad() || Subtarget.hasAVX512()) &&\n         \"Expanding masked load is supported on AVX-512 target only!\");\n\n  assert((!N->isExpandingLoad() || ScalarVT.getSizeInBits() >= 32) &&\n         \"Expanding masked load is supported for 32 and 64-bit types only!\");\n\n  assert(Subtarget.hasAVX512() && !Subtarget.hasVLX() && !VT.is512BitVector() &&\n         \"Cannot lower masked load op.\");\n\n  assert((ScalarVT.getSizeInBits() >= 32 ||\n          (Subtarget.hasBWI() &&\n              (ScalarVT == MVT::i8 || ScalarVT == MVT::i16))) &&\n         \"Unsupported masked load op.\");\n\n  // This operation is legal for targets with VLX, but without\n  // VLX the vector should be widened to 512 bit\n  unsigned NumEltsInWideVec = 512 / VT.getScalarSizeInBits();\n  MVT WideDataVT = MVT::getVectorVT(ScalarVT, NumEltsInWideVec);\n  PassThru = ExtendToType(PassThru, WideDataVT, DAG);\n\n  // Mask element has to be i1.\n  assert(Mask.getSimpleValueType().getScalarType() == MVT::i1 &&\n         \"Unexpected mask type\");\n\n  MVT WideMaskVT = MVT::getVectorVT(MVT::i1, NumEltsInWideVec);\n\n  Mask = ExtendToType(Mask, WideMaskVT, DAG, true);\n  SDValue NewLoad = DAG.getMaskedLoad(\n      WideDataVT, dl, N->getChain(), N->getBasePtr(), N->getOffset(), Mask,\n      PassThru, N->getMemoryVT(), N->getMemOperand(), N->getAddressingMode(),\n      N->getExtensionType(), N->isExpandingLoad());\n\n  SDValue Extract =\n      DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, NewLoad.getValue(0),\n                  DAG.getIntPtrConstant(0, dl));\n  SDValue RetOps[] = {Extract, NewLoad.getValue(1)};\n  return DAG.getMergeValues(RetOps, dl);\n}\n\nstatic SDValue LowerMSTORE(SDValue Op, const X86Subtarget &Subtarget,\n                           SelectionDAG &DAG) {\n  MaskedStoreSDNode *N = cast<MaskedStoreSDNode>(Op.getNode());\n  SDValue DataToStore = N->getValue();\n  MVT VT = DataToStore.getSimpleValueType();\n  MVT ScalarVT = VT.getScalarType();\n  SDValue Mask = N->getMask();\n  SDLoc dl(Op);\n\n  assert((!N->isCompressingStore() || Subtarget.hasAVX512()) &&\n         \"Expanding masked load is supported on AVX-512 target only!\");\n\n  assert((!N->isCompressingStore() || ScalarVT.getSizeInBits() >= 32) &&\n         \"Expanding masked load is supported for 32 and 64-bit types only!\");\n\n  assert(Subtarget.hasAVX512() && !Subtarget.hasVLX() && !VT.is512BitVector() &&\n         \"Cannot lower masked store op.\");\n\n  assert((ScalarVT.getSizeInBits() >= 32 ||\n          (Subtarget.hasBWI() &&\n              (ScalarVT == MVT::i8 || ScalarVT == MVT::i16))) &&\n          \"Unsupported masked store op.\");\n\n  // This operation is legal for targets with VLX, but without\n  // VLX the vector should be widened to 512 bit\n  unsigned NumEltsInWideVec = 512/VT.getScalarSizeInBits();\n  MVT WideDataVT = MVT::getVectorVT(ScalarVT, NumEltsInWideVec);\n\n  // Mask element has to be i1.\n  assert(Mask.getSimpleValueType().getScalarType() == MVT::i1 &&\n         \"Unexpected mask type\");\n\n  MVT WideMaskVT = MVT::getVectorVT(MVT::i1, NumEltsInWideVec);\n\n  DataToStore = ExtendToType(DataToStore, WideDataVT, DAG);\n  Mask = ExtendToType(Mask, WideMaskVT, DAG, true);\n  return DAG.getMaskedStore(N->getChain(), dl, DataToStore, N->getBasePtr(),\n                            N->getOffset(), Mask, N->getMemoryVT(),\n                            N->getMemOperand(), N->getAddressingMode(),\n                            N->isTruncatingStore(), N->isCompressingStore());\n}\n\nstatic SDValue LowerMGATHER(SDValue Op, const X86Subtarget &Subtarget,\n                            SelectionDAG &DAG) {\n  assert(Subtarget.hasAVX2() &&\n         \"MGATHER/MSCATTER are supported on AVX-512/AVX-2 arch only\");\n\n  MaskedGatherSDNode *N = cast<MaskedGatherSDNode>(Op.getNode());\n  SDLoc dl(Op);\n  MVT VT = Op.getSimpleValueType();\n  SDValue Index = N->getIndex();\n  SDValue Mask = N->getMask();\n  SDValue PassThru = N->getPassThru();\n  MVT IndexVT = Index.getSimpleValueType();\n\n  assert(VT.getScalarSizeInBits() >= 32 && \"Unsupported gather op\");\n\n  // If the index is v2i32, we're being called by type legalization.\n  if (IndexVT == MVT::v2i32)\n    return SDValue();\n\n  // If we don't have VLX and neither the passthru or index is 512-bits, we\n  // need to widen until one is.\n  MVT OrigVT = VT;\n  if (Subtarget.hasAVX512() && !Subtarget.hasVLX() && !VT.is512BitVector() &&\n      !IndexVT.is512BitVector()) {\n    // Determine how much we need to widen by to get a 512-bit type.\n    unsigned Factor = std::min(512/VT.getSizeInBits(),\n                               512/IndexVT.getSizeInBits());\n\n    unsigned NumElts = VT.getVectorNumElements() * Factor;\n\n    VT = MVT::getVectorVT(VT.getVectorElementType(), NumElts);\n    IndexVT = MVT::getVectorVT(IndexVT.getVectorElementType(), NumElts);\n    MVT MaskVT = MVT::getVectorVT(MVT::i1, NumElts);\n\n    PassThru = ExtendToType(PassThru, VT, DAG);\n    Index = ExtendToType(Index, IndexVT, DAG);\n    Mask = ExtendToType(Mask, MaskVT, DAG, true);\n  }\n\n  SDValue Ops[] = { N->getChain(), PassThru, Mask, N->getBasePtr(), Index,\n                    N->getScale() };\n  SDValue NewGather = DAG.getMemIntrinsicNode(\n      X86ISD::MGATHER, dl, DAG.getVTList(VT, MVT::Other), Ops, N->getMemoryVT(),\n      N->getMemOperand());\n  SDValue Extract = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, OrigVT,\n                                NewGather, DAG.getIntPtrConstant(0, dl));\n  return DAG.getMergeValues({Extract, NewGather.getValue(1)}, dl);\n}\n\nstatic SDValue LowerADDRSPACECAST(SDValue Op, SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  SDValue Src = Op.getOperand(0);\n  MVT DstVT = Op.getSimpleValueType();\n\n  AddrSpaceCastSDNode *N = cast<AddrSpaceCastSDNode>(Op.getNode());\n  unsigned SrcAS = N->getSrcAddressSpace();\n\n  assert(SrcAS != N->getDestAddressSpace() &&\n         \"addrspacecast must be between different address spaces\");\n\n  if (SrcAS == X86AS::PTR32_UPTR && DstVT == MVT::i64) {\n    Op = DAG.getNode(ISD::ZERO_EXTEND, dl, DstVT, Src);\n  } else if (DstVT == MVT::i64) {\n    Op = DAG.getNode(ISD::SIGN_EXTEND, dl, DstVT, Src);\n  } else if (DstVT == MVT::i32) {\n    Op = DAG.getNode(ISD::TRUNCATE, dl, DstVT, Src);\n  } else {\n    report_fatal_error(\"Bad address space in addrspacecast\");\n  }\n  return Op;\n}\n\nSDValue X86TargetLowering::LowerGC_TRANSITION(SDValue Op,\n                                              SelectionDAG &DAG) const {\n  // TODO: Eventually, the lowering of these nodes should be informed by or\n  // deferred to the GC strategy for the function in which they appear. For\n  // now, however, they must be lowered to something. Since they are logically\n  // no-ops in the case of a null GC strategy (or a GC strategy which does not\n  // require special handling for these nodes), lower them as literal NOOPs for\n  // the time being.\n  SmallVector<SDValue, 2> Ops;\n\n  Ops.push_back(Op.getOperand(0));\n  if (Op->getGluedNode())\n    Ops.push_back(Op->getOperand(Op->getNumOperands() - 1));\n\n  SDLoc OpDL(Op);\n  SDVTList VTs = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue NOOP(DAG.getMachineNode(X86::NOOP, SDLoc(Op), VTs, Ops), 0);\n\n  return NOOP;\n}\n\n// Custom split CVTPS2PH with wide types.\nstatic SDValue LowerCVTPS2PH(SDValue Op, SelectionDAG &DAG) {\n  SDLoc dl(Op);\n  EVT VT = Op.getValueType();\n  SDValue Lo, Hi;\n  std::tie(Lo, Hi) = DAG.SplitVectorOperand(Op.getNode(), 0);\n  EVT LoVT, HiVT;\n  std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n  SDValue RC = Op.getOperand(1);\n  Lo = DAG.getNode(X86ISD::CVTPS2PH, dl, LoVT, Lo, RC);\n  Hi = DAG.getNode(X86ISD::CVTPS2PH, dl, HiVT, Hi, RC);\n  return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n}\n\n/// Provide custom lowering hooks for some operations.\nSDValue X86TargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {\n  switch (Op.getOpcode()) {\n  default: llvm_unreachable(\"Should not custom lower this!\");\n  case ISD::ATOMIC_FENCE:       return LowerATOMIC_FENCE(Op, Subtarget, DAG);\n  case ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS:\n    return LowerCMP_SWAP(Op, Subtarget, DAG);\n  case ISD::CTPOP:              return LowerCTPOP(Op, Subtarget, DAG);\n  case ISD::ATOMIC_LOAD_ADD:\n  case ISD::ATOMIC_LOAD_SUB:\n  case ISD::ATOMIC_LOAD_OR:\n  case ISD::ATOMIC_LOAD_XOR:\n  case ISD::ATOMIC_LOAD_AND:    return lowerAtomicArith(Op, DAG, Subtarget);\n  case ISD::ATOMIC_STORE:       return LowerATOMIC_STORE(Op, DAG, Subtarget);\n  case ISD::BITREVERSE:         return LowerBITREVERSE(Op, Subtarget, DAG);\n  case ISD::PARITY:             return LowerPARITY(Op, Subtarget, DAG);\n  case ISD::BUILD_VECTOR:       return LowerBUILD_VECTOR(Op, DAG);\n  case ISD::CONCAT_VECTORS:     return LowerCONCAT_VECTORS(Op, Subtarget, DAG);\n  case ISD::VECTOR_SHUFFLE:     return lowerVECTOR_SHUFFLE(Op, Subtarget, DAG);\n  case ISD::VSELECT:            return LowerVSELECT(Op, DAG);\n  case ISD::EXTRACT_VECTOR_ELT: return LowerEXTRACT_VECTOR_ELT(Op, DAG);\n  case ISD::INSERT_VECTOR_ELT:  return LowerINSERT_VECTOR_ELT(Op, DAG);\n  case ISD::INSERT_SUBVECTOR:   return LowerINSERT_SUBVECTOR(Op, Subtarget,DAG);\n  case ISD::EXTRACT_SUBVECTOR:  return LowerEXTRACT_SUBVECTOR(Op,Subtarget,DAG);\n  case ISD::SCALAR_TO_VECTOR:   return LowerSCALAR_TO_VECTOR(Op, Subtarget,DAG);\n  case ISD::ConstantPool:       return LowerConstantPool(Op, DAG);\n  case ISD::GlobalAddress:      return LowerGlobalAddress(Op, DAG);\n  case ISD::GlobalTLSAddress:   return LowerGlobalTLSAddress(Op, DAG);\n  case ISD::ExternalSymbol:     return LowerExternalSymbol(Op, DAG);\n  case ISD::BlockAddress:       return LowerBlockAddress(Op, DAG);\n  case ISD::SHL_PARTS:\n  case ISD::SRA_PARTS:\n  case ISD::SRL_PARTS:          return LowerShiftParts(Op, DAG);\n  case ISD::FSHL:\n  case ISD::FSHR:               return LowerFunnelShift(Op, Subtarget, DAG);\n  case ISD::STRICT_SINT_TO_FP:\n  case ISD::SINT_TO_FP:         return LowerSINT_TO_FP(Op, DAG);\n  case ISD::STRICT_UINT_TO_FP:\n  case ISD::UINT_TO_FP:         return LowerUINT_TO_FP(Op, DAG);\n  case ISD::TRUNCATE:           return LowerTRUNCATE(Op, DAG);\n  case ISD::ZERO_EXTEND:        return LowerZERO_EXTEND(Op, Subtarget, DAG);\n  case ISD::SIGN_EXTEND:        return LowerSIGN_EXTEND(Op, Subtarget, DAG);\n  case ISD::ANY_EXTEND:         return LowerANY_EXTEND(Op, Subtarget, DAG);\n  case ISD::ZERO_EXTEND_VECTOR_INREG:\n  case ISD::SIGN_EXTEND_VECTOR_INREG:\n    return LowerEXTEND_VECTOR_INREG(Op, Subtarget, DAG);\n  case ISD::FP_TO_SINT:\n  case ISD::STRICT_FP_TO_SINT:\n  case ISD::FP_TO_UINT:\n  case ISD::STRICT_FP_TO_UINT:  return LowerFP_TO_INT(Op, DAG);\n  case ISD::FP_TO_SINT_SAT:\n  case ISD::FP_TO_UINT_SAT:     return LowerFP_TO_INT_SAT(Op, DAG);\n  case ISD::FP_EXTEND:\n  case ISD::STRICT_FP_EXTEND:   return LowerFP_EXTEND(Op, DAG);\n  case ISD::FP_ROUND:\n  case ISD::STRICT_FP_ROUND:    return LowerFP_ROUND(Op, DAG);\n  case ISD::FP16_TO_FP:\n  case ISD::STRICT_FP16_TO_FP:  return LowerFP16_TO_FP(Op, DAG);\n  case ISD::FP_TO_FP16:\n  case ISD::STRICT_FP_TO_FP16:  return LowerFP_TO_FP16(Op, DAG);\n  case ISD::LOAD:               return LowerLoad(Op, Subtarget, DAG);\n  case ISD::STORE:              return LowerStore(Op, Subtarget, DAG);\n  case ISD::FADD:\n  case ISD::FSUB:               return lowerFaddFsub(Op, DAG);\n  case ISD::FROUND:             return LowerFROUND(Op, DAG);\n  case ISD::FABS:\n  case ISD::FNEG:               return LowerFABSorFNEG(Op, DAG);\n  case ISD::FCOPYSIGN:          return LowerFCOPYSIGN(Op, DAG);\n  case ISD::FGETSIGN:           return LowerFGETSIGN(Op, DAG);\n  case ISD::LRINT:\n  case ISD::LLRINT:             return LowerLRINT_LLRINT(Op, DAG);\n  case ISD::SETCC:\n  case ISD::STRICT_FSETCC:\n  case ISD::STRICT_FSETCCS:     return LowerSETCC(Op, DAG);\n  case ISD::SETCCCARRY:         return LowerSETCCCARRY(Op, DAG);\n  case ISD::SELECT:             return LowerSELECT(Op, DAG);\n  case ISD::BRCOND:             return LowerBRCOND(Op, DAG);\n  case ISD::JumpTable:          return LowerJumpTable(Op, DAG);\n  case ISD::VASTART:            return LowerVASTART(Op, DAG);\n  case ISD::VAARG:              return LowerVAARG(Op, DAG);\n  case ISD::VACOPY:             return LowerVACOPY(Op, Subtarget, DAG);\n  case ISD::INTRINSIC_WO_CHAIN: return LowerINTRINSIC_WO_CHAIN(Op, DAG);\n  case ISD::INTRINSIC_VOID:\n  case ISD::INTRINSIC_W_CHAIN:  return LowerINTRINSIC_W_CHAIN(Op, Subtarget, DAG);\n  case ISD::RETURNADDR:         return LowerRETURNADDR(Op, DAG);\n  case ISD::ADDROFRETURNADDR:   return LowerADDROFRETURNADDR(Op, DAG);\n  case ISD::FRAMEADDR:          return LowerFRAMEADDR(Op, DAG);\n  case ISD::FRAME_TO_ARGS_OFFSET:\n                                return LowerFRAME_TO_ARGS_OFFSET(Op, DAG);\n  case ISD::DYNAMIC_STACKALLOC: return LowerDYNAMIC_STACKALLOC(Op, DAG);\n  case ISD::EH_RETURN:          return LowerEH_RETURN(Op, DAG);\n  case ISD::EH_SJLJ_SETJMP:     return lowerEH_SJLJ_SETJMP(Op, DAG);\n  case ISD::EH_SJLJ_LONGJMP:    return lowerEH_SJLJ_LONGJMP(Op, DAG);\n  case ISD::EH_SJLJ_SETUP_DISPATCH:\n    return lowerEH_SJLJ_SETUP_DISPATCH(Op, DAG);\n  case ISD::INIT_TRAMPOLINE:    return LowerINIT_TRAMPOLINE(Op, DAG);\n  case ISD::ADJUST_TRAMPOLINE:  return LowerADJUST_TRAMPOLINE(Op, DAG);\n  case ISD::FLT_ROUNDS_:        return LowerFLT_ROUNDS_(Op, DAG);\n  case ISD::CTLZ:\n  case ISD::CTLZ_ZERO_UNDEF:    return LowerCTLZ(Op, Subtarget, DAG);\n  case ISD::CTTZ:\n  case ISD::CTTZ_ZERO_UNDEF:    return LowerCTTZ(Op, Subtarget, DAG);\n  case ISD::MUL:                return LowerMUL(Op, Subtarget, DAG);\n  case ISD::MULHS:\n  case ISD::MULHU:              return LowerMULH(Op, Subtarget, DAG);\n  case ISD::ROTL:\n  case ISD::ROTR:               return LowerRotate(Op, Subtarget, DAG);\n  case ISD::SRA:\n  case ISD::SRL:\n  case ISD::SHL:                return LowerShift(Op, Subtarget, DAG);\n  case ISD::SADDO:\n  case ISD::UADDO:\n  case ISD::SSUBO:\n  case ISD::USUBO:\n  case ISD::SMULO:\n  case ISD::UMULO:              return LowerXALUO(Op, DAG);\n  case ISD::READCYCLECOUNTER:   return LowerREADCYCLECOUNTER(Op, Subtarget,DAG);\n  case ISD::BITCAST:            return LowerBITCAST(Op, Subtarget, DAG);\n  case ISD::SADDO_CARRY:\n  case ISD::SSUBO_CARRY:\n  case ISD::ADDCARRY:\n  case ISD::SUBCARRY:           return LowerADDSUBCARRY(Op, DAG);\n  case ISD::ADD:\n  case ISD::SUB:                return lowerAddSub(Op, DAG, Subtarget);\n  case ISD::UADDSAT:\n  case ISD::SADDSAT:\n  case ISD::USUBSAT:\n  case ISD::SSUBSAT:            return LowerADDSAT_SUBSAT(Op, DAG, Subtarget);\n  case ISD::SMAX:\n  case ISD::SMIN:\n  case ISD::UMAX:\n  case ISD::UMIN:               return LowerMINMAX(Op, DAG);\n  case ISD::ABS:                return LowerABS(Op, Subtarget, DAG);\n  case ISD::FSINCOS:            return LowerFSINCOS(Op, Subtarget, DAG);\n  case ISD::MLOAD:              return LowerMLOAD(Op, Subtarget, DAG);\n  case ISD::MSTORE:             return LowerMSTORE(Op, Subtarget, DAG);\n  case ISD::MGATHER:            return LowerMGATHER(Op, Subtarget, DAG);\n  case ISD::MSCATTER:           return LowerMSCATTER(Op, Subtarget, DAG);\n  case ISD::GC_TRANSITION_START:\n  case ISD::GC_TRANSITION_END:  return LowerGC_TRANSITION(Op, DAG);\n  case ISD::ADDRSPACECAST:      return LowerADDRSPACECAST(Op, DAG);\n  case X86ISD::CVTPS2PH:        return LowerCVTPS2PH(Op, DAG);\n  }\n}\n\n/// Replace a node with an illegal result type with a new node built out of\n/// custom code.\nvoid X86TargetLowering::ReplaceNodeResults(SDNode *N,\n                                           SmallVectorImpl<SDValue>&Results,\n                                           SelectionDAG &DAG) const {\n  SDLoc dl(N);\n  switch (N->getOpcode()) {\n  default:\n#ifndef NDEBUG\n    dbgs() << \"ReplaceNodeResults: \";\n    N->dump(&DAG);\n#endif\n    llvm_unreachable(\"Do not know how to custom type legalize this operation!\");\n  case X86ISD::CVTPH2PS: {\n    EVT VT = N->getValueType(0);\n    SDValue Lo, Hi;\n    std::tie(Lo, Hi) = DAG.SplitVectorOperand(N, 0);\n    EVT LoVT, HiVT;\n    std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n    Lo = DAG.getNode(X86ISD::CVTPH2PS, dl, LoVT, Lo);\n    Hi = DAG.getNode(X86ISD::CVTPH2PS, dl, HiVT, Hi);\n    SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n    Results.push_back(Res);\n    return;\n  }\n  case X86ISD::STRICT_CVTPH2PS: {\n    EVT VT = N->getValueType(0);\n    SDValue Lo, Hi;\n    std::tie(Lo, Hi) = DAG.SplitVectorOperand(N, 1);\n    EVT LoVT, HiVT;\n    std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(VT);\n    Lo = DAG.getNode(X86ISD::STRICT_CVTPH2PS, dl, {LoVT, MVT::Other},\n                     {N->getOperand(0), Lo});\n    Hi = DAG.getNode(X86ISD::STRICT_CVTPH2PS, dl, {HiVT, MVT::Other},\n                     {N->getOperand(0), Hi});\n    SDValue Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other,\n                                Lo.getValue(1), Hi.getValue(1));\n    SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n    Results.push_back(Res);\n    Results.push_back(Chain);\n    return;\n  }\n  case X86ISD::CVTPS2PH:\n    Results.push_back(LowerCVTPS2PH(SDValue(N, 0), DAG));\n    return;\n  case ISD::CTPOP: {\n    assert(N->getValueType(0) == MVT::i64 && \"Unexpected VT!\");\n    // Use a v2i64 if possible.\n    bool NoImplicitFloatOps =\n        DAG.getMachineFunction().getFunction().hasFnAttribute(\n            Attribute::NoImplicitFloat);\n    if (isTypeLegal(MVT::v2i64) && !NoImplicitFloatOps) {\n      SDValue Wide =\n          DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v2i64, N->getOperand(0));\n      Wide = DAG.getNode(ISD::CTPOP, dl, MVT::v2i64, Wide);\n      // Bit count should fit in 32-bits, extract it as that and then zero\n      // extend to i64. Otherwise we end up extracting bits 63:32 separately.\n      Wide = DAG.getNode(ISD::BITCAST, dl, MVT::v4i32, Wide);\n      Wide = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i32, Wide,\n                         DAG.getIntPtrConstant(0, dl));\n      Wide = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i64, Wide);\n      Results.push_back(Wide);\n    }\n    return;\n  }\n  case ISD::MUL: {\n    EVT VT = N->getValueType(0);\n    assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n           VT.getVectorElementType() == MVT::i8 && \"Unexpected VT!\");\n    // Pre-promote these to vXi16 to avoid op legalization thinking all 16\n    // elements are needed.\n    MVT MulVT = MVT::getVectorVT(MVT::i16, VT.getVectorNumElements());\n    SDValue Op0 = DAG.getNode(ISD::ANY_EXTEND, dl, MulVT, N->getOperand(0));\n    SDValue Op1 = DAG.getNode(ISD::ANY_EXTEND, dl, MulVT, N->getOperand(1));\n    SDValue Res = DAG.getNode(ISD::MUL, dl, MulVT, Op0, Op1);\n    Res = DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n    unsigned NumConcats = 16 / VT.getVectorNumElements();\n    SmallVector<SDValue, 8> ConcatOps(NumConcats, DAG.getUNDEF(VT));\n    ConcatOps[0] = Res;\n    Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v16i8, ConcatOps);\n    Results.push_back(Res);\n    return;\n  }\n  case X86ISD::VPMADDWD:\n  case X86ISD::AVG: {\n    // Legalize types for X86ISD::AVG/VPMADDWD by widening.\n    assert(Subtarget.hasSSE2() && \"Requires at least SSE2!\");\n\n    EVT VT = N->getValueType(0);\n    EVT InVT = N->getOperand(0).getValueType();\n    assert(VT.getSizeInBits() < 128 && 128 % VT.getSizeInBits() == 0 &&\n           \"Expected a VT that divides into 128 bits.\");\n    assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n           \"Unexpected type action!\");\n    unsigned NumConcat = 128 / InVT.getSizeInBits();\n\n    EVT InWideVT = EVT::getVectorVT(*DAG.getContext(),\n                                    InVT.getVectorElementType(),\n                                    NumConcat * InVT.getVectorNumElements());\n    EVT WideVT = EVT::getVectorVT(*DAG.getContext(),\n                                  VT.getVectorElementType(),\n                                  NumConcat * VT.getVectorNumElements());\n\n    SmallVector<SDValue, 16> Ops(NumConcat, DAG.getUNDEF(InVT));\n    Ops[0] = N->getOperand(0);\n    SDValue InVec0 = DAG.getNode(ISD::CONCAT_VECTORS, dl, InWideVT, Ops);\n    Ops[0] = N->getOperand(1);\n    SDValue InVec1 = DAG.getNode(ISD::CONCAT_VECTORS, dl, InWideVT, Ops);\n\n    SDValue Res = DAG.getNode(N->getOpcode(), dl, WideVT, InVec0, InVec1);\n    Results.push_back(Res);\n    return;\n  }\n  // We might have generated v2f32 FMIN/FMAX operations. Widen them to v4f32.\n  case X86ISD::FMINC:\n  case X86ISD::FMIN:\n  case X86ISD::FMAXC:\n  case X86ISD::FMAX: {\n    EVT VT = N->getValueType(0);\n    assert(VT == MVT::v2f32 && \"Unexpected type (!= v2f32) on FMIN/FMAX.\");\n    SDValue UNDEF = DAG.getUNDEF(VT);\n    SDValue LHS = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f32,\n                              N->getOperand(0), UNDEF);\n    SDValue RHS = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f32,\n                              N->getOperand(1), UNDEF);\n    Results.push_back(DAG.getNode(N->getOpcode(), dl, MVT::v4f32, LHS, RHS));\n    return;\n  }\n  case ISD::SDIV:\n  case ISD::UDIV:\n  case ISD::SREM:\n  case ISD::UREM: {\n    EVT VT = N->getValueType(0);\n    if (VT.isVector()) {\n      assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n      // If this RHS is a constant splat vector we can widen this and let\n      // division/remainder by constant optimize it.\n      // TODO: Can we do something for non-splat?\n      APInt SplatVal;\n      if (ISD::isConstantSplatVector(N->getOperand(1).getNode(), SplatVal)) {\n        unsigned NumConcats = 128 / VT.getSizeInBits();\n        SmallVector<SDValue, 8> Ops0(NumConcats, DAG.getUNDEF(VT));\n        Ops0[0] = N->getOperand(0);\n        EVT ResVT = getTypeToTransformTo(*DAG.getContext(), VT);\n        SDValue N0 = DAG.getNode(ISD::CONCAT_VECTORS, dl, ResVT, Ops0);\n        SDValue N1 = DAG.getConstant(SplatVal, dl, ResVT);\n        SDValue Res = DAG.getNode(N->getOpcode(), dl, ResVT, N0, N1);\n        Results.push_back(Res);\n      }\n      return;\n    }\n\n    SDValue V = LowerWin64_i128OP(SDValue(N,0), DAG);\n    Results.push_back(V);\n    return;\n  }\n  case ISD::TRUNCATE: {\n    MVT VT = N->getSimpleValueType(0);\n    if (getTypeAction(*DAG.getContext(), VT) != TypeWidenVector)\n      return;\n\n    // The generic legalizer will try to widen the input type to the same\n    // number of elements as the widened result type. But this isn't always\n    // the best thing so do some custom legalization to avoid some cases.\n    MVT WidenVT = getTypeToTransformTo(*DAG.getContext(), VT).getSimpleVT();\n    SDValue In = N->getOperand(0);\n    EVT InVT = In.getValueType();\n\n    unsigned InBits = InVT.getSizeInBits();\n    if (128 % InBits == 0) {\n      // 128 bit and smaller inputs should avoid truncate all together and\n      // just use a build_vector that will become a shuffle.\n      // TODO: Widen and use a shuffle directly?\n      MVT InEltVT = InVT.getSimpleVT().getVectorElementType();\n      EVT EltVT = VT.getVectorElementType();\n      unsigned WidenNumElts = WidenVT.getVectorNumElements();\n      SmallVector<SDValue, 16> Ops(WidenNumElts, DAG.getUNDEF(EltVT));\n      // Use the original element count so we don't do more scalar opts than\n      // necessary.\n      unsigned MinElts = VT.getVectorNumElements();\n      for (unsigned i=0; i < MinElts; ++i) {\n        SDValue Val = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, InEltVT, In,\n                                  DAG.getIntPtrConstant(i, dl));\n        Ops[i] = DAG.getNode(ISD::TRUNCATE, dl, EltVT, Val);\n      }\n      Results.push_back(DAG.getBuildVector(WidenVT, dl, Ops));\n      return;\n    }\n    // With AVX512 there are some cases that can use a target specific\n    // truncate node to go from 256/512 to less than 128 with zeros in the\n    // upper elements of the 128 bit result.\n    if (Subtarget.hasAVX512() && isTypeLegal(InVT)) {\n      // We can use VTRUNC directly if for 256 bits with VLX or for any 512.\n      if ((InBits == 256 && Subtarget.hasVLX()) || InBits == 512) {\n        Results.push_back(DAG.getNode(X86ISD::VTRUNC, dl, WidenVT, In));\n        return;\n      }\n      // There's one case we can widen to 512 bits and use VTRUNC.\n      if (InVT == MVT::v4i64 && VT == MVT::v4i8 && isTypeLegal(MVT::v8i64)) {\n        In = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8i64, In,\n                         DAG.getUNDEF(MVT::v4i64));\n        Results.push_back(DAG.getNode(X86ISD::VTRUNC, dl, WidenVT, In));\n        return;\n      }\n    }\n    if (Subtarget.hasVLX() && InVT == MVT::v8i64 && VT == MVT::v8i8 &&\n        getTypeAction(*DAG.getContext(), InVT) == TypeSplitVector &&\n        isTypeLegal(MVT::v4i64)) {\n      // Input needs to be split and output needs to widened. Let's use two\n      // VTRUNCs, and shuffle their results together into the wider type.\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = DAG.SplitVector(In, dl);\n\n      Lo = DAG.getNode(X86ISD::VTRUNC, dl, MVT::v16i8, Lo);\n      Hi = DAG.getNode(X86ISD::VTRUNC, dl, MVT::v16i8, Hi);\n      SDValue Res = DAG.getVectorShuffle(MVT::v16i8, dl, Lo, Hi,\n                                         { 0,  1,  2,  3, 16, 17, 18, 19,\n                                          -1, -1, -1, -1, -1, -1, -1, -1 });\n      Results.push_back(Res);\n      return;\n    }\n\n    return;\n  }\n  case ISD::ANY_EXTEND:\n    // Right now, only MVT::v8i8 has Custom action for an illegal type.\n    // It's intended to custom handle the input type.\n    assert(N->getValueType(0) == MVT::v8i8 &&\n           \"Do not know how to legalize this Node\");\n    return;\n  case ISD::SIGN_EXTEND:\n  case ISD::ZERO_EXTEND: {\n    EVT VT = N->getValueType(0);\n    SDValue In = N->getOperand(0);\n    EVT InVT = In.getValueType();\n    if (!Subtarget.hasSSE41() && VT == MVT::v4i64 &&\n        (InVT == MVT::v4i16 || InVT == MVT::v4i8)){\n      assert(getTypeAction(*DAG.getContext(), InVT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n      assert(N->getOpcode() == ISD::SIGN_EXTEND && \"Unexpected opcode\");\n      // Custom split this so we can extend i8/i16->i32 invec. This is better\n      // since sign_extend_inreg i8/i16->i64 requires an extend to i32 using\n      // sra. Then extending from i32 to i64 using pcmpgt. By custom splitting\n      // we allow the sra from the extend to i32 to be shared by the split.\n      In = DAG.getNode(ISD::SIGN_EXTEND, dl, MVT::v4i32, In);\n\n      // Fill a vector with sign bits for each element.\n      SDValue Zero = DAG.getConstant(0, dl, MVT::v4i32);\n      SDValue SignBits = DAG.getSetCC(dl, MVT::v4i32, Zero, In, ISD::SETGT);\n\n      // Create an unpackl and unpackh to interleave the sign bits then bitcast\n      // to v2i64.\n      SDValue Lo = DAG.getVectorShuffle(MVT::v4i32, dl, In, SignBits,\n                                        {0, 4, 1, 5});\n      Lo = DAG.getNode(ISD::BITCAST, dl, MVT::v2i64, Lo);\n      SDValue Hi = DAG.getVectorShuffle(MVT::v4i32, dl, In, SignBits,\n                                        {2, 6, 3, 7});\n      Hi = DAG.getNode(ISD::BITCAST, dl, MVT::v2i64, Hi);\n\n      SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n      Results.push_back(Res);\n      return;\n    }\n\n    if (VT == MVT::v16i32 || VT == MVT::v8i64) {\n      if (!InVT.is128BitVector()) {\n        // Not a 128 bit vector, but maybe type legalization will promote\n        // it to 128 bits.\n        if (getTypeAction(*DAG.getContext(), InVT) != TypePromoteInteger)\n          return;\n        InVT = getTypeToTransformTo(*DAG.getContext(), InVT);\n        if (!InVT.is128BitVector())\n          return;\n\n        // Promote the input to 128 bits. Type legalization will turn this into\n        // zext_inreg/sext_inreg.\n        In = DAG.getNode(N->getOpcode(), dl, InVT, In);\n      }\n\n      // Perform custom splitting instead of the two stage extend we would get\n      // by default.\n      EVT LoVT, HiVT;\n      std::tie(LoVT, HiVT) = DAG.GetSplitDestVTs(N->getValueType(0));\n      assert(isTypeLegal(LoVT) && \"Split VT not legal?\");\n\n      SDValue Lo = getEXTEND_VECTOR_INREG(N->getOpcode(), dl, LoVT, In, DAG);\n\n      // We need to shift the input over by half the number of elements.\n      unsigned NumElts = InVT.getVectorNumElements();\n      unsigned HalfNumElts = NumElts / 2;\n      SmallVector<int, 16> ShufMask(NumElts, SM_SentinelUndef);\n      for (unsigned i = 0; i != HalfNumElts; ++i)\n        ShufMask[i] = i + HalfNumElts;\n\n      SDValue Hi = DAG.getVectorShuffle(InVT, dl, In, In, ShufMask);\n      Hi = getEXTEND_VECTOR_INREG(N->getOpcode(), dl, HiVT, Hi, DAG);\n\n      SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Lo, Hi);\n      Results.push_back(Res);\n    }\n    return;\n  }\n  case ISD::FP_TO_SINT:\n  case ISD::STRICT_FP_TO_SINT:\n  case ISD::FP_TO_UINT:\n  case ISD::STRICT_FP_TO_UINT: {\n    bool IsStrict = N->isStrictFPOpcode();\n    bool IsSigned = N->getOpcode() == ISD::FP_TO_SINT ||\n                    N->getOpcode() == ISD::STRICT_FP_TO_SINT;\n    EVT VT = N->getValueType(0);\n    SDValue Src = N->getOperand(IsStrict ? 1 : 0);\n    EVT SrcVT = Src.getValueType();\n\n    if (VT.isVector() && VT.getScalarSizeInBits() < 32) {\n      assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n\n      // Try to create a 128 bit vector, but don't exceed a 32 bit element.\n      unsigned NewEltWidth = std::min(128 / VT.getVectorNumElements(), 32U);\n      MVT PromoteVT = MVT::getVectorVT(MVT::getIntegerVT(NewEltWidth),\n                                       VT.getVectorNumElements());\n      SDValue Res;\n      SDValue Chain;\n      if (IsStrict) {\n        Res = DAG.getNode(ISD::STRICT_FP_TO_SINT, dl, {PromoteVT, MVT::Other},\n                          {N->getOperand(0), Src});\n        Chain = Res.getValue(1);\n      } else\n        Res = DAG.getNode(ISD::FP_TO_SINT, dl, PromoteVT, Src);\n\n      // Preserve what we know about the size of the original result. Except\n      // when the result is v2i32 since we can't widen the assert.\n      if (PromoteVT != MVT::v2i32)\n        Res = DAG.getNode(!IsSigned ? ISD::AssertZext : ISD::AssertSext,\n                          dl, PromoteVT, Res,\n                          DAG.getValueType(VT.getVectorElementType()));\n\n      // Truncate back to the original width.\n      Res = DAG.getNode(ISD::TRUNCATE, dl, VT, Res);\n\n      // Now widen to 128 bits.\n      unsigned NumConcats = 128 / VT.getSizeInBits();\n      MVT ConcatVT = MVT::getVectorVT(VT.getSimpleVT().getVectorElementType(),\n                                      VT.getVectorNumElements() * NumConcats);\n      SmallVector<SDValue, 8> ConcatOps(NumConcats, DAG.getUNDEF(VT));\n      ConcatOps[0] = Res;\n      Res = DAG.getNode(ISD::CONCAT_VECTORS, dl, ConcatVT, ConcatOps);\n      Results.push_back(Res);\n      if (IsStrict)\n        Results.push_back(Chain);\n      return;\n    }\n\n\n    if (VT == MVT::v2i32) {\n      assert((IsSigned || Subtarget.hasAVX512()) &&\n             \"Can only handle signed conversion without AVX512\");\n      assert(Subtarget.hasSSE2() && \"Requires at least SSE2!\");\n      assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n      if (Src.getValueType() == MVT::v2f64) {\n        unsigned Opc;\n        if (IsStrict)\n          Opc = IsSigned ? X86ISD::STRICT_CVTTP2SI : X86ISD::STRICT_CVTTP2UI;\n        else\n          Opc = IsSigned ? X86ISD::CVTTP2SI : X86ISD::CVTTP2UI;\n\n        // If we have VLX we can emit a target specific FP_TO_UINT node,.\n        if (!IsSigned && !Subtarget.hasVLX()) {\n          // Otherwise we can defer to the generic legalizer which will widen\n          // the input as well. This will be further widened during op\n          // legalization to v8i32<-v8f64.\n          // For strict nodes we'll need to widen ourselves.\n          // FIXME: Fix the type legalizer to safely widen strict nodes?\n          if (!IsStrict)\n            return;\n          Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f64, Src,\n                            DAG.getConstantFP(0.0, dl, MVT::v2f64));\n          Opc = N->getOpcode();\n        }\n        SDValue Res;\n        SDValue Chain;\n        if (IsStrict) {\n          Res = DAG.getNode(Opc, dl, {MVT::v4i32, MVT::Other},\n                            {N->getOperand(0), Src});\n          Chain = Res.getValue(1);\n        } else {\n          Res = DAG.getNode(Opc, dl, MVT::v4i32, Src);\n        }\n        Results.push_back(Res);\n        if (IsStrict)\n          Results.push_back(Chain);\n        return;\n      }\n\n      // Custom widen strict v2f32->v2i32 by padding with zeros.\n      // FIXME: Should generic type legalizer do this?\n      if (Src.getValueType() == MVT::v2f32 && IsStrict) {\n        Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f32, Src,\n                          DAG.getConstantFP(0.0, dl, MVT::v2f32));\n        SDValue Res = DAG.getNode(N->getOpcode(), dl, {MVT::v4i32, MVT::Other},\n                                  {N->getOperand(0), Src});\n        Results.push_back(Res);\n        Results.push_back(Res.getValue(1));\n        return;\n      }\n\n      // The FP_TO_INTHelper below only handles f32/f64/f80 scalar inputs,\n      // so early out here.\n      return;\n    }\n\n    assert(!VT.isVector() && \"Vectors should have been handled above!\");\n\n    if (Subtarget.hasDQI() && VT == MVT::i64 &&\n        (SrcVT == MVT::f32 || SrcVT == MVT::f64)) {\n      assert(!Subtarget.is64Bit() && \"i64 should be legal\");\n      unsigned NumElts = Subtarget.hasVLX() ? 2 : 8;\n      // If we use a 128-bit result we might need to use a target specific node.\n      unsigned SrcElts =\n          std::max(NumElts, 128U / (unsigned)SrcVT.getSizeInBits());\n      MVT VecVT = MVT::getVectorVT(MVT::i64, NumElts);\n      MVT VecInVT = MVT::getVectorVT(SrcVT.getSimpleVT(), SrcElts);\n      unsigned Opc = N->getOpcode();\n      if (NumElts != SrcElts) {\n        if (IsStrict)\n          Opc = IsSigned ? X86ISD::STRICT_CVTTP2SI : X86ISD::STRICT_CVTTP2UI;\n        else\n          Opc = IsSigned ? X86ISD::CVTTP2SI : X86ISD::CVTTP2UI;\n      }\n\n      SDValue ZeroIdx = DAG.getIntPtrConstant(0, dl);\n      SDValue Res = DAG.getNode(ISD::INSERT_VECTOR_ELT, dl, VecInVT,\n                                DAG.getConstantFP(0.0, dl, VecInVT), Src,\n                                ZeroIdx);\n      SDValue Chain;\n      if (IsStrict) {\n        SDVTList Tys = DAG.getVTList(VecVT, MVT::Other);\n        Res = DAG.getNode(Opc, SDLoc(N), Tys, N->getOperand(0), Res);\n        Chain = Res.getValue(1);\n      } else\n        Res = DAG.getNode(Opc, SDLoc(N), VecVT, Res);\n      Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VT, Res, ZeroIdx);\n      Results.push_back(Res);\n      if (IsStrict)\n        Results.push_back(Chain);\n      return;\n    }\n\n    SDValue Chain;\n    if (SDValue V = FP_TO_INTHelper(SDValue(N, 0), DAG, IsSigned, Chain)) {\n      Results.push_back(V);\n      if (IsStrict)\n        Results.push_back(Chain);\n    }\n    return;\n  }\n  case ISD::LRINT:\n  case ISD::LLRINT: {\n    if (SDValue V = LRINT_LLRINTHelper(N, DAG))\n      Results.push_back(V);\n    return;\n  }\n\n  case ISD::SINT_TO_FP:\n  case ISD::STRICT_SINT_TO_FP:\n  case ISD::UINT_TO_FP:\n  case ISD::STRICT_UINT_TO_FP: {\n    bool IsStrict = N->isStrictFPOpcode();\n    bool IsSigned = N->getOpcode() == ISD::SINT_TO_FP ||\n                    N->getOpcode() == ISD::STRICT_SINT_TO_FP;\n    EVT VT = N->getValueType(0);\n    if (VT != MVT::v2f32)\n      return;\n    SDValue Src = N->getOperand(IsStrict ? 1 : 0);\n    EVT SrcVT = Src.getValueType();\n    if (Subtarget.hasDQI() && Subtarget.hasVLX() && SrcVT == MVT::v2i64) {\n      if (IsStrict) {\n        unsigned Opc = IsSigned ? X86ISD::STRICT_CVTSI2P\n                                : X86ISD::STRICT_CVTUI2P;\n        SDValue Res = DAG.getNode(Opc, dl, {MVT::v4f32, MVT::Other},\n                                  {N->getOperand(0), Src});\n        Results.push_back(Res);\n        Results.push_back(Res.getValue(1));\n      } else {\n        unsigned Opc = IsSigned ? X86ISD::CVTSI2P : X86ISD::CVTUI2P;\n        Results.push_back(DAG.getNode(Opc, dl, MVT::v4f32, Src));\n      }\n      return;\n    }\n    if (SrcVT == MVT::v2i64 && !IsSigned && Subtarget.is64Bit() &&\n        Subtarget.hasSSE41() && !Subtarget.hasAVX512()) {\n      SDValue Zero = DAG.getConstant(0, dl, SrcVT);\n      SDValue One  = DAG.getConstant(1, dl, SrcVT);\n      SDValue Sign = DAG.getNode(ISD::OR, dl, SrcVT,\n                                 DAG.getNode(ISD::SRL, dl, SrcVT, Src, One),\n                                 DAG.getNode(ISD::AND, dl, SrcVT, Src, One));\n      SDValue IsNeg = DAG.getSetCC(dl, MVT::v2i64, Src, Zero, ISD::SETLT);\n      SDValue SignSrc = DAG.getSelect(dl, SrcVT, IsNeg, Sign, Src);\n      SmallVector<SDValue, 4> SignCvts(4, DAG.getConstantFP(0.0, dl, MVT::f32));\n      for (int i = 0; i != 2; ++i) {\n        SDValue Elt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i64,\n                                  SignSrc, DAG.getIntPtrConstant(i, dl));\n        if (IsStrict)\n          SignCvts[i] =\n              DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {MVT::f32, MVT::Other},\n                          {N->getOperand(0), Elt});\n        else\n          SignCvts[i] = DAG.getNode(ISD::SINT_TO_FP, dl, MVT::f32, Elt);\n      };\n      SDValue SignCvt = DAG.getBuildVector(MVT::v4f32, dl, SignCvts);\n      SDValue Slow, Chain;\n      if (IsStrict) {\n        Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other,\n                            SignCvts[0].getValue(1), SignCvts[1].getValue(1));\n        Slow = DAG.getNode(ISD::STRICT_FADD, dl, {MVT::v4f32, MVT::Other},\n                           {Chain, SignCvt, SignCvt});\n        Chain = Slow.getValue(1);\n      } else {\n        Slow = DAG.getNode(ISD::FADD, dl, MVT::v4f32, SignCvt, SignCvt);\n      }\n      IsNeg = DAG.getBitcast(MVT::v4i32, IsNeg);\n      IsNeg =\n          DAG.getVectorShuffle(MVT::v4i32, dl, IsNeg, IsNeg, {1, 3, -1, -1});\n      SDValue Cvt = DAG.getSelect(dl, MVT::v4f32, IsNeg, Slow, SignCvt);\n      Results.push_back(Cvt);\n      if (IsStrict)\n        Results.push_back(Chain);\n      return;\n    }\n\n    if (SrcVT != MVT::v2i32)\n      return;\n\n    if (IsSigned || Subtarget.hasAVX512()) {\n      if (!IsStrict)\n        return;\n\n      // Custom widen strict v2i32->v2f32 to avoid scalarization.\n      // FIXME: Should generic type legalizer do this?\n      Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4i32, Src,\n                        DAG.getConstant(0, dl, MVT::v2i32));\n      SDValue Res = DAG.getNode(N->getOpcode(), dl, {MVT::v4f32, MVT::Other},\n                                {N->getOperand(0), Src});\n      Results.push_back(Res);\n      Results.push_back(Res.getValue(1));\n      return;\n    }\n\n    assert(Subtarget.hasSSE2() && \"Requires at least SSE2!\");\n    SDValue ZExtIn = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::v2i64, Src);\n    SDValue VBias =\n        DAG.getConstantFP(BitsToDouble(0x4330000000000000ULL), dl, MVT::v2f64);\n    SDValue Or = DAG.getNode(ISD::OR, dl, MVT::v2i64, ZExtIn,\n                             DAG.getBitcast(MVT::v2i64, VBias));\n    Or = DAG.getBitcast(MVT::v2f64, Or);\n    if (IsStrict) {\n      SDValue Sub = DAG.getNode(ISD::STRICT_FSUB, dl, {MVT::v2f64, MVT::Other},\n                                {N->getOperand(0), Or, VBias});\n      SDValue Res = DAG.getNode(X86ISD::STRICT_VFPROUND, dl,\n                                {MVT::v4f32, MVT::Other},\n                                {Sub.getValue(1), Sub});\n      Results.push_back(Res);\n      Results.push_back(Res.getValue(1));\n    } else {\n      // TODO: Are there any fast-math-flags to propagate here?\n      SDValue Sub = DAG.getNode(ISD::FSUB, dl, MVT::v2f64, Or, VBias);\n      Results.push_back(DAG.getNode(X86ISD::VFPROUND, dl, MVT::v4f32, Sub));\n    }\n    return;\n  }\n  case ISD::STRICT_FP_ROUND:\n  case ISD::FP_ROUND: {\n    bool IsStrict = N->isStrictFPOpcode();\n    SDValue Src = N->getOperand(IsStrict ? 1 : 0);\n    if (!isTypeLegal(Src.getValueType()))\n      return;\n    SDValue V;\n    if (IsStrict)\n      V = DAG.getNode(X86ISD::STRICT_VFPROUND, dl, {MVT::v4f32, MVT::Other},\n                      {N->getOperand(0), N->getOperand(1)});\n    else\n      V = DAG.getNode(X86ISD::VFPROUND, dl, MVT::v4f32, N->getOperand(0));\n    Results.push_back(V);\n    if (IsStrict)\n      Results.push_back(V.getValue(1));\n    return;\n  }\n  case ISD::FP_EXTEND:\n  case ISD::STRICT_FP_EXTEND: {\n    // Right now, only MVT::v2f32 has OperationAction for FP_EXTEND.\n    // No other ValueType for FP_EXTEND should reach this point.\n    assert(N->getValueType(0) == MVT::v2f32 &&\n           \"Do not know how to legalize this Node\");\n    return;\n  }\n  case ISD::INTRINSIC_W_CHAIN: {\n    unsigned IntNo = N->getConstantOperandVal(1);\n    switch (IntNo) {\n    default : llvm_unreachable(\"Do not know how to custom type \"\n                               \"legalize this intrinsic operation!\");\n    case Intrinsic::x86_rdtsc:\n      return getReadTimeStampCounter(N, dl, X86::RDTSC, DAG, Subtarget,\n                                     Results);\n    case Intrinsic::x86_rdtscp:\n      return getReadTimeStampCounter(N, dl, X86::RDTSCP, DAG, Subtarget,\n                                     Results);\n    case Intrinsic::x86_rdpmc:\n      expandIntrinsicWChainHelper(N, dl, DAG, X86::RDPMC, X86::ECX, Subtarget,\n                                  Results);\n      return;\n    case Intrinsic::x86_xgetbv:\n      expandIntrinsicWChainHelper(N, dl, DAG, X86::XGETBV, X86::ECX, Subtarget,\n                                  Results);\n      return;\n    }\n  }\n  case ISD::READCYCLECOUNTER: {\n    return getReadTimeStampCounter(N, dl, X86::RDTSC, DAG, Subtarget, Results);\n  }\n  case ISD::ATOMIC_CMP_SWAP_WITH_SUCCESS: {\n    EVT T = N->getValueType(0);\n    assert((T == MVT::i64 || T == MVT::i128) && \"can only expand cmpxchg pair\");\n    bool Regs64bit = T == MVT::i128;\n    assert((!Regs64bit || Subtarget.hasCmpxchg16b()) &&\n           \"64-bit ATOMIC_CMP_SWAP_WITH_SUCCESS requires CMPXCHG16B\");\n    MVT HalfT = Regs64bit ? MVT::i64 : MVT::i32;\n    SDValue cpInL, cpInH;\n    cpInL = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, HalfT, N->getOperand(2),\n                        DAG.getConstant(0, dl, HalfT));\n    cpInH = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, HalfT, N->getOperand(2),\n                        DAG.getConstant(1, dl, HalfT));\n    cpInL = DAG.getCopyToReg(N->getOperand(0), dl,\n                             Regs64bit ? X86::RAX : X86::EAX,\n                             cpInL, SDValue());\n    cpInH = DAG.getCopyToReg(cpInL.getValue(0), dl,\n                             Regs64bit ? X86::RDX : X86::EDX,\n                             cpInH, cpInL.getValue(1));\n    SDValue swapInL, swapInH;\n    swapInL = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, HalfT, N->getOperand(3),\n                          DAG.getConstant(0, dl, HalfT));\n    swapInH = DAG.getNode(ISD::EXTRACT_ELEMENT, dl, HalfT, N->getOperand(3),\n                          DAG.getConstant(1, dl, HalfT));\n    swapInH =\n        DAG.getCopyToReg(cpInH.getValue(0), dl, Regs64bit ? X86::RCX : X86::ECX,\n                         swapInH, cpInH.getValue(1));\n\n    // In 64-bit mode we might need the base pointer in RBX, but we can't know\n    // until later. So we keep the RBX input in a vreg and use a custom\n    // inserter.\n    // Since RBX will be a reserved register the register allocator will not\n    // make sure its value will be properly saved and restored around this\n    // live-range.\n    SDValue Result;\n    SDVTList Tys = DAG.getVTList(MVT::Other, MVT::Glue);\n    MachineMemOperand *MMO = cast<AtomicSDNode>(N)->getMemOperand();\n    if (Regs64bit) {\n      SDValue Ops[] = {swapInH.getValue(0), N->getOperand(1), swapInL,\n                       swapInH.getValue(1)};\n      Result =\n          DAG.getMemIntrinsicNode(X86ISD::LCMPXCHG16_DAG, dl, Tys, Ops, T, MMO);\n    } else {\n      swapInL = DAG.getCopyToReg(swapInH.getValue(0), dl, X86::EBX, swapInL,\n                                 swapInH.getValue(1));\n      SDValue Ops[] = {swapInL.getValue(0), N->getOperand(1),\n                       swapInL.getValue(1)};\n      Result =\n          DAG.getMemIntrinsicNode(X86ISD::LCMPXCHG8_DAG, dl, Tys, Ops, T, MMO);\n    }\n\n    SDValue cpOutL = DAG.getCopyFromReg(Result.getValue(0), dl,\n                                        Regs64bit ? X86::RAX : X86::EAX,\n                                        HalfT, Result.getValue(1));\n    SDValue cpOutH = DAG.getCopyFromReg(cpOutL.getValue(1), dl,\n                                        Regs64bit ? X86::RDX : X86::EDX,\n                                        HalfT, cpOutL.getValue(2));\n    SDValue OpsF[] = { cpOutL.getValue(0), cpOutH.getValue(0)};\n\n    SDValue EFLAGS = DAG.getCopyFromReg(cpOutH.getValue(1), dl, X86::EFLAGS,\n                                        MVT::i32, cpOutH.getValue(2));\n    SDValue Success = getSETCC(X86::COND_E, EFLAGS, dl, DAG);\n    Success = DAG.getZExtOrTrunc(Success, dl, N->getValueType(1));\n\n    Results.push_back(DAG.getNode(ISD::BUILD_PAIR, dl, T, OpsF));\n    Results.push_back(Success);\n    Results.push_back(EFLAGS.getValue(1));\n    return;\n  }\n  case ISD::ATOMIC_LOAD: {\n    assert(N->getValueType(0) == MVT::i64 && \"Unexpected VT!\");\n    bool NoImplicitFloatOps =\n        DAG.getMachineFunction().getFunction().hasFnAttribute(\n            Attribute::NoImplicitFloat);\n    if (!Subtarget.useSoftFloat() && !NoImplicitFloatOps) {\n      auto *Node = cast<AtomicSDNode>(N);\n      if (Subtarget.hasSSE1()) {\n        // Use a VZEXT_LOAD which will be selected as MOVQ or XORPS+MOVLPS.\n        // Then extract the lower 64-bits.\n        MVT LdVT = Subtarget.hasSSE2() ? MVT::v2i64 : MVT::v4f32;\n        SDVTList Tys = DAG.getVTList(LdVT, MVT::Other);\n        SDValue Ops[] = { Node->getChain(), Node->getBasePtr() };\n        SDValue Ld = DAG.getMemIntrinsicNode(X86ISD::VZEXT_LOAD, dl, Tys, Ops,\n                                             MVT::i64, Node->getMemOperand());\n        if (Subtarget.hasSSE2()) {\n          SDValue Res = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::i64, Ld,\n                                    DAG.getIntPtrConstant(0, dl));\n          Results.push_back(Res);\n          Results.push_back(Ld.getValue(1));\n          return;\n        }\n        // We use an alternative sequence for SSE1 that extracts as v2f32 and\n        // then casts to i64. This avoids a 128-bit stack temporary being\n        // created by type legalization if we were to cast v4f32->v2i64.\n        SDValue Res = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v2f32, Ld,\n                                  DAG.getIntPtrConstant(0, dl));\n        Res = DAG.getBitcast(MVT::i64, Res);\n        Results.push_back(Res);\n        Results.push_back(Ld.getValue(1));\n        return;\n      }\n      if (Subtarget.hasX87()) {\n        // First load this into an 80-bit X87 register. This will put the whole\n        // integer into the significand.\n        SDVTList Tys = DAG.getVTList(MVT::f80, MVT::Other);\n        SDValue Ops[] = { Node->getChain(), Node->getBasePtr() };\n        SDValue Result = DAG.getMemIntrinsicNode(X86ISD::FILD,\n                                                 dl, Tys, Ops, MVT::i64,\n                                                 Node->getMemOperand());\n        SDValue Chain = Result.getValue(1);\n\n        // Now store the X87 register to a stack temporary and convert to i64.\n        // This store is not atomic and doesn't need to be.\n        // FIXME: We don't need a stack temporary if the result of the load\n        // is already being stored. We could just directly store there.\n        SDValue StackPtr = DAG.CreateStackTemporary(MVT::i64);\n        int SPFI = cast<FrameIndexSDNode>(StackPtr.getNode())->getIndex();\n        MachinePointerInfo MPI =\n            MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), SPFI);\n        SDValue StoreOps[] = { Chain, Result, StackPtr };\n        Chain = DAG.getMemIntrinsicNode(\n            X86ISD::FIST, dl, DAG.getVTList(MVT::Other), StoreOps, MVT::i64,\n            MPI, None /*Align*/, MachineMemOperand::MOStore);\n\n        // Finally load the value back from the stack temporary and return it.\n        // This load is not atomic and doesn't need to be.\n        // This load will be further type legalized.\n        Result = DAG.getLoad(MVT::i64, dl, Chain, StackPtr, MPI);\n        Results.push_back(Result);\n        Results.push_back(Result.getValue(1));\n        return;\n      }\n    }\n    // TODO: Use MOVLPS when SSE1 is available?\n    // Delegate to generic TypeLegalization. Situations we can really handle\n    // should have already been dealt with by AtomicExpandPass.cpp.\n    break;\n  }\n  case ISD::ATOMIC_SWAP:\n  case ISD::ATOMIC_LOAD_ADD:\n  case ISD::ATOMIC_LOAD_SUB:\n  case ISD::ATOMIC_LOAD_AND:\n  case ISD::ATOMIC_LOAD_OR:\n  case ISD::ATOMIC_LOAD_XOR:\n  case ISD::ATOMIC_LOAD_NAND:\n  case ISD::ATOMIC_LOAD_MIN:\n  case ISD::ATOMIC_LOAD_MAX:\n  case ISD::ATOMIC_LOAD_UMIN:\n  case ISD::ATOMIC_LOAD_UMAX:\n    // Delegate to generic TypeLegalization. Situations we can really handle\n    // should have already been dealt with by AtomicExpandPass.cpp.\n    break;\n\n  case ISD::BITCAST: {\n    assert(Subtarget.hasSSE2() && \"Requires at least SSE2!\");\n    EVT DstVT = N->getValueType(0);\n    EVT SrcVT = N->getOperand(0).getValueType();\n\n    // If this is a bitcast from a v64i1 k-register to a i64 on a 32-bit target\n    // we can split using the k-register rather than memory.\n    if (SrcVT == MVT::v64i1 && DstVT == MVT::i64 && Subtarget.hasBWI()) {\n      assert(!Subtarget.is64Bit() && \"Expected 32-bit mode\");\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = DAG.SplitVectorOperand(N, 0);\n      Lo = DAG.getBitcast(MVT::i32, Lo);\n      Hi = DAG.getBitcast(MVT::i32, Hi);\n      SDValue Res = DAG.getNode(ISD::BUILD_PAIR, dl, MVT::i64, Lo, Hi);\n      Results.push_back(Res);\n      return;\n    }\n\n    if (DstVT.isVector() && SrcVT == MVT::x86mmx) {\n      // FIXME: Use v4f32 for SSE1?\n      assert(Subtarget.hasSSE2() && \"Requires SSE2\");\n      assert(getTypeAction(*DAG.getContext(), DstVT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n      EVT WideVT = getTypeToTransformTo(*DAG.getContext(), DstVT);\n      SDValue Res = DAG.getNode(X86ISD::MOVQ2DQ, dl, MVT::v2i64,\n                                N->getOperand(0));\n      Res = DAG.getBitcast(WideVT, Res);\n      Results.push_back(Res);\n      return;\n    }\n\n    return;\n  }\n  case ISD::MGATHER: {\n    EVT VT = N->getValueType(0);\n    if ((VT == MVT::v2f32 || VT == MVT::v2i32) &&\n        (Subtarget.hasVLX() || !Subtarget.hasAVX512())) {\n      auto *Gather = cast<MaskedGatherSDNode>(N);\n      SDValue Index = Gather->getIndex();\n      if (Index.getValueType() != MVT::v2i64)\n        return;\n      assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n             \"Unexpected type action!\");\n      EVT WideVT = getTypeToTransformTo(*DAG.getContext(), VT);\n      SDValue Mask = Gather->getMask();\n      assert(Mask.getValueType() == MVT::v2i1 && \"Unexpected mask type\");\n      SDValue PassThru = DAG.getNode(ISD::CONCAT_VECTORS, dl, WideVT,\n                                     Gather->getPassThru(),\n                                     DAG.getUNDEF(VT));\n      if (!Subtarget.hasVLX()) {\n        // We need to widen the mask, but the instruction will only use 2\n        // of its elements. So we can use undef.\n        Mask = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4i1, Mask,\n                           DAG.getUNDEF(MVT::v2i1));\n        Mask = DAG.getNode(ISD::SIGN_EXTEND, dl, MVT::v4i32, Mask);\n      }\n      SDValue Ops[] = { Gather->getChain(), PassThru, Mask,\n                        Gather->getBasePtr(), Index, Gather->getScale() };\n      SDValue Res = DAG.getMemIntrinsicNode(\n          X86ISD::MGATHER, dl, DAG.getVTList(WideVT, MVT::Other), Ops,\n          Gather->getMemoryVT(), Gather->getMemOperand());\n      Results.push_back(Res);\n      Results.push_back(Res.getValue(1));\n      return;\n    }\n    return;\n  }\n  case ISD::LOAD: {\n    // Use an f64/i64 load and a scalar_to_vector for v2f32/v2i32 loads. This\n    // avoids scalarizing in 32-bit mode. In 64-bit mode this avoids a int->fp\n    // cast since type legalization will try to use an i64 load.\n    MVT VT = N->getSimpleValueType(0);\n    assert(VT.isVector() && VT.getSizeInBits() == 64 && \"Unexpected VT\");\n    assert(getTypeAction(*DAG.getContext(), VT) == TypeWidenVector &&\n           \"Unexpected type action!\");\n    if (!ISD::isNON_EXTLoad(N))\n      return;\n    auto *Ld = cast<LoadSDNode>(N);\n    if (Subtarget.hasSSE2()) {\n      MVT LdVT = Subtarget.is64Bit() && VT.isInteger() ? MVT::i64 : MVT::f64;\n      SDValue Res = DAG.getLoad(LdVT, dl, Ld->getChain(), Ld->getBasePtr(),\n                                Ld->getPointerInfo(), Ld->getOriginalAlign(),\n                                Ld->getMemOperand()->getFlags());\n      SDValue Chain = Res.getValue(1);\n      MVT VecVT = MVT::getVectorVT(LdVT, 2);\n      Res = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, VecVT, Res);\n      EVT WideVT = getTypeToTransformTo(*DAG.getContext(), VT);\n      Res = DAG.getBitcast(WideVT, Res);\n      Results.push_back(Res);\n      Results.push_back(Chain);\n      return;\n    }\n    assert(Subtarget.hasSSE1() && \"Expected SSE\");\n    SDVTList Tys = DAG.getVTList(MVT::v4f32, MVT::Other);\n    SDValue Ops[] = {Ld->getChain(), Ld->getBasePtr()};\n    SDValue Res = DAG.getMemIntrinsicNode(X86ISD::VZEXT_LOAD, dl, Tys, Ops,\n                                          MVT::i64, Ld->getMemOperand());\n    Results.push_back(Res);\n    Results.push_back(Res.getValue(1));\n    return;\n  }\n  case ISD::ADDRSPACECAST: {\n    SDValue V = LowerADDRSPACECAST(SDValue(N,0), DAG);\n    Results.push_back(V);\n    return;\n  }\n  case ISD::BITREVERSE:\n    assert(N->getValueType(0) == MVT::i64 && \"Unexpected VT!\");\n    assert(Subtarget.hasXOP() && \"Expected XOP\");\n    // We can use VPPERM by copying to a vector register and back. We'll need\n    // to move the scalar in two i32 pieces.\n    Results.push_back(LowerBITREVERSE(SDValue(N, 0), Subtarget, DAG));\n    return;\n  }\n}\n\nconst char *X86TargetLowering::getTargetNodeName(unsigned Opcode) const {\n  switch ((X86ISD::NodeType)Opcode) {\n  case X86ISD::FIRST_NUMBER:       break;\n#define NODE_NAME_CASE(NODE) case X86ISD::NODE: return \"X86ISD::\" #NODE;\n  NODE_NAME_CASE(BSF)\n  NODE_NAME_CASE(BSR)\n  NODE_NAME_CASE(FSHL)\n  NODE_NAME_CASE(FSHR)\n  NODE_NAME_CASE(FAND)\n  NODE_NAME_CASE(FANDN)\n  NODE_NAME_CASE(FOR)\n  NODE_NAME_CASE(FXOR)\n  NODE_NAME_CASE(FILD)\n  NODE_NAME_CASE(FIST)\n  NODE_NAME_CASE(FP_TO_INT_IN_MEM)\n  NODE_NAME_CASE(FLD)\n  NODE_NAME_CASE(FST)\n  NODE_NAME_CASE(CALL)\n  NODE_NAME_CASE(BT)\n  NODE_NAME_CASE(CMP)\n  NODE_NAME_CASE(FCMP)\n  NODE_NAME_CASE(STRICT_FCMP)\n  NODE_NAME_CASE(STRICT_FCMPS)\n  NODE_NAME_CASE(COMI)\n  NODE_NAME_CASE(UCOMI)\n  NODE_NAME_CASE(CMPM)\n  NODE_NAME_CASE(CMPMM)\n  NODE_NAME_CASE(STRICT_CMPM)\n  NODE_NAME_CASE(CMPMM_SAE)\n  NODE_NAME_CASE(SETCC)\n  NODE_NAME_CASE(SETCC_CARRY)\n  NODE_NAME_CASE(FSETCC)\n  NODE_NAME_CASE(FSETCCM)\n  NODE_NAME_CASE(FSETCCM_SAE)\n  NODE_NAME_CASE(CMOV)\n  NODE_NAME_CASE(BRCOND)\n  NODE_NAME_CASE(RET_FLAG)\n  NODE_NAME_CASE(IRET)\n  NODE_NAME_CASE(REP_STOS)\n  NODE_NAME_CASE(REP_MOVS)\n  NODE_NAME_CASE(GlobalBaseReg)\n  NODE_NAME_CASE(Wrapper)\n  NODE_NAME_CASE(WrapperRIP)\n  NODE_NAME_CASE(MOVQ2DQ)\n  NODE_NAME_CASE(MOVDQ2Q)\n  NODE_NAME_CASE(MMX_MOVD2W)\n  NODE_NAME_CASE(MMX_MOVW2D)\n  NODE_NAME_CASE(PEXTRB)\n  NODE_NAME_CASE(PEXTRW)\n  NODE_NAME_CASE(INSERTPS)\n  NODE_NAME_CASE(PINSRB)\n  NODE_NAME_CASE(PINSRW)\n  NODE_NAME_CASE(PSHUFB)\n  NODE_NAME_CASE(ANDNP)\n  NODE_NAME_CASE(BLENDI)\n  NODE_NAME_CASE(BLENDV)\n  NODE_NAME_CASE(HADD)\n  NODE_NAME_CASE(HSUB)\n  NODE_NAME_CASE(FHADD)\n  NODE_NAME_CASE(FHSUB)\n  NODE_NAME_CASE(CONFLICT)\n  NODE_NAME_CASE(FMAX)\n  NODE_NAME_CASE(FMAXS)\n  NODE_NAME_CASE(FMAX_SAE)\n  NODE_NAME_CASE(FMAXS_SAE)\n  NODE_NAME_CASE(FMIN)\n  NODE_NAME_CASE(FMINS)\n  NODE_NAME_CASE(FMIN_SAE)\n  NODE_NAME_CASE(FMINS_SAE)\n  NODE_NAME_CASE(FMAXC)\n  NODE_NAME_CASE(FMINC)\n  NODE_NAME_CASE(FRSQRT)\n  NODE_NAME_CASE(FRCP)\n  NODE_NAME_CASE(EXTRQI)\n  NODE_NAME_CASE(INSERTQI)\n  NODE_NAME_CASE(TLSADDR)\n  NODE_NAME_CASE(TLSBASEADDR)\n  NODE_NAME_CASE(TLSCALL)\n  NODE_NAME_CASE(EH_SJLJ_SETJMP)\n  NODE_NAME_CASE(EH_SJLJ_LONGJMP)\n  NODE_NAME_CASE(EH_SJLJ_SETUP_DISPATCH)\n  NODE_NAME_CASE(EH_RETURN)\n  NODE_NAME_CASE(TC_RETURN)\n  NODE_NAME_CASE(FNSTCW16m)\n  NODE_NAME_CASE(LCMPXCHG_DAG)\n  NODE_NAME_CASE(LCMPXCHG8_DAG)\n  NODE_NAME_CASE(LCMPXCHG16_DAG)\n  NODE_NAME_CASE(LCMPXCHG16_SAVE_RBX_DAG)\n  NODE_NAME_CASE(LADD)\n  NODE_NAME_CASE(LSUB)\n  NODE_NAME_CASE(LOR)\n  NODE_NAME_CASE(LXOR)\n  NODE_NAME_CASE(LAND)\n  NODE_NAME_CASE(VZEXT_MOVL)\n  NODE_NAME_CASE(VZEXT_LOAD)\n  NODE_NAME_CASE(VEXTRACT_STORE)\n  NODE_NAME_CASE(VTRUNC)\n  NODE_NAME_CASE(VTRUNCS)\n  NODE_NAME_CASE(VTRUNCUS)\n  NODE_NAME_CASE(VMTRUNC)\n  NODE_NAME_CASE(VMTRUNCS)\n  NODE_NAME_CASE(VMTRUNCUS)\n  NODE_NAME_CASE(VTRUNCSTORES)\n  NODE_NAME_CASE(VTRUNCSTOREUS)\n  NODE_NAME_CASE(VMTRUNCSTORES)\n  NODE_NAME_CASE(VMTRUNCSTOREUS)\n  NODE_NAME_CASE(VFPEXT)\n  NODE_NAME_CASE(STRICT_VFPEXT)\n  NODE_NAME_CASE(VFPEXT_SAE)\n  NODE_NAME_CASE(VFPEXTS)\n  NODE_NAME_CASE(VFPEXTS_SAE)\n  NODE_NAME_CASE(VFPROUND)\n  NODE_NAME_CASE(STRICT_VFPROUND)\n  NODE_NAME_CASE(VMFPROUND)\n  NODE_NAME_CASE(VFPROUND_RND)\n  NODE_NAME_CASE(VFPROUNDS)\n  NODE_NAME_CASE(VFPROUNDS_RND)\n  NODE_NAME_CASE(VSHLDQ)\n  NODE_NAME_CASE(VSRLDQ)\n  NODE_NAME_CASE(VSHL)\n  NODE_NAME_CASE(VSRL)\n  NODE_NAME_CASE(VSRA)\n  NODE_NAME_CASE(VSHLI)\n  NODE_NAME_CASE(VSRLI)\n  NODE_NAME_CASE(VSRAI)\n  NODE_NAME_CASE(VSHLV)\n  NODE_NAME_CASE(VSRLV)\n  NODE_NAME_CASE(VSRAV)\n  NODE_NAME_CASE(VROTLI)\n  NODE_NAME_CASE(VROTRI)\n  NODE_NAME_CASE(VPPERM)\n  NODE_NAME_CASE(CMPP)\n  NODE_NAME_CASE(STRICT_CMPP)\n  NODE_NAME_CASE(PCMPEQ)\n  NODE_NAME_CASE(PCMPGT)\n  NODE_NAME_CASE(PHMINPOS)\n  NODE_NAME_CASE(ADD)\n  NODE_NAME_CASE(SUB)\n  NODE_NAME_CASE(ADC)\n  NODE_NAME_CASE(SBB)\n  NODE_NAME_CASE(SMUL)\n  NODE_NAME_CASE(UMUL)\n  NODE_NAME_CASE(OR)\n  NODE_NAME_CASE(XOR)\n  NODE_NAME_CASE(AND)\n  NODE_NAME_CASE(BEXTR)\n  NODE_NAME_CASE(BEXTRI)\n  NODE_NAME_CASE(BZHI)\n  NODE_NAME_CASE(PDEP)\n  NODE_NAME_CASE(PEXT)\n  NODE_NAME_CASE(MUL_IMM)\n  NODE_NAME_CASE(MOVMSK)\n  NODE_NAME_CASE(PTEST)\n  NODE_NAME_CASE(TESTP)\n  NODE_NAME_CASE(KORTEST)\n  NODE_NAME_CASE(KTEST)\n  NODE_NAME_CASE(KADD)\n  NODE_NAME_CASE(KSHIFTL)\n  NODE_NAME_CASE(KSHIFTR)\n  NODE_NAME_CASE(PACKSS)\n  NODE_NAME_CASE(PACKUS)\n  NODE_NAME_CASE(PALIGNR)\n  NODE_NAME_CASE(VALIGN)\n  NODE_NAME_CASE(VSHLD)\n  NODE_NAME_CASE(VSHRD)\n  NODE_NAME_CASE(VSHLDV)\n  NODE_NAME_CASE(VSHRDV)\n  NODE_NAME_CASE(PSHUFD)\n  NODE_NAME_CASE(PSHUFHW)\n  NODE_NAME_CASE(PSHUFLW)\n  NODE_NAME_CASE(SHUFP)\n  NODE_NAME_CASE(SHUF128)\n  NODE_NAME_CASE(MOVLHPS)\n  NODE_NAME_CASE(MOVHLPS)\n  NODE_NAME_CASE(MOVDDUP)\n  NODE_NAME_CASE(MOVSHDUP)\n  NODE_NAME_CASE(MOVSLDUP)\n  NODE_NAME_CASE(MOVSD)\n  NODE_NAME_CASE(MOVSS)\n  NODE_NAME_CASE(UNPCKL)\n  NODE_NAME_CASE(UNPCKH)\n  NODE_NAME_CASE(VBROADCAST)\n  NODE_NAME_CASE(VBROADCAST_LOAD)\n  NODE_NAME_CASE(VBROADCASTM)\n  NODE_NAME_CASE(SUBV_BROADCAST_LOAD)\n  NODE_NAME_CASE(VPERMILPV)\n  NODE_NAME_CASE(VPERMILPI)\n  NODE_NAME_CASE(VPERM2X128)\n  NODE_NAME_CASE(VPERMV)\n  NODE_NAME_CASE(VPERMV3)\n  NODE_NAME_CASE(VPERMI)\n  NODE_NAME_CASE(VPTERNLOG)\n  NODE_NAME_CASE(VFIXUPIMM)\n  NODE_NAME_CASE(VFIXUPIMM_SAE)\n  NODE_NAME_CASE(VFIXUPIMMS)\n  NODE_NAME_CASE(VFIXUPIMMS_SAE)\n  NODE_NAME_CASE(VRANGE)\n  NODE_NAME_CASE(VRANGE_SAE)\n  NODE_NAME_CASE(VRANGES)\n  NODE_NAME_CASE(VRANGES_SAE)\n  NODE_NAME_CASE(PMULUDQ)\n  NODE_NAME_CASE(PMULDQ)\n  NODE_NAME_CASE(PSADBW)\n  NODE_NAME_CASE(DBPSADBW)\n  NODE_NAME_CASE(VASTART_SAVE_XMM_REGS)\n  NODE_NAME_CASE(VAARG_64)\n  NODE_NAME_CASE(VAARG_X32)\n  NODE_NAME_CASE(WIN_ALLOCA)\n  NODE_NAME_CASE(MEMBARRIER)\n  NODE_NAME_CASE(MFENCE)\n  NODE_NAME_CASE(SEG_ALLOCA)\n  NODE_NAME_CASE(PROBED_ALLOCA)\n  NODE_NAME_CASE(RDRAND)\n  NODE_NAME_CASE(RDSEED)\n  NODE_NAME_CASE(RDPKRU)\n  NODE_NAME_CASE(WRPKRU)\n  NODE_NAME_CASE(VPMADDUBSW)\n  NODE_NAME_CASE(VPMADDWD)\n  NODE_NAME_CASE(VPSHA)\n  NODE_NAME_CASE(VPSHL)\n  NODE_NAME_CASE(VPCOM)\n  NODE_NAME_CASE(VPCOMU)\n  NODE_NAME_CASE(VPERMIL2)\n  NODE_NAME_CASE(FMSUB)\n  NODE_NAME_CASE(STRICT_FMSUB)\n  NODE_NAME_CASE(FNMADD)\n  NODE_NAME_CASE(STRICT_FNMADD)\n  NODE_NAME_CASE(FNMSUB)\n  NODE_NAME_CASE(STRICT_FNMSUB)\n  NODE_NAME_CASE(FMADDSUB)\n  NODE_NAME_CASE(FMSUBADD)\n  NODE_NAME_CASE(FMADD_RND)\n  NODE_NAME_CASE(FNMADD_RND)\n  NODE_NAME_CASE(FMSUB_RND)\n  NODE_NAME_CASE(FNMSUB_RND)\n  NODE_NAME_CASE(FMADDSUB_RND)\n  NODE_NAME_CASE(FMSUBADD_RND)\n  NODE_NAME_CASE(VPMADD52H)\n  NODE_NAME_CASE(VPMADD52L)\n  NODE_NAME_CASE(VRNDSCALE)\n  NODE_NAME_CASE(STRICT_VRNDSCALE)\n  NODE_NAME_CASE(VRNDSCALE_SAE)\n  NODE_NAME_CASE(VRNDSCALES)\n  NODE_NAME_CASE(VRNDSCALES_SAE)\n  NODE_NAME_CASE(VREDUCE)\n  NODE_NAME_CASE(VREDUCE_SAE)\n  NODE_NAME_CASE(VREDUCES)\n  NODE_NAME_CASE(VREDUCES_SAE)\n  NODE_NAME_CASE(VGETMANT)\n  NODE_NAME_CASE(VGETMANT_SAE)\n  NODE_NAME_CASE(VGETMANTS)\n  NODE_NAME_CASE(VGETMANTS_SAE)\n  NODE_NAME_CASE(PCMPESTR)\n  NODE_NAME_CASE(PCMPISTR)\n  NODE_NAME_CASE(XTEST)\n  NODE_NAME_CASE(COMPRESS)\n  NODE_NAME_CASE(EXPAND)\n  NODE_NAME_CASE(SELECTS)\n  NODE_NAME_CASE(ADDSUB)\n  NODE_NAME_CASE(RCP14)\n  NODE_NAME_CASE(RCP14S)\n  NODE_NAME_CASE(RCP28)\n  NODE_NAME_CASE(RCP28_SAE)\n  NODE_NAME_CASE(RCP28S)\n  NODE_NAME_CASE(RCP28S_SAE)\n  NODE_NAME_CASE(EXP2)\n  NODE_NAME_CASE(EXP2_SAE)\n  NODE_NAME_CASE(RSQRT14)\n  NODE_NAME_CASE(RSQRT14S)\n  NODE_NAME_CASE(RSQRT28)\n  NODE_NAME_CASE(RSQRT28_SAE)\n  NODE_NAME_CASE(RSQRT28S)\n  NODE_NAME_CASE(RSQRT28S_SAE)\n  NODE_NAME_CASE(FADD_RND)\n  NODE_NAME_CASE(FADDS)\n  NODE_NAME_CASE(FADDS_RND)\n  NODE_NAME_CASE(FSUB_RND)\n  NODE_NAME_CASE(FSUBS)\n  NODE_NAME_CASE(FSUBS_RND)\n  NODE_NAME_CASE(FMUL_RND)\n  NODE_NAME_CASE(FMULS)\n  NODE_NAME_CASE(FMULS_RND)\n  NODE_NAME_CASE(FDIV_RND)\n  NODE_NAME_CASE(FDIVS)\n  NODE_NAME_CASE(FDIVS_RND)\n  NODE_NAME_CASE(FSQRT_RND)\n  NODE_NAME_CASE(FSQRTS)\n  NODE_NAME_CASE(FSQRTS_RND)\n  NODE_NAME_CASE(FGETEXP)\n  NODE_NAME_CASE(FGETEXP_SAE)\n  NODE_NAME_CASE(FGETEXPS)\n  NODE_NAME_CASE(FGETEXPS_SAE)\n  NODE_NAME_CASE(SCALEF)\n  NODE_NAME_CASE(SCALEF_RND)\n  NODE_NAME_CASE(SCALEFS)\n  NODE_NAME_CASE(SCALEFS_RND)\n  NODE_NAME_CASE(AVG)\n  NODE_NAME_CASE(MULHRS)\n  NODE_NAME_CASE(SINT_TO_FP_RND)\n  NODE_NAME_CASE(UINT_TO_FP_RND)\n  NODE_NAME_CASE(CVTTP2SI)\n  NODE_NAME_CASE(CVTTP2UI)\n  NODE_NAME_CASE(STRICT_CVTTP2SI)\n  NODE_NAME_CASE(STRICT_CVTTP2UI)\n  NODE_NAME_CASE(MCVTTP2SI)\n  NODE_NAME_CASE(MCVTTP2UI)\n  NODE_NAME_CASE(CVTTP2SI_SAE)\n  NODE_NAME_CASE(CVTTP2UI_SAE)\n  NODE_NAME_CASE(CVTTS2SI)\n  NODE_NAME_CASE(CVTTS2UI)\n  NODE_NAME_CASE(CVTTS2SI_SAE)\n  NODE_NAME_CASE(CVTTS2UI_SAE)\n  NODE_NAME_CASE(CVTSI2P)\n  NODE_NAME_CASE(CVTUI2P)\n  NODE_NAME_CASE(STRICT_CVTSI2P)\n  NODE_NAME_CASE(STRICT_CVTUI2P)\n  NODE_NAME_CASE(MCVTSI2P)\n  NODE_NAME_CASE(MCVTUI2P)\n  NODE_NAME_CASE(VFPCLASS)\n  NODE_NAME_CASE(VFPCLASSS)\n  NODE_NAME_CASE(MULTISHIFT)\n  NODE_NAME_CASE(SCALAR_SINT_TO_FP)\n  NODE_NAME_CASE(SCALAR_SINT_TO_FP_RND)\n  NODE_NAME_CASE(SCALAR_UINT_TO_FP)\n  NODE_NAME_CASE(SCALAR_UINT_TO_FP_RND)\n  NODE_NAME_CASE(CVTPS2PH)\n  NODE_NAME_CASE(STRICT_CVTPS2PH)\n  NODE_NAME_CASE(MCVTPS2PH)\n  NODE_NAME_CASE(CVTPH2PS)\n  NODE_NAME_CASE(STRICT_CVTPH2PS)\n  NODE_NAME_CASE(CVTPH2PS_SAE)\n  NODE_NAME_CASE(CVTP2SI)\n  NODE_NAME_CASE(CVTP2UI)\n  NODE_NAME_CASE(MCVTP2SI)\n  NODE_NAME_CASE(MCVTP2UI)\n  NODE_NAME_CASE(CVTP2SI_RND)\n  NODE_NAME_CASE(CVTP2UI_RND)\n  NODE_NAME_CASE(CVTS2SI)\n  NODE_NAME_CASE(CVTS2UI)\n  NODE_NAME_CASE(CVTS2SI_RND)\n  NODE_NAME_CASE(CVTS2UI_RND)\n  NODE_NAME_CASE(CVTNE2PS2BF16)\n  NODE_NAME_CASE(CVTNEPS2BF16)\n  NODE_NAME_CASE(MCVTNEPS2BF16)\n  NODE_NAME_CASE(DPBF16PS)\n  NODE_NAME_CASE(LWPINS)\n  NODE_NAME_CASE(MGATHER)\n  NODE_NAME_CASE(MSCATTER)\n  NODE_NAME_CASE(VPDPBUSD)\n  NODE_NAME_CASE(VPDPBUSDS)\n  NODE_NAME_CASE(VPDPWSSD)\n  NODE_NAME_CASE(VPDPWSSDS)\n  NODE_NAME_CASE(VPSHUFBITQMB)\n  NODE_NAME_CASE(GF2P8MULB)\n  NODE_NAME_CASE(GF2P8AFFINEQB)\n  NODE_NAME_CASE(GF2P8AFFINEINVQB)\n  NODE_NAME_CASE(NT_CALL)\n  NODE_NAME_CASE(NT_BRIND)\n  NODE_NAME_CASE(UMWAIT)\n  NODE_NAME_CASE(TPAUSE)\n  NODE_NAME_CASE(ENQCMD)\n  NODE_NAME_CASE(ENQCMDS)\n  NODE_NAME_CASE(VP2INTERSECT)\n  NODE_NAME_CASE(AESENC128KL)\n  NODE_NAME_CASE(AESDEC128KL)\n  NODE_NAME_CASE(AESENC256KL)\n  NODE_NAME_CASE(AESDEC256KL)\n  NODE_NAME_CASE(AESENCWIDE128KL)\n  NODE_NAME_CASE(AESDECWIDE128KL)\n  NODE_NAME_CASE(AESENCWIDE256KL)\n  NODE_NAME_CASE(AESDECWIDE256KL)\n  NODE_NAME_CASE(TESTUI)\n  }\n  return nullptr;\n#undef NODE_NAME_CASE\n}\n\n/// Return true if the addressing mode represented by AM is legal for this\n/// target, for a load/store of the specified type.\nbool X86TargetLowering::isLegalAddressingMode(const DataLayout &DL,\n                                              const AddrMode &AM, Type *Ty,\n                                              unsigned AS,\n                                              Instruction *I) const {\n  // X86 supports extremely general addressing modes.\n  CodeModel::Model M = getTargetMachine().getCodeModel();\n\n  // X86 allows a sign-extended 32-bit immediate field as a displacement.\n  if (!X86::isOffsetSuitableForCodeModel(AM.BaseOffs, M, AM.BaseGV != nullptr))\n    return false;\n\n  if (AM.BaseGV) {\n    unsigned GVFlags = Subtarget.classifyGlobalReference(AM.BaseGV);\n\n    // If a reference to this global requires an extra load, we can't fold it.\n    if (isGlobalStubReference(GVFlags))\n      return false;\n\n    // If BaseGV requires a register for the PIC base, we cannot also have a\n    // BaseReg specified.\n    if (AM.HasBaseReg && isGlobalRelativeToPICBase(GVFlags))\n      return false;\n\n    // If lower 4G is not available, then we must use rip-relative addressing.\n    if ((M != CodeModel::Small || isPositionIndependent()) &&\n        Subtarget.is64Bit() && (AM.BaseOffs || AM.Scale > 1))\n      return false;\n  }\n\n  switch (AM.Scale) {\n  case 0:\n  case 1:\n  case 2:\n  case 4:\n  case 8:\n    // These scales always work.\n    break;\n  case 3:\n  case 5:\n  case 9:\n    // These scales are formed with basereg+scalereg.  Only accept if there is\n    // no basereg yet.\n    if (AM.HasBaseReg)\n      return false;\n    break;\n  default:  // Other stuff never works.\n    return false;\n  }\n\n  return true;\n}\n\nbool X86TargetLowering::isVectorShiftByScalarCheap(Type *Ty) const {\n  unsigned Bits = Ty->getScalarSizeInBits();\n\n  // 8-bit shifts are always expensive, but versions with a scalar amount aren't\n  // particularly cheaper than those without.\n  if (Bits == 8)\n    return false;\n\n  // XOP has v16i8/v8i16/v4i32/v2i64 variable vector shifts.\n  // Splitting for v32i8/v16i16 on XOP+AVX2 targets is still preferred.\n  if (Subtarget.hasXOP() &&\n      (Bits == 8 || Bits == 16 || Bits == 32 || Bits == 64))\n    return false;\n\n  // AVX2 has vpsllv[dq] instructions (and other shifts) that make variable\n  // shifts just as cheap as scalar ones.\n  if (Subtarget.hasAVX2() && (Bits == 32 || Bits == 64))\n    return false;\n\n  // AVX512BW has shifts such as vpsllvw.\n  if (Subtarget.hasBWI() && Bits == 16)\n      return false;\n\n  // Otherwise, it's significantly cheaper to shift by a scalar amount than by a\n  // fully general vector.\n  return true;\n}\n\nbool X86TargetLowering::isBinOp(unsigned Opcode) const {\n  switch (Opcode) {\n  // These are non-commutative binops.\n  // TODO: Add more X86ISD opcodes once we have test coverage.\n  case X86ISD::ANDNP:\n  case X86ISD::PCMPGT:\n  case X86ISD::FMAX:\n  case X86ISD::FMIN:\n  case X86ISD::FANDN:\n    return true;\n  }\n\n  return TargetLoweringBase::isBinOp(Opcode);\n}\n\nbool X86TargetLowering::isCommutativeBinOp(unsigned Opcode) const {\n  switch (Opcode) {\n  // TODO: Add more X86ISD opcodes once we have test coverage.\n  case X86ISD::PCMPEQ:\n  case X86ISD::PMULDQ:\n  case X86ISD::PMULUDQ:\n  case X86ISD::FMAXC:\n  case X86ISD::FMINC:\n  case X86ISD::FAND:\n  case X86ISD::FOR:\n  case X86ISD::FXOR:\n    return true;\n  }\n\n  return TargetLoweringBase::isCommutativeBinOp(Opcode);\n}\n\nbool X86TargetLowering::isTruncateFree(Type *Ty1, Type *Ty2) const {\n  if (!Ty1->isIntegerTy() || !Ty2->isIntegerTy())\n    return false;\n  unsigned NumBits1 = Ty1->getPrimitiveSizeInBits();\n  unsigned NumBits2 = Ty2->getPrimitiveSizeInBits();\n  return NumBits1 > NumBits2;\n}\n\nbool X86TargetLowering::allowTruncateForTailCall(Type *Ty1, Type *Ty2) const {\n  if (!Ty1->isIntegerTy() || !Ty2->isIntegerTy())\n    return false;\n\n  if (!isTypeLegal(EVT::getEVT(Ty1)))\n    return false;\n\n  assert(Ty1->getPrimitiveSizeInBits() <= 64 && \"i128 is probably not a noop\");\n\n  // Assuming the caller doesn't have a zeroext or signext return parameter,\n  // truncation all the way down to i1 is valid.\n  return true;\n}\n\nbool X86TargetLowering::isLegalICmpImmediate(int64_t Imm) const {\n  return isInt<32>(Imm);\n}\n\nbool X86TargetLowering::isLegalAddImmediate(int64_t Imm) const {\n  // Can also use sub to handle negated immediates.\n  return isInt<32>(Imm);\n}\n\nbool X86TargetLowering::isLegalStoreImmediate(int64_t Imm) const {\n  return isInt<32>(Imm);\n}\n\nbool X86TargetLowering::isTruncateFree(EVT VT1, EVT VT2) const {\n  if (!VT1.isScalarInteger() || !VT2.isScalarInteger())\n    return false;\n  unsigned NumBits1 = VT1.getSizeInBits();\n  unsigned NumBits2 = VT2.getSizeInBits();\n  return NumBits1 > NumBits2;\n}\n\nbool X86TargetLowering::isZExtFree(Type *Ty1, Type *Ty2) const {\n  // x86-64 implicitly zero-extends 32-bit results in 64-bit registers.\n  return Ty1->isIntegerTy(32) && Ty2->isIntegerTy(64) && Subtarget.is64Bit();\n}\n\nbool X86TargetLowering::isZExtFree(EVT VT1, EVT VT2) const {\n  // x86-64 implicitly zero-extends 32-bit results in 64-bit registers.\n  return VT1 == MVT::i32 && VT2 == MVT::i64 && Subtarget.is64Bit();\n}\n\nbool X86TargetLowering::isZExtFree(SDValue Val, EVT VT2) const {\n  EVT VT1 = Val.getValueType();\n  if (isZExtFree(VT1, VT2))\n    return true;\n\n  if (Val.getOpcode() != ISD::LOAD)\n    return false;\n\n  if (!VT1.isSimple() || !VT1.isInteger() ||\n      !VT2.isSimple() || !VT2.isInteger())\n    return false;\n\n  switch (VT1.getSimpleVT().SimpleTy) {\n  default: break;\n  case MVT::i8:\n  case MVT::i16:\n  case MVT::i32:\n    // X86 has 8, 16, and 32-bit zero-extending loads.\n    return true;\n  }\n\n  return false;\n}\n\nbool X86TargetLowering::shouldSinkOperands(Instruction *I,\n                                           SmallVectorImpl<Use *> &Ops) const {\n  // A uniform shift amount in a vector shift or funnel shift may be much\n  // cheaper than a generic variable vector shift, so make that pattern visible\n  // to SDAG by sinking the shuffle instruction next to the shift.\n  int ShiftAmountOpNum = -1;\n  if (I->isShift())\n    ShiftAmountOpNum = 1;\n  else if (auto *II = dyn_cast<IntrinsicInst>(I)) {\n    if (II->getIntrinsicID() == Intrinsic::fshl ||\n        II->getIntrinsicID() == Intrinsic::fshr)\n      ShiftAmountOpNum = 2;\n  }\n\n  if (ShiftAmountOpNum == -1)\n    return false;\n\n  auto *Shuf = dyn_cast<ShuffleVectorInst>(I->getOperand(ShiftAmountOpNum));\n  if (Shuf && getSplatIndex(Shuf->getShuffleMask()) >= 0 &&\n      isVectorShiftByScalarCheap(I->getType())) {\n    Ops.push_back(&I->getOperandUse(ShiftAmountOpNum));\n    return true;\n  }\n\n  return false;\n}\n\nbool X86TargetLowering::shouldConvertPhiType(Type *From, Type *To) const {\n  if (!Subtarget.is64Bit())\n    return false;\n  return TargetLowering::shouldConvertPhiType(From, To);\n}\n\nbool X86TargetLowering::isVectorLoadExtDesirable(SDValue ExtVal) const {\n  if (isa<MaskedLoadSDNode>(ExtVal.getOperand(0)))\n    return false;\n\n  EVT SrcVT = ExtVal.getOperand(0).getValueType();\n\n  // There is no extending load for vXi1.\n  if (SrcVT.getScalarType() == MVT::i1)\n    return false;\n\n  return true;\n}\n\nbool X86TargetLowering::isFMAFasterThanFMulAndFAdd(const MachineFunction &MF,\n                                                   EVT VT) const {\n  if (!Subtarget.hasAnyFMA())\n    return false;\n\n  VT = VT.getScalarType();\n\n  if (!VT.isSimple())\n    return false;\n\n  switch (VT.getSimpleVT().SimpleTy) {\n  case MVT::f32:\n  case MVT::f64:\n    return true;\n  default:\n    break;\n  }\n\n  return false;\n}\n\nbool X86TargetLowering::isNarrowingProfitable(EVT VT1, EVT VT2) const {\n  // i16 instructions are longer (0x66 prefix) and potentially slower.\n  return !(VT1 == MVT::i32 && VT2 == MVT::i16);\n}\n\n/// Targets can use this to indicate that they only support *some*\n/// VECTOR_SHUFFLE operations, those with specific masks.\n/// By default, if a target supports the VECTOR_SHUFFLE node, all mask values\n/// are assumed to be legal.\nbool X86TargetLowering::isShuffleMaskLegal(ArrayRef<int> Mask, EVT VT) const {\n  if (!VT.isSimple())\n    return false;\n\n  // Not for i1 vectors\n  if (VT.getSimpleVT().getScalarType() == MVT::i1)\n    return false;\n\n  // Very little shuffling can be done for 64-bit vectors right now.\n  if (VT.getSimpleVT().getSizeInBits() == 64)\n    return false;\n\n  // We only care that the types being shuffled are legal. The lowering can\n  // handle any possible shuffle mask that results.\n  return isTypeLegal(VT.getSimpleVT());\n}\n\nbool X86TargetLowering::isVectorClearMaskLegal(ArrayRef<int> Mask,\n                                               EVT VT) const {\n  // Don't convert an 'and' into a shuffle that we don't directly support.\n  // vpblendw and vpshufb for 256-bit vectors are not available on AVX1.\n  if (!Subtarget.hasAVX2())\n    if (VT == MVT::v32i8 || VT == MVT::v16i16)\n      return false;\n\n  // Just delegate to the generic legality, clear masks aren't special.\n  return isShuffleMaskLegal(Mask, VT);\n}\n\nbool X86TargetLowering::areJTsAllowed(const Function *Fn) const {\n  // If the subtarget is using thunks, we need to not generate jump tables.\n  if (Subtarget.useIndirectThunkBranches())\n    return false;\n\n  // Otherwise, fallback on the generic logic.\n  return TargetLowering::areJTsAllowed(Fn);\n}\n\n//===----------------------------------------------------------------------===//\n//                           X86 Scheduler Hooks\n//===----------------------------------------------------------------------===//\n\n// Returns true if EFLAG is consumed after this iterator in the rest of the\n// basic block or any successors of the basic block.\nstatic bool isEFLAGSLiveAfter(MachineBasicBlock::iterator Itr,\n                              MachineBasicBlock *BB) {\n  // Scan forward through BB for a use/def of EFLAGS.\n  for (MachineBasicBlock::iterator miI = std::next(Itr), miE = BB->end();\n         miI != miE; ++miI) {\n    const MachineInstr& mi = *miI;\n    if (mi.readsRegister(X86::EFLAGS))\n      return true;\n    // If we found a def, we can stop searching.\n    if (mi.definesRegister(X86::EFLAGS))\n      return false;\n  }\n\n  // If we hit the end of the block, check whether EFLAGS is live into a\n  // successor.\n  for (MachineBasicBlock::succ_iterator sItr = BB->succ_begin(),\n                                        sEnd = BB->succ_end();\n       sItr != sEnd; ++sItr) {\n    MachineBasicBlock* succ = *sItr;\n    if (succ->isLiveIn(X86::EFLAGS))\n      return true;\n  }\n\n  return false;\n}\n\n/// Utility function to emit xbegin specifying the start of an RTM region.\nstatic MachineBasicBlock *emitXBegin(MachineInstr &MI, MachineBasicBlock *MBB,\n                                     const TargetInstrInfo *TII) {\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  const BasicBlock *BB = MBB->getBasicBlock();\n  MachineFunction::iterator I = ++MBB->getIterator();\n\n  // For the v = xbegin(), we generate\n  //\n  // thisMBB:\n  //  xbegin sinkMBB\n  //\n  // mainMBB:\n  //  s0 = -1\n  //\n  // fallBB:\n  //  eax = # XABORT_DEF\n  //  s1 = eax\n  //\n  // sinkMBB:\n  //  v = phi(s0/mainBB, s1/fallBB)\n\n  MachineBasicBlock *thisMBB = MBB;\n  MachineFunction *MF = MBB->getParent();\n  MachineBasicBlock *mainMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *fallMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(BB);\n  MF->insert(I, mainMBB);\n  MF->insert(I, fallMBB);\n  MF->insert(I, sinkMBB);\n\n  if (isEFLAGSLiveAfter(MI, MBB)) {\n    mainMBB->addLiveIn(X86::EFLAGS);\n    fallMBB->addLiveIn(X86::EFLAGS);\n    sinkMBB->addLiveIn(X86::EFLAGS);\n  }\n\n  // Transfer the remainder of BB and its successor edges to sinkMBB.\n  sinkMBB->splice(sinkMBB->begin(), MBB,\n                  std::next(MachineBasicBlock::iterator(MI)), MBB->end());\n  sinkMBB->transferSuccessorsAndUpdatePHIs(MBB);\n\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n  Register DstReg = MI.getOperand(0).getReg();\n  const TargetRegisterClass *RC = MRI.getRegClass(DstReg);\n  Register mainDstReg = MRI.createVirtualRegister(RC);\n  Register fallDstReg = MRI.createVirtualRegister(RC);\n\n  // thisMBB:\n  //  xbegin fallMBB\n  //  # fallthrough to mainMBB\n  //  # abortion to fallMBB\n  BuildMI(thisMBB, DL, TII->get(X86::XBEGIN_4)).addMBB(fallMBB);\n  thisMBB->addSuccessor(mainMBB);\n  thisMBB->addSuccessor(fallMBB);\n\n  // mainMBB:\n  //  mainDstReg := -1\n  BuildMI(mainMBB, DL, TII->get(X86::MOV32ri), mainDstReg).addImm(-1);\n  BuildMI(mainMBB, DL, TII->get(X86::JMP_1)).addMBB(sinkMBB);\n  mainMBB->addSuccessor(sinkMBB);\n\n  // fallMBB:\n  //  ; pseudo instruction to model hardware's definition from XABORT\n  //  EAX := XABORT_DEF\n  //  fallDstReg := EAX\n  BuildMI(fallMBB, DL, TII->get(X86::XABORT_DEF));\n  BuildMI(fallMBB, DL, TII->get(TargetOpcode::COPY), fallDstReg)\n      .addReg(X86::EAX);\n  fallMBB->addSuccessor(sinkMBB);\n\n  // sinkMBB:\n  //  DstReg := phi(mainDstReg/mainBB, fallDstReg/fallBB)\n  BuildMI(*sinkMBB, sinkMBB->begin(), DL, TII->get(X86::PHI), DstReg)\n      .addReg(mainDstReg).addMBB(mainMBB)\n      .addReg(fallDstReg).addMBB(fallMBB);\n\n  MI.eraseFromParent();\n  return sinkMBB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitVAARGWithCustomInserter(MachineInstr &MI,\n                                               MachineBasicBlock *MBB) const {\n  // Emit va_arg instruction on X86-64.\n\n  // Operands to this pseudo-instruction:\n  // 0  ) Output        : destination address (reg)\n  // 1-5) Input         : va_list address (addr, i64mem)\n  // 6  ) ArgSize       : Size (in bytes) of vararg type\n  // 7  ) ArgMode       : 0=overflow only, 1=use gp_offset, 2=use fp_offset\n  // 8  ) Align         : Alignment of type\n  // 9  ) EFLAGS (implicit-def)\n\n  assert(MI.getNumOperands() == 10 && \"VAARG should have 10 operands!\");\n  static_assert(X86::AddrNumOperands == 5, \"VAARG assumes 5 address operands\");\n\n  Register DestReg = MI.getOperand(0).getReg();\n  MachineOperand &Base = MI.getOperand(1);\n  MachineOperand &Scale = MI.getOperand(2);\n  MachineOperand &Index = MI.getOperand(3);\n  MachineOperand &Disp = MI.getOperand(4);\n  MachineOperand &Segment = MI.getOperand(5);\n  unsigned ArgSize = MI.getOperand(6).getImm();\n  unsigned ArgMode = MI.getOperand(7).getImm();\n  Align Alignment = Align(MI.getOperand(8).getImm());\n\n  MachineFunction *MF = MBB->getParent();\n\n  // Memory Reference\n  assert(MI.hasOneMemOperand() && \"Expected VAARG to have one memoperand\");\n\n  MachineMemOperand *OldMMO = MI.memoperands().front();\n\n  // Clone the MMO into two separate MMOs for loading and storing\n  MachineMemOperand *LoadOnlyMMO = MF->getMachineMemOperand(\n      OldMMO, OldMMO->getFlags() & ~MachineMemOperand::MOStore);\n  MachineMemOperand *StoreOnlyMMO = MF->getMachineMemOperand(\n      OldMMO, OldMMO->getFlags() & ~MachineMemOperand::MOLoad);\n\n  // Machine Information\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  MachineRegisterInfo &MRI = MBB->getParent()->getRegInfo();\n  const TargetRegisterClass *AddrRegClass =\n      getRegClassFor(getPointerTy(MBB->getParent()->getDataLayout()));\n  const TargetRegisterClass *OffsetRegClass = getRegClassFor(MVT::i32);\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  // struct va_list {\n  //   i32   gp_offset\n  //   i32   fp_offset\n  //   i64   overflow_area (address)\n  //   i64   reg_save_area (address)\n  // }\n  // sizeof(va_list) = 24\n  // alignment(va_list) = 8\n\n  unsigned TotalNumIntRegs = 6;\n  unsigned TotalNumXMMRegs = 8;\n  bool UseGPOffset = (ArgMode == 1);\n  bool UseFPOffset = (ArgMode == 2);\n  unsigned MaxOffset = TotalNumIntRegs * 8 +\n                       (UseFPOffset ? TotalNumXMMRegs * 16 : 0);\n\n  /* Align ArgSize to a multiple of 8 */\n  unsigned ArgSizeA8 = (ArgSize + 7) & ~7;\n  bool NeedsAlign = (Alignment > 8);\n\n  MachineBasicBlock *thisMBB = MBB;\n  MachineBasicBlock *overflowMBB;\n  MachineBasicBlock *offsetMBB;\n  MachineBasicBlock *endMBB;\n\n  unsigned OffsetDestReg = 0;    // Argument address computed by offsetMBB\n  unsigned OverflowDestReg = 0;  // Argument address computed by overflowMBB\n  unsigned OffsetReg = 0;\n\n  if (!UseGPOffset && !UseFPOffset) {\n    // If we only pull from the overflow region, we don't create a branch.\n    // We don't need to alter control flow.\n    OffsetDestReg = 0; // unused\n    OverflowDestReg = DestReg;\n\n    offsetMBB = nullptr;\n    overflowMBB = thisMBB;\n    endMBB = thisMBB;\n  } else {\n    // First emit code to check if gp_offset (or fp_offset) is below the bound.\n    // If so, pull the argument from reg_save_area. (branch to offsetMBB)\n    // If not, pull from overflow_area. (branch to overflowMBB)\n    //\n    //       thisMBB\n    //         |     .\n    //         |        .\n    //     offsetMBB   overflowMBB\n    //         |        .\n    //         |     .\n    //        endMBB\n\n    // Registers for the PHI in endMBB\n    OffsetDestReg = MRI.createVirtualRegister(AddrRegClass);\n    OverflowDestReg = MRI.createVirtualRegister(AddrRegClass);\n\n    const BasicBlock *LLVM_BB = MBB->getBasicBlock();\n    overflowMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n    offsetMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n    endMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n\n    MachineFunction::iterator MBBIter = ++MBB->getIterator();\n\n    // Insert the new basic blocks\n    MF->insert(MBBIter, offsetMBB);\n    MF->insert(MBBIter, overflowMBB);\n    MF->insert(MBBIter, endMBB);\n\n    // Transfer the remainder of MBB and its successor edges to endMBB.\n    endMBB->splice(endMBB->begin(), thisMBB,\n                   std::next(MachineBasicBlock::iterator(MI)), thisMBB->end());\n    endMBB->transferSuccessorsAndUpdatePHIs(thisMBB);\n\n    // Make offsetMBB and overflowMBB successors of thisMBB\n    thisMBB->addSuccessor(offsetMBB);\n    thisMBB->addSuccessor(overflowMBB);\n\n    // endMBB is a successor of both offsetMBB and overflowMBB\n    offsetMBB->addSuccessor(endMBB);\n    overflowMBB->addSuccessor(endMBB);\n\n    // Load the offset value into a register\n    OffsetReg = MRI.createVirtualRegister(OffsetRegClass);\n    BuildMI(thisMBB, DL, TII->get(X86::MOV32rm), OffsetReg)\n        .add(Base)\n        .add(Scale)\n        .add(Index)\n        .addDisp(Disp, UseFPOffset ? 4 : 0)\n        .add(Segment)\n        .setMemRefs(LoadOnlyMMO);\n\n    // Check if there is enough room left to pull this argument.\n    BuildMI(thisMBB, DL, TII->get(X86::CMP32ri))\n      .addReg(OffsetReg)\n      .addImm(MaxOffset + 8 - ArgSizeA8);\n\n    // Branch to \"overflowMBB\" if offset >= max\n    // Fall through to \"offsetMBB\" otherwise\n    BuildMI(thisMBB, DL, TII->get(X86::JCC_1))\n      .addMBB(overflowMBB).addImm(X86::COND_AE);\n  }\n\n  // In offsetMBB, emit code to use the reg_save_area.\n  if (offsetMBB) {\n    assert(OffsetReg != 0);\n\n    // Read the reg_save_area address.\n    Register RegSaveReg = MRI.createVirtualRegister(AddrRegClass);\n    BuildMI(\n        offsetMBB, DL,\n        TII->get(Subtarget.isTarget64BitLP64() ? X86::MOV64rm : X86::MOV32rm),\n        RegSaveReg)\n        .add(Base)\n        .add(Scale)\n        .add(Index)\n        .addDisp(Disp, Subtarget.isTarget64BitLP64() ? 16 : 12)\n        .add(Segment)\n        .setMemRefs(LoadOnlyMMO);\n\n    if (Subtarget.isTarget64BitLP64()) {\n      // Zero-extend the offset\n      Register OffsetReg64 = MRI.createVirtualRegister(AddrRegClass);\n      BuildMI(offsetMBB, DL, TII->get(X86::SUBREG_TO_REG), OffsetReg64)\n          .addImm(0)\n          .addReg(OffsetReg)\n          .addImm(X86::sub_32bit);\n\n      // Add the offset to the reg_save_area to get the final address.\n      BuildMI(offsetMBB, DL, TII->get(X86::ADD64rr), OffsetDestReg)\n          .addReg(OffsetReg64)\n          .addReg(RegSaveReg);\n    } else {\n      // Add the offset to the reg_save_area to get the final address.\n      BuildMI(offsetMBB, DL, TII->get(X86::ADD32rr), OffsetDestReg)\n          .addReg(OffsetReg)\n          .addReg(RegSaveReg);\n    }\n\n    // Compute the offset for the next argument\n    Register NextOffsetReg = MRI.createVirtualRegister(OffsetRegClass);\n    BuildMI(offsetMBB, DL, TII->get(X86::ADD32ri), NextOffsetReg)\n      .addReg(OffsetReg)\n      .addImm(UseFPOffset ? 16 : 8);\n\n    // Store it back into the va_list.\n    BuildMI(offsetMBB, DL, TII->get(X86::MOV32mr))\n        .add(Base)\n        .add(Scale)\n        .add(Index)\n        .addDisp(Disp, UseFPOffset ? 4 : 0)\n        .add(Segment)\n        .addReg(NextOffsetReg)\n        .setMemRefs(StoreOnlyMMO);\n\n    // Jump to endMBB\n    BuildMI(offsetMBB, DL, TII->get(X86::JMP_1))\n      .addMBB(endMBB);\n  }\n\n  //\n  // Emit code to use overflow area\n  //\n\n  // Load the overflow_area address into a register.\n  Register OverflowAddrReg = MRI.createVirtualRegister(AddrRegClass);\n  BuildMI(overflowMBB, DL,\n          TII->get(Subtarget.isTarget64BitLP64() ? X86::MOV64rm : X86::MOV32rm),\n          OverflowAddrReg)\n      .add(Base)\n      .add(Scale)\n      .add(Index)\n      .addDisp(Disp, 8)\n      .add(Segment)\n      .setMemRefs(LoadOnlyMMO);\n\n  // If we need to align it, do so. Otherwise, just copy the address\n  // to OverflowDestReg.\n  if (NeedsAlign) {\n    // Align the overflow address\n    Register TmpReg = MRI.createVirtualRegister(AddrRegClass);\n\n    // aligned_addr = (addr + (align-1)) & ~(align-1)\n    BuildMI(\n        overflowMBB, DL,\n        TII->get(Subtarget.isTarget64BitLP64() ? X86::ADD64ri32 : X86::ADD32ri),\n        TmpReg)\n        .addReg(OverflowAddrReg)\n        .addImm(Alignment.value() - 1);\n\n    BuildMI(\n        overflowMBB, DL,\n        TII->get(Subtarget.isTarget64BitLP64() ? X86::AND64ri32 : X86::AND32ri),\n        OverflowDestReg)\n        .addReg(TmpReg)\n        .addImm(~(uint64_t)(Alignment.value() - 1));\n  } else {\n    BuildMI(overflowMBB, DL, TII->get(TargetOpcode::COPY), OverflowDestReg)\n      .addReg(OverflowAddrReg);\n  }\n\n  // Compute the next overflow address after this argument.\n  // (the overflow address should be kept 8-byte aligned)\n  Register NextAddrReg = MRI.createVirtualRegister(AddrRegClass);\n  BuildMI(\n      overflowMBB, DL,\n      TII->get(Subtarget.isTarget64BitLP64() ? X86::ADD64ri32 : X86::ADD32ri),\n      NextAddrReg)\n      .addReg(OverflowDestReg)\n      .addImm(ArgSizeA8);\n\n  // Store the new overflow address.\n  BuildMI(overflowMBB, DL,\n          TII->get(Subtarget.isTarget64BitLP64() ? X86::MOV64mr : X86::MOV32mr))\n      .add(Base)\n      .add(Scale)\n      .add(Index)\n      .addDisp(Disp, 8)\n      .add(Segment)\n      .addReg(NextAddrReg)\n      .setMemRefs(StoreOnlyMMO);\n\n  // If we branched, emit the PHI to the front of endMBB.\n  if (offsetMBB) {\n    BuildMI(*endMBB, endMBB->begin(), DL,\n            TII->get(X86::PHI), DestReg)\n      .addReg(OffsetDestReg).addMBB(offsetMBB)\n      .addReg(OverflowDestReg).addMBB(overflowMBB);\n  }\n\n  // Erase the pseudo instruction\n  MI.eraseFromParent();\n\n  return endMBB;\n}\n\nMachineBasicBlock *X86TargetLowering::EmitVAStartSaveXMMRegsWithCustomInserter(\n    MachineInstr &MI, MachineBasicBlock *MBB) const {\n  // Emit code to save XMM registers to the stack. The ABI says that the\n  // number of registers to save is given in %al, so it's theoretically\n  // possible to do an indirect jump trick to avoid saving all of them,\n  // however this code takes a simpler approach and just executes all\n  // of the stores if %al is non-zero. It's less code, and it's probably\n  // easier on the hardware branch predictor, and stores aren't all that\n  // expensive anyway.\n\n  // Create the new basic blocks. One block contains all the XMM stores,\n  // and one block is the final destination regardless of whether any\n  // stores were performed.\n  const BasicBlock *LLVM_BB = MBB->getBasicBlock();\n  MachineFunction *F = MBB->getParent();\n  MachineFunction::iterator MBBIter = ++MBB->getIterator();\n  MachineBasicBlock *XMMSaveMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *EndMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  F->insert(MBBIter, XMMSaveMBB);\n  F->insert(MBBIter, EndMBB);\n\n  // Transfer the remainder of MBB and its successor edges to EndMBB.\n  EndMBB->splice(EndMBB->begin(), MBB,\n                 std::next(MachineBasicBlock::iterator(MI)), MBB->end());\n  EndMBB->transferSuccessorsAndUpdatePHIs(MBB);\n\n  // The original block will now fall through to the XMM save block.\n  MBB->addSuccessor(XMMSaveMBB);\n  // The XMMSaveMBB will fall through to the end block.\n  XMMSaveMBB->addSuccessor(EndMBB);\n\n  // Now add the instructions.\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  Register CountReg = MI.getOperand(0).getReg();\n  int RegSaveFrameIndex = MI.getOperand(1).getImm();\n  int64_t VarArgsFPOffset = MI.getOperand(2).getImm();\n\n  if (!Subtarget.isCallingConvWin64(F->getFunction().getCallingConv())) {\n    // If %al is 0, branch around the XMM save block.\n    BuildMI(MBB, DL, TII->get(X86::TEST8rr)).addReg(CountReg).addReg(CountReg);\n    BuildMI(MBB, DL, TII->get(X86::JCC_1)).addMBB(EndMBB).addImm(X86::COND_E);\n    MBB->addSuccessor(EndMBB);\n  }\n\n  // Make sure the last operand is EFLAGS, which gets clobbered by the branch\n  // that was just emitted, but clearly shouldn't be \"saved\".\n  assert((MI.getNumOperands() <= 3 ||\n          !MI.getOperand(MI.getNumOperands() - 1).isReg() ||\n          MI.getOperand(MI.getNumOperands() - 1).getReg() == X86::EFLAGS) &&\n         \"Expected last argument to be EFLAGS\");\n  unsigned MOVOpc = Subtarget.hasAVX() ? X86::VMOVAPSmr : X86::MOVAPSmr;\n  // In the XMM save block, save all the XMM argument registers.\n  for (int i = 3, e = MI.getNumOperands() - 1; i != e; ++i) {\n    int64_t Offset = (i - 3) * 16 + VarArgsFPOffset;\n    MachineMemOperand *MMO = F->getMachineMemOperand(\n        MachinePointerInfo::getFixedStack(*F, RegSaveFrameIndex, Offset),\n        MachineMemOperand::MOStore,\n        /*Size=*/16, Align(16));\n    BuildMI(XMMSaveMBB, DL, TII->get(MOVOpc))\n        .addFrameIndex(RegSaveFrameIndex)\n        .addImm(/*Scale=*/1)\n        .addReg(/*IndexReg=*/0)\n        .addImm(/*Disp=*/Offset)\n        .addReg(/*Segment=*/0)\n        .addReg(MI.getOperand(i).getReg())\n        .addMemOperand(MMO);\n  }\n\n  MI.eraseFromParent(); // The pseudo instruction is gone now.\n\n  return EndMBB;\n}\n\n// The EFLAGS operand of SelectItr might be missing a kill marker\n// because there were multiple uses of EFLAGS, and ISel didn't know\n// which to mark. Figure out whether SelectItr should have had a\n// kill marker, and set it if it should. Returns the correct kill\n// marker value.\nstatic bool checkAndUpdateEFLAGSKill(MachineBasicBlock::iterator SelectItr,\n                                     MachineBasicBlock* BB,\n                                     const TargetRegisterInfo* TRI) {\n  if (isEFLAGSLiveAfter(SelectItr, BB))\n    return false;\n\n  // We found a def, or hit the end of the basic block and EFLAGS wasn't live\n  // out. SelectMI should have a kill flag on EFLAGS.\n  SelectItr->addRegisterKilled(X86::EFLAGS, TRI);\n  return true;\n}\n\n// Return true if it is OK for this CMOV pseudo-opcode to be cascaded\n// together with other CMOV pseudo-opcodes into a single basic-block with\n// conditional jump around it.\nstatic bool isCMOVPseudo(MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  case X86::CMOV_FR32:\n  case X86::CMOV_FR32X:\n  case X86::CMOV_FR64:\n  case X86::CMOV_FR64X:\n  case X86::CMOV_GR8:\n  case X86::CMOV_GR16:\n  case X86::CMOV_GR32:\n  case X86::CMOV_RFP32:\n  case X86::CMOV_RFP64:\n  case X86::CMOV_RFP80:\n  case X86::CMOV_VR64:\n  case X86::CMOV_VR128:\n  case X86::CMOV_VR128X:\n  case X86::CMOV_VR256:\n  case X86::CMOV_VR256X:\n  case X86::CMOV_VR512:\n  case X86::CMOV_VK1:\n  case X86::CMOV_VK2:\n  case X86::CMOV_VK4:\n  case X86::CMOV_VK8:\n  case X86::CMOV_VK16:\n  case X86::CMOV_VK32:\n  case X86::CMOV_VK64:\n    return true;\n\n  default:\n    return false;\n  }\n}\n\n// Helper function, which inserts PHI functions into SinkMBB:\n//   %Result(i) = phi [ %FalseValue(i), FalseMBB ], [ %TrueValue(i), TrueMBB ],\n// where %FalseValue(i) and %TrueValue(i) are taken from the consequent CMOVs\n// in [MIItBegin, MIItEnd) range. It returns the last MachineInstrBuilder for\n// the last PHI function inserted.\nstatic MachineInstrBuilder createPHIsForCMOVsInSinkBB(\n    MachineBasicBlock::iterator MIItBegin, MachineBasicBlock::iterator MIItEnd,\n    MachineBasicBlock *TrueMBB, MachineBasicBlock *FalseMBB,\n    MachineBasicBlock *SinkMBB) {\n  MachineFunction *MF = TrueMBB->getParent();\n  const TargetInstrInfo *TII = MF->getSubtarget().getInstrInfo();\n  DebugLoc DL = MIItBegin->getDebugLoc();\n\n  X86::CondCode CC = X86::CondCode(MIItBegin->getOperand(3).getImm());\n  X86::CondCode OppCC = X86::GetOppositeBranchCondition(CC);\n\n  MachineBasicBlock::iterator SinkInsertionPoint = SinkMBB->begin();\n\n  // As we are creating the PHIs, we have to be careful if there is more than\n  // one.  Later CMOVs may reference the results of earlier CMOVs, but later\n  // PHIs have to reference the individual true/false inputs from earlier PHIs.\n  // That also means that PHI construction must work forward from earlier to\n  // later, and that the code must maintain a mapping from earlier PHI's\n  // destination registers, and the registers that went into the PHI.\n  DenseMap<unsigned, std::pair<unsigned, unsigned>> RegRewriteTable;\n  MachineInstrBuilder MIB;\n\n  for (MachineBasicBlock::iterator MIIt = MIItBegin; MIIt != MIItEnd; ++MIIt) {\n    Register DestReg = MIIt->getOperand(0).getReg();\n    Register Op1Reg = MIIt->getOperand(1).getReg();\n    Register Op2Reg = MIIt->getOperand(2).getReg();\n\n    // If this CMOV we are generating is the opposite condition from\n    // the jump we generated, then we have to swap the operands for the\n    // PHI that is going to be generated.\n    if (MIIt->getOperand(3).getImm() == OppCC)\n      std::swap(Op1Reg, Op2Reg);\n\n    if (RegRewriteTable.find(Op1Reg) != RegRewriteTable.end())\n      Op1Reg = RegRewriteTable[Op1Reg].first;\n\n    if (RegRewriteTable.find(Op2Reg) != RegRewriteTable.end())\n      Op2Reg = RegRewriteTable[Op2Reg].second;\n\n    MIB = BuildMI(*SinkMBB, SinkInsertionPoint, DL, TII->get(X86::PHI), DestReg)\n              .addReg(Op1Reg)\n              .addMBB(FalseMBB)\n              .addReg(Op2Reg)\n              .addMBB(TrueMBB);\n\n    // Add this PHI to the rewrite table.\n    RegRewriteTable[DestReg] = std::make_pair(Op1Reg, Op2Reg);\n  }\n\n  return MIB;\n}\n\n// Lower cascaded selects in form of (SecondCmov (FirstCMOV F, T, cc1), T, cc2).\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredCascadedSelect(MachineInstr &FirstCMOV,\n                                             MachineInstr &SecondCascadedCMOV,\n                                             MachineBasicBlock *ThisMBB) const {\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  DebugLoc DL = FirstCMOV.getDebugLoc();\n\n  // We lower cascaded CMOVs such as\n  //\n  //   (SecondCascadedCMOV (FirstCMOV F, T, cc1), T, cc2)\n  //\n  // to two successive branches.\n  //\n  // Without this, we would add a PHI between the two jumps, which ends up\n  // creating a few copies all around. For instance, for\n  //\n  //    (sitofp (zext (fcmp une)))\n  //\n  // we would generate:\n  //\n  //         ucomiss %xmm1, %xmm0\n  //         movss  <1.0f>, %xmm0\n  //         movaps  %xmm0, %xmm1\n  //         jne     .LBB5_2\n  //         xorps   %xmm1, %xmm1\n  // .LBB5_2:\n  //         jp      .LBB5_4\n  //         movaps  %xmm1, %xmm0\n  // .LBB5_4:\n  //         retq\n  //\n  // because this custom-inserter would have generated:\n  //\n  //   A\n  //   | \\\n  //   |  B\n  //   | /\n  //   C\n  //   | \\\n  //   |  D\n  //   | /\n  //   E\n  //\n  // A: X = ...; Y = ...\n  // B: empty\n  // C: Z = PHI [X, A], [Y, B]\n  // D: empty\n  // E: PHI [X, C], [Z, D]\n  //\n  // If we lower both CMOVs in a single step, we can instead generate:\n  //\n  //   A\n  //   | \\\n  //   |  C\n  //   | /|\n  //   |/ |\n  //   |  |\n  //   |  D\n  //   | /\n  //   E\n  //\n  // A: X = ...; Y = ...\n  // D: empty\n  // E: PHI [X, A], [X, C], [Y, D]\n  //\n  // Which, in our sitofp/fcmp example, gives us something like:\n  //\n  //         ucomiss %xmm1, %xmm0\n  //         movss  <1.0f>, %xmm0\n  //         jne     .LBB5_4\n  //         jp      .LBB5_4\n  //         xorps   %xmm0, %xmm0\n  // .LBB5_4:\n  //         retq\n  //\n\n  // We lower cascaded CMOV into two successive branches to the same block.\n  // EFLAGS is used by both, so mark it as live in the second.\n  const BasicBlock *LLVM_BB = ThisMBB->getBasicBlock();\n  MachineFunction *F = ThisMBB->getParent();\n  MachineBasicBlock *FirstInsertedMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *SecondInsertedMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *SinkMBB = F->CreateMachineBasicBlock(LLVM_BB);\n\n  MachineFunction::iterator It = ++ThisMBB->getIterator();\n  F->insert(It, FirstInsertedMBB);\n  F->insert(It, SecondInsertedMBB);\n  F->insert(It, SinkMBB);\n\n  // For a cascaded CMOV, we lower it to two successive branches to\n  // the same block (SinkMBB).  EFLAGS is used by both, so mark it as live in\n  // the FirstInsertedMBB.\n  FirstInsertedMBB->addLiveIn(X86::EFLAGS);\n\n  // If the EFLAGS register isn't dead in the terminator, then claim that it's\n  // live into the sink and copy blocks.\n  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n  if (!SecondCascadedCMOV.killsRegister(X86::EFLAGS) &&\n      !checkAndUpdateEFLAGSKill(SecondCascadedCMOV, ThisMBB, TRI)) {\n    SecondInsertedMBB->addLiveIn(X86::EFLAGS);\n    SinkMBB->addLiveIn(X86::EFLAGS);\n  }\n\n  // Transfer the remainder of ThisMBB and its successor edges to SinkMBB.\n  SinkMBB->splice(SinkMBB->begin(), ThisMBB,\n                  std::next(MachineBasicBlock::iterator(FirstCMOV)),\n                  ThisMBB->end());\n  SinkMBB->transferSuccessorsAndUpdatePHIs(ThisMBB);\n\n  // Fallthrough block for ThisMBB.\n  ThisMBB->addSuccessor(FirstInsertedMBB);\n  // The true block target of the first branch is always SinkMBB.\n  ThisMBB->addSuccessor(SinkMBB);\n  // Fallthrough block for FirstInsertedMBB.\n  FirstInsertedMBB->addSuccessor(SecondInsertedMBB);\n  // The true block for the branch of FirstInsertedMBB.\n  FirstInsertedMBB->addSuccessor(SinkMBB);\n  // This is fallthrough.\n  SecondInsertedMBB->addSuccessor(SinkMBB);\n\n  // Create the conditional branch instructions.\n  X86::CondCode FirstCC = X86::CondCode(FirstCMOV.getOperand(3).getImm());\n  BuildMI(ThisMBB, DL, TII->get(X86::JCC_1)).addMBB(SinkMBB).addImm(FirstCC);\n\n  X86::CondCode SecondCC =\n      X86::CondCode(SecondCascadedCMOV.getOperand(3).getImm());\n  BuildMI(FirstInsertedMBB, DL, TII->get(X86::JCC_1)).addMBB(SinkMBB).addImm(SecondCC);\n\n  //  SinkMBB:\n  //   %Result = phi [ %FalseValue, SecondInsertedMBB ], [ %TrueValue, ThisMBB ]\n  Register DestReg = FirstCMOV.getOperand(0).getReg();\n  Register Op1Reg = FirstCMOV.getOperand(1).getReg();\n  Register Op2Reg = FirstCMOV.getOperand(2).getReg();\n  MachineInstrBuilder MIB =\n      BuildMI(*SinkMBB, SinkMBB->begin(), DL, TII->get(X86::PHI), DestReg)\n          .addReg(Op1Reg)\n          .addMBB(SecondInsertedMBB)\n          .addReg(Op2Reg)\n          .addMBB(ThisMBB);\n\n  // The second SecondInsertedMBB provides the same incoming value as the\n  // FirstInsertedMBB (the True operand of the SELECT_CC/CMOV nodes).\n  MIB.addReg(FirstCMOV.getOperand(2).getReg()).addMBB(FirstInsertedMBB);\n  // Copy the PHI result to the register defined by the second CMOV.\n  BuildMI(*SinkMBB, std::next(MachineBasicBlock::iterator(MIB.getInstr())), DL,\n          TII->get(TargetOpcode::COPY),\n          SecondCascadedCMOV.getOperand(0).getReg())\n      .addReg(FirstCMOV.getOperand(0).getReg());\n\n  // Now remove the CMOVs.\n  FirstCMOV.eraseFromParent();\n  SecondCascadedCMOV.eraseFromParent();\n\n  return SinkMBB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredSelect(MachineInstr &MI,\n                                     MachineBasicBlock *ThisMBB) const {\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  // To \"insert\" a SELECT_CC instruction, we actually have to insert the\n  // diamond control-flow pattern.  The incoming instruction knows the\n  // destination vreg to set, the condition code register to branch on, the\n  // true/false values to select between and a branch opcode to use.\n\n  //  ThisMBB:\n  //  ...\n  //   TrueVal = ...\n  //   cmpTY ccX, r1, r2\n  //   bCC copy1MBB\n  //   fallthrough --> FalseMBB\n\n  // This code lowers all pseudo-CMOV instructions. Generally it lowers these\n  // as described above, by inserting a BB, and then making a PHI at the join\n  // point to select the true and false operands of the CMOV in the PHI.\n  //\n  // The code also handles two different cases of multiple CMOV opcodes\n  // in a row.\n  //\n  // Case 1:\n  // In this case, there are multiple CMOVs in a row, all which are based on\n  // the same condition setting (or the exact opposite condition setting).\n  // In this case we can lower all the CMOVs using a single inserted BB, and\n  // then make a number of PHIs at the join point to model the CMOVs. The only\n  // trickiness here, is that in a case like:\n  //\n  // t2 = CMOV cond1 t1, f1\n  // t3 = CMOV cond1 t2, f2\n  //\n  // when rewriting this into PHIs, we have to perform some renaming on the\n  // temps since you cannot have a PHI operand refer to a PHI result earlier\n  // in the same block.  The \"simple\" but wrong lowering would be:\n  //\n  // t2 = PHI t1(BB1), f1(BB2)\n  // t3 = PHI t2(BB1), f2(BB2)\n  //\n  // but clearly t2 is not defined in BB1, so that is incorrect. The proper\n  // renaming is to note that on the path through BB1, t2 is really just a\n  // copy of t1, and do that renaming, properly generating:\n  //\n  // t2 = PHI t1(BB1), f1(BB2)\n  // t3 = PHI t1(BB1), f2(BB2)\n  //\n  // Case 2:\n  // CMOV ((CMOV F, T, cc1), T, cc2) is checked here and handled by a separate\n  // function - EmitLoweredCascadedSelect.\n\n  X86::CondCode CC = X86::CondCode(MI.getOperand(3).getImm());\n  X86::CondCode OppCC = X86::GetOppositeBranchCondition(CC);\n  MachineInstr *LastCMOV = &MI;\n  MachineBasicBlock::iterator NextMIIt = MachineBasicBlock::iterator(MI);\n\n  // Check for case 1, where there are multiple CMOVs with the same condition\n  // first.  Of the two cases of multiple CMOV lowerings, case 1 reduces the\n  // number of jumps the most.\n\n  if (isCMOVPseudo(MI)) {\n    // See if we have a string of CMOVS with the same condition. Skip over\n    // intervening debug insts.\n    while (NextMIIt != ThisMBB->end() && isCMOVPseudo(*NextMIIt) &&\n           (NextMIIt->getOperand(3).getImm() == CC ||\n            NextMIIt->getOperand(3).getImm() == OppCC)) {\n      LastCMOV = &*NextMIIt;\n      NextMIIt = next_nodbg(NextMIIt, ThisMBB->end());\n    }\n  }\n\n  // This checks for case 2, but only do this if we didn't already find\n  // case 1, as indicated by LastCMOV == MI.\n  if (LastCMOV == &MI && NextMIIt != ThisMBB->end() &&\n      NextMIIt->getOpcode() == MI.getOpcode() &&\n      NextMIIt->getOperand(2).getReg() == MI.getOperand(2).getReg() &&\n      NextMIIt->getOperand(1).getReg() == MI.getOperand(0).getReg() &&\n      NextMIIt->getOperand(1).isKill()) {\n    return EmitLoweredCascadedSelect(MI, *NextMIIt, ThisMBB);\n  }\n\n  const BasicBlock *LLVM_BB = ThisMBB->getBasicBlock();\n  MachineFunction *F = ThisMBB->getParent();\n  MachineBasicBlock *FalseMBB = F->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *SinkMBB = F->CreateMachineBasicBlock(LLVM_BB);\n\n  MachineFunction::iterator It = ++ThisMBB->getIterator();\n  F->insert(It, FalseMBB);\n  F->insert(It, SinkMBB);\n\n  // If the EFLAGS register isn't dead in the terminator, then claim that it's\n  // live into the sink and copy blocks.\n  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n  if (!LastCMOV->killsRegister(X86::EFLAGS) &&\n      !checkAndUpdateEFLAGSKill(LastCMOV, ThisMBB, TRI)) {\n    FalseMBB->addLiveIn(X86::EFLAGS);\n    SinkMBB->addLiveIn(X86::EFLAGS);\n  }\n\n  // Transfer any debug instructions inside the CMOV sequence to the sunk block.\n  auto DbgEnd = MachineBasicBlock::iterator(LastCMOV);\n  auto DbgIt = MachineBasicBlock::iterator(MI);\n  while (DbgIt != DbgEnd) {\n    auto Next = std::next(DbgIt);\n    if (DbgIt->isDebugInstr())\n      SinkMBB->push_back(DbgIt->removeFromParent());\n    DbgIt = Next;\n  }\n\n  // Transfer the remainder of ThisMBB and its successor edges to SinkMBB.\n  SinkMBB->splice(SinkMBB->end(), ThisMBB,\n                  std::next(MachineBasicBlock::iterator(LastCMOV)),\n                  ThisMBB->end());\n  SinkMBB->transferSuccessorsAndUpdatePHIs(ThisMBB);\n\n  // Fallthrough block for ThisMBB.\n  ThisMBB->addSuccessor(FalseMBB);\n  // The true block target of the first (or only) branch is always a SinkMBB.\n  ThisMBB->addSuccessor(SinkMBB);\n  // Fallthrough block for FalseMBB.\n  FalseMBB->addSuccessor(SinkMBB);\n\n  // Create the conditional branch instruction.\n  BuildMI(ThisMBB, DL, TII->get(X86::JCC_1)).addMBB(SinkMBB).addImm(CC);\n\n  //  SinkMBB:\n  //   %Result = phi [ %FalseValue, FalseMBB ], [ %TrueValue, ThisMBB ]\n  //  ...\n  MachineBasicBlock::iterator MIItBegin = MachineBasicBlock::iterator(MI);\n  MachineBasicBlock::iterator MIItEnd =\n      std::next(MachineBasicBlock::iterator(LastCMOV));\n  createPHIsForCMOVsInSinkBB(MIItBegin, MIItEnd, ThisMBB, FalseMBB, SinkMBB);\n\n  // Now remove the CMOV(s).\n  ThisMBB->erase(MIItBegin, MIItEnd);\n\n  return SinkMBB;\n}\n\nstatic unsigned getSUBriOpcode(bool IsLP64, int64_t Imm) {\n  if (IsLP64) {\n    if (isInt<8>(Imm))\n      return X86::SUB64ri8;\n    return X86::SUB64ri32;\n  } else {\n    if (isInt<8>(Imm))\n      return X86::SUB32ri8;\n    return X86::SUB32ri;\n  }\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredProbedAlloca(MachineInstr &MI,\n                                           MachineBasicBlock *MBB) const {\n  MachineFunction *MF = MBB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const X86FrameLowering &TFI = *Subtarget.getFrameLowering();\n  const DebugLoc &DL = MI.getDebugLoc();\n  const BasicBlock *LLVM_BB = MBB->getBasicBlock();\n\n  const unsigned ProbeSize = getStackProbeSize(*MF);\n\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n  MachineBasicBlock *testMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *tailMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *blockMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n\n  MachineFunction::iterator MBBIter = ++MBB->getIterator();\n  MF->insert(MBBIter, testMBB);\n  MF->insert(MBBIter, blockMBB);\n  MF->insert(MBBIter, tailMBB);\n\n  Register sizeVReg = MI.getOperand(1).getReg();\n\n  Register physSPReg = TFI.Uses64BitFramePtr ? X86::RSP : X86::ESP;\n\n  Register TmpStackPtr = MRI.createVirtualRegister(\n      TFI.Uses64BitFramePtr ? &X86::GR64RegClass : &X86::GR32RegClass);\n  Register FinalStackPtr = MRI.createVirtualRegister(\n      TFI.Uses64BitFramePtr ? &X86::GR64RegClass : &X86::GR32RegClass);\n\n  BuildMI(*MBB, {MI}, DL, TII->get(TargetOpcode::COPY), TmpStackPtr)\n      .addReg(physSPReg);\n  {\n    const unsigned Opc = TFI.Uses64BitFramePtr ? X86::SUB64rr : X86::SUB32rr;\n    BuildMI(*MBB, {MI}, DL, TII->get(Opc), FinalStackPtr)\n        .addReg(TmpStackPtr)\n        .addReg(sizeVReg);\n  }\n\n  // test rsp size\n\n  BuildMI(testMBB, DL,\n          TII->get(TFI.Uses64BitFramePtr ? X86::CMP64rr : X86::CMP32rr))\n      .addReg(FinalStackPtr)\n      .addReg(physSPReg);\n\n  BuildMI(testMBB, DL, TII->get(X86::JCC_1))\n      .addMBB(tailMBB)\n      .addImm(X86::COND_GE);\n  testMBB->addSuccessor(blockMBB);\n  testMBB->addSuccessor(tailMBB);\n\n  // Touch the block then extend it. This is done on the opposite side of\n  // static probe where we allocate then touch, to avoid the need of probing the\n  // tail of the static alloca. Possible scenarios are:\n  //\n  //       + ---- <- ------------ <- ------------- <- ------------ +\n  //       |                                                       |\n  // [free probe] -> [page alloc] -> [alloc probe] -> [tail alloc] + -> [dyn probe] -> [page alloc] -> [dyn probe] -> [tail alloc] +\n  //                                                               |                                                               |\n  //                                                               + <- ----------- <- ------------ <- ----------- <- ------------ +\n  //\n  // The property we want to enforce is to never have more than [page alloc] between two probes.\n\n  const unsigned XORMIOpc =\n      TFI.Uses64BitFramePtr ? X86::XOR64mi8 : X86::XOR32mi8;\n  addRegOffset(BuildMI(blockMBB, DL, TII->get(XORMIOpc)), physSPReg, false, 0)\n      .addImm(0);\n\n  BuildMI(blockMBB, DL,\n          TII->get(getSUBriOpcode(TFI.Uses64BitFramePtr, ProbeSize)), physSPReg)\n      .addReg(physSPReg)\n      .addImm(ProbeSize);\n\n\n  BuildMI(blockMBB, DL, TII->get(X86::JMP_1)).addMBB(testMBB);\n  blockMBB->addSuccessor(testMBB);\n\n  // Replace original instruction by the expected stack ptr\n  BuildMI(tailMBB, DL, TII->get(TargetOpcode::COPY), MI.getOperand(0).getReg())\n      .addReg(FinalStackPtr);\n\n  tailMBB->splice(tailMBB->end(), MBB,\n                  std::next(MachineBasicBlock::iterator(MI)), MBB->end());\n  tailMBB->transferSuccessorsAndUpdatePHIs(MBB);\n  MBB->addSuccessor(testMBB);\n\n  // Delete the original pseudo instruction.\n  MI.eraseFromParent();\n\n  // And we're done.\n  return tailMBB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredSegAlloca(MachineInstr &MI,\n                                        MachineBasicBlock *BB) const {\n  MachineFunction *MF = BB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n  const BasicBlock *LLVM_BB = BB->getBasicBlock();\n\n  assert(MF->shouldSplitStack());\n\n  const bool Is64Bit = Subtarget.is64Bit();\n  const bool IsLP64 = Subtarget.isTarget64BitLP64();\n\n  const unsigned TlsReg = Is64Bit ? X86::FS : X86::GS;\n  const unsigned TlsOffset = IsLP64 ? 0x70 : Is64Bit ? 0x40 : 0x30;\n\n  // BB:\n  //  ... [Till the alloca]\n  // If stacklet is not large enough, jump to mallocMBB\n  //\n  // bumpMBB:\n  //  Allocate by subtracting from RSP\n  //  Jump to continueMBB\n  //\n  // mallocMBB:\n  //  Allocate by call to runtime\n  //\n  // continueMBB:\n  //  ...\n  //  [rest of original BB]\n  //\n\n  MachineBasicBlock *mallocMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *bumpMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n  MachineBasicBlock *continueMBB = MF->CreateMachineBasicBlock(LLVM_BB);\n\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n  const TargetRegisterClass *AddrRegClass =\n      getRegClassFor(getPointerTy(MF->getDataLayout()));\n\n  Register mallocPtrVReg = MRI.createVirtualRegister(AddrRegClass),\n           bumpSPPtrVReg = MRI.createVirtualRegister(AddrRegClass),\n           tmpSPVReg = MRI.createVirtualRegister(AddrRegClass),\n           SPLimitVReg = MRI.createVirtualRegister(AddrRegClass),\n           sizeVReg = MI.getOperand(1).getReg(),\n           physSPReg =\n               IsLP64 || Subtarget.isTargetNaCl64() ? X86::RSP : X86::ESP;\n\n  MachineFunction::iterator MBBIter = ++BB->getIterator();\n\n  MF->insert(MBBIter, bumpMBB);\n  MF->insert(MBBIter, mallocMBB);\n  MF->insert(MBBIter, continueMBB);\n\n  continueMBB->splice(continueMBB->begin(), BB,\n                      std::next(MachineBasicBlock::iterator(MI)), BB->end());\n  continueMBB->transferSuccessorsAndUpdatePHIs(BB);\n\n  // Add code to the main basic block to check if the stack limit has been hit,\n  // and if so, jump to mallocMBB otherwise to bumpMBB.\n  BuildMI(BB, DL, TII->get(TargetOpcode::COPY), tmpSPVReg).addReg(physSPReg);\n  BuildMI(BB, DL, TII->get(IsLP64 ? X86::SUB64rr:X86::SUB32rr), SPLimitVReg)\n    .addReg(tmpSPVReg).addReg(sizeVReg);\n  BuildMI(BB, DL, TII->get(IsLP64 ? X86::CMP64mr:X86::CMP32mr))\n    .addReg(0).addImm(1).addReg(0).addImm(TlsOffset).addReg(TlsReg)\n    .addReg(SPLimitVReg);\n  BuildMI(BB, DL, TII->get(X86::JCC_1)).addMBB(mallocMBB).addImm(X86::COND_G);\n\n  // bumpMBB simply decreases the stack pointer, since we know the current\n  // stacklet has enough space.\n  BuildMI(bumpMBB, DL, TII->get(TargetOpcode::COPY), physSPReg)\n    .addReg(SPLimitVReg);\n  BuildMI(bumpMBB, DL, TII->get(TargetOpcode::COPY), bumpSPPtrVReg)\n    .addReg(SPLimitVReg);\n  BuildMI(bumpMBB, DL, TII->get(X86::JMP_1)).addMBB(continueMBB);\n\n  // Calls into a routine in libgcc to allocate more space from the heap.\n  const uint32_t *RegMask =\n      Subtarget.getRegisterInfo()->getCallPreservedMask(*MF, CallingConv::C);\n  if (IsLP64) {\n    BuildMI(mallocMBB, DL, TII->get(X86::MOV64rr), X86::RDI)\n      .addReg(sizeVReg);\n    BuildMI(mallocMBB, DL, TII->get(X86::CALL64pcrel32))\n      .addExternalSymbol(\"__morestack_allocate_stack_space\")\n      .addRegMask(RegMask)\n      .addReg(X86::RDI, RegState::Implicit)\n      .addReg(X86::RAX, RegState::ImplicitDefine);\n  } else if (Is64Bit) {\n    BuildMI(mallocMBB, DL, TII->get(X86::MOV32rr), X86::EDI)\n      .addReg(sizeVReg);\n    BuildMI(mallocMBB, DL, TII->get(X86::CALL64pcrel32))\n      .addExternalSymbol(\"__morestack_allocate_stack_space\")\n      .addRegMask(RegMask)\n      .addReg(X86::EDI, RegState::Implicit)\n      .addReg(X86::EAX, RegState::ImplicitDefine);\n  } else {\n    BuildMI(mallocMBB, DL, TII->get(X86::SUB32ri), physSPReg).addReg(physSPReg)\n      .addImm(12);\n    BuildMI(mallocMBB, DL, TII->get(X86::PUSH32r)).addReg(sizeVReg);\n    BuildMI(mallocMBB, DL, TII->get(X86::CALLpcrel32))\n      .addExternalSymbol(\"__morestack_allocate_stack_space\")\n      .addRegMask(RegMask)\n      .addReg(X86::EAX, RegState::ImplicitDefine);\n  }\n\n  if (!Is64Bit)\n    BuildMI(mallocMBB, DL, TII->get(X86::ADD32ri), physSPReg).addReg(physSPReg)\n      .addImm(16);\n\n  BuildMI(mallocMBB, DL, TII->get(TargetOpcode::COPY), mallocPtrVReg)\n    .addReg(IsLP64 ? X86::RAX : X86::EAX);\n  BuildMI(mallocMBB, DL, TII->get(X86::JMP_1)).addMBB(continueMBB);\n\n  // Set up the CFG correctly.\n  BB->addSuccessor(bumpMBB);\n  BB->addSuccessor(mallocMBB);\n  mallocMBB->addSuccessor(continueMBB);\n  bumpMBB->addSuccessor(continueMBB);\n\n  // Take care of the PHI nodes.\n  BuildMI(*continueMBB, continueMBB->begin(), DL, TII->get(X86::PHI),\n          MI.getOperand(0).getReg())\n      .addReg(mallocPtrVReg)\n      .addMBB(mallocMBB)\n      .addReg(bumpSPPtrVReg)\n      .addMBB(bumpMBB);\n\n  // Delete the original pseudo instruction.\n  MI.eraseFromParent();\n\n  // And we're done.\n  return continueMBB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredCatchRet(MachineInstr &MI,\n                                       MachineBasicBlock *BB) const {\n  MachineFunction *MF = BB->getParent();\n  const TargetInstrInfo &TII = *Subtarget.getInstrInfo();\n  MachineBasicBlock *TargetMBB = MI.getOperand(0).getMBB();\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  assert(!isAsynchronousEHPersonality(\n             classifyEHPersonality(MF->getFunction().getPersonalityFn())) &&\n         \"SEH does not use catchret!\");\n\n  // Only 32-bit EH needs to worry about manually restoring stack pointers.\n  if (!Subtarget.is32Bit())\n    return BB;\n\n  // C++ EH creates a new target block to hold the restore code, and wires up\n  // the new block to the return destination with a normal JMP_4.\n  MachineBasicBlock *RestoreMBB =\n      MF->CreateMachineBasicBlock(BB->getBasicBlock());\n  assert(BB->succ_size() == 1);\n  MF->insert(std::next(BB->getIterator()), RestoreMBB);\n  RestoreMBB->transferSuccessorsAndUpdatePHIs(BB);\n  BB->addSuccessor(RestoreMBB);\n  MI.getOperand(0).setMBB(RestoreMBB);\n\n  // Marking this as an EH pad but not a funclet entry block causes PEI to\n  // restore stack pointers in the block.\n  RestoreMBB->setIsEHPad(true);\n\n  auto RestoreMBBI = RestoreMBB->begin();\n  BuildMI(*RestoreMBB, RestoreMBBI, DL, TII.get(X86::JMP_4)).addMBB(TargetMBB);\n  return BB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredTLSAddr(MachineInstr &MI,\n                                      MachineBasicBlock *BB) const {\n  // So, here we replace TLSADDR with the sequence:\n  // adjust_stackdown -> TLSADDR -> adjust_stackup.\n  // We need this because TLSADDR is lowered into calls\n  // inside MC, therefore without the two markers shrink-wrapping\n  // may push the prologue/epilogue pass them.\n  const TargetInstrInfo &TII = *Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction &MF = *BB->getParent();\n\n  // Emit CALLSEQ_START right before the instruction.\n  unsigned AdjStackDown = TII.getCallFrameSetupOpcode();\n  MachineInstrBuilder CallseqStart =\n    BuildMI(MF, DL, TII.get(AdjStackDown)).addImm(0).addImm(0).addImm(0);\n  BB->insert(MachineBasicBlock::iterator(MI), CallseqStart);\n\n  // Emit CALLSEQ_END right after the instruction.\n  // We don't call erase from parent because we want to keep the\n  // original instruction around.\n  unsigned AdjStackUp = TII.getCallFrameDestroyOpcode();\n  MachineInstrBuilder CallseqEnd =\n    BuildMI(MF, DL, TII.get(AdjStackUp)).addImm(0).addImm(0);\n  BB->insertAfter(MachineBasicBlock::iterator(MI), CallseqEnd);\n\n  return BB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredTLSCall(MachineInstr &MI,\n                                      MachineBasicBlock *BB) const {\n  // This is pretty easy.  We're taking the value that we received from\n  // our load from the relocation, sticking it in either RDI (x86-64)\n  // or EAX and doing an indirect call.  The return value will then\n  // be in the normal return register.\n  MachineFunction *F = BB->getParent();\n  const X86InstrInfo *TII = Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  assert(Subtarget.isTargetDarwin() && \"Darwin only instr emitted?\");\n  assert(MI.getOperand(3).isGlobal() && \"This should be a global\");\n\n  // Get a register mask for the lowered call.\n  // FIXME: The 32-bit calls have non-standard calling conventions. Use a\n  // proper register mask.\n  const uint32_t *RegMask =\n      Subtarget.is64Bit() ?\n      Subtarget.getRegisterInfo()->getDarwinTLSCallPreservedMask() :\n      Subtarget.getRegisterInfo()->getCallPreservedMask(*F, CallingConv::C);\n  if (Subtarget.is64Bit()) {\n    MachineInstrBuilder MIB =\n        BuildMI(*BB, MI, DL, TII->get(X86::MOV64rm), X86::RDI)\n            .addReg(X86::RIP)\n            .addImm(0)\n            .addReg(0)\n            .addGlobalAddress(MI.getOperand(3).getGlobal(), 0,\n                              MI.getOperand(3).getTargetFlags())\n            .addReg(0);\n    MIB = BuildMI(*BB, MI, DL, TII->get(X86::CALL64m));\n    addDirectMem(MIB, X86::RDI);\n    MIB.addReg(X86::RAX, RegState::ImplicitDefine).addRegMask(RegMask);\n  } else if (!isPositionIndependent()) {\n    MachineInstrBuilder MIB =\n        BuildMI(*BB, MI, DL, TII->get(X86::MOV32rm), X86::EAX)\n            .addReg(0)\n            .addImm(0)\n            .addReg(0)\n            .addGlobalAddress(MI.getOperand(3).getGlobal(), 0,\n                              MI.getOperand(3).getTargetFlags())\n            .addReg(0);\n    MIB = BuildMI(*BB, MI, DL, TII->get(X86::CALL32m));\n    addDirectMem(MIB, X86::EAX);\n    MIB.addReg(X86::EAX, RegState::ImplicitDefine).addRegMask(RegMask);\n  } else {\n    MachineInstrBuilder MIB =\n        BuildMI(*BB, MI, DL, TII->get(X86::MOV32rm), X86::EAX)\n            .addReg(TII->getGlobalBaseReg(F))\n            .addImm(0)\n            .addReg(0)\n            .addGlobalAddress(MI.getOperand(3).getGlobal(), 0,\n                              MI.getOperand(3).getTargetFlags())\n            .addReg(0);\n    MIB = BuildMI(*BB, MI, DL, TII->get(X86::CALL32m));\n    addDirectMem(MIB, X86::EAX);\n    MIB.addReg(X86::EAX, RegState::ImplicitDefine).addRegMask(RegMask);\n  }\n\n  MI.eraseFromParent(); // The pseudo instruction is gone now.\n  return BB;\n}\n\nstatic unsigned getOpcodeForIndirectThunk(unsigned RPOpc) {\n  switch (RPOpc) {\n  case X86::INDIRECT_THUNK_CALL32:\n    return X86::CALLpcrel32;\n  case X86::INDIRECT_THUNK_CALL64:\n    return X86::CALL64pcrel32;\n  case X86::INDIRECT_THUNK_TCRETURN32:\n    return X86::TCRETURNdi;\n  case X86::INDIRECT_THUNK_TCRETURN64:\n    return X86::TCRETURNdi64;\n  }\n  llvm_unreachable(\"not indirect thunk opcode\");\n}\n\nstatic const char *getIndirectThunkSymbol(const X86Subtarget &Subtarget,\n                                          unsigned Reg) {\n  if (Subtarget.useRetpolineExternalThunk()) {\n    // When using an external thunk for retpolines, we pick names that match the\n    // names GCC happens to use as well. This helps simplify the implementation\n    // of the thunks for kernels where they have no easy ability to create\n    // aliases and are doing non-trivial configuration of the thunk's body. For\n    // example, the Linux kernel will do boot-time hot patching of the thunk\n    // bodies and cannot easily export aliases of these to loaded modules.\n    //\n    // Note that at any point in the future, we may need to change the semantics\n    // of how we implement retpolines and at that time will likely change the\n    // name of the called thunk. Essentially, there is no hard guarantee that\n    // LLVM will generate calls to specific thunks, we merely make a best-effort\n    // attempt to help out kernels and other systems where duplicating the\n    // thunks is costly.\n    switch (Reg) {\n    case X86::EAX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__x86_indirect_thunk_eax\";\n    case X86::ECX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__x86_indirect_thunk_ecx\";\n    case X86::EDX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__x86_indirect_thunk_edx\";\n    case X86::EDI:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__x86_indirect_thunk_edi\";\n    case X86::R11:\n      assert(Subtarget.is64Bit() && \"Should not be using a 64-bit thunk!\");\n      return \"__x86_indirect_thunk_r11\";\n    }\n    llvm_unreachable(\"unexpected reg for external indirect thunk\");\n  }\n\n  if (Subtarget.useRetpolineIndirectCalls() ||\n      Subtarget.useRetpolineIndirectBranches()) {\n    // When targeting an internal COMDAT thunk use an LLVM-specific name.\n    switch (Reg) {\n    case X86::EAX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__llvm_retpoline_eax\";\n    case X86::ECX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__llvm_retpoline_ecx\";\n    case X86::EDX:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__llvm_retpoline_edx\";\n    case X86::EDI:\n      assert(!Subtarget.is64Bit() && \"Should not be using a 32-bit thunk!\");\n      return \"__llvm_retpoline_edi\";\n    case X86::R11:\n      assert(Subtarget.is64Bit() && \"Should not be using a 64-bit thunk!\");\n      return \"__llvm_retpoline_r11\";\n    }\n    llvm_unreachable(\"unexpected reg for retpoline\");\n  }\n\n  if (Subtarget.useLVIControlFlowIntegrity()) {\n    assert(Subtarget.is64Bit() && \"Should not be using a 64-bit thunk!\");\n    return \"__llvm_lvi_thunk_r11\";\n  }\n  llvm_unreachable(\"getIndirectThunkSymbol() invoked without thunk feature\");\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitLoweredIndirectThunk(MachineInstr &MI,\n                                            MachineBasicBlock *BB) const {\n  // Copy the virtual register into the R11 physical register and\n  // call the retpoline thunk.\n  const DebugLoc &DL = MI.getDebugLoc();\n  const X86InstrInfo *TII = Subtarget.getInstrInfo();\n  Register CalleeVReg = MI.getOperand(0).getReg();\n  unsigned Opc = getOpcodeForIndirectThunk(MI.getOpcode());\n\n  // Find an available scratch register to hold the callee. On 64-bit, we can\n  // just use R11, but we scan for uses anyway to ensure we don't generate\n  // incorrect code. On 32-bit, we use one of EAX, ECX, or EDX that isn't\n  // already a register use operand to the call to hold the callee. If none\n  // are available, use EDI instead. EDI is chosen because EBX is the PIC base\n  // register and ESI is the base pointer to realigned stack frames with VLAs.\n  SmallVector<unsigned, 3> AvailableRegs;\n  if (Subtarget.is64Bit())\n    AvailableRegs.push_back(X86::R11);\n  else\n    AvailableRegs.append({X86::EAX, X86::ECX, X86::EDX, X86::EDI});\n\n  // Zero out any registers that are already used.\n  for (const auto &MO : MI.operands()) {\n    if (MO.isReg() && MO.isUse())\n      for (unsigned &Reg : AvailableRegs)\n        if (Reg == MO.getReg())\n          Reg = 0;\n  }\n\n  // Choose the first remaining non-zero available register.\n  unsigned AvailableReg = 0;\n  for (unsigned MaybeReg : AvailableRegs) {\n    if (MaybeReg) {\n      AvailableReg = MaybeReg;\n      break;\n    }\n  }\n  if (!AvailableReg)\n    report_fatal_error(\"calling convention incompatible with retpoline, no \"\n                       \"available registers\");\n\n  const char *Symbol = getIndirectThunkSymbol(Subtarget, AvailableReg);\n\n  BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), AvailableReg)\n      .addReg(CalleeVReg);\n  MI.getOperand(0).ChangeToES(Symbol);\n  MI.setDesc(TII->get(Opc));\n  MachineInstrBuilder(*BB->getParent(), &MI)\n      .addReg(AvailableReg, RegState::Implicit | RegState::Kill);\n  return BB;\n}\n\n/// SetJmp implies future control flow change upon calling the corresponding\n/// LongJmp.\n/// Instead of using the 'return' instruction, the long jump fixes the stack and\n/// performs an indirect branch. To do so it uses the registers that were stored\n/// in the jump buffer (when calling SetJmp).\n/// In case the shadow stack is enabled we need to fix it as well, because some\n/// return addresses will be skipped.\n/// The function will save the SSP for future fixing in the function\n/// emitLongJmpShadowStackFix.\n/// \\sa emitLongJmpShadowStackFix\n/// \\param [in] MI The temporary Machine Instruction for the builtin.\n/// \\param [in] MBB The Machine Basic Block that will be modified.\nvoid X86TargetLowering::emitSetJmpShadowStackFix(MachineInstr &MI,\n                                                 MachineBasicBlock *MBB) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n  MachineInstrBuilder MIB;\n\n  // Memory Reference.\n  SmallVector<MachineMemOperand *, 2> MMOs(MI.memoperands_begin(),\n                                           MI.memoperands_end());\n\n  // Initialize a register with zero.\n  MVT PVT = getPointerTy(MF->getDataLayout());\n  const TargetRegisterClass *PtrRC = getRegClassFor(PVT);\n  Register ZReg = MRI.createVirtualRegister(PtrRC);\n  unsigned XorRROpc = (PVT == MVT::i64) ? X86::XOR64rr : X86::XOR32rr;\n  BuildMI(*MBB, MI, DL, TII->get(XorRROpc))\n      .addDef(ZReg)\n      .addReg(ZReg, RegState::Undef)\n      .addReg(ZReg, RegState::Undef);\n\n  // Read the current SSP Register value to the zeroed register.\n  Register SSPCopyReg = MRI.createVirtualRegister(PtrRC);\n  unsigned RdsspOpc = (PVT == MVT::i64) ? X86::RDSSPQ : X86::RDSSPD;\n  BuildMI(*MBB, MI, DL, TII->get(RdsspOpc), SSPCopyReg).addReg(ZReg);\n\n  // Write the SSP register value to offset 3 in input memory buffer.\n  unsigned PtrStoreOpc = (PVT == MVT::i64) ? X86::MOV64mr : X86::MOV32mr;\n  MIB = BuildMI(*MBB, MI, DL, TII->get(PtrStoreOpc));\n  const int64_t SSPOffset = 3 * PVT.getStoreSize();\n  const unsigned MemOpndSlot = 1;\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    if (i == X86::AddrDisp)\n      MIB.addDisp(MI.getOperand(MemOpndSlot + i), SSPOffset);\n    else\n      MIB.add(MI.getOperand(MemOpndSlot + i));\n  }\n  MIB.addReg(SSPCopyReg);\n  MIB.setMemRefs(MMOs);\n}\n\nMachineBasicBlock *\nX86TargetLowering::emitEHSjLjSetJmp(MachineInstr &MI,\n                                    MachineBasicBlock *MBB) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n\n  const BasicBlock *BB = MBB->getBasicBlock();\n  MachineFunction::iterator I = ++MBB->getIterator();\n\n  // Memory Reference\n  SmallVector<MachineMemOperand *, 2> MMOs(MI.memoperands_begin(),\n                                           MI.memoperands_end());\n\n  unsigned DstReg;\n  unsigned MemOpndSlot = 0;\n\n  unsigned CurOp = 0;\n\n  DstReg = MI.getOperand(CurOp++).getReg();\n  const TargetRegisterClass *RC = MRI.getRegClass(DstReg);\n  assert(TRI->isTypeLegalForClass(*RC, MVT::i32) && \"Invalid destination!\");\n  (void)TRI;\n  Register mainDstReg = MRI.createVirtualRegister(RC);\n  Register restoreDstReg = MRI.createVirtualRegister(RC);\n\n  MemOpndSlot = CurOp;\n\n  MVT PVT = getPointerTy(MF->getDataLayout());\n  assert((PVT == MVT::i64 || PVT == MVT::i32) &&\n         \"Invalid Pointer Size!\");\n\n  // For v = setjmp(buf), we generate\n  //\n  // thisMBB:\n  //  buf[LabelOffset] = restoreMBB <-- takes address of restoreMBB\n  //  SjLjSetup restoreMBB\n  //\n  // mainMBB:\n  //  v_main = 0\n  //\n  // sinkMBB:\n  //  v = phi(main, restore)\n  //\n  // restoreMBB:\n  //  if base pointer being used, load it from frame\n  //  v_restore = 1\n\n  MachineBasicBlock *thisMBB = MBB;\n  MachineBasicBlock *mainMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *restoreMBB = MF->CreateMachineBasicBlock(BB);\n  MF->insert(I, mainMBB);\n  MF->insert(I, sinkMBB);\n  MF->push_back(restoreMBB);\n  restoreMBB->setHasAddressTaken();\n\n  MachineInstrBuilder MIB;\n\n  // Transfer the remainder of BB and its successor edges to sinkMBB.\n  sinkMBB->splice(sinkMBB->begin(), MBB,\n                  std::next(MachineBasicBlock::iterator(MI)), MBB->end());\n  sinkMBB->transferSuccessorsAndUpdatePHIs(MBB);\n\n  // thisMBB:\n  unsigned PtrStoreOpc = 0;\n  unsigned LabelReg = 0;\n  const int64_t LabelOffset = 1 * PVT.getStoreSize();\n  bool UseImmLabel = (MF->getTarget().getCodeModel() == CodeModel::Small) &&\n                     !isPositionIndependent();\n\n  // Prepare IP either in reg or imm.\n  if (!UseImmLabel) {\n    PtrStoreOpc = (PVT == MVT::i64) ? X86::MOV64mr : X86::MOV32mr;\n    const TargetRegisterClass *PtrRC = getRegClassFor(PVT);\n    LabelReg = MRI.createVirtualRegister(PtrRC);\n    if (Subtarget.is64Bit()) {\n      MIB = BuildMI(*thisMBB, MI, DL, TII->get(X86::LEA64r), LabelReg)\n              .addReg(X86::RIP)\n              .addImm(0)\n              .addReg(0)\n              .addMBB(restoreMBB)\n              .addReg(0);\n    } else {\n      const X86InstrInfo *XII = static_cast<const X86InstrInfo*>(TII);\n      MIB = BuildMI(*thisMBB, MI, DL, TII->get(X86::LEA32r), LabelReg)\n              .addReg(XII->getGlobalBaseReg(MF))\n              .addImm(0)\n              .addReg(0)\n              .addMBB(restoreMBB, Subtarget.classifyBlockAddressReference())\n              .addReg(0);\n    }\n  } else\n    PtrStoreOpc = (PVT == MVT::i64) ? X86::MOV64mi32 : X86::MOV32mi;\n  // Store IP\n  MIB = BuildMI(*thisMBB, MI, DL, TII->get(PtrStoreOpc));\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    if (i == X86::AddrDisp)\n      MIB.addDisp(MI.getOperand(MemOpndSlot + i), LabelOffset);\n    else\n      MIB.add(MI.getOperand(MemOpndSlot + i));\n  }\n  if (!UseImmLabel)\n    MIB.addReg(LabelReg);\n  else\n    MIB.addMBB(restoreMBB);\n  MIB.setMemRefs(MMOs);\n\n  if (MF->getMMI().getModule()->getModuleFlag(\"cf-protection-return\")) {\n    emitSetJmpShadowStackFix(MI, thisMBB);\n  }\n\n  // Setup\n  MIB = BuildMI(*thisMBB, MI, DL, TII->get(X86::EH_SjLj_Setup))\n          .addMBB(restoreMBB);\n\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  MIB.addRegMask(RegInfo->getNoPreservedMask());\n  thisMBB->addSuccessor(mainMBB);\n  thisMBB->addSuccessor(restoreMBB);\n\n  // mainMBB:\n  //  EAX = 0\n  BuildMI(mainMBB, DL, TII->get(X86::MOV32r0), mainDstReg);\n  mainMBB->addSuccessor(sinkMBB);\n\n  // sinkMBB:\n  BuildMI(*sinkMBB, sinkMBB->begin(), DL,\n          TII->get(X86::PHI), DstReg)\n    .addReg(mainDstReg).addMBB(mainMBB)\n    .addReg(restoreDstReg).addMBB(restoreMBB);\n\n  // restoreMBB:\n  if (RegInfo->hasBasePointer(*MF)) {\n    const bool Uses64BitFramePtr =\n        Subtarget.isTarget64BitLP64() || Subtarget.isTargetNaCl64();\n    X86MachineFunctionInfo *X86FI = MF->getInfo<X86MachineFunctionInfo>();\n    X86FI->setRestoreBasePointer(MF);\n    Register FramePtr = RegInfo->getFrameRegister(*MF);\n    Register BasePtr = RegInfo->getBaseRegister();\n    unsigned Opm = Uses64BitFramePtr ? X86::MOV64rm : X86::MOV32rm;\n    addRegOffset(BuildMI(restoreMBB, DL, TII->get(Opm), BasePtr),\n                 FramePtr, true, X86FI->getRestoreBasePointerOffset())\n      .setMIFlag(MachineInstr::FrameSetup);\n  }\n  BuildMI(restoreMBB, DL, TII->get(X86::MOV32ri), restoreDstReg).addImm(1);\n  BuildMI(restoreMBB, DL, TII->get(X86::JMP_1)).addMBB(sinkMBB);\n  restoreMBB->addSuccessor(sinkMBB);\n\n  MI.eraseFromParent();\n  return sinkMBB;\n}\n\n/// Fix the shadow stack using the previously saved SSP pointer.\n/// \\sa emitSetJmpShadowStackFix\n/// \\param [in] MI The temporary Machine Instruction for the builtin.\n/// \\param [in] MBB The Machine Basic Block that will be modified.\n/// \\return The sink MBB that will perform the future indirect branch.\nMachineBasicBlock *\nX86TargetLowering::emitLongJmpShadowStackFix(MachineInstr &MI,\n                                             MachineBasicBlock *MBB) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n\n  // Memory Reference\n  SmallVector<MachineMemOperand *, 2> MMOs(MI.memoperands_begin(),\n                                           MI.memoperands_end());\n\n  MVT PVT = getPointerTy(MF->getDataLayout());\n  const TargetRegisterClass *PtrRC = getRegClassFor(PVT);\n\n  // checkSspMBB:\n  //         xor vreg1, vreg1\n  //         rdssp vreg1\n  //         test vreg1, vreg1\n  //         je sinkMBB   # Jump if Shadow Stack is not supported\n  // fallMBB:\n  //         mov buf+24/12(%rip), vreg2\n  //         sub vreg1, vreg2\n  //         jbe sinkMBB  # No need to fix the Shadow Stack\n  // fixShadowMBB:\n  //         shr 3/2, vreg2\n  //         incssp vreg2  # fix the SSP according to the lower 8 bits\n  //         shr 8, vreg2\n  //         je sinkMBB\n  // fixShadowLoopPrepareMBB:\n  //         shl vreg2\n  //         mov 128, vreg3\n  // fixShadowLoopMBB:\n  //         incssp vreg3\n  //         dec vreg2\n  //         jne fixShadowLoopMBB # Iterate until you finish fixing\n  //                              # the Shadow Stack\n  // sinkMBB:\n\n  MachineFunction::iterator I = ++MBB->getIterator();\n  const BasicBlock *BB = MBB->getBasicBlock();\n\n  MachineBasicBlock *checkSspMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *fallMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *fixShadowMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *fixShadowLoopPrepareMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *fixShadowLoopMBB = MF->CreateMachineBasicBlock(BB);\n  MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(BB);\n  MF->insert(I, checkSspMBB);\n  MF->insert(I, fallMBB);\n  MF->insert(I, fixShadowMBB);\n  MF->insert(I, fixShadowLoopPrepareMBB);\n  MF->insert(I, fixShadowLoopMBB);\n  MF->insert(I, sinkMBB);\n\n  // Transfer the remainder of BB and its successor edges to sinkMBB.\n  sinkMBB->splice(sinkMBB->begin(), MBB, MachineBasicBlock::iterator(MI),\n                  MBB->end());\n  sinkMBB->transferSuccessorsAndUpdatePHIs(MBB);\n\n  MBB->addSuccessor(checkSspMBB);\n\n  // Initialize a register with zero.\n  Register ZReg = MRI.createVirtualRegister(&X86::GR32RegClass);\n  BuildMI(checkSspMBB, DL, TII->get(X86::MOV32r0), ZReg);\n\n  if (PVT == MVT::i64) {\n    Register TmpZReg = MRI.createVirtualRegister(PtrRC);\n    BuildMI(checkSspMBB, DL, TII->get(X86::SUBREG_TO_REG), TmpZReg)\n      .addImm(0)\n      .addReg(ZReg)\n      .addImm(X86::sub_32bit);\n    ZReg = TmpZReg;\n  }\n\n  // Read the current SSP Register value to the zeroed register.\n  Register SSPCopyReg = MRI.createVirtualRegister(PtrRC);\n  unsigned RdsspOpc = (PVT == MVT::i64) ? X86::RDSSPQ : X86::RDSSPD;\n  BuildMI(checkSspMBB, DL, TII->get(RdsspOpc), SSPCopyReg).addReg(ZReg);\n\n  // Check whether the result of the SSP register is zero and jump directly\n  // to the sink.\n  unsigned TestRROpc = (PVT == MVT::i64) ? X86::TEST64rr : X86::TEST32rr;\n  BuildMI(checkSspMBB, DL, TII->get(TestRROpc))\n      .addReg(SSPCopyReg)\n      .addReg(SSPCopyReg);\n  BuildMI(checkSspMBB, DL, TII->get(X86::JCC_1)).addMBB(sinkMBB).addImm(X86::COND_E);\n  checkSspMBB->addSuccessor(sinkMBB);\n  checkSspMBB->addSuccessor(fallMBB);\n\n  // Reload the previously saved SSP register value.\n  Register PrevSSPReg = MRI.createVirtualRegister(PtrRC);\n  unsigned PtrLoadOpc = (PVT == MVT::i64) ? X86::MOV64rm : X86::MOV32rm;\n  const int64_t SPPOffset = 3 * PVT.getStoreSize();\n  MachineInstrBuilder MIB =\n      BuildMI(fallMBB, DL, TII->get(PtrLoadOpc), PrevSSPReg);\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    const MachineOperand &MO = MI.getOperand(i);\n    if (i == X86::AddrDisp)\n      MIB.addDisp(MO, SPPOffset);\n    else if (MO.isReg()) // Don't add the whole operand, we don't want to\n                         // preserve kill flags.\n      MIB.addReg(MO.getReg());\n    else\n      MIB.add(MO);\n  }\n  MIB.setMemRefs(MMOs);\n\n  // Subtract the current SSP from the previous SSP.\n  Register SspSubReg = MRI.createVirtualRegister(PtrRC);\n  unsigned SubRROpc = (PVT == MVT::i64) ? X86::SUB64rr : X86::SUB32rr;\n  BuildMI(fallMBB, DL, TII->get(SubRROpc), SspSubReg)\n      .addReg(PrevSSPReg)\n      .addReg(SSPCopyReg);\n\n  // Jump to sink in case PrevSSPReg <= SSPCopyReg.\n  BuildMI(fallMBB, DL, TII->get(X86::JCC_1)).addMBB(sinkMBB).addImm(X86::COND_BE);\n  fallMBB->addSuccessor(sinkMBB);\n  fallMBB->addSuccessor(fixShadowMBB);\n\n  // Shift right by 2/3 for 32/64 because incssp multiplies the argument by 4/8.\n  unsigned ShrRIOpc = (PVT == MVT::i64) ? X86::SHR64ri : X86::SHR32ri;\n  unsigned Offset = (PVT == MVT::i64) ? 3 : 2;\n  Register SspFirstShrReg = MRI.createVirtualRegister(PtrRC);\n  BuildMI(fixShadowMBB, DL, TII->get(ShrRIOpc), SspFirstShrReg)\n      .addReg(SspSubReg)\n      .addImm(Offset);\n\n  // Increase SSP when looking only on the lower 8 bits of the delta.\n  unsigned IncsspOpc = (PVT == MVT::i64) ? X86::INCSSPQ : X86::INCSSPD;\n  BuildMI(fixShadowMBB, DL, TII->get(IncsspOpc)).addReg(SspFirstShrReg);\n\n  // Reset the lower 8 bits.\n  Register SspSecondShrReg = MRI.createVirtualRegister(PtrRC);\n  BuildMI(fixShadowMBB, DL, TII->get(ShrRIOpc), SspSecondShrReg)\n      .addReg(SspFirstShrReg)\n      .addImm(8);\n\n  // Jump if the result of the shift is zero.\n  BuildMI(fixShadowMBB, DL, TII->get(X86::JCC_1)).addMBB(sinkMBB).addImm(X86::COND_E);\n  fixShadowMBB->addSuccessor(sinkMBB);\n  fixShadowMBB->addSuccessor(fixShadowLoopPrepareMBB);\n\n  // Do a single shift left.\n  unsigned ShlR1Opc = (PVT == MVT::i64) ? X86::SHL64r1 : X86::SHL32r1;\n  Register SspAfterShlReg = MRI.createVirtualRegister(PtrRC);\n  BuildMI(fixShadowLoopPrepareMBB, DL, TII->get(ShlR1Opc), SspAfterShlReg)\n      .addReg(SspSecondShrReg);\n\n  // Save the value 128 to a register (will be used next with incssp).\n  Register Value128InReg = MRI.createVirtualRegister(PtrRC);\n  unsigned MovRIOpc = (PVT == MVT::i64) ? X86::MOV64ri32 : X86::MOV32ri;\n  BuildMI(fixShadowLoopPrepareMBB, DL, TII->get(MovRIOpc), Value128InReg)\n      .addImm(128);\n  fixShadowLoopPrepareMBB->addSuccessor(fixShadowLoopMBB);\n\n  // Since incssp only looks at the lower 8 bits, we might need to do several\n  // iterations of incssp until we finish fixing the shadow stack.\n  Register DecReg = MRI.createVirtualRegister(PtrRC);\n  Register CounterReg = MRI.createVirtualRegister(PtrRC);\n  BuildMI(fixShadowLoopMBB, DL, TII->get(X86::PHI), CounterReg)\n      .addReg(SspAfterShlReg)\n      .addMBB(fixShadowLoopPrepareMBB)\n      .addReg(DecReg)\n      .addMBB(fixShadowLoopMBB);\n\n  // Every iteration we increase the SSP by 128.\n  BuildMI(fixShadowLoopMBB, DL, TII->get(IncsspOpc)).addReg(Value128InReg);\n\n  // Every iteration we decrement the counter by 1.\n  unsigned DecROpc = (PVT == MVT::i64) ? X86::DEC64r : X86::DEC32r;\n  BuildMI(fixShadowLoopMBB, DL, TII->get(DecROpc), DecReg).addReg(CounterReg);\n\n  // Jump if the counter is not zero yet.\n  BuildMI(fixShadowLoopMBB, DL, TII->get(X86::JCC_1)).addMBB(fixShadowLoopMBB).addImm(X86::COND_NE);\n  fixShadowLoopMBB->addSuccessor(sinkMBB);\n  fixShadowLoopMBB->addSuccessor(fixShadowLoopMBB);\n\n  return sinkMBB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::emitEHSjLjLongJmp(MachineInstr &MI,\n                                     MachineBasicBlock *MBB) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  MachineRegisterInfo &MRI = MF->getRegInfo();\n\n  // Memory Reference\n  SmallVector<MachineMemOperand *, 2> MMOs(MI.memoperands_begin(),\n                                           MI.memoperands_end());\n\n  MVT PVT = getPointerTy(MF->getDataLayout());\n  assert((PVT == MVT::i64 || PVT == MVT::i32) &&\n         \"Invalid Pointer Size!\");\n\n  const TargetRegisterClass *RC =\n    (PVT == MVT::i64) ? &X86::GR64RegClass : &X86::GR32RegClass;\n  Register Tmp = MRI.createVirtualRegister(RC);\n  // Since FP is only updated here but NOT referenced, it's treated as GPR.\n  const X86RegisterInfo *RegInfo = Subtarget.getRegisterInfo();\n  Register FP = (PVT == MVT::i64) ? X86::RBP : X86::EBP;\n  Register SP = RegInfo->getStackRegister();\n\n  MachineInstrBuilder MIB;\n\n  const int64_t LabelOffset = 1 * PVT.getStoreSize();\n  const int64_t SPOffset = 2 * PVT.getStoreSize();\n\n  unsigned PtrLoadOpc = (PVT == MVT::i64) ? X86::MOV64rm : X86::MOV32rm;\n  unsigned IJmpOpc = (PVT == MVT::i64) ? X86::JMP64r : X86::JMP32r;\n\n  MachineBasicBlock *thisMBB = MBB;\n\n  // When CET and shadow stack is enabled, we need to fix the Shadow Stack.\n  if (MF->getMMI().getModule()->getModuleFlag(\"cf-protection-return\")) {\n    thisMBB = emitLongJmpShadowStackFix(MI, thisMBB);\n  }\n\n  // Reload FP\n  MIB = BuildMI(*thisMBB, MI, DL, TII->get(PtrLoadOpc), FP);\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    const MachineOperand &MO = MI.getOperand(i);\n    if (MO.isReg()) // Don't add the whole operand, we don't want to\n                    // preserve kill flags.\n      MIB.addReg(MO.getReg());\n    else\n      MIB.add(MO);\n  }\n  MIB.setMemRefs(MMOs);\n\n  // Reload IP\n  MIB = BuildMI(*thisMBB, MI, DL, TII->get(PtrLoadOpc), Tmp);\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    const MachineOperand &MO = MI.getOperand(i);\n    if (i == X86::AddrDisp)\n      MIB.addDisp(MO, LabelOffset);\n    else if (MO.isReg()) // Don't add the whole operand, we don't want to\n                         // preserve kill flags.\n      MIB.addReg(MO.getReg());\n    else\n      MIB.add(MO);\n  }\n  MIB.setMemRefs(MMOs);\n\n  // Reload SP\n  MIB = BuildMI(*thisMBB, MI, DL, TII->get(PtrLoadOpc), SP);\n  for (unsigned i = 0; i < X86::AddrNumOperands; ++i) {\n    if (i == X86::AddrDisp)\n      MIB.addDisp(MI.getOperand(i), SPOffset);\n    else\n      MIB.add(MI.getOperand(i)); // We can preserve the kill flags here, it's\n                                 // the last instruction of the expansion.\n  }\n  MIB.setMemRefs(MMOs);\n\n  // Jump\n  BuildMI(*thisMBB, MI, DL, TII->get(IJmpOpc)).addReg(Tmp);\n\n  MI.eraseFromParent();\n  return thisMBB;\n}\n\nvoid X86TargetLowering::SetupEntryBlockForSjLj(MachineInstr &MI,\n                                               MachineBasicBlock *MBB,\n                                               MachineBasicBlock *DispatchBB,\n                                               int FI) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = MBB->getParent();\n  MachineRegisterInfo *MRI = &MF->getRegInfo();\n  const X86InstrInfo *TII = Subtarget.getInstrInfo();\n\n  MVT PVT = getPointerTy(MF->getDataLayout());\n  assert((PVT == MVT::i64 || PVT == MVT::i32) && \"Invalid Pointer Size!\");\n\n  unsigned Op = 0;\n  unsigned VR = 0;\n\n  bool UseImmLabel = (MF->getTarget().getCodeModel() == CodeModel::Small) &&\n                     !isPositionIndependent();\n\n  if (UseImmLabel) {\n    Op = (PVT == MVT::i64) ? X86::MOV64mi32 : X86::MOV32mi;\n  } else {\n    const TargetRegisterClass *TRC =\n        (PVT == MVT::i64) ? &X86::GR64RegClass : &X86::GR32RegClass;\n    VR = MRI->createVirtualRegister(TRC);\n    Op = (PVT == MVT::i64) ? X86::MOV64mr : X86::MOV32mr;\n\n    if (Subtarget.is64Bit())\n      BuildMI(*MBB, MI, DL, TII->get(X86::LEA64r), VR)\n          .addReg(X86::RIP)\n          .addImm(1)\n          .addReg(0)\n          .addMBB(DispatchBB)\n          .addReg(0);\n    else\n      BuildMI(*MBB, MI, DL, TII->get(X86::LEA32r), VR)\n          .addReg(0) /* TII->getGlobalBaseReg(MF) */\n          .addImm(1)\n          .addReg(0)\n          .addMBB(DispatchBB, Subtarget.classifyBlockAddressReference())\n          .addReg(0);\n  }\n\n  MachineInstrBuilder MIB = BuildMI(*MBB, MI, DL, TII->get(Op));\n  addFrameReference(MIB, FI, Subtarget.is64Bit() ? 56 : 36);\n  if (UseImmLabel)\n    MIB.addMBB(DispatchBB);\n  else\n    MIB.addReg(VR);\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitSjLjDispatchBlock(MachineInstr &MI,\n                                         MachineBasicBlock *BB) const {\n  const DebugLoc &DL = MI.getDebugLoc();\n  MachineFunction *MF = BB->getParent();\n  MachineRegisterInfo *MRI = &MF->getRegInfo();\n  const X86InstrInfo *TII = Subtarget.getInstrInfo();\n  int FI = MF->getFrameInfo().getFunctionContextIndex();\n\n  // Get a mapping of the call site numbers to all of the landing pads they're\n  // associated with.\n  DenseMap<unsigned, SmallVector<MachineBasicBlock *, 2>> CallSiteNumToLPad;\n  unsigned MaxCSNum = 0;\n  for (auto &MBB : *MF) {\n    if (!MBB.isEHPad())\n      continue;\n\n    MCSymbol *Sym = nullptr;\n    for (const auto &MI : MBB) {\n      if (MI.isDebugInstr())\n        continue;\n\n      assert(MI.isEHLabel() && \"expected EH_LABEL\");\n      Sym = MI.getOperand(0).getMCSymbol();\n      break;\n    }\n\n    if (!MF->hasCallSiteLandingPad(Sym))\n      continue;\n\n    for (unsigned CSI : MF->getCallSiteLandingPad(Sym)) {\n      CallSiteNumToLPad[CSI].push_back(&MBB);\n      MaxCSNum = std::max(MaxCSNum, CSI);\n    }\n  }\n\n  // Get an ordered list of the machine basic blocks for the jump table.\n  std::vector<MachineBasicBlock *> LPadList;\n  SmallPtrSet<MachineBasicBlock *, 32> InvokeBBs;\n  LPadList.reserve(CallSiteNumToLPad.size());\n\n  for (unsigned CSI = 1; CSI <= MaxCSNum; ++CSI) {\n    for (auto &LP : CallSiteNumToLPad[CSI]) {\n      LPadList.push_back(LP);\n      InvokeBBs.insert(LP->pred_begin(), LP->pred_end());\n    }\n  }\n\n  assert(!LPadList.empty() &&\n         \"No landing pad destinations for the dispatch jump table!\");\n\n  // Create the MBBs for the dispatch code.\n\n  // Shove the dispatch's address into the return slot in the function context.\n  MachineBasicBlock *DispatchBB = MF->CreateMachineBasicBlock();\n  DispatchBB->setIsEHPad(true);\n\n  MachineBasicBlock *TrapBB = MF->CreateMachineBasicBlock();\n  BuildMI(TrapBB, DL, TII->get(X86::TRAP));\n  DispatchBB->addSuccessor(TrapBB);\n\n  MachineBasicBlock *DispContBB = MF->CreateMachineBasicBlock();\n  DispatchBB->addSuccessor(DispContBB);\n\n  // Insert MBBs.\n  MF->push_back(DispatchBB);\n  MF->push_back(DispContBB);\n  MF->push_back(TrapBB);\n\n  // Insert code into the entry block that creates and registers the function\n  // context.\n  SetupEntryBlockForSjLj(MI, BB, DispatchBB, FI);\n\n  // Create the jump table and associated information\n  unsigned JTE = getJumpTableEncoding();\n  MachineJumpTableInfo *JTI = MF->getOrCreateJumpTableInfo(JTE);\n  unsigned MJTI = JTI->createJumpTableIndex(LPadList);\n\n  const X86RegisterInfo &RI = TII->getRegisterInfo();\n  // Add a register mask with no preserved registers.  This results in all\n  // registers being marked as clobbered.\n  if (RI.hasBasePointer(*MF)) {\n    const bool FPIs64Bit =\n        Subtarget.isTarget64BitLP64() || Subtarget.isTargetNaCl64();\n    X86MachineFunctionInfo *MFI = MF->getInfo<X86MachineFunctionInfo>();\n    MFI->setRestoreBasePointer(MF);\n\n    Register FP = RI.getFrameRegister(*MF);\n    Register BP = RI.getBaseRegister();\n    unsigned Op = FPIs64Bit ? X86::MOV64rm : X86::MOV32rm;\n    addRegOffset(BuildMI(DispatchBB, DL, TII->get(Op), BP), FP, true,\n                 MFI->getRestoreBasePointerOffset())\n        .addRegMask(RI.getNoPreservedMask());\n  } else {\n    BuildMI(DispatchBB, DL, TII->get(X86::NOOP))\n        .addRegMask(RI.getNoPreservedMask());\n  }\n\n  // IReg is used as an index in a memory operand and therefore can't be SP\n  Register IReg = MRI->createVirtualRegister(&X86::GR32_NOSPRegClass);\n  addFrameReference(BuildMI(DispatchBB, DL, TII->get(X86::MOV32rm), IReg), FI,\n                    Subtarget.is64Bit() ? 8 : 4);\n  BuildMI(DispatchBB, DL, TII->get(X86::CMP32ri))\n      .addReg(IReg)\n      .addImm(LPadList.size());\n  BuildMI(DispatchBB, DL, TII->get(X86::JCC_1)).addMBB(TrapBB).addImm(X86::COND_AE);\n\n  if (Subtarget.is64Bit()) {\n    Register BReg = MRI->createVirtualRegister(&X86::GR64RegClass);\n    Register IReg64 = MRI->createVirtualRegister(&X86::GR64_NOSPRegClass);\n\n    // leaq .LJTI0_0(%rip), BReg\n    BuildMI(DispContBB, DL, TII->get(X86::LEA64r), BReg)\n        .addReg(X86::RIP)\n        .addImm(1)\n        .addReg(0)\n        .addJumpTableIndex(MJTI)\n        .addReg(0);\n    // movzx IReg64, IReg\n    BuildMI(DispContBB, DL, TII->get(TargetOpcode::SUBREG_TO_REG), IReg64)\n        .addImm(0)\n        .addReg(IReg)\n        .addImm(X86::sub_32bit);\n\n    switch (JTE) {\n    case MachineJumpTableInfo::EK_BlockAddress:\n      // jmpq *(BReg,IReg64,8)\n      BuildMI(DispContBB, DL, TII->get(X86::JMP64m))\n          .addReg(BReg)\n          .addImm(8)\n          .addReg(IReg64)\n          .addImm(0)\n          .addReg(0);\n      break;\n    case MachineJumpTableInfo::EK_LabelDifference32: {\n      Register OReg = MRI->createVirtualRegister(&X86::GR32RegClass);\n      Register OReg64 = MRI->createVirtualRegister(&X86::GR64RegClass);\n      Register TReg = MRI->createVirtualRegister(&X86::GR64RegClass);\n\n      // movl (BReg,IReg64,4), OReg\n      BuildMI(DispContBB, DL, TII->get(X86::MOV32rm), OReg)\n          .addReg(BReg)\n          .addImm(4)\n          .addReg(IReg64)\n          .addImm(0)\n          .addReg(0);\n      // movsx OReg64, OReg\n      BuildMI(DispContBB, DL, TII->get(X86::MOVSX64rr32), OReg64).addReg(OReg);\n      // addq BReg, OReg64, TReg\n      BuildMI(DispContBB, DL, TII->get(X86::ADD64rr), TReg)\n          .addReg(OReg64)\n          .addReg(BReg);\n      // jmpq *TReg\n      BuildMI(DispContBB, DL, TII->get(X86::JMP64r)).addReg(TReg);\n      break;\n    }\n    default:\n      llvm_unreachable(\"Unexpected jump table encoding\");\n    }\n  } else {\n    // jmpl *.LJTI0_0(,IReg,4)\n    BuildMI(DispContBB, DL, TII->get(X86::JMP32m))\n        .addReg(0)\n        .addImm(4)\n        .addReg(IReg)\n        .addJumpTableIndex(MJTI)\n        .addReg(0);\n  }\n\n  // Add the jump table entries as successors to the MBB.\n  SmallPtrSet<MachineBasicBlock *, 8> SeenMBBs;\n  for (auto &LP : LPadList)\n    if (SeenMBBs.insert(LP).second)\n      DispContBB->addSuccessor(LP);\n\n  // N.B. the order the invoke BBs are processed in doesn't matter here.\n  SmallVector<MachineBasicBlock *, 64> MBBLPads;\n  const MCPhysReg *SavedRegs = MF->getRegInfo().getCalleeSavedRegs();\n  for (MachineBasicBlock *MBB : InvokeBBs) {\n    // Remove the landing pad successor from the invoke block and replace it\n    // with the new dispatch block.\n    // Keep a copy of Successors since it's modified inside the loop.\n    SmallVector<MachineBasicBlock *, 8> Successors(MBB->succ_rbegin(),\n                                                   MBB->succ_rend());\n    // FIXME: Avoid quadratic complexity.\n    for (auto MBBS : Successors) {\n      if (MBBS->isEHPad()) {\n        MBB->removeSuccessor(MBBS);\n        MBBLPads.push_back(MBBS);\n      }\n    }\n\n    MBB->addSuccessor(DispatchBB);\n\n    // Find the invoke call and mark all of the callee-saved registers as\n    // 'implicit defined' so that they're spilled.  This prevents code from\n    // moving instructions to before the EH block, where they will never be\n    // executed.\n    for (auto &II : reverse(*MBB)) {\n      if (!II.isCall())\n        continue;\n\n      DenseMap<unsigned, bool> DefRegs;\n      for (auto &MOp : II.operands())\n        if (MOp.isReg())\n          DefRegs[MOp.getReg()] = true;\n\n      MachineInstrBuilder MIB(*MF, &II);\n      for (unsigned RegIdx = 0; SavedRegs[RegIdx]; ++RegIdx) {\n        unsigned Reg = SavedRegs[RegIdx];\n        if (!DefRegs[Reg])\n          MIB.addReg(Reg, RegState::ImplicitDefine | RegState::Dead);\n      }\n\n      break;\n    }\n  }\n\n  // Mark all former landing pads as non-landing pads.  The dispatch is the only\n  // landing pad now.\n  for (auto &LP : MBBLPads)\n    LP->setIsEHPad(false);\n\n  // The instruction is gone now.\n  MI.eraseFromParent();\n  return BB;\n}\n\nMachineBasicBlock *\nX86TargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,\n                                               MachineBasicBlock *BB) const {\n  MachineFunction *MF = BB->getParent();\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  const DebugLoc &DL = MI.getDebugLoc();\n\n  auto TMMImmToTMMReg = [](unsigned Imm) {\n    assert (Imm < 8 && \"Illegal tmm index\");\n    return X86::TMM0 + Imm;\n  };\n  switch (MI.getOpcode()) {\n  default: llvm_unreachable(\"Unexpected instr type to insert\");\n  case X86::TLS_addr32:\n  case X86::TLS_addr64:\n  case X86::TLS_addrX32:\n  case X86::TLS_base_addr32:\n  case X86::TLS_base_addr64:\n  case X86::TLS_base_addrX32:\n    return EmitLoweredTLSAddr(MI, BB);\n  case X86::INDIRECT_THUNK_CALL32:\n  case X86::INDIRECT_THUNK_CALL64:\n  case X86::INDIRECT_THUNK_TCRETURN32:\n  case X86::INDIRECT_THUNK_TCRETURN64:\n    return EmitLoweredIndirectThunk(MI, BB);\n  case X86::CATCHRET:\n    return EmitLoweredCatchRet(MI, BB);\n  case X86::SEG_ALLOCA_32:\n  case X86::SEG_ALLOCA_64:\n    return EmitLoweredSegAlloca(MI, BB);\n  case X86::PROBED_ALLOCA_32:\n  case X86::PROBED_ALLOCA_64:\n    return EmitLoweredProbedAlloca(MI, BB);\n  case X86::TLSCall_32:\n  case X86::TLSCall_64:\n    return EmitLoweredTLSCall(MI, BB);\n  case X86::CMOV_FR32:\n  case X86::CMOV_FR32X:\n  case X86::CMOV_FR64:\n  case X86::CMOV_FR64X:\n  case X86::CMOV_GR8:\n  case X86::CMOV_GR16:\n  case X86::CMOV_GR32:\n  case X86::CMOV_RFP32:\n  case X86::CMOV_RFP64:\n  case X86::CMOV_RFP80:\n  case X86::CMOV_VR64:\n  case X86::CMOV_VR128:\n  case X86::CMOV_VR128X:\n  case X86::CMOV_VR256:\n  case X86::CMOV_VR256X:\n  case X86::CMOV_VR512:\n  case X86::CMOV_VK1:\n  case X86::CMOV_VK2:\n  case X86::CMOV_VK4:\n  case X86::CMOV_VK8:\n  case X86::CMOV_VK16:\n  case X86::CMOV_VK32:\n  case X86::CMOV_VK64:\n    return EmitLoweredSelect(MI, BB);\n\n  case X86::RDFLAGS32:\n  case X86::RDFLAGS64: {\n    unsigned PushF =\n        MI.getOpcode() == X86::RDFLAGS32 ? X86::PUSHF32 : X86::PUSHF64;\n    unsigned Pop = MI.getOpcode() == X86::RDFLAGS32 ? X86::POP32r : X86::POP64r;\n    MachineInstr *Push = BuildMI(*BB, MI, DL, TII->get(PushF));\n    // Permit reads of the EFLAGS and DF registers without them being defined.\n    // This intrinsic exists to read external processor state in flags, such as\n    // the trap flag, interrupt flag, and direction flag, none of which are\n    // modeled by the backend.\n    assert(Push->getOperand(2).getReg() == X86::EFLAGS &&\n           \"Unexpected register in operand!\");\n    Push->getOperand(2).setIsUndef();\n    assert(Push->getOperand(3).getReg() == X86::DF &&\n           \"Unexpected register in operand!\");\n    Push->getOperand(3).setIsUndef();\n    BuildMI(*BB, MI, DL, TII->get(Pop), MI.getOperand(0).getReg());\n\n    MI.eraseFromParent(); // The pseudo is gone now.\n    return BB;\n  }\n\n  case X86::WRFLAGS32:\n  case X86::WRFLAGS64: {\n    unsigned Push =\n        MI.getOpcode() == X86::WRFLAGS32 ? X86::PUSH32r : X86::PUSH64r;\n    unsigned PopF =\n        MI.getOpcode() == X86::WRFLAGS32 ? X86::POPF32 : X86::POPF64;\n    BuildMI(*BB, MI, DL, TII->get(Push)).addReg(MI.getOperand(0).getReg());\n    BuildMI(*BB, MI, DL, TII->get(PopF));\n\n    MI.eraseFromParent(); // The pseudo is gone now.\n    return BB;\n  }\n\n  case X86::FP32_TO_INT16_IN_MEM:\n  case X86::FP32_TO_INT32_IN_MEM:\n  case X86::FP32_TO_INT64_IN_MEM:\n  case X86::FP64_TO_INT16_IN_MEM:\n  case X86::FP64_TO_INT32_IN_MEM:\n  case X86::FP64_TO_INT64_IN_MEM:\n  case X86::FP80_TO_INT16_IN_MEM:\n  case X86::FP80_TO_INT32_IN_MEM:\n  case X86::FP80_TO_INT64_IN_MEM: {\n    // Change the floating point control register to use \"round towards zero\"\n    // mode when truncating to an integer value.\n    int OrigCWFrameIdx =\n        MF->getFrameInfo().CreateStackObject(2, Align(2), false);\n    addFrameReference(BuildMI(*BB, MI, DL,\n                              TII->get(X86::FNSTCW16m)), OrigCWFrameIdx);\n\n    // Load the old value of the control word...\n    Register OldCW = MF->getRegInfo().createVirtualRegister(&X86::GR32RegClass);\n    addFrameReference(BuildMI(*BB, MI, DL, TII->get(X86::MOVZX32rm16), OldCW),\n                      OrigCWFrameIdx);\n\n    // OR 0b11 into bit 10 and 11. 0b11 is the encoding for round toward zero.\n    Register NewCW = MF->getRegInfo().createVirtualRegister(&X86::GR32RegClass);\n    BuildMI(*BB, MI, DL, TII->get(X86::OR32ri), NewCW)\n      .addReg(OldCW, RegState::Kill).addImm(0xC00);\n\n    // Extract to 16 bits.\n    Register NewCW16 =\n        MF->getRegInfo().createVirtualRegister(&X86::GR16RegClass);\n    BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), NewCW16)\n      .addReg(NewCW, RegState::Kill, X86::sub_16bit);\n\n    // Prepare memory for FLDCW.\n    int NewCWFrameIdx =\n        MF->getFrameInfo().CreateStackObject(2, Align(2), false);\n    addFrameReference(BuildMI(*BB, MI, DL, TII->get(X86::MOV16mr)),\n                      NewCWFrameIdx)\n      .addReg(NewCW16, RegState::Kill);\n\n    // Reload the modified control word now...\n    addFrameReference(BuildMI(*BB, MI, DL,\n                              TII->get(X86::FLDCW16m)), NewCWFrameIdx);\n\n    // Get the X86 opcode to use.\n    unsigned Opc;\n    switch (MI.getOpcode()) {\n    default: llvm_unreachable(\"illegal opcode!\");\n    case X86::FP32_TO_INT16_IN_MEM: Opc = X86::IST_Fp16m32; break;\n    case X86::FP32_TO_INT32_IN_MEM: Opc = X86::IST_Fp32m32; break;\n    case X86::FP32_TO_INT64_IN_MEM: Opc = X86::IST_Fp64m32; break;\n    case X86::FP64_TO_INT16_IN_MEM: Opc = X86::IST_Fp16m64; break;\n    case X86::FP64_TO_INT32_IN_MEM: Opc = X86::IST_Fp32m64; break;\n    case X86::FP64_TO_INT64_IN_MEM: Opc = X86::IST_Fp64m64; break;\n    case X86::FP80_TO_INT16_IN_MEM: Opc = X86::IST_Fp16m80; break;\n    case X86::FP80_TO_INT32_IN_MEM: Opc = X86::IST_Fp32m80; break;\n    case X86::FP80_TO_INT64_IN_MEM: Opc = X86::IST_Fp64m80; break;\n    }\n\n    X86AddressMode AM = getAddressFromInstr(&MI, 0);\n    addFullAddress(BuildMI(*BB, MI, DL, TII->get(Opc)), AM)\n        .addReg(MI.getOperand(X86::AddrNumOperands).getReg());\n\n    // Reload the original control word now.\n    addFrameReference(BuildMI(*BB, MI, DL,\n                              TII->get(X86::FLDCW16m)), OrigCWFrameIdx);\n\n    MI.eraseFromParent(); // The pseudo instruction is gone now.\n    return BB;\n  }\n\n  // xbegin\n  case X86::XBEGIN:\n    return emitXBegin(MI, BB, Subtarget.getInstrInfo());\n\n  case X86::VASTART_SAVE_XMM_REGS:\n    return EmitVAStartSaveXMMRegsWithCustomInserter(MI, BB);\n\n  case X86::VAARG_64:\n  case X86::VAARG_X32:\n    return EmitVAARGWithCustomInserter(MI, BB);\n\n  case X86::EH_SjLj_SetJmp32:\n  case X86::EH_SjLj_SetJmp64:\n    return emitEHSjLjSetJmp(MI, BB);\n\n  case X86::EH_SjLj_LongJmp32:\n  case X86::EH_SjLj_LongJmp64:\n    return emitEHSjLjLongJmp(MI, BB);\n\n  case X86::Int_eh_sjlj_setup_dispatch:\n    return EmitSjLjDispatchBlock(MI, BB);\n\n  case TargetOpcode::STATEPOINT:\n    // As an implementation detail, STATEPOINT shares the STACKMAP format at\n    // this point in the process.  We diverge later.\n    return emitPatchPoint(MI, BB);\n\n  case TargetOpcode::STACKMAP:\n  case TargetOpcode::PATCHPOINT:\n    return emitPatchPoint(MI, BB);\n\n  case TargetOpcode::PATCHABLE_EVENT_CALL:\n  case TargetOpcode::PATCHABLE_TYPED_EVENT_CALL:\n    return BB;\n\n  case X86::LCMPXCHG8B: {\n    const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n    // In addition to 4 E[ABCD] registers implied by encoding, CMPXCHG8B\n    // requires a memory operand. If it happens that current architecture is\n    // i686 and for current function we need a base pointer\n    // - which is ESI for i686 - register allocator would not be able to\n    // allocate registers for an address in form of X(%reg, %reg, Y)\n    // - there never would be enough unreserved registers during regalloc\n    // (without the need for base ptr the only option would be X(%edi, %esi, Y).\n    // We are giving a hand to register allocator by precomputing the address in\n    // a new vreg using LEA.\n\n    // If it is not i686 or there is no base pointer - nothing to do here.\n    if (!Subtarget.is32Bit() || !TRI->hasBasePointer(*MF))\n      return BB;\n\n    // Even though this code does not necessarily needs the base pointer to\n    // be ESI, we check for that. The reason: if this assert fails, there are\n    // some changes happened in the compiler base pointer handling, which most\n    // probably have to be addressed somehow here.\n    assert(TRI->getBaseRegister() == X86::ESI &&\n           \"LCMPXCHG8B custom insertion for i686 is written with X86::ESI as a \"\n           \"base pointer in mind\");\n\n    MachineRegisterInfo &MRI = MF->getRegInfo();\n    MVT SPTy = getPointerTy(MF->getDataLayout());\n    const TargetRegisterClass *AddrRegClass = getRegClassFor(SPTy);\n    Register computedAddrVReg = MRI.createVirtualRegister(AddrRegClass);\n\n    X86AddressMode AM = getAddressFromInstr(&MI, 0);\n    // Regalloc does not need any help when the memory operand of CMPXCHG8B\n    // does not use index register.\n    if (AM.IndexReg == X86::NoRegister)\n      return BB;\n\n    // After X86TargetLowering::ReplaceNodeResults CMPXCHG8B is glued to its\n    // four operand definitions that are E[ABCD] registers. We skip them and\n    // then insert the LEA.\n    MachineBasicBlock::reverse_iterator RMBBI(MI.getReverseIterator());\n    while (RMBBI != BB->rend() && (RMBBI->definesRegister(X86::EAX) ||\n                                   RMBBI->definesRegister(X86::EBX) ||\n                                   RMBBI->definesRegister(X86::ECX) ||\n                                   RMBBI->definesRegister(X86::EDX))) {\n      ++RMBBI;\n    }\n    MachineBasicBlock::iterator MBBI(RMBBI);\n    addFullAddress(\n        BuildMI(*BB, *MBBI, DL, TII->get(X86::LEA32r), computedAddrVReg), AM);\n\n    setDirectAddressInInstr(&MI, 0, computedAddrVReg);\n\n    return BB;\n  }\n  case X86::LCMPXCHG16B_NO_RBX: {\n    const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n    Register BasePtr = TRI->getBaseRegister();\n    if (TRI->hasBasePointer(*MF) &&\n        (BasePtr == X86::RBX || BasePtr == X86::EBX)) {\n      if (!BB->isLiveIn(BasePtr))\n        BB->addLiveIn(BasePtr);\n      // Save RBX into a virtual register.\n      Register SaveRBX =\n          MF->getRegInfo().createVirtualRegister(&X86::GR64RegClass);\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), SaveRBX)\n          .addReg(X86::RBX);\n      Register Dst = MF->getRegInfo().createVirtualRegister(&X86::GR64RegClass);\n      MachineInstrBuilder MIB =\n          BuildMI(*BB, MI, DL, TII->get(X86::LCMPXCHG16B_SAVE_RBX), Dst);\n      for (unsigned Idx = 0; Idx < X86::AddrNumOperands; ++Idx)\n        MIB.add(MI.getOperand(Idx));\n      MIB.add(MI.getOperand(X86::AddrNumOperands));\n      MIB.addReg(SaveRBX);\n    } else {\n      // Simple case, just copy the virtual register to RBX.\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::RBX)\n          .add(MI.getOperand(X86::AddrNumOperands));\n      MachineInstrBuilder MIB =\n          BuildMI(*BB, MI, DL, TII->get(X86::LCMPXCHG16B));\n      for (unsigned Idx = 0; Idx < X86::AddrNumOperands; ++Idx)\n        MIB.add(MI.getOperand(Idx));\n    }\n    MI.eraseFromParent();\n    return BB;\n  }\n  case X86::MWAITX: {\n    const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n    Register BasePtr = TRI->getBaseRegister();\n    bool IsRBX = (BasePtr == X86::RBX || BasePtr == X86::EBX);\n    // If no need to save the base pointer, we generate MWAITXrrr,\n    // else we generate pseudo MWAITX_SAVE_RBX.\n    if (!IsRBX || !TRI->hasBasePointer(*MF)) {\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::ECX)\n          .addReg(MI.getOperand(0).getReg());\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::EAX)\n          .addReg(MI.getOperand(1).getReg());\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::EBX)\n          .addReg(MI.getOperand(2).getReg());\n      BuildMI(*BB, MI, DL, TII->get(X86::MWAITXrrr));\n      MI.eraseFromParent();\n    } else {\n      if (!BB->isLiveIn(BasePtr)) {\n        BB->addLiveIn(BasePtr);\n      }\n      // Parameters can be copied into ECX and EAX but not EBX yet.\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::ECX)\n          .addReg(MI.getOperand(0).getReg());\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), X86::EAX)\n          .addReg(MI.getOperand(1).getReg());\n      assert(Subtarget.is64Bit() && \"Expected 64-bit mode!\");\n      // Save RBX into a virtual register.\n      Register SaveRBX =\n          MF->getRegInfo().createVirtualRegister(&X86::GR64RegClass);\n      BuildMI(*BB, MI, DL, TII->get(TargetOpcode::COPY), SaveRBX)\n          .addReg(X86::RBX);\n      // Generate mwaitx pseudo.\n      Register Dst = MF->getRegInfo().createVirtualRegister(&X86::GR64RegClass);\n      BuildMI(*BB, MI, DL, TII->get(X86::MWAITX_SAVE_RBX))\n          .addDef(Dst) // Destination tied in with SaveRBX.\n          .addReg(MI.getOperand(2).getReg()) // input value of EBX.\n          .addUse(SaveRBX);                  // Save of base pointer.\n      MI.eraseFromParent();\n    }\n    return BB;\n  }\n  case TargetOpcode::PREALLOCATED_SETUP: {\n    assert(Subtarget.is32Bit() && \"preallocated only used in 32-bit\");\n    auto MFI = MF->getInfo<X86MachineFunctionInfo>();\n    MFI->setHasPreallocatedCall(true);\n    int64_t PreallocatedId = MI.getOperand(0).getImm();\n    size_t StackAdjustment = MFI->getPreallocatedStackSize(PreallocatedId);\n    assert(StackAdjustment != 0 && \"0 stack adjustment\");\n    LLVM_DEBUG(dbgs() << \"PREALLOCATED_SETUP stack adjustment \"\n                      << StackAdjustment << \"\\n\");\n    BuildMI(*BB, MI, DL, TII->get(X86::SUB32ri), X86::ESP)\n        .addReg(X86::ESP)\n        .addImm(StackAdjustment);\n    MI.eraseFromParent();\n    return BB;\n  }\n  case TargetOpcode::PREALLOCATED_ARG: {\n    assert(Subtarget.is32Bit() && \"preallocated calls only used in 32-bit\");\n    int64_t PreallocatedId = MI.getOperand(1).getImm();\n    int64_t ArgIdx = MI.getOperand(2).getImm();\n    auto MFI = MF->getInfo<X86MachineFunctionInfo>();\n    size_t ArgOffset = MFI->getPreallocatedArgOffsets(PreallocatedId)[ArgIdx];\n    LLVM_DEBUG(dbgs() << \"PREALLOCATED_ARG arg index \" << ArgIdx\n                      << \", arg offset \" << ArgOffset << \"\\n\");\n    // stack pointer + offset\n    addRegOffset(\n        BuildMI(*BB, MI, DL, TII->get(X86::LEA32r), MI.getOperand(0).getReg()),\n        X86::ESP, false, ArgOffset);\n    MI.eraseFromParent();\n    return BB;\n  }\n  case X86::PTDPBSSD:\n  case X86::PTDPBSUD:\n  case X86::PTDPBUSD:\n  case X86::PTDPBUUD:\n  case X86::PTDPBF16PS: {\n    unsigned Opc;\n    switch (MI.getOpcode()) {\n    case X86::PTDPBSSD: Opc = X86::TDPBSSD; break;\n    case X86::PTDPBSUD: Opc = X86::TDPBSUD; break;\n    case X86::PTDPBUSD: Opc = X86::TDPBUSD; break;\n    case X86::PTDPBUUD: Opc = X86::TDPBUUD; break;\n    case X86::PTDPBF16PS: Opc = X86::TDPBF16PS; break;\n    }\n\n    MachineInstrBuilder MIB = BuildMI(*BB, MI, DL, TII->get(Opc));\n    MIB.addReg(TMMImmToTMMReg(MI.getOperand(0).getImm()), RegState::Define);\n    MIB.addReg(TMMImmToTMMReg(MI.getOperand(0).getImm()), RegState::Undef);\n    MIB.addReg(TMMImmToTMMReg(MI.getOperand(1).getImm()), RegState::Undef);\n    MIB.addReg(TMMImmToTMMReg(MI.getOperand(2).getImm()), RegState::Undef);\n\n    MI.eraseFromParent(); // The pseudo is gone now.\n    return BB;\n  }\n  case X86::PTILEZERO: {\n    unsigned Imm = MI.getOperand(0).getImm();\n    BuildMI(*BB, MI, DL, TII->get(X86::TILEZERO), TMMImmToTMMReg(Imm));\n    MI.eraseFromParent(); // The pseudo is gone now.\n    return BB;\n  }\n  case X86::PTILELOADD:\n  case X86::PTILELOADDT1:\n  case X86::PTILESTORED: {\n    unsigned Opc;\n    switch (MI.getOpcode()) {\n    case X86::PTILELOADD:   Opc = X86::TILELOADD;   break;\n    case X86::PTILELOADDT1: Opc = X86::TILELOADDT1; break;\n    case X86::PTILESTORED:  Opc = X86::TILESTORED;  break;\n    }\n\n    MachineInstrBuilder MIB = BuildMI(*BB, MI, DL, TII->get(Opc));\n    unsigned CurOp = 0;\n    if (Opc != X86::TILESTORED)\n      MIB.addReg(TMMImmToTMMReg(MI.getOperand(CurOp++).getImm()),\n                 RegState::Define);\n\n    MIB.add(MI.getOperand(CurOp++)); // base\n    MIB.add(MI.getOperand(CurOp++)); // scale\n    MIB.add(MI.getOperand(CurOp++)); // index -- stride\n    MIB.add(MI.getOperand(CurOp++)); // displacement\n    MIB.add(MI.getOperand(CurOp++)); // segment\n\n    if (Opc == X86::TILESTORED)\n      MIB.addReg(TMMImmToTMMReg(MI.getOperand(CurOp++).getImm()),\n                 RegState::Undef);\n\n    MI.eraseFromParent(); // The pseudo is gone now.\n    return BB;\n  }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n//                           X86 Optimization Hooks\n//===----------------------------------------------------------------------===//\n\nbool\nX86TargetLowering::targetShrinkDemandedConstant(SDValue Op,\n                                                const APInt &DemandedBits,\n                                                const APInt &DemandedElts,\n                                                TargetLoweringOpt &TLO) const {\n  EVT VT = Op.getValueType();\n  unsigned Opcode = Op.getOpcode();\n  unsigned EltSize = VT.getScalarSizeInBits();\n\n  if (VT.isVector()) {\n    // If the constant is only all signbits in the active bits, then we should\n    // extend it to the entire constant to allow it act as a boolean constant\n    // vector.\n    auto NeedsSignExtension = [&](SDValue V, unsigned ActiveBits) {\n      if (!ISD::isBuildVectorOfConstantSDNodes(V.getNode()))\n        return false;\n      for (unsigned i = 0, e = V.getNumOperands(); i != e; ++i) {\n        if (!DemandedElts[i] || V.getOperand(i).isUndef())\n          continue;\n        const APInt &Val = V.getConstantOperandAPInt(i);\n        if (Val.getBitWidth() > Val.getNumSignBits() &&\n            Val.trunc(ActiveBits).getNumSignBits() == ActiveBits)\n          return true;\n      }\n      return false;\n    };\n    // For vectors - if we have a constant, then try to sign extend.\n    // TODO: Handle AND/ANDN cases.\n    unsigned ActiveBits = DemandedBits.getActiveBits();\n    if (EltSize > ActiveBits && EltSize > 1 && isTypeLegal(VT) &&\n        (Opcode == ISD::OR || Opcode == ISD::XOR) &&\n        NeedsSignExtension(Op.getOperand(1), ActiveBits)) {\n      EVT ExtSVT = EVT::getIntegerVT(*TLO.DAG.getContext(), ActiveBits);\n      EVT ExtVT = EVT::getVectorVT(*TLO.DAG.getContext(), ExtSVT,\n                                    VT.getVectorNumElements());\n      SDValue NewC =\n          TLO.DAG.getNode(ISD::SIGN_EXTEND_INREG, SDLoc(Op), VT,\n                          Op.getOperand(1), TLO.DAG.getValueType(ExtVT));\n      SDValue NewOp =\n          TLO.DAG.getNode(Opcode, SDLoc(Op), VT, Op.getOperand(0), NewC);\n      return TLO.CombineTo(Op, NewOp);\n    }\n    return false;\n  }\n\n  // Only optimize Ands to prevent shrinking a constant that could be\n  // matched by movzx.\n  if (Opcode != ISD::AND)\n    return false;\n\n  // Make sure the RHS really is a constant.\n  ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op.getOperand(1));\n  if (!C)\n    return false;\n\n  const APInt &Mask = C->getAPIntValue();\n\n  // Clear all non-demanded bits initially.\n  APInt ShrunkMask = Mask & DemandedBits;\n\n  // Find the width of the shrunk mask.\n  unsigned Width = ShrunkMask.getActiveBits();\n\n  // If the mask is all 0s there's nothing to do here.\n  if (Width == 0)\n    return false;\n\n  // Find the next power of 2 width, rounding up to a byte.\n  Width = PowerOf2Ceil(std::max(Width, 8U));\n  // Truncate the width to size to handle illegal types.\n  Width = std::min(Width, EltSize);\n\n  // Calculate a possible zero extend mask for this constant.\n  APInt ZeroExtendMask = APInt::getLowBitsSet(EltSize, Width);\n\n  // If we aren't changing the mask, just return true to keep it and prevent\n  // the caller from optimizing.\n  if (ZeroExtendMask == Mask)\n    return true;\n\n  // Make sure the new mask can be represented by a combination of mask bits\n  // and non-demanded bits.\n  if (!ZeroExtendMask.isSubsetOf(Mask | ~DemandedBits))\n    return false;\n\n  // Replace the constant with the zero extend mask.\n  SDLoc DL(Op);\n  SDValue NewC = TLO.DAG.getConstant(ZeroExtendMask, DL, VT);\n  SDValue NewOp = TLO.DAG.getNode(ISD::AND, DL, VT, Op.getOperand(0), NewC);\n  return TLO.CombineTo(Op, NewOp);\n}\n\nvoid X86TargetLowering::computeKnownBitsForTargetNode(const SDValue Op,\n                                                      KnownBits &Known,\n                                                      const APInt &DemandedElts,\n                                                      const SelectionDAG &DAG,\n                                                      unsigned Depth) const {\n  unsigned BitWidth = Known.getBitWidth();\n  unsigned NumElts = DemandedElts.getBitWidth();\n  unsigned Opc = Op.getOpcode();\n  EVT VT = Op.getValueType();\n  assert((Opc >= ISD::BUILTIN_OP_END ||\n          Opc == ISD::INTRINSIC_WO_CHAIN ||\n          Opc == ISD::INTRINSIC_W_CHAIN ||\n          Opc == ISD::INTRINSIC_VOID) &&\n         \"Should use MaskedValueIsZero if you don't know whether Op\"\n         \" is a target node!\");\n\n  Known.resetAll();\n  switch (Opc) {\n  default: break;\n  case X86ISD::SETCC:\n    Known.Zero.setBitsFrom(1);\n    break;\n  case X86ISD::MOVMSK: {\n    unsigned NumLoBits = Op.getOperand(0).getValueType().getVectorNumElements();\n    Known.Zero.setBitsFrom(NumLoBits);\n    break;\n  }\n  case X86ISD::PEXTRB:\n  case X86ISD::PEXTRW: {\n    SDValue Src = Op.getOperand(0);\n    EVT SrcVT = Src.getValueType();\n    APInt DemandedElt = APInt::getOneBitSet(SrcVT.getVectorNumElements(),\n                                            Op.getConstantOperandVal(1));\n    Known = DAG.computeKnownBits(Src, DemandedElt, Depth + 1);\n    Known = Known.anyextOrTrunc(BitWidth);\n    Known.Zero.setBitsFrom(SrcVT.getScalarSizeInBits());\n    break;\n  }\n  case X86ISD::VSRAI:\n  case X86ISD::VSHLI:\n  case X86ISD::VSRLI: {\n    unsigned ShAmt = Op.getConstantOperandVal(1);\n    if (ShAmt >= VT.getScalarSizeInBits()) {\n      Known.setAllZero();\n      break;\n    }\n\n    Known = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    if (Opc == X86ISD::VSHLI) {\n      Known.Zero <<= ShAmt;\n      Known.One <<= ShAmt;\n      // Low bits are known zero.\n      Known.Zero.setLowBits(ShAmt);\n    } else if (Opc == X86ISD::VSRLI) {\n      Known.Zero.lshrInPlace(ShAmt);\n      Known.One.lshrInPlace(ShAmt);\n      // High bits are known zero.\n      Known.Zero.setHighBits(ShAmt);\n    } else {\n      Known.Zero.ashrInPlace(ShAmt);\n      Known.One.ashrInPlace(ShAmt);\n    }\n    break;\n  }\n  case X86ISD::PACKUS: {\n    // PACKUS is just a truncation if the upper half is zero.\n    APInt DemandedLHS, DemandedRHS;\n    getPackDemandedElts(VT, DemandedElts, DemandedLHS, DemandedRHS);\n\n    Known.One = APInt::getAllOnesValue(BitWidth * 2);\n    Known.Zero = APInt::getAllOnesValue(BitWidth * 2);\n\n    KnownBits Known2;\n    if (!!DemandedLHS) {\n      Known2 = DAG.computeKnownBits(Op.getOperand(0), DemandedLHS, Depth + 1);\n      Known = KnownBits::commonBits(Known, Known2);\n    }\n    if (!!DemandedRHS) {\n      Known2 = DAG.computeKnownBits(Op.getOperand(1), DemandedRHS, Depth + 1);\n      Known = KnownBits::commonBits(Known, Known2);\n    }\n\n    if (Known.countMinLeadingZeros() < BitWidth)\n      Known.resetAll();\n    Known = Known.trunc(BitWidth);\n    break;\n  }\n  case X86ISD::ANDNP: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n\n    // ANDNP = (~X & Y);\n    Known.One &= Known2.Zero;\n    Known.Zero |= Known2.One;\n    break;\n  }\n  case X86ISD::FOR: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n\n    Known |= Known2;\n    break;\n  }\n  case X86ISD::PSADBW: {\n    assert(VT.getScalarType() == MVT::i64 &&\n           Op.getOperand(0).getValueType().getScalarType() == MVT::i8 &&\n           \"Unexpected PSADBW types\");\n\n    // PSADBW - fills low 16 bits and zeros upper 48 bits of each i64 result.\n    Known.Zero.setBitsFrom(16);\n    break;\n  }\n  case X86ISD::CMOV: {\n    Known = DAG.computeKnownBits(Op.getOperand(1), Depth + 1);\n    // If we don't know any bits, early out.\n    if (Known.isUnknown())\n      break;\n    KnownBits Known2 = DAG.computeKnownBits(Op.getOperand(0), Depth + 1);\n\n    // Only known if known in both the LHS and RHS.\n    Known = KnownBits::commonBits(Known, Known2);\n    break;\n  }\n  case X86ISD::BEXTR:\n  case X86ISD::BEXTRI: {\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    if (auto* Cst1 = dyn_cast<ConstantSDNode>(Op1)) {\n      unsigned Shift = Cst1->getAPIntValue().extractBitsAsZExtValue(8, 0);\n      unsigned Length = Cst1->getAPIntValue().extractBitsAsZExtValue(8, 8);\n\n      // If the length is 0, the result is 0.\n      if (Length == 0) {\n        Known.setAllZero();\n        break;\n      }\n\n      if ((Shift + Length) <= BitWidth) {\n        Known = DAG.computeKnownBits(Op0, Depth + 1);\n        Known = Known.extractBits(Length, Shift);\n        Known = Known.zextOrTrunc(BitWidth);\n      }\n    }\n    break;\n  }\n  case X86ISD::PDEP: {\n    KnownBits Known2;\n    Known = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    Known2 = DAG.computeKnownBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    // Zeros are retained from the mask operand. But not ones.\n    Known.One.clearAllBits();\n    // The result will have at least as many trailing zeros as the non-mask\n    // operand since bits can only map to the same or higher bit position.\n    Known.Zero.setLowBits(Known2.countMinTrailingZeros());\n    break;\n  }\n  case X86ISD::PEXT: {\n    Known = DAG.computeKnownBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    // The result has as many leading zeros as the number of zeroes in the mask.\n    unsigned Count = Known.Zero.countPopulation();\n    Known.Zero = APInt::getHighBitsSet(BitWidth, Count);\n    Known.One.clearAllBits();\n    break;\n  }\n  case X86ISD::VTRUNC:\n  case X86ISD::VTRUNCS:\n  case X86ISD::VTRUNCUS:\n  case X86ISD::CVTSI2P:\n  case X86ISD::CVTUI2P:\n  case X86ISD::CVTP2SI:\n  case X86ISD::CVTP2UI:\n  case X86ISD::MCVTP2SI:\n  case X86ISD::MCVTP2UI:\n  case X86ISD::CVTTP2SI:\n  case X86ISD::CVTTP2UI:\n  case X86ISD::MCVTTP2SI:\n  case X86ISD::MCVTTP2UI:\n  case X86ISD::MCVTSI2P:\n  case X86ISD::MCVTUI2P:\n  case X86ISD::VFPROUND:\n  case X86ISD::VMFPROUND:\n  case X86ISD::CVTPS2PH:\n  case X86ISD::MCVTPS2PH: {\n    // Truncations/Conversions - upper elements are known zero.\n    EVT SrcVT = Op.getOperand(0).getValueType();\n    if (SrcVT.isVector()) {\n      unsigned NumSrcElts = SrcVT.getVectorNumElements();\n      if (NumElts > NumSrcElts &&\n          DemandedElts.countTrailingZeros() >= NumSrcElts)\n        Known.setAllZero();\n    }\n    break;\n  }\n  case X86ISD::STRICT_CVTTP2SI:\n  case X86ISD::STRICT_CVTTP2UI:\n  case X86ISD::STRICT_CVTSI2P:\n  case X86ISD::STRICT_CVTUI2P:\n  case X86ISD::STRICT_VFPROUND:\n  case X86ISD::STRICT_CVTPS2PH: {\n    // Strict Conversions - upper elements are known zero.\n    EVT SrcVT = Op.getOperand(1).getValueType();\n    if (SrcVT.isVector()) {\n      unsigned NumSrcElts = SrcVT.getVectorNumElements();\n      if (NumElts > NumSrcElts &&\n          DemandedElts.countTrailingZeros() >= NumSrcElts)\n        Known.setAllZero();\n    }\n    break;\n  }\n  case X86ISD::MOVQ2DQ: {\n    // Move from MMX to XMM. Upper half of XMM should be 0.\n    if (DemandedElts.countTrailingZeros() >= (NumElts / 2))\n      Known.setAllZero();\n    break;\n  }\n  }\n\n  // Handle target shuffles.\n  // TODO - use resolveTargetShuffleInputs once we can limit recursive depth.\n  if (isTargetShuffle(Opc)) {\n    bool IsUnary;\n    SmallVector<int, 64> Mask;\n    SmallVector<SDValue, 2> Ops;\n    if (getTargetShuffleMask(Op.getNode(), VT.getSimpleVT(), true, Ops, Mask,\n                             IsUnary)) {\n      unsigned NumOps = Ops.size();\n      unsigned NumElts = VT.getVectorNumElements();\n      if (Mask.size() == NumElts) {\n        SmallVector<APInt, 2> DemandedOps(NumOps, APInt(NumElts, 0));\n        Known.Zero.setAllBits(); Known.One.setAllBits();\n        for (unsigned i = 0; i != NumElts; ++i) {\n          if (!DemandedElts[i])\n            continue;\n          int M = Mask[i];\n          if (M == SM_SentinelUndef) {\n            // For UNDEF elements, we don't know anything about the common state\n            // of the shuffle result.\n            Known.resetAll();\n            break;\n          } else if (M == SM_SentinelZero) {\n            Known.One.clearAllBits();\n            continue;\n          }\n          assert(0 <= M && (unsigned)M < (NumOps * NumElts) &&\n                 \"Shuffle index out of range\");\n\n          unsigned OpIdx = (unsigned)M / NumElts;\n          unsigned EltIdx = (unsigned)M % NumElts;\n          if (Ops[OpIdx].getValueType() != VT) {\n            // TODO - handle target shuffle ops with different value types.\n            Known.resetAll();\n            break;\n          }\n          DemandedOps[OpIdx].setBit(EltIdx);\n        }\n        // Known bits are the values that are shared by every demanded element.\n        for (unsigned i = 0; i != NumOps && !Known.isUnknown(); ++i) {\n          if (!DemandedOps[i])\n            continue;\n          KnownBits Known2 =\n              DAG.computeKnownBits(Ops[i], DemandedOps[i], Depth + 1);\n          Known = KnownBits::commonBits(Known, Known2);\n        }\n      }\n    }\n  }\n}\n\nunsigned X86TargetLowering::ComputeNumSignBitsForTargetNode(\n    SDValue Op, const APInt &DemandedElts, const SelectionDAG &DAG,\n    unsigned Depth) const {\n  EVT VT = Op.getValueType();\n  unsigned VTBits = VT.getScalarSizeInBits();\n  unsigned Opcode = Op.getOpcode();\n  switch (Opcode) {\n  case X86ISD::SETCC_CARRY:\n    // SETCC_CARRY sets the dest to ~0 for true or 0 for false.\n    return VTBits;\n\n  case X86ISD::VTRUNC: {\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n    unsigned NumSrcBits = SrcVT.getScalarSizeInBits();\n    assert(VTBits < NumSrcBits && \"Illegal truncation input type\");\n    APInt DemandedSrc = DemandedElts.zextOrTrunc(SrcVT.getVectorNumElements());\n    unsigned Tmp = DAG.ComputeNumSignBits(Src, DemandedSrc, Depth + 1);\n    if (Tmp > (NumSrcBits - VTBits))\n      return Tmp - (NumSrcBits - VTBits);\n    return 1;\n  }\n\n  case X86ISD::PACKSS: {\n    // PACKSS is just a truncation if the sign bits extend to the packed size.\n    APInt DemandedLHS, DemandedRHS;\n    getPackDemandedElts(Op.getValueType(), DemandedElts, DemandedLHS,\n                        DemandedRHS);\n\n    unsigned SrcBits = Op.getOperand(0).getScalarValueSizeInBits();\n    unsigned Tmp0 = SrcBits, Tmp1 = SrcBits;\n    if (!!DemandedLHS)\n      Tmp0 = DAG.ComputeNumSignBits(Op.getOperand(0), DemandedLHS, Depth + 1);\n    if (!!DemandedRHS)\n      Tmp1 = DAG.ComputeNumSignBits(Op.getOperand(1), DemandedRHS, Depth + 1);\n    unsigned Tmp = std::min(Tmp0, Tmp1);\n    if (Tmp > (SrcBits - VTBits))\n      return Tmp - (SrcBits - VTBits);\n    return 1;\n  }\n\n  case X86ISD::VSHLI: {\n    SDValue Src = Op.getOperand(0);\n    const APInt &ShiftVal = Op.getConstantOperandAPInt(1);\n    if (ShiftVal.uge(VTBits))\n      return VTBits; // Shifted all bits out --> zero.\n    unsigned Tmp = DAG.ComputeNumSignBits(Src, DemandedElts, Depth + 1);\n    if (ShiftVal.uge(Tmp))\n      return 1; // Shifted all sign bits out --> unknown.\n    return Tmp - ShiftVal.getZExtValue();\n  }\n\n  case X86ISD::VSRAI: {\n    SDValue Src = Op.getOperand(0);\n    APInt ShiftVal = Op.getConstantOperandAPInt(1);\n    if (ShiftVal.uge(VTBits - 1))\n      return VTBits; // Sign splat.\n    unsigned Tmp = DAG.ComputeNumSignBits(Src, DemandedElts, Depth + 1);\n    ShiftVal += Tmp;\n    return ShiftVal.uge(VTBits) ? VTBits : ShiftVal.getZExtValue();\n  }\n\n  case X86ISD::PCMPGT:\n  case X86ISD::PCMPEQ:\n  case X86ISD::CMPP:\n  case X86ISD::VPCOM:\n  case X86ISD::VPCOMU:\n    // Vector compares return zero/all-bits result values.\n    return VTBits;\n\n  case X86ISD::ANDNP: {\n    unsigned Tmp0 =\n        DAG.ComputeNumSignBits(Op.getOperand(0), DemandedElts, Depth + 1);\n    if (Tmp0 == 1) return 1; // Early out.\n    unsigned Tmp1 =\n        DAG.ComputeNumSignBits(Op.getOperand(1), DemandedElts, Depth + 1);\n    return std::min(Tmp0, Tmp1);\n  }\n\n  case X86ISD::CMOV: {\n    unsigned Tmp0 = DAG.ComputeNumSignBits(Op.getOperand(0), Depth+1);\n    if (Tmp0 == 1) return 1;  // Early out.\n    unsigned Tmp1 = DAG.ComputeNumSignBits(Op.getOperand(1), Depth+1);\n    return std::min(Tmp0, Tmp1);\n  }\n  }\n\n  // Handle target shuffles.\n  // TODO - use resolveTargetShuffleInputs once we can limit recursive depth.\n  if (isTargetShuffle(Opcode)) {\n    bool IsUnary;\n    SmallVector<int, 64> Mask;\n    SmallVector<SDValue, 2> Ops;\n    if (getTargetShuffleMask(Op.getNode(), VT.getSimpleVT(), true, Ops, Mask,\n                             IsUnary)) {\n      unsigned NumOps = Ops.size();\n      unsigned NumElts = VT.getVectorNumElements();\n      if (Mask.size() == NumElts) {\n        SmallVector<APInt, 2> DemandedOps(NumOps, APInt(NumElts, 0));\n        for (unsigned i = 0; i != NumElts; ++i) {\n          if (!DemandedElts[i])\n            continue;\n          int M = Mask[i];\n          if (M == SM_SentinelUndef) {\n            // For UNDEF elements, we don't know anything about the common state\n            // of the shuffle result.\n            return 1;\n          } else if (M == SM_SentinelZero) {\n            // Zero = all sign bits.\n            continue;\n          }\n          assert(0 <= M && (unsigned)M < (NumOps * NumElts) &&\n                 \"Shuffle index out of range\");\n\n          unsigned OpIdx = (unsigned)M / NumElts;\n          unsigned EltIdx = (unsigned)M % NumElts;\n          if (Ops[OpIdx].getValueType() != VT) {\n            // TODO - handle target shuffle ops with different value types.\n            return 1;\n          }\n          DemandedOps[OpIdx].setBit(EltIdx);\n        }\n        unsigned Tmp0 = VTBits;\n        for (unsigned i = 0; i != NumOps && Tmp0 > 1; ++i) {\n          if (!DemandedOps[i])\n            continue;\n          unsigned Tmp1 =\n              DAG.ComputeNumSignBits(Ops[i], DemandedOps[i], Depth + 1);\n          Tmp0 = std::min(Tmp0, Tmp1);\n        }\n        return Tmp0;\n      }\n    }\n  }\n\n  // Fallback case.\n  return 1;\n}\n\nSDValue X86TargetLowering::unwrapAddress(SDValue N) const {\n  if (N->getOpcode() == X86ISD::Wrapper || N->getOpcode() == X86ISD::WrapperRIP)\n    return N->getOperand(0);\n  return N;\n}\n\n// Helper to look for a normal load that can be narrowed into a vzload with the\n// specified VT and memory VT. Returns SDValue() on failure.\nstatic SDValue narrowLoadToVZLoad(LoadSDNode *LN, MVT MemVT, MVT VT,\n                                  SelectionDAG &DAG) {\n  // Can't if the load is volatile or atomic.\n  if (!LN->isSimple())\n    return SDValue();\n\n  SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n  SDValue Ops[] = {LN->getChain(), LN->getBasePtr()};\n  return DAG.getMemIntrinsicNode(X86ISD::VZEXT_LOAD, SDLoc(LN), Tys, Ops, MemVT,\n                                 LN->getPointerInfo(), LN->getOriginalAlign(),\n                                 LN->getMemOperand()->getFlags());\n}\n\n// Attempt to match a combined shuffle mask against supported unary shuffle\n// instructions.\n// TODO: Investigate sharing more of this with shuffle lowering.\nstatic bool matchUnaryShuffle(MVT MaskVT, ArrayRef<int> Mask,\n                              bool AllowFloatDomain, bool AllowIntDomain,\n                              SDValue &V1, const SDLoc &DL, SelectionDAG &DAG,\n                              const X86Subtarget &Subtarget, unsigned &Shuffle,\n                              MVT &SrcVT, MVT &DstVT) {\n  unsigned NumMaskElts = Mask.size();\n  unsigned MaskEltSize = MaskVT.getScalarSizeInBits();\n\n  // Match against a VZEXT_MOVL vXi32 zero-extending instruction.\n  if (MaskEltSize == 32 && Mask[0] == 0) {\n    if (isUndefOrZero(Mask[1]) && isUndefInRange(Mask, 2, NumMaskElts - 2)) {\n      Shuffle = X86ISD::VZEXT_MOVL;\n      SrcVT = DstVT = !Subtarget.hasSSE2() ? MVT::v4f32 : MaskVT;\n      return true;\n    }\n    if (V1.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n        isUndefOrZeroInRange(Mask, 1, NumMaskElts - 1)) {\n      Shuffle = X86ISD::VZEXT_MOVL;\n      SrcVT = DstVT = !Subtarget.hasSSE2() ? MVT::v4f32 : MaskVT;\n      return true;\n    }\n  }\n\n  // Match against a ANY/ZERO_EXTEND_VECTOR_INREG instruction.\n  // TODO: Add 512-bit vector support (split AVX512F and AVX512BW).\n  if (AllowIntDomain && ((MaskVT.is128BitVector() && Subtarget.hasSSE41()) ||\n                         (MaskVT.is256BitVector() && Subtarget.hasInt256()))) {\n    unsigned MaxScale = 64 / MaskEltSize;\n    for (unsigned Scale = 2; Scale <= MaxScale; Scale *= 2) {\n      bool MatchAny = true;\n      bool MatchZero = true;\n      unsigned NumDstElts = NumMaskElts / Scale;\n      for (unsigned i = 0; i != NumDstElts && (MatchAny || MatchZero); ++i) {\n        if (!isUndefOrEqual(Mask[i * Scale], (int)i)) {\n          MatchAny = MatchZero = false;\n          break;\n        }\n        MatchAny &= isUndefInRange(Mask, (i * Scale) + 1, Scale - 1);\n        MatchZero &= isUndefOrZeroInRange(Mask, (i * Scale) + 1, Scale - 1);\n      }\n      if (MatchAny || MatchZero) {\n        assert(MatchZero && \"Failed to match zext but matched aext?\");\n        unsigned SrcSize = std::max(128u, NumDstElts * MaskEltSize);\n        MVT ScalarTy = MaskVT.isInteger() ? MaskVT.getScalarType() :\n                                            MVT::getIntegerVT(MaskEltSize);\n        SrcVT = MVT::getVectorVT(ScalarTy, SrcSize / MaskEltSize);\n\n        if (SrcVT.getSizeInBits() != MaskVT.getSizeInBits())\n          V1 = extractSubVector(V1, 0, DAG, DL, SrcSize);\n\n        Shuffle = unsigned(MatchAny ? ISD::ANY_EXTEND : ISD::ZERO_EXTEND);\n        if (SrcVT.getVectorNumElements() != NumDstElts)\n          Shuffle = getOpcode_EXTEND_VECTOR_INREG(Shuffle);\n\n        DstVT = MVT::getIntegerVT(Scale * MaskEltSize);\n        DstVT = MVT::getVectorVT(DstVT, NumDstElts);\n        return true;\n      }\n    }\n  }\n\n  // Match against a VZEXT_MOVL instruction, SSE1 only supports 32-bits (MOVSS).\n  if (((MaskEltSize == 32) || (MaskEltSize == 64 && Subtarget.hasSSE2())) &&\n      isUndefOrEqual(Mask[0], 0) &&\n      isUndefOrZeroInRange(Mask, 1, NumMaskElts - 1)) {\n    Shuffle = X86ISD::VZEXT_MOVL;\n    SrcVT = DstVT = !Subtarget.hasSSE2() ? MVT::v4f32 : MaskVT;\n    return true;\n  }\n\n  // Check if we have SSE3 which will let us use MOVDDUP etc. The\n  // instructions are no slower than UNPCKLPD but has the option to\n  // fold the input operand into even an unaligned memory load.\n  if (MaskVT.is128BitVector() && Subtarget.hasSSE3() && AllowFloatDomain) {\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0}, V1)) {\n      Shuffle = X86ISD::MOVDDUP;\n      SrcVT = DstVT = MVT::v2f64;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0, 2, 2}, V1)) {\n      Shuffle = X86ISD::MOVSLDUP;\n      SrcVT = DstVT = MVT::v4f32;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {1, 1, 3, 3}, V1)) {\n      Shuffle = X86ISD::MOVSHDUP;\n      SrcVT = DstVT = MVT::v4f32;\n      return true;\n    }\n  }\n\n  if (MaskVT.is256BitVector() && AllowFloatDomain) {\n    assert(Subtarget.hasAVX() && \"AVX required for 256-bit vector shuffles\");\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0, 2, 2}, V1)) {\n      Shuffle = X86ISD::MOVDDUP;\n      SrcVT = DstVT = MVT::v4f64;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0, 2, 2, 4, 4, 6, 6}, V1)) {\n      Shuffle = X86ISD::MOVSLDUP;\n      SrcVT = DstVT = MVT::v8f32;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {1, 1, 3, 3, 5, 5, 7, 7}, V1)) {\n      Shuffle = X86ISD::MOVSHDUP;\n      SrcVT = DstVT = MVT::v8f32;\n      return true;\n    }\n  }\n\n  if (MaskVT.is512BitVector() && AllowFloatDomain) {\n    assert(Subtarget.hasAVX512() &&\n           \"AVX512 required for 512-bit vector shuffles\");\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0, 2, 2, 4, 4, 6, 6}, V1)) {\n      Shuffle = X86ISD::MOVDDUP;\n      SrcVT = DstVT = MVT::v8f64;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(\n            MaskVT, Mask,\n            {0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14}, V1)) {\n      Shuffle = X86ISD::MOVSLDUP;\n      SrcVT = DstVT = MVT::v16f32;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(\n            MaskVT, Mask,\n            {1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15}, V1)) {\n      Shuffle = X86ISD::MOVSHDUP;\n      SrcVT = DstVT = MVT::v16f32;\n      return true;\n    }\n  }\n\n  return false;\n}\n\n// Attempt to match a combined shuffle mask against supported unary immediate\n// permute instructions.\n// TODO: Investigate sharing more of this with shuffle lowering.\nstatic bool matchUnaryPermuteShuffle(MVT MaskVT, ArrayRef<int> Mask,\n                                     const APInt &Zeroable,\n                                     bool AllowFloatDomain, bool AllowIntDomain,\n                                     const X86Subtarget &Subtarget,\n                                     unsigned &Shuffle, MVT &ShuffleVT,\n                                     unsigned &PermuteImm) {\n  unsigned NumMaskElts = Mask.size();\n  unsigned InputSizeInBits = MaskVT.getSizeInBits();\n  unsigned MaskScalarSizeInBits = InputSizeInBits / NumMaskElts;\n  MVT MaskEltVT = MVT::getIntegerVT(MaskScalarSizeInBits);\n  bool ContainsZeros = isAnyZero(Mask);\n\n  // Handle VPERMI/VPERMILPD vXi64/vXi64 patterns.\n  if (!ContainsZeros && MaskScalarSizeInBits == 64) {\n    // Check for lane crossing permutes.\n    if (is128BitLaneCrossingShuffleMask(MaskEltVT, Mask)) {\n      // PERMPD/PERMQ permutes within a 256-bit vector (AVX2+).\n      if (Subtarget.hasAVX2() && MaskVT.is256BitVector()) {\n        Shuffle = X86ISD::VPERMI;\n        ShuffleVT = (AllowFloatDomain ? MVT::v4f64 : MVT::v4i64);\n        PermuteImm = getV4X86ShuffleImm(Mask);\n        return true;\n      }\n      if (Subtarget.hasAVX512() && MaskVT.is512BitVector()) {\n        SmallVector<int, 4> RepeatedMask;\n        if (is256BitLaneRepeatedShuffleMask(MVT::v8f64, Mask, RepeatedMask)) {\n          Shuffle = X86ISD::VPERMI;\n          ShuffleVT = (AllowFloatDomain ? MVT::v8f64 : MVT::v8i64);\n          PermuteImm = getV4X86ShuffleImm(RepeatedMask);\n          return true;\n        }\n      }\n    } else if (AllowFloatDomain && Subtarget.hasAVX()) {\n      // VPERMILPD can permute with a non-repeating shuffle.\n      Shuffle = X86ISD::VPERMILPI;\n      ShuffleVT = MVT::getVectorVT(MVT::f64, Mask.size());\n      PermuteImm = 0;\n      for (int i = 0, e = Mask.size(); i != e; ++i) {\n        int M = Mask[i];\n        if (M == SM_SentinelUndef)\n          continue;\n        assert(((M / 2) == (i / 2)) && \"Out of range shuffle mask index\");\n        PermuteImm |= (M & 1) << i;\n      }\n      return true;\n    }\n  }\n\n  // Handle PSHUFD/VPERMILPI vXi32/vXf32 repeated patterns.\n  // AVX introduced the VPERMILPD/VPERMILPS float permutes, before then we\n  // had to use 2-input SHUFPD/SHUFPS shuffles (not handled here).\n  if ((MaskScalarSizeInBits == 64 || MaskScalarSizeInBits == 32) &&\n      !ContainsZeros && (AllowIntDomain || Subtarget.hasAVX())) {\n    SmallVector<int, 4> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MaskEltVT, Mask, RepeatedMask)) {\n      // Narrow the repeated mask to create 32-bit element permutes.\n      SmallVector<int, 4> WordMask = RepeatedMask;\n      if (MaskScalarSizeInBits == 64)\n        narrowShuffleMaskElts(2, RepeatedMask, WordMask);\n\n      Shuffle = (AllowIntDomain ? X86ISD::PSHUFD : X86ISD::VPERMILPI);\n      ShuffleVT = (AllowIntDomain ? MVT::i32 : MVT::f32);\n      ShuffleVT = MVT::getVectorVT(ShuffleVT, InputSizeInBits / 32);\n      PermuteImm = getV4X86ShuffleImm(WordMask);\n      return true;\n    }\n  }\n\n  // Handle PSHUFLW/PSHUFHW vXi16 repeated patterns.\n  if (!ContainsZeros && AllowIntDomain && MaskScalarSizeInBits == 16 &&\n      ((MaskVT.is128BitVector() && Subtarget.hasSSE2()) ||\n       (MaskVT.is256BitVector() && Subtarget.hasAVX2()) ||\n       (MaskVT.is512BitVector() && Subtarget.hasBWI()))) {\n    SmallVector<int, 4> RepeatedMask;\n    if (is128BitLaneRepeatedShuffleMask(MaskEltVT, Mask, RepeatedMask)) {\n      ArrayRef<int> LoMask(RepeatedMask.data() + 0, 4);\n      ArrayRef<int> HiMask(RepeatedMask.data() + 4, 4);\n\n      // PSHUFLW: permute lower 4 elements only.\n      if (isUndefOrInRange(LoMask, 0, 4) &&\n          isSequentialOrUndefInRange(HiMask, 0, 4, 4)) {\n        Shuffle = X86ISD::PSHUFLW;\n        ShuffleVT = MVT::getVectorVT(MVT::i16, InputSizeInBits / 16);\n        PermuteImm = getV4X86ShuffleImm(LoMask);\n        return true;\n      }\n\n      // PSHUFHW: permute upper 4 elements only.\n      if (isUndefOrInRange(HiMask, 4, 8) &&\n          isSequentialOrUndefInRange(LoMask, 0, 4, 0)) {\n        // Offset the HiMask so that we can create the shuffle immediate.\n        int OffsetHiMask[4];\n        for (int i = 0; i != 4; ++i)\n          OffsetHiMask[i] = (HiMask[i] < 0 ? HiMask[i] : HiMask[i] - 4);\n\n        Shuffle = X86ISD::PSHUFHW;\n        ShuffleVT = MVT::getVectorVT(MVT::i16, InputSizeInBits / 16);\n        PermuteImm = getV4X86ShuffleImm(OffsetHiMask);\n        return true;\n      }\n    }\n  }\n\n  // Attempt to match against byte/bit shifts.\n  if (AllowIntDomain &&\n      ((MaskVT.is128BitVector() && Subtarget.hasSSE2()) ||\n       (MaskVT.is256BitVector() && Subtarget.hasAVX2()) ||\n       (MaskVT.is512BitVector() && Subtarget.hasAVX512()))) {\n    int ShiftAmt = matchShuffleAsShift(ShuffleVT, Shuffle, MaskScalarSizeInBits,\n                                       Mask, 0, Zeroable, Subtarget);\n    if (0 < ShiftAmt && (!ShuffleVT.is512BitVector() || Subtarget.hasBWI() ||\n                         32 <= ShuffleVT.getScalarSizeInBits())) {\n      PermuteImm = (unsigned)ShiftAmt;\n      return true;\n    }\n  }\n\n  // Attempt to match against bit rotates.\n  if (!ContainsZeros && AllowIntDomain && MaskScalarSizeInBits < 64 &&\n      ((MaskVT.is128BitVector() && Subtarget.hasXOP()) ||\n       Subtarget.hasAVX512())) {\n    int RotateAmt = matchShuffleAsBitRotate(ShuffleVT, MaskScalarSizeInBits,\n                                            Subtarget, Mask);\n    if (0 < RotateAmt) {\n      Shuffle = X86ISD::VROTLI;\n      PermuteImm = (unsigned)RotateAmt;\n      return true;\n    }\n  }\n\n  return false;\n}\n\n// Attempt to match a combined unary shuffle mask against supported binary\n// shuffle instructions.\n// TODO: Investigate sharing more of this with shuffle lowering.\nstatic bool matchBinaryShuffle(MVT MaskVT, ArrayRef<int> Mask,\n                               bool AllowFloatDomain, bool AllowIntDomain,\n                               SDValue &V1, SDValue &V2, const SDLoc &DL,\n                               SelectionDAG &DAG, const X86Subtarget &Subtarget,\n                               unsigned &Shuffle, MVT &SrcVT, MVT &DstVT,\n                               bool IsUnary) {\n  unsigned NumMaskElts = Mask.size();\n  unsigned EltSizeInBits = MaskVT.getScalarSizeInBits();\n\n  if (MaskVT.is128BitVector()) {\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 0}) && AllowFloatDomain) {\n      V2 = V1;\n      V1 = (SM_SentinelUndef == Mask[0] ? DAG.getUNDEF(MVT::v4f32) : V1);\n      Shuffle = Subtarget.hasSSE2() ? X86ISD::UNPCKL : X86ISD::MOVLHPS;\n      SrcVT = DstVT = Subtarget.hasSSE2() ? MVT::v2f64 : MVT::v4f32;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {1, 1}) && AllowFloatDomain) {\n      V2 = V1;\n      Shuffle = Subtarget.hasSSE2() ? X86ISD::UNPCKH : X86ISD::MOVHLPS;\n      SrcVT = DstVT = Subtarget.hasSSE2() ? MVT::v2f64 : MVT::v4f32;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {0, 3}) &&\n        Subtarget.hasSSE2() && (AllowFloatDomain || !Subtarget.hasSSE41())) {\n      std::swap(V1, V2);\n      Shuffle = X86ISD::MOVSD;\n      SrcVT = DstVT = MVT::v2f64;\n      return true;\n    }\n    if (isTargetShuffleEquivalent(MaskVT, Mask, {4, 1, 2, 3}) &&\n        (AllowFloatDomain || !Subtarget.hasSSE41())) {\n      Shuffle = X86ISD::MOVSS;\n      SrcVT = DstVT = MVT::v4f32;\n      return true;\n    }\n  }\n\n  // Attempt to match against either an unary or binary PACKSS/PACKUS shuffle.\n  if (((MaskVT == MVT::v8i16 || MaskVT == MVT::v16i8) && Subtarget.hasSSE2()) ||\n      ((MaskVT == MVT::v16i16 || MaskVT == MVT::v32i8) && Subtarget.hasInt256()) ||\n      ((MaskVT == MVT::v32i16 || MaskVT == MVT::v64i8) && Subtarget.hasBWI())) {\n    if (matchShuffleWithPACK(MaskVT, SrcVT, V1, V2, Shuffle, Mask, DAG,\n                             Subtarget)) {\n      DstVT = MaskVT;\n      return true;\n    }\n  }\n\n  // Attempt to match against either a unary or binary UNPCKL/UNPCKH shuffle.\n  if ((MaskVT == MVT::v4f32 && Subtarget.hasSSE1()) ||\n      (MaskVT.is128BitVector() && Subtarget.hasSSE2()) ||\n      (MaskVT.is256BitVector() && 32 <= EltSizeInBits && Subtarget.hasAVX()) ||\n      (MaskVT.is256BitVector() && Subtarget.hasAVX2()) ||\n      (MaskVT.is512BitVector() && Subtarget.hasAVX512())) {\n    if (matchShuffleWithUNPCK(MaskVT, V1, V2, Shuffle, IsUnary, Mask, DL, DAG,\n                              Subtarget)) {\n      SrcVT = DstVT = MaskVT;\n      if (MaskVT.is256BitVector() && !Subtarget.hasAVX2())\n        SrcVT = DstVT = (32 == EltSizeInBits ? MVT::v8f32 : MVT::v4f64);\n      return true;\n    }\n  }\n\n  // Attempt to match against a OR if we're performing a blend shuffle and the\n  // non-blended source element is zero in each case.\n  if ((EltSizeInBits % V1.getScalarValueSizeInBits()) == 0 &&\n      (EltSizeInBits % V2.getScalarValueSizeInBits()) == 0) {\n    bool IsBlend = true;\n    unsigned NumV1Elts = V1.getValueType().getVectorNumElements();\n    unsigned NumV2Elts = V2.getValueType().getVectorNumElements();\n    unsigned Scale1 = NumV1Elts / NumMaskElts;\n    unsigned Scale2 = NumV2Elts / NumMaskElts;\n    APInt DemandedZeroV1 = APInt::getNullValue(NumV1Elts);\n    APInt DemandedZeroV2 = APInt::getNullValue(NumV2Elts);\n    for (unsigned i = 0; i != NumMaskElts; ++i) {\n      int M = Mask[i];\n      if (M == SM_SentinelUndef)\n        continue;\n      if (M == SM_SentinelZero) {\n        DemandedZeroV1.setBits(i * Scale1, (i + 1) * Scale1);\n        DemandedZeroV2.setBits(i * Scale2, (i + 1) * Scale2);\n        continue;\n      }\n      if (M == (int)i) {\n        DemandedZeroV2.setBits(i * Scale2, (i + 1) * Scale2);\n        continue;\n      }\n      if (M == (int)(i + NumMaskElts)) {\n        DemandedZeroV1.setBits(i * Scale1, (i + 1) * Scale1);\n        continue;\n      }\n      IsBlend = false;\n      break;\n    }\n    if (IsBlend &&\n        DAG.computeKnownBits(V1, DemandedZeroV1).isZero() &&\n        DAG.computeKnownBits(V2, DemandedZeroV2).isZero()) {\n      Shuffle = ISD::OR;\n      SrcVT = DstVT = MaskVT.changeTypeToInteger();\n      return true;\n    }\n  }\n\n  return false;\n}\n\nstatic bool matchBinaryPermuteShuffle(\n    MVT MaskVT, ArrayRef<int> Mask, const APInt &Zeroable,\n    bool AllowFloatDomain, bool AllowIntDomain, SDValue &V1, SDValue &V2,\n    const SDLoc &DL, SelectionDAG &DAG, const X86Subtarget &Subtarget,\n    unsigned &Shuffle, MVT &ShuffleVT, unsigned &PermuteImm) {\n  unsigned NumMaskElts = Mask.size();\n  unsigned EltSizeInBits = MaskVT.getScalarSizeInBits();\n\n  // Attempt to match against VALIGND/VALIGNQ rotate.\n  if (AllowIntDomain && (EltSizeInBits == 64 || EltSizeInBits == 32) &&\n      ((MaskVT.is128BitVector() && Subtarget.hasVLX()) ||\n       (MaskVT.is256BitVector() && Subtarget.hasVLX()) ||\n       (MaskVT.is512BitVector() && Subtarget.hasAVX512()))) {\n    if (!isAnyZero(Mask)) {\n      int Rotation = matchShuffleAsElementRotate(V1, V2, Mask);\n      if (0 < Rotation) {\n        Shuffle = X86ISD::VALIGN;\n        if (EltSizeInBits == 64)\n          ShuffleVT = MVT::getVectorVT(MVT::i64, MaskVT.getSizeInBits() / 64);\n        else\n          ShuffleVT = MVT::getVectorVT(MVT::i32, MaskVT.getSizeInBits() / 32);\n        PermuteImm = Rotation;\n        return true;\n      }\n    }\n  }\n\n  // Attempt to match against PALIGNR byte rotate.\n  if (AllowIntDomain && ((MaskVT.is128BitVector() && Subtarget.hasSSSE3()) ||\n                         (MaskVT.is256BitVector() && Subtarget.hasAVX2()) ||\n                         (MaskVT.is512BitVector() && Subtarget.hasBWI()))) {\n    int ByteRotation = matchShuffleAsByteRotate(MaskVT, V1, V2, Mask);\n    if (0 < ByteRotation) {\n      Shuffle = X86ISD::PALIGNR;\n      ShuffleVT = MVT::getVectorVT(MVT::i8, MaskVT.getSizeInBits() / 8);\n      PermuteImm = ByteRotation;\n      return true;\n    }\n  }\n\n  // Attempt to combine to X86ISD::BLENDI.\n  if ((NumMaskElts <= 8 && ((Subtarget.hasSSE41() && MaskVT.is128BitVector()) ||\n                            (Subtarget.hasAVX() && MaskVT.is256BitVector()))) ||\n      (MaskVT == MVT::v16i16 && Subtarget.hasAVX2())) {\n    uint64_t BlendMask = 0;\n    bool ForceV1Zero = false, ForceV2Zero = false;\n    SmallVector<int, 8> TargetMask(Mask.begin(), Mask.end());\n    if (matchShuffleAsBlend(V1, V2, TargetMask, Zeroable, ForceV1Zero,\n                            ForceV2Zero, BlendMask)) {\n      if (MaskVT == MVT::v16i16) {\n        // We can only use v16i16 PBLENDW if the lanes are repeated.\n        SmallVector<int, 8> RepeatedMask;\n        if (isRepeatedTargetShuffleMask(128, MaskVT, TargetMask,\n                                        RepeatedMask)) {\n          assert(RepeatedMask.size() == 8 &&\n                 \"Repeated mask size doesn't match!\");\n          PermuteImm = 0;\n          for (int i = 0; i < 8; ++i)\n            if (RepeatedMask[i] >= 8)\n              PermuteImm |= 1 << i;\n          V1 = ForceV1Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V1;\n          V2 = ForceV2Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V2;\n          Shuffle = X86ISD::BLENDI;\n          ShuffleVT = MaskVT;\n          return true;\n        }\n      } else {\n        V1 = ForceV1Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V1;\n        V2 = ForceV2Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V2;\n        PermuteImm = (unsigned)BlendMask;\n        Shuffle = X86ISD::BLENDI;\n        ShuffleVT = MaskVT;\n        return true;\n      }\n    }\n  }\n\n  // Attempt to combine to INSERTPS, but only if it has elements that need to\n  // be set to zero.\n  if (AllowFloatDomain && EltSizeInBits == 32 && Subtarget.hasSSE41() &&\n      MaskVT.is128BitVector() && isAnyZero(Mask) &&\n      matchShuffleAsInsertPS(V1, V2, PermuteImm, Zeroable, Mask, DAG)) {\n    Shuffle = X86ISD::INSERTPS;\n    ShuffleVT = MVT::v4f32;\n    return true;\n  }\n\n  // Attempt to combine to SHUFPD.\n  if (AllowFloatDomain && EltSizeInBits == 64 &&\n      ((MaskVT.is128BitVector() && Subtarget.hasSSE2()) ||\n       (MaskVT.is256BitVector() && Subtarget.hasAVX()) ||\n       (MaskVT.is512BitVector() && Subtarget.hasAVX512()))) {\n    bool ForceV1Zero = false, ForceV2Zero = false;\n    if (matchShuffleWithSHUFPD(MaskVT, V1, V2, ForceV1Zero, ForceV2Zero,\n                               PermuteImm, Mask, Zeroable)) {\n      V1 = ForceV1Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V1;\n      V2 = ForceV2Zero ? getZeroVector(MaskVT, Subtarget, DAG, DL) : V2;\n      Shuffle = X86ISD::SHUFP;\n      ShuffleVT = MVT::getVectorVT(MVT::f64, MaskVT.getSizeInBits() / 64);\n      return true;\n    }\n  }\n\n  // Attempt to combine to SHUFPS.\n  if (AllowFloatDomain && EltSizeInBits == 32 &&\n      ((MaskVT.is128BitVector() && Subtarget.hasSSE1()) ||\n       (MaskVT.is256BitVector() && Subtarget.hasAVX()) ||\n       (MaskVT.is512BitVector() && Subtarget.hasAVX512()))) {\n    SmallVector<int, 4> RepeatedMask;\n    if (isRepeatedTargetShuffleMask(128, MaskVT, Mask, RepeatedMask)) {\n      // Match each half of the repeated mask, to determine if its just\n      // referencing one of the vectors, is zeroable or entirely undef.\n      auto MatchHalf = [&](unsigned Offset, int &S0, int &S1) {\n        int M0 = RepeatedMask[Offset];\n        int M1 = RepeatedMask[Offset + 1];\n\n        if (isUndefInRange(RepeatedMask, Offset, 2)) {\n          return DAG.getUNDEF(MaskVT);\n        } else if (isUndefOrZeroInRange(RepeatedMask, Offset, 2)) {\n          S0 = (SM_SentinelUndef == M0 ? -1 : 0);\n          S1 = (SM_SentinelUndef == M1 ? -1 : 1);\n          return getZeroVector(MaskVT, Subtarget, DAG, DL);\n        } else if (isUndefOrInRange(M0, 0, 4) && isUndefOrInRange(M1, 0, 4)) {\n          S0 = (SM_SentinelUndef == M0 ? -1 : M0 & 3);\n          S1 = (SM_SentinelUndef == M1 ? -1 : M1 & 3);\n          return V1;\n        } else if (isUndefOrInRange(M0, 4, 8) && isUndefOrInRange(M1, 4, 8)) {\n          S0 = (SM_SentinelUndef == M0 ? -1 : M0 & 3);\n          S1 = (SM_SentinelUndef == M1 ? -1 : M1 & 3);\n          return V2;\n        }\n\n        return SDValue();\n      };\n\n      int ShufMask[4] = {-1, -1, -1, -1};\n      SDValue Lo = MatchHalf(0, ShufMask[0], ShufMask[1]);\n      SDValue Hi = MatchHalf(2, ShufMask[2], ShufMask[3]);\n\n      if (Lo && Hi) {\n        V1 = Lo;\n        V2 = Hi;\n        Shuffle = X86ISD::SHUFP;\n        ShuffleVT = MVT::getVectorVT(MVT::f32, MaskVT.getSizeInBits() / 32);\n        PermuteImm = getV4X86ShuffleImm(ShufMask);\n        return true;\n      }\n    }\n  }\n\n  // Attempt to combine to INSERTPS more generally if X86ISD::SHUFP failed.\n  if (AllowFloatDomain && EltSizeInBits == 32 && Subtarget.hasSSE41() &&\n      MaskVT.is128BitVector() &&\n      matchShuffleAsInsertPS(V1, V2, PermuteImm, Zeroable, Mask, DAG)) {\n    Shuffle = X86ISD::INSERTPS;\n    ShuffleVT = MVT::v4f32;\n    return true;\n  }\n\n  return false;\n}\n\nstatic SDValue combineX86ShuffleChainWithExtract(\n    ArrayRef<SDValue> Inputs, SDValue Root, ArrayRef<int> BaseMask, int Depth,\n    bool HasVariableMask, bool AllowVariableMask, SelectionDAG &DAG,\n    const X86Subtarget &Subtarget);\n\n/// Combine an arbitrary chain of shuffles into a single instruction if\n/// possible.\n///\n/// This is the leaf of the recursive combine below. When we have found some\n/// chain of single-use x86 shuffle instructions and accumulated the combined\n/// shuffle mask represented by them, this will try to pattern match that mask\n/// into either a single instruction if there is a special purpose instruction\n/// for this operation, or into a PSHUFB instruction which is a fully general\n/// instruction but should only be used to replace chains over a certain depth.\nstatic SDValue combineX86ShuffleChain(ArrayRef<SDValue> Inputs, SDValue Root,\n                                      ArrayRef<int> BaseMask, int Depth,\n                                      bool HasVariableMask,\n                                      bool AllowVariableMask, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  assert(!BaseMask.empty() && \"Cannot combine an empty shuffle mask!\");\n  assert((Inputs.size() == 1 || Inputs.size() == 2) &&\n         \"Unexpected number of shuffle inputs!\");\n\n  MVT RootVT = Root.getSimpleValueType();\n  unsigned RootSizeInBits = RootVT.getSizeInBits();\n  unsigned NumRootElts = RootVT.getVectorNumElements();\n\n  // Canonicalize shuffle input op to the requested type.\n  // TODO: Support cases where Op is smaller than VT.\n  auto CanonicalizeShuffleInput = [&](MVT VT, SDValue Op) {\n    return DAG.getBitcast(VT, Op);\n  };\n\n  // Find the inputs that enter the chain. Note that multiple uses are OK\n  // here, we're not going to remove the operands we find.\n  bool UnaryShuffle = (Inputs.size() == 1);\n  SDValue V1 = peekThroughBitcasts(Inputs[0]);\n  SDValue V2 = (UnaryShuffle ? DAG.getUNDEF(V1.getValueType())\n                             : peekThroughBitcasts(Inputs[1]));\n\n  MVT VT1 = V1.getSimpleValueType();\n  MVT VT2 = V2.getSimpleValueType();\n  assert(VT1.getSizeInBits() == RootSizeInBits &&\n         VT2.getSizeInBits() == RootSizeInBits && \"Vector size mismatch\");\n\n  SDLoc DL(Root);\n  SDValue Res;\n\n  unsigned NumBaseMaskElts = BaseMask.size();\n  if (NumBaseMaskElts == 1) {\n    assert(BaseMask[0] == 0 && \"Invalid shuffle index found!\");\n    return CanonicalizeShuffleInput(RootVT, V1);\n  }\n\n  bool OptForSize = DAG.shouldOptForSize();\n  unsigned BaseMaskEltSizeInBits = RootSizeInBits / NumBaseMaskElts;\n  bool FloatDomain = VT1.isFloatingPoint() || VT2.isFloatingPoint() ||\n                     (RootVT.isFloatingPoint() && Depth >= 1) ||\n                     (RootVT.is256BitVector() && !Subtarget.hasAVX2());\n\n  // Don't combine if we are a AVX512/EVEX target and the mask element size\n  // is different from the root element size - this would prevent writemasks\n  // from being reused.\n  bool IsMaskedShuffle = false;\n  if (RootSizeInBits == 512 || (Subtarget.hasVLX() && RootSizeInBits >= 128)) {\n    if (Root.hasOneUse() && Root->use_begin()->getOpcode() == ISD::VSELECT &&\n        Root->use_begin()->getOperand(0).getScalarValueSizeInBits() == 1) {\n      IsMaskedShuffle = true;\n    }\n  }\n\n  // If we are shuffling a broadcast (and not introducing zeros) then\n  // we can just use the broadcast directly. This works for smaller broadcast\n  // elements as well as they already repeat across each mask element\n  if (UnaryShuffle && isTargetShuffleSplat(V1) && !isAnyZero(BaseMask) &&\n      (BaseMaskEltSizeInBits % V1.getScalarValueSizeInBits()) == 0 &&\n      V1.getValueSizeInBits() >= RootSizeInBits) {\n    return CanonicalizeShuffleInput(RootVT, V1);\n  }\n\n  // Handle 128/256-bit lane shuffles of 512-bit vectors.\n  if (RootVT.is512BitVector() &&\n      (NumBaseMaskElts == 2 || NumBaseMaskElts == 4)) {\n    // If the upper subvectors are zeroable, then an extract+insert is more\n    // optimal than using X86ISD::SHUF128. The insertion is free, even if it has\n    // to zero the upper subvectors.\n    if (isUndefOrZeroInRange(BaseMask, 1, NumBaseMaskElts - 1)) {\n      if (Depth == 0 && Root.getOpcode() == ISD::INSERT_SUBVECTOR)\n        return SDValue(); // Nothing to do!\n      assert(isInRange(BaseMask[0], 0, NumBaseMaskElts) &&\n             \"Unexpected lane shuffle\");\n      Res = CanonicalizeShuffleInput(RootVT, V1);\n      unsigned SubIdx = BaseMask[0] * (NumRootElts / NumBaseMaskElts);\n      bool UseZero = isAnyZero(BaseMask);\n      Res = extractSubVector(Res, SubIdx, DAG, DL, BaseMaskEltSizeInBits);\n      return widenSubVector(Res, UseZero, Subtarget, DAG, DL, RootSizeInBits);\n    }\n\n    // Narrow shuffle mask to v4x128.\n    SmallVector<int, 4> Mask;\n    assert((BaseMaskEltSizeInBits % 128) == 0 && \"Illegal mask size\");\n    narrowShuffleMaskElts(BaseMaskEltSizeInBits / 128, BaseMask, Mask);\n\n    // Try to lower to vshuf64x2/vshuf32x4.\n    auto MatchSHUF128 = [&](MVT ShuffleVT, const SDLoc &DL, ArrayRef<int> Mask,\n                            SDValue V1, SDValue V2, SelectionDAG &DAG) {\n      unsigned PermMask = 0;\n      // Insure elements came from the same Op.\n      SDValue Ops[2] = {DAG.getUNDEF(ShuffleVT), DAG.getUNDEF(ShuffleVT)};\n      for (int i = 0; i < 4; ++i) {\n        assert(Mask[i] >= -1 && \"Illegal shuffle sentinel value\");\n        if (Mask[i] < 0)\n          continue;\n\n        SDValue Op = Mask[i] >= 4 ? V2 : V1;\n        unsigned OpIndex = i / 2;\n        if (Ops[OpIndex].isUndef())\n          Ops[OpIndex] = Op;\n        else if (Ops[OpIndex] != Op)\n          return SDValue();\n\n        // Convert the 128-bit shuffle mask selection values into 128-bit\n        // selection bits defined by a vshuf64x2 instruction's immediate control\n        // byte.\n        PermMask |= (Mask[i] % 4) << (i * 2);\n      }\n\n      return DAG.getNode(X86ISD::SHUF128, DL, ShuffleVT,\n                         CanonicalizeShuffleInput(ShuffleVT, Ops[0]),\n                         CanonicalizeShuffleInput(ShuffleVT, Ops[1]),\n                         DAG.getTargetConstant(PermMask, DL, MVT::i8));\n    };\n\n    // FIXME: Is there a better way to do this? is256BitLaneRepeatedShuffleMask\n    // doesn't work because our mask is for 128 bits and we don't have an MVT\n    // to match that.\n    bool PreferPERMQ =\n        UnaryShuffle && isUndefOrInRange(Mask[0], 0, 2) &&\n        isUndefOrInRange(Mask[1], 0, 2) && isUndefOrInRange(Mask[2], 2, 4) &&\n        isUndefOrInRange(Mask[3], 2, 4) &&\n        (Mask[0] < 0 || Mask[2] < 0 || Mask[0] == (Mask[2] % 2)) &&\n        (Mask[1] < 0 || Mask[3] < 0 || Mask[1] == (Mask[3] % 2));\n\n    if (!isAnyZero(Mask) && !PreferPERMQ) {\n      if (Depth == 0 && Root.getOpcode() == X86ISD::SHUF128)\n        return SDValue(); // Nothing to do!\n      MVT ShuffleVT = (FloatDomain ? MVT::v8f64 : MVT::v8i64);\n      if (SDValue V = MatchSHUF128(ShuffleVT, DL, Mask, V1, V2, DAG))\n        return DAG.getBitcast(RootVT, V);\n    }\n  }\n\n  // Handle 128-bit lane shuffles of 256-bit vectors.\n  if (RootVT.is256BitVector() && NumBaseMaskElts == 2) {\n    // If the upper half is zeroable, then an extract+insert is more optimal\n    // than using X86ISD::VPERM2X128. The insertion is free, even if it has to\n    // zero the upper half.\n    if (isUndefOrZero(BaseMask[1])) {\n      if (Depth == 0 && Root.getOpcode() == ISD::INSERT_SUBVECTOR)\n        return SDValue(); // Nothing to do!\n      assert(isInRange(BaseMask[0], 0, 2) && \"Unexpected lane shuffle\");\n      Res = CanonicalizeShuffleInput(RootVT, V1);\n      Res = extract128BitVector(Res, BaseMask[0] * (NumRootElts / 2), DAG, DL);\n      return widenSubVector(Res, BaseMask[1] == SM_SentinelZero, Subtarget, DAG,\n                            DL, 256);\n    }\n\n    if (Depth == 0 && Root.getOpcode() == X86ISD::VPERM2X128)\n      return SDValue(); // Nothing to do!\n\n    // If we have AVX2, prefer to use VPERMQ/VPERMPD for unary shuffles unless\n    // we need to use the zeroing feature.\n    // Prefer blends for sequential shuffles unless we are optimizing for size.\n    if (UnaryShuffle &&\n        !(Subtarget.hasAVX2() && isUndefOrInRange(BaseMask, 0, 2)) &&\n        (OptForSize || !isSequentialOrUndefOrZeroInRange(BaseMask, 0, 2, 0))) {\n      unsigned PermMask = 0;\n      PermMask |= ((BaseMask[0] < 0 ? 0x8 : (BaseMask[0] & 1)) << 0);\n      PermMask |= ((BaseMask[1] < 0 ? 0x8 : (BaseMask[1] & 1)) << 4);\n      return DAG.getNode(\n          X86ISD::VPERM2X128, DL, RootVT, CanonicalizeShuffleInput(RootVT, V1),\n          DAG.getUNDEF(RootVT), DAG.getTargetConstant(PermMask, DL, MVT::i8));\n    }\n\n    if (Depth == 0 && Root.getOpcode() == X86ISD::SHUF128)\n      return SDValue(); // Nothing to do!\n\n    // TODO - handle AVX512VL cases with X86ISD::SHUF128.\n    if (!UnaryShuffle && !IsMaskedShuffle) {\n      assert(llvm::all_of(BaseMask, [](int M) { return 0 <= M && M < 4; }) &&\n             \"Unexpected shuffle sentinel value\");\n      // Prefer blends to X86ISD::VPERM2X128.\n      if (!((BaseMask[0] == 0 && BaseMask[1] == 3) ||\n            (BaseMask[0] == 2 && BaseMask[1] == 1))) {\n        unsigned PermMask = 0;\n        PermMask |= ((BaseMask[0] & 3) << 0);\n        PermMask |= ((BaseMask[1] & 3) << 4);\n        SDValue LHS = isInRange(BaseMask[0], 0, 2) ? V1 : V2;\n        SDValue RHS = isInRange(BaseMask[1], 0, 2) ? V1 : V2;\n        return DAG.getNode(X86ISD::VPERM2X128, DL, RootVT,\n                          CanonicalizeShuffleInput(RootVT, LHS),\n                          CanonicalizeShuffleInput(RootVT, RHS),\n                          DAG.getTargetConstant(PermMask, DL, MVT::i8));\n      }\n    }\n  }\n\n  // For masks that have been widened to 128-bit elements or more,\n  // narrow back down to 64-bit elements.\n  SmallVector<int, 64> Mask;\n  if (BaseMaskEltSizeInBits > 64) {\n    assert((BaseMaskEltSizeInBits % 64) == 0 && \"Illegal mask size\");\n    int MaskScale = BaseMaskEltSizeInBits / 64;\n    narrowShuffleMaskElts(MaskScale, BaseMask, Mask);\n  } else {\n    Mask.assign(BaseMask.begin(), BaseMask.end());\n  }\n\n  // For masked shuffles, we're trying to match the root width for better\n  // writemask folding, attempt to scale the mask.\n  // TODO - variable shuffles might need this to be widened again.\n  if (IsMaskedShuffle && NumRootElts > Mask.size()) {\n    assert((NumRootElts % Mask.size()) == 0 && \"Illegal mask size\");\n    int MaskScale = NumRootElts / Mask.size();\n    SmallVector<int, 64> ScaledMask;\n    narrowShuffleMaskElts(MaskScale, Mask, ScaledMask);\n    Mask = std::move(ScaledMask);\n  }\n\n  unsigned NumMaskElts = Mask.size();\n  unsigned MaskEltSizeInBits = RootSizeInBits / NumMaskElts;\n\n  // Determine the effective mask value type.\n  FloatDomain &= (32 <= MaskEltSizeInBits);\n  MVT MaskVT = FloatDomain ? MVT::getFloatingPointVT(MaskEltSizeInBits)\n                           : MVT::getIntegerVT(MaskEltSizeInBits);\n  MaskVT = MVT::getVectorVT(MaskVT, NumMaskElts);\n\n  // Only allow legal mask types.\n  if (!DAG.getTargetLoweringInfo().isTypeLegal(MaskVT))\n    return SDValue();\n\n  // Attempt to match the mask against known shuffle patterns.\n  MVT ShuffleSrcVT, ShuffleVT;\n  unsigned Shuffle, PermuteImm;\n\n  // Which shuffle domains are permitted?\n  // Permit domain crossing at higher combine depths.\n  // TODO: Should we indicate which domain is preferred if both are allowed?\n  bool AllowFloatDomain = FloatDomain || (Depth >= 3);\n  bool AllowIntDomain = (!FloatDomain || (Depth >= 3)) && Subtarget.hasSSE2() &&\n                        (!MaskVT.is256BitVector() || Subtarget.hasAVX2());\n\n  // Determine zeroable mask elements.\n  APInt KnownUndef, KnownZero;\n  resolveZeroablesFromTargetShuffle(Mask, KnownUndef, KnownZero);\n  APInt Zeroable = KnownUndef | KnownZero;\n\n  if (UnaryShuffle) {\n    // Attempt to match against broadcast-from-vector.\n    // Limit AVX1 to cases where we're loading+broadcasting a scalar element.\n    if ((Subtarget.hasAVX2() ||\n         (Subtarget.hasAVX() && 32 <= MaskEltSizeInBits)) &&\n        (!IsMaskedShuffle || NumRootElts == NumMaskElts)) {\n      if (isUndefOrEqual(Mask, 0)) {\n        if (V1.getValueType() == MaskVT &&\n            V1.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n            MayFoldLoad(V1.getOperand(0))) {\n          if (Depth == 0 && Root.getOpcode() == X86ISD::VBROADCAST)\n            return SDValue(); // Nothing to do!\n          Res = V1.getOperand(0);\n          Res = DAG.getNode(X86ISD::VBROADCAST, DL, MaskVT, Res);\n          return DAG.getBitcast(RootVT, Res);\n        }\n        if (Subtarget.hasAVX2()) {\n          if (Depth == 0 && Root.getOpcode() == X86ISD::VBROADCAST)\n            return SDValue(); // Nothing to do!\n          Res = CanonicalizeShuffleInput(MaskVT, V1);\n          Res = DAG.getNode(X86ISD::VBROADCAST, DL, MaskVT, Res);\n          return DAG.getBitcast(RootVT, Res);\n        }\n      }\n    }\n\n    SDValue NewV1 = V1; // Save operand in case early exit happens.\n    if (matchUnaryShuffle(MaskVT, Mask, AllowFloatDomain, AllowIntDomain, NewV1,\n                          DL, DAG, Subtarget, Shuffle, ShuffleSrcVT,\n                          ShuffleVT) &&\n        (!IsMaskedShuffle ||\n         (NumRootElts == ShuffleVT.getVectorNumElements()))) {\n      if (Depth == 0 && Root.getOpcode() == Shuffle)\n        return SDValue(); // Nothing to do!\n      Res = CanonicalizeShuffleInput(ShuffleSrcVT, NewV1);\n      Res = DAG.getNode(Shuffle, DL, ShuffleVT, Res);\n      return DAG.getBitcast(RootVT, Res);\n    }\n\n    if (matchUnaryPermuteShuffle(MaskVT, Mask, Zeroable, AllowFloatDomain,\n                                 AllowIntDomain, Subtarget, Shuffle, ShuffleVT,\n                                 PermuteImm) &&\n        (!IsMaskedShuffle ||\n         (NumRootElts == ShuffleVT.getVectorNumElements()))) {\n      if (Depth == 0 && Root.getOpcode() == Shuffle)\n        return SDValue(); // Nothing to do!\n      Res = CanonicalizeShuffleInput(ShuffleVT, V1);\n      Res = DAG.getNode(Shuffle, DL, ShuffleVT, Res,\n                        DAG.getTargetConstant(PermuteImm, DL, MVT::i8));\n      return DAG.getBitcast(RootVT, Res);\n    }\n  }\n\n  // Attempt to combine to INSERTPS, but only if the inserted element has come\n  // from a scalar.\n  // TODO: Handle other insertions here as well?\n  if (!UnaryShuffle && AllowFloatDomain && RootSizeInBits == 128 &&\n      Subtarget.hasSSE41() &&\n      !isTargetShuffleEquivalent(MaskVT, Mask, {4, 1, 2, 3})) {\n    if (MaskEltSizeInBits == 32) {\n      SDValue SrcV1 = V1, SrcV2 = V2;\n      if (matchShuffleAsInsertPS(SrcV1, SrcV2, PermuteImm, Zeroable, Mask,\n                                 DAG) &&\n          SrcV2.getOpcode() == ISD::SCALAR_TO_VECTOR) {\n        if (Depth == 0 && Root.getOpcode() == X86ISD::INSERTPS)\n          return SDValue(); // Nothing to do!\n        Res = DAG.getNode(X86ISD::INSERTPS, DL, MVT::v4f32,\n                          CanonicalizeShuffleInput(MVT::v4f32, SrcV1),\n                          CanonicalizeShuffleInput(MVT::v4f32, SrcV2),\n                          DAG.getTargetConstant(PermuteImm, DL, MVT::i8));\n        return DAG.getBitcast(RootVT, Res);\n      }\n    }\n    if (MaskEltSizeInBits == 64 &&\n        isTargetShuffleEquivalent(MaskVT, Mask, {0, 2}) &&\n        V2.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n        V2.getScalarValueSizeInBits() <= 32) {\n      if (Depth == 0 && Root.getOpcode() == X86ISD::INSERTPS)\n        return SDValue(); // Nothing to do!\n      PermuteImm = (/*DstIdx*/2 << 4) | (/*SrcIdx*/0 << 0);\n      Res = DAG.getNode(X86ISD::INSERTPS, DL, MVT::v4f32,\n                        CanonicalizeShuffleInput(MVT::v4f32, V1),\n                        CanonicalizeShuffleInput(MVT::v4f32, V2),\n                        DAG.getTargetConstant(PermuteImm, DL, MVT::i8));\n      return DAG.getBitcast(RootVT, Res);\n    }\n  }\n\n  SDValue NewV1 = V1; // Save operands in case early exit happens.\n  SDValue NewV2 = V2;\n  if (matchBinaryShuffle(MaskVT, Mask, AllowFloatDomain, AllowIntDomain, NewV1,\n                         NewV2, DL, DAG, Subtarget, Shuffle, ShuffleSrcVT,\n                         ShuffleVT, UnaryShuffle) &&\n      (!IsMaskedShuffle || (NumRootElts == ShuffleVT.getVectorNumElements()))) {\n    if (Depth == 0 && Root.getOpcode() == Shuffle)\n      return SDValue(); // Nothing to do!\n    NewV1 = CanonicalizeShuffleInput(ShuffleSrcVT, NewV1);\n    NewV2 = CanonicalizeShuffleInput(ShuffleSrcVT, NewV2);\n    Res = DAG.getNode(Shuffle, DL, ShuffleVT, NewV1, NewV2);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  NewV1 = V1; // Save operands in case early exit happens.\n  NewV2 = V2;\n  if (matchBinaryPermuteShuffle(MaskVT, Mask, Zeroable, AllowFloatDomain,\n                                AllowIntDomain, NewV1, NewV2, DL, DAG,\n                                Subtarget, Shuffle, ShuffleVT, PermuteImm) &&\n      (!IsMaskedShuffle || (NumRootElts == ShuffleVT.getVectorNumElements()))) {\n    if (Depth == 0 && Root.getOpcode() == Shuffle)\n      return SDValue(); // Nothing to do!\n    NewV1 = CanonicalizeShuffleInput(ShuffleVT, NewV1);\n    NewV2 = CanonicalizeShuffleInput(ShuffleVT, NewV2);\n    Res = DAG.getNode(Shuffle, DL, ShuffleVT, NewV1, NewV2,\n                      DAG.getTargetConstant(PermuteImm, DL, MVT::i8));\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // Typically from here on, we need an integer version of MaskVT.\n  MVT IntMaskVT = MVT::getIntegerVT(MaskEltSizeInBits);\n  IntMaskVT = MVT::getVectorVT(IntMaskVT, NumMaskElts);\n\n  // Annoyingly, SSE4A instructions don't map into the above match helpers.\n  if (Subtarget.hasSSE4A() && AllowIntDomain && RootSizeInBits == 128) {\n    uint64_t BitLen, BitIdx;\n    if (matchShuffleAsEXTRQ(IntMaskVT, V1, V2, Mask, BitLen, BitIdx,\n                            Zeroable)) {\n      if (Depth == 0 && Root.getOpcode() == X86ISD::EXTRQI)\n        return SDValue(); // Nothing to do!\n      V1 = CanonicalizeShuffleInput(IntMaskVT, V1);\n      Res = DAG.getNode(X86ISD::EXTRQI, DL, IntMaskVT, V1,\n                        DAG.getTargetConstant(BitLen, DL, MVT::i8),\n                        DAG.getTargetConstant(BitIdx, DL, MVT::i8));\n      return DAG.getBitcast(RootVT, Res);\n    }\n\n    if (matchShuffleAsINSERTQ(IntMaskVT, V1, V2, Mask, BitLen, BitIdx)) {\n      if (Depth == 0 && Root.getOpcode() == X86ISD::INSERTQI)\n        return SDValue(); // Nothing to do!\n      V1 = CanonicalizeShuffleInput(IntMaskVT, V1);\n      V2 = CanonicalizeShuffleInput(IntMaskVT, V2);\n      Res = DAG.getNode(X86ISD::INSERTQI, DL, IntMaskVT, V1, V2,\n                        DAG.getTargetConstant(BitLen, DL, MVT::i8),\n                        DAG.getTargetConstant(BitIdx, DL, MVT::i8));\n      return DAG.getBitcast(RootVT, Res);\n    }\n  }\n\n  // Match shuffle against TRUNCATE patterns.\n  if (AllowIntDomain && MaskEltSizeInBits < 64 && Subtarget.hasAVX512()) {\n    // Match against a VTRUNC instruction, accounting for src/dst sizes.\n    if (matchShuffleAsVTRUNC(ShuffleSrcVT, ShuffleVT, IntMaskVT, Mask, Zeroable,\n                             Subtarget)) {\n      bool IsTRUNCATE = ShuffleVT.getVectorNumElements() ==\n                        ShuffleSrcVT.getVectorNumElements();\n      unsigned Opc =\n          IsTRUNCATE ? (unsigned)ISD::TRUNCATE : (unsigned)X86ISD::VTRUNC;\n      if (Depth == 0 && Root.getOpcode() == Opc)\n        return SDValue(); // Nothing to do!\n      V1 = CanonicalizeShuffleInput(ShuffleSrcVT, V1);\n      Res = DAG.getNode(Opc, DL, ShuffleVT, V1);\n      if (ShuffleVT.getSizeInBits() < RootSizeInBits)\n        Res = widenSubVector(Res, true, Subtarget, DAG, DL, RootSizeInBits);\n      return DAG.getBitcast(RootVT, Res);\n    }\n\n    // Do we need a more general binary truncation pattern?\n    if (RootSizeInBits < 512 &&\n        ((RootVT.is256BitVector() && Subtarget.useAVX512Regs()) ||\n         (RootVT.is128BitVector() && Subtarget.hasVLX())) &&\n        (MaskEltSizeInBits > 8 || Subtarget.hasBWI()) &&\n        isSequentialOrUndefInRange(Mask, 0, NumMaskElts, 0, 2)) {\n      if (Depth == 0 && Root.getOpcode() == ISD::TRUNCATE)\n        return SDValue(); // Nothing to do!\n      ShuffleSrcVT = MVT::getIntegerVT(MaskEltSizeInBits * 2);\n      ShuffleSrcVT = MVT::getVectorVT(ShuffleSrcVT, NumMaskElts / 2);\n      V1 = CanonicalizeShuffleInput(ShuffleSrcVT, V1);\n      V2 = CanonicalizeShuffleInput(ShuffleSrcVT, V2);\n      ShuffleSrcVT = MVT::getIntegerVT(MaskEltSizeInBits * 2);\n      ShuffleSrcVT = MVT::getVectorVT(ShuffleSrcVT, NumMaskElts);\n      Res = DAG.getNode(ISD::CONCAT_VECTORS, DL, ShuffleSrcVT, V1, V2);\n      Res = DAG.getNode(ISD::TRUNCATE, DL, IntMaskVT, Res);\n      return DAG.getBitcast(RootVT, Res);\n    }\n  }\n\n  // Don't try to re-form single instruction chains under any circumstances now\n  // that we've done encoding canonicalization for them.\n  if (Depth < 1)\n    return SDValue();\n\n  // Depth threshold above which we can efficiently use variable mask shuffles.\n  int VariableShuffleDepth = Subtarget.hasFastVariableShuffle() ? 1 : 2;\n  AllowVariableMask &= (Depth >= VariableShuffleDepth) || HasVariableMask;\n  // VPERMI2W/VPERMI2B are 3 uops on Skylake and Icelake so we require a\n  // higher depth before combining them.\n  bool AllowBWIVPERMV3 = (Depth >= 2 || HasVariableMask);\n\n  bool MaskContainsZeros = isAnyZero(Mask);\n\n  if (is128BitLaneCrossingShuffleMask(MaskVT, Mask)) {\n    // If we have a single input lane-crossing shuffle then lower to VPERMV.\n    if (UnaryShuffle && AllowVariableMask && !MaskContainsZeros) {\n      if (Subtarget.hasAVX2() &&\n          (MaskVT == MVT::v8f32 || MaskVT == MVT::v8i32)) {\n        SDValue VPermMask = getConstVector(Mask, IntMaskVT, DAG, DL, true);\n        Res = CanonicalizeShuffleInput(MaskVT, V1);\n        Res = DAG.getNode(X86ISD::VPERMV, DL, MaskVT, VPermMask, Res);\n        return DAG.getBitcast(RootVT, Res);\n      }\n      // AVX512 variants (non-VLX will pad to 512-bit shuffles).\n      if ((Subtarget.hasAVX512() &&\n           (MaskVT == MVT::v8f64 || MaskVT == MVT::v8i64 ||\n            MaskVT == MVT::v16f32 || MaskVT == MVT::v16i32)) ||\n          (Subtarget.hasBWI() &&\n           (MaskVT == MVT::v16i16 || MaskVT == MVT::v32i16)) ||\n          (Subtarget.hasVBMI() &&\n           (MaskVT == MVT::v32i8 || MaskVT == MVT::v64i8))) {\n        V1 = CanonicalizeShuffleInput(MaskVT, V1);\n        V2 = DAG.getUNDEF(MaskVT);\n        Res = lowerShuffleWithPERMV(DL, MaskVT, Mask, V1, V2, Subtarget, DAG);\n        return DAG.getBitcast(RootVT, Res);\n      }\n    }\n\n    // Lower a unary+zero lane-crossing shuffle as VPERMV3 with a zero\n    // vector as the second source (non-VLX will pad to 512-bit shuffles).\n    if (UnaryShuffle && AllowVariableMask &&\n        ((Subtarget.hasAVX512() &&\n          (MaskVT == MVT::v8f64 || MaskVT == MVT::v8i64 ||\n           MaskVT == MVT::v4f64 || MaskVT == MVT::v4i64 ||\n           MaskVT == MVT::v8f32 || MaskVT == MVT::v8i32 ||\n           MaskVT == MVT::v16f32 || MaskVT == MVT::v16i32)) ||\n         (Subtarget.hasBWI() && AllowBWIVPERMV3 &&\n          (MaskVT == MVT::v16i16 || MaskVT == MVT::v32i16)) ||\n         (Subtarget.hasVBMI() && AllowBWIVPERMV3 &&\n          (MaskVT == MVT::v32i8 || MaskVT == MVT::v64i8)))) {\n      // Adjust shuffle mask - replace SM_SentinelZero with second source index.\n      for (unsigned i = 0; i != NumMaskElts; ++i)\n        if (Mask[i] == SM_SentinelZero)\n          Mask[i] = NumMaskElts + i;\n      V1 = CanonicalizeShuffleInput(MaskVT, V1);\n      V2 = getZeroVector(MaskVT, Subtarget, DAG, DL);\n      Res = lowerShuffleWithPERMV(DL, MaskVT, Mask, V1, V2, Subtarget, DAG);\n      return DAG.getBitcast(RootVT, Res);\n    }\n\n    // If that failed and either input is extracted then try to combine as a\n    // shuffle with the larger type.\n    if (SDValue WideShuffle = combineX86ShuffleChainWithExtract(\n            Inputs, Root, BaseMask, Depth, HasVariableMask, AllowVariableMask,\n            DAG, Subtarget))\n      return WideShuffle;\n\n    // If we have a dual input lane-crossing shuffle then lower to VPERMV3,\n    // (non-VLX will pad to 512-bit shuffles).\n    if (AllowVariableMask && !MaskContainsZeros &&\n        ((Subtarget.hasAVX512() &&\n          (MaskVT == MVT::v8f64 || MaskVT == MVT::v8i64 ||\n           MaskVT == MVT::v4f64 || MaskVT == MVT::v4i64 ||\n           MaskVT == MVT::v16f32 || MaskVT == MVT::v16i32 ||\n           MaskVT == MVT::v8f32 || MaskVT == MVT::v8i32)) ||\n         (Subtarget.hasBWI() && AllowBWIVPERMV3 &&\n          (MaskVT == MVT::v16i16 || MaskVT == MVT::v32i16)) ||\n         (Subtarget.hasVBMI() && AllowBWIVPERMV3 &&\n          (MaskVT == MVT::v32i8 || MaskVT == MVT::v64i8)))) {\n      V1 = CanonicalizeShuffleInput(MaskVT, V1);\n      V2 = CanonicalizeShuffleInput(MaskVT, V2);\n      Res = lowerShuffleWithPERMV(DL, MaskVT, Mask, V1, V2, Subtarget, DAG);\n      return DAG.getBitcast(RootVT, Res);\n    }\n    return SDValue();\n  }\n\n  // See if we can combine a single input shuffle with zeros to a bit-mask,\n  // which is much simpler than any shuffle.\n  if (UnaryShuffle && MaskContainsZeros && AllowVariableMask &&\n      isSequentialOrUndefOrZeroInRange(Mask, 0, NumMaskElts, 0) &&\n      DAG.getTargetLoweringInfo().isTypeLegal(MaskVT)) {\n    APInt Zero = APInt::getNullValue(MaskEltSizeInBits);\n    APInt AllOnes = APInt::getAllOnesValue(MaskEltSizeInBits);\n    APInt UndefElts(NumMaskElts, 0);\n    SmallVector<APInt, 64> EltBits(NumMaskElts, Zero);\n    for (unsigned i = 0; i != NumMaskElts; ++i) {\n      int M = Mask[i];\n      if (M == SM_SentinelUndef) {\n        UndefElts.setBit(i);\n        continue;\n      }\n      if (M == SM_SentinelZero)\n        continue;\n      EltBits[i] = AllOnes;\n    }\n    SDValue BitMask = getConstVector(EltBits, UndefElts, MaskVT, DAG, DL);\n    Res = CanonicalizeShuffleInput(MaskVT, V1);\n    unsigned AndOpcode =\n        MaskVT.isFloatingPoint() ? unsigned(X86ISD::FAND) : unsigned(ISD::AND);\n    Res = DAG.getNode(AndOpcode, DL, MaskVT, Res, BitMask);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // If we have a single input shuffle with different shuffle patterns in the\n  // the 128-bit lanes use the variable mask to VPERMILPS.\n  // TODO Combine other mask types at higher depths.\n  if (UnaryShuffle && AllowVariableMask && !MaskContainsZeros &&\n      ((MaskVT == MVT::v8f32 && Subtarget.hasAVX()) ||\n       (MaskVT == MVT::v16f32 && Subtarget.hasAVX512()))) {\n    SmallVector<SDValue, 16> VPermIdx;\n    for (int M : Mask) {\n      SDValue Idx =\n          M < 0 ? DAG.getUNDEF(MVT::i32) : DAG.getConstant(M % 4, DL, MVT::i32);\n      VPermIdx.push_back(Idx);\n    }\n    SDValue VPermMask = DAG.getBuildVector(IntMaskVT, DL, VPermIdx);\n    Res = CanonicalizeShuffleInput(MaskVT, V1);\n    Res = DAG.getNode(X86ISD::VPERMILPV, DL, MaskVT, Res, VPermMask);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // With XOP, binary shuffles of 128/256-bit floating point vectors can combine\n  // to VPERMIL2PD/VPERMIL2PS.\n  if (AllowVariableMask && Subtarget.hasXOP() &&\n      (MaskVT == MVT::v2f64 || MaskVT == MVT::v4f64 || MaskVT == MVT::v4f32 ||\n       MaskVT == MVT::v8f32)) {\n    // VPERMIL2 Operation.\n    // Bits[3] - Match Bit.\n    // Bits[2:1] - (Per Lane) PD Shuffle Mask.\n    // Bits[2:0] - (Per Lane) PS Shuffle Mask.\n    unsigned NumLanes = MaskVT.getSizeInBits() / 128;\n    unsigned NumEltsPerLane = NumMaskElts / NumLanes;\n    SmallVector<int, 8> VPerm2Idx;\n    unsigned M2ZImm = 0;\n    for (int M : Mask) {\n      if (M == SM_SentinelUndef) {\n        VPerm2Idx.push_back(-1);\n        continue;\n      }\n      if (M == SM_SentinelZero) {\n        M2ZImm = 2;\n        VPerm2Idx.push_back(8);\n        continue;\n      }\n      int Index = (M % NumEltsPerLane) + ((M / NumMaskElts) * NumEltsPerLane);\n      Index = (MaskVT.getScalarSizeInBits() == 64 ? Index << 1 : Index);\n      VPerm2Idx.push_back(Index);\n    }\n    V1 = CanonicalizeShuffleInput(MaskVT, V1);\n    V2 = CanonicalizeShuffleInput(MaskVT, V2);\n    SDValue VPerm2MaskOp = getConstVector(VPerm2Idx, IntMaskVT, DAG, DL, true);\n    Res = DAG.getNode(X86ISD::VPERMIL2, DL, MaskVT, V1, V2, VPerm2MaskOp,\n                      DAG.getTargetConstant(M2ZImm, DL, MVT::i8));\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // If we have 3 or more shuffle instructions or a chain involving a variable\n  // mask, we can replace them with a single PSHUFB instruction profitably.\n  // Intel's manuals suggest only using PSHUFB if doing so replacing 5\n  // instructions, but in practice PSHUFB tends to be *very* fast so we're\n  // more aggressive.\n  if (UnaryShuffle && AllowVariableMask &&\n      ((RootVT.is128BitVector() && Subtarget.hasSSSE3()) ||\n       (RootVT.is256BitVector() && Subtarget.hasAVX2()) ||\n       (RootVT.is512BitVector() && Subtarget.hasBWI()))) {\n    SmallVector<SDValue, 16> PSHUFBMask;\n    int NumBytes = RootVT.getSizeInBits() / 8;\n    int Ratio = NumBytes / NumMaskElts;\n    for (int i = 0; i < NumBytes; ++i) {\n      int M = Mask[i / Ratio];\n      if (M == SM_SentinelUndef) {\n        PSHUFBMask.push_back(DAG.getUNDEF(MVT::i8));\n        continue;\n      }\n      if (M == SM_SentinelZero) {\n        PSHUFBMask.push_back(DAG.getConstant(0x80, DL, MVT::i8));\n        continue;\n      }\n      M = Ratio * M + i % Ratio;\n      assert((M / 16) == (i / 16) && \"Lane crossing detected\");\n      PSHUFBMask.push_back(DAG.getConstant(M, DL, MVT::i8));\n    }\n    MVT ByteVT = MVT::getVectorVT(MVT::i8, NumBytes);\n    Res = CanonicalizeShuffleInput(ByteVT, V1);\n    SDValue PSHUFBMaskOp = DAG.getBuildVector(ByteVT, DL, PSHUFBMask);\n    Res = DAG.getNode(X86ISD::PSHUFB, DL, ByteVT, Res, PSHUFBMaskOp);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // With XOP, if we have a 128-bit binary input shuffle we can always combine\n  // to VPPERM. We match the depth requirement of PSHUFB - VPPERM is never\n  // slower than PSHUFB on targets that support both.\n  if (AllowVariableMask && RootVT.is128BitVector() && Subtarget.hasXOP()) {\n    // VPPERM Mask Operation\n    // Bits[4:0] - Byte Index (0 - 31)\n    // Bits[7:5] - Permute Operation (0 - Source byte, 4 - ZERO)\n    SmallVector<SDValue, 16> VPPERMMask;\n    int NumBytes = 16;\n    int Ratio = NumBytes / NumMaskElts;\n    for (int i = 0; i < NumBytes; ++i) {\n      int M = Mask[i / Ratio];\n      if (M == SM_SentinelUndef) {\n        VPPERMMask.push_back(DAG.getUNDEF(MVT::i8));\n        continue;\n      }\n      if (M == SM_SentinelZero) {\n        VPPERMMask.push_back(DAG.getConstant(0x80, DL, MVT::i8));\n        continue;\n      }\n      M = Ratio * M + i % Ratio;\n      VPPERMMask.push_back(DAG.getConstant(M, DL, MVT::i8));\n    }\n    MVT ByteVT = MVT::v16i8;\n    V1 = CanonicalizeShuffleInput(ByteVT, V1);\n    V2 = CanonicalizeShuffleInput(ByteVT, V2);\n    SDValue VPPERMMaskOp = DAG.getBuildVector(ByteVT, DL, VPPERMMask);\n    Res = DAG.getNode(X86ISD::VPPERM, DL, ByteVT, V1, V2, VPPERMMaskOp);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // If that failed and either input is extracted then try to combine as a\n  // shuffle with the larger type.\n  if (SDValue WideShuffle = combineX86ShuffleChainWithExtract(\n          Inputs, Root, BaseMask, Depth, HasVariableMask, AllowVariableMask,\n          DAG, Subtarget))\n    return WideShuffle;\n\n  // If we have a dual input shuffle then lower to VPERMV3,\n  // (non-VLX will pad to 512-bit shuffles)\n  if (!UnaryShuffle && AllowVariableMask && !MaskContainsZeros &&\n      ((Subtarget.hasAVX512() &&\n        (MaskVT == MVT::v2f64 || MaskVT == MVT::v4f64 || MaskVT == MVT::v8f64 ||\n         MaskVT == MVT::v2i64 || MaskVT == MVT::v4i64 || MaskVT == MVT::v8i64 ||\n         MaskVT == MVT::v4f32 || MaskVT == MVT::v4i32 || MaskVT == MVT::v8f32 ||\n         MaskVT == MVT::v8i32 || MaskVT == MVT::v16f32 ||\n         MaskVT == MVT::v16i32)) ||\n       (Subtarget.hasBWI() && AllowBWIVPERMV3 &&\n        (MaskVT == MVT::v8i16 || MaskVT == MVT::v16i16 || MaskVT == MVT::v32i16)) ||\n       (Subtarget.hasVBMI() && AllowBWIVPERMV3 &&\n        (MaskVT == MVT::v16i8 || MaskVT == MVT::v32i8 || MaskVT == MVT::v64i8)))) {\n    V1 = CanonicalizeShuffleInput(MaskVT, V1);\n    V2 = CanonicalizeShuffleInput(MaskVT, V2);\n    Res = lowerShuffleWithPERMV(DL, MaskVT, Mask, V1, V2, Subtarget, DAG);\n    return DAG.getBitcast(RootVT, Res);\n  }\n\n  // Failed to find any combines.\n  return SDValue();\n}\n\n// Combine an arbitrary chain of shuffles + extract_subvectors into a single\n// instruction if possible.\n//\n// Wrapper for combineX86ShuffleChain that extends the shuffle mask to a larger\n// type size to attempt to combine:\n// shuffle(extract_subvector(x,c1),extract_subvector(y,c2),m1)\n// -->\n// extract_subvector(shuffle(x,y,m2),0)\nstatic SDValue combineX86ShuffleChainWithExtract(\n    ArrayRef<SDValue> Inputs, SDValue Root, ArrayRef<int> BaseMask, int Depth,\n    bool HasVariableMask, bool AllowVariableMask, SelectionDAG &DAG,\n    const X86Subtarget &Subtarget) {\n  unsigned NumMaskElts = BaseMask.size();\n  unsigned NumInputs = Inputs.size();\n  if (NumInputs == 0)\n    return SDValue();\n\n  EVT RootVT = Root.getValueType();\n  unsigned RootSizeInBits = RootVT.getSizeInBits();\n  assert((RootSizeInBits % NumMaskElts) == 0 && \"Unexpected root shuffle mask\");\n\n  SmallVector<SDValue, 4> WideInputs(Inputs.begin(), Inputs.end());\n  SmallVector<unsigned, 4> Offsets(NumInputs, 0);\n\n  // Peek through subvectors.\n  // TODO: Support inter-mixed EXTRACT_SUBVECTORs + BITCASTs?\n  unsigned WideSizeInBits = RootSizeInBits;\n  for (unsigned i = 0; i != NumInputs; ++i) {\n    SDValue &Src = WideInputs[i];\n    unsigned &Offset = Offsets[i];\n    Src = peekThroughBitcasts(Src);\n    EVT BaseVT = Src.getValueType();\n    while (Src.getOpcode() == ISD::EXTRACT_SUBVECTOR) {\n      Offset += Src.getConstantOperandVal(1);\n      Src = Src.getOperand(0);\n    }\n    WideSizeInBits = std::max(WideSizeInBits,\n                              (unsigned)Src.getValueSizeInBits());\n    assert((Offset % BaseVT.getVectorNumElements()) == 0 &&\n           \"Unexpected subvector extraction\");\n    Offset /= BaseVT.getVectorNumElements();\n    Offset *= NumMaskElts;\n  }\n\n  // Bail if we're always extracting from the lowest subvectors,\n  // combineX86ShuffleChain should match this for the current width.\n  if (llvm::all_of(Offsets, [](unsigned Offset) { return Offset == 0; }))\n    return SDValue();\n\n  unsigned Scale = WideSizeInBits / RootSizeInBits;\n  assert((WideSizeInBits % RootSizeInBits) == 0 &&\n         \"Unexpected subvector extraction\");\n\n  // If the src vector types aren't the same, see if we can extend\n  // them to match each other.\n  // TODO: Support different scalar types?\n  EVT WideSVT = WideInputs[0].getValueType().getScalarType();\n  if (llvm::any_of(WideInputs, [&WideSVT, &DAG](SDValue Op) {\n        return !DAG.getTargetLoweringInfo().isTypeLegal(Op.getValueType()) ||\n               Op.getValueType().getScalarType() != WideSVT;\n      }))\n    return SDValue();\n\n  for (SDValue &NewInput : WideInputs) {\n    assert((WideSizeInBits % NewInput.getValueSizeInBits()) == 0 &&\n           \"Shuffle vector size mismatch\");\n    if (WideSizeInBits > NewInput.getValueSizeInBits())\n      NewInput = widenSubVector(NewInput, false, Subtarget, DAG,\n                                SDLoc(NewInput), WideSizeInBits);\n    assert(WideSizeInBits == NewInput.getValueSizeInBits() &&\n           \"Unexpected subvector extraction\");\n  }\n\n  // Create new mask for larger type.\n  for (unsigned i = 1; i != NumInputs; ++i)\n    Offsets[i] += i * Scale * NumMaskElts;\n\n  SmallVector<int, 64> WideMask(BaseMask.begin(), BaseMask.end());\n  for (int &M : WideMask) {\n    if (M < 0)\n      continue;\n    M = (M % NumMaskElts) + Offsets[M / NumMaskElts];\n  }\n  WideMask.append((Scale - 1) * NumMaskElts, SM_SentinelUndef);\n\n  // Remove unused/repeated shuffle source ops.\n  resolveTargetShuffleInputsAndMask(WideInputs, WideMask);\n  assert(!WideInputs.empty() && \"Shuffle with no inputs detected\");\n\n  if (WideInputs.size() > 2)\n    return SDValue();\n\n  // Increase depth for every upper subvector we've peeked through.\n  Depth += count_if(Offsets, [](unsigned Offset) { return Offset > 0; });\n\n  // Attempt to combine wider chain.\n  // TODO: Can we use a better Root?\n  SDValue WideRoot = WideInputs[0];\n  if (SDValue WideShuffle = combineX86ShuffleChain(\n          WideInputs, WideRoot, WideMask, Depth, HasVariableMask,\n          AllowVariableMask, DAG, Subtarget)) {\n    WideShuffle =\n        extractSubVector(WideShuffle, 0, DAG, SDLoc(Root), RootSizeInBits);\n    return DAG.getBitcast(RootVT, WideShuffle);\n  }\n  return SDValue();\n}\n\n// Canonicalize the combined shuffle mask chain with horizontal ops.\n// NOTE: This may update the Ops and Mask.\nstatic SDValue canonicalizeShuffleMaskWithHorizOp(\n    MutableArrayRef<SDValue> Ops, MutableArrayRef<int> Mask,\n    unsigned RootSizeInBits, const SDLoc &DL, SelectionDAG &DAG,\n    const X86Subtarget &Subtarget) {\n  if (Mask.empty() || Ops.empty())\n    return SDValue();\n\n  SmallVector<SDValue> BC;\n  for (SDValue Op : Ops)\n    BC.push_back(peekThroughBitcasts(Op));\n\n  // All ops must be the same horizop + type.\n  SDValue BC0 = BC[0];\n  EVT VT0 = BC0.getValueType();\n  unsigned Opcode0 = BC0.getOpcode();\n  if (VT0.getSizeInBits() != RootSizeInBits || llvm::any_of(BC, [&](SDValue V) {\n        return V.getOpcode() != Opcode0 || V.getValueType() != VT0;\n      }))\n    return SDValue();\n\n  bool isHoriz = (Opcode0 == X86ISD::FHADD || Opcode0 == X86ISD::HADD ||\n                  Opcode0 == X86ISD::FHSUB || Opcode0 == X86ISD::HSUB);\n  bool isPack = (Opcode0 == X86ISD::PACKSS || Opcode0 == X86ISD::PACKUS);\n  if (!isHoriz && !isPack)\n    return SDValue();\n\n  int NumElts = VT0.getVectorNumElements();\n  int NumLanes = VT0.getSizeInBits() / 128;\n  int NumEltsPerLane = NumElts / NumLanes;\n  int NumHalfEltsPerLane = NumEltsPerLane / 2;\n\n  // See if we can remove the shuffle by resorting the HOP chain so that\n  // the HOP args are pre-shuffled.\n  // TODO: Generalize to any sized/depth chain.\n  // TODO: Add support for PACKSS/PACKUS.\n  if (isHoriz && NumEltsPerLane == 4 && VT0.is128BitVector() &&\n      shouldUseHorizontalOp(Ops.size() == 1, DAG, Subtarget)) {\n    SmallVector<int> ScaledMask;\n    if (scaleShuffleElements(Mask, 4, ScaledMask)) {\n      // Attempt to find a HOP(HOP(X,Y),HOP(Z,W)) source operand.\n      auto GetHOpSrc = [&](int M) {\n        if (M == SM_SentinelUndef)\n          return DAG.getUNDEF(VT0);\n        if (M == SM_SentinelZero)\n          return getZeroVector(VT0.getSimpleVT(), Subtarget, DAG, DL);\n        SDValue Src0 = BC[M / NumElts];\n        SDValue Src1 = Src0.getOperand((M % 4) >= 2);\n        if (Src1.getOpcode() == Opcode0 && Src0->isOnlyUserOf(Src1.getNode()))\n          return Src1.getOperand(M % 2);\n        return SDValue();\n      };\n      SDValue M0 = GetHOpSrc(ScaledMask[0]);\n      SDValue M1 = GetHOpSrc(ScaledMask[1]);\n      SDValue M2 = GetHOpSrc(ScaledMask[2]);\n      SDValue M3 = GetHOpSrc(ScaledMask[3]);\n      if (M0 && M1 && M2 && M3) {\n        SDValue LHS = DAG.getNode(Opcode0, DL, VT0, M0, M1);\n        SDValue RHS = DAG.getNode(Opcode0, DL, VT0, M2, M3);\n        return DAG.getNode(Opcode0, DL, VT0, LHS, RHS);\n      }\n    }\n  }\n\n  if (2 < Ops.size())\n    return SDValue();\n\n  SDValue BC1 = BC[BC.size() - 1];\n  if (Mask.size() == VT0.getVectorNumElements()) {\n    // Canonicalize binary shuffles of horizontal ops that use the\n    // same sources to an unary shuffle.\n    // TODO: Try to perform this fold even if the shuffle remains.\n    if (Ops.size() == 2) {\n      auto ContainsOps = [](SDValue HOp, SDValue Op) {\n        return Op == HOp.getOperand(0) || Op == HOp.getOperand(1);\n      };\n      // Commute if all BC0's ops are contained in BC1.\n      if (ContainsOps(BC1, BC0.getOperand(0)) &&\n          ContainsOps(BC1, BC0.getOperand(1))) {\n        ShuffleVectorSDNode::commuteMask(Mask);\n        std::swap(Ops[0], Ops[1]);\n        std::swap(BC0, BC1);\n      }\n\n      // If BC1 can be represented by BC0, then convert to unary shuffle.\n      if (ContainsOps(BC0, BC1.getOperand(0)) &&\n          ContainsOps(BC0, BC1.getOperand(1))) {\n        for (int &M : Mask) {\n          if (M < NumElts) // BC0 element or UNDEF/Zero sentinel.\n            continue;\n          int SubLane = ((M % NumEltsPerLane) >= NumHalfEltsPerLane) ? 1 : 0;\n          M -= NumElts + (SubLane * NumHalfEltsPerLane);\n          if (BC1.getOperand(SubLane) != BC0.getOperand(0))\n            M += NumHalfEltsPerLane;\n        }\n      }\n    }\n\n    // Canonicalize unary horizontal ops to only refer to lower halves.\n    for (int i = 0; i != NumElts; ++i) {\n      int &M = Mask[i];\n      if (isUndefOrZero(M))\n        continue;\n      if (M < NumElts && BC0.getOperand(0) == BC0.getOperand(1) &&\n          (M % NumEltsPerLane) >= NumHalfEltsPerLane)\n        M -= NumHalfEltsPerLane;\n      if (NumElts <= M && BC1.getOperand(0) == BC1.getOperand(1) &&\n          (M % NumEltsPerLane) >= NumHalfEltsPerLane)\n        M -= NumHalfEltsPerLane;\n    }\n  }\n\n  // Combine binary shuffle of 2 similar 'Horizontal' instructions into a\n  // single instruction. Attempt to match a v2X64 repeating shuffle pattern that\n  // represents the LHS/RHS inputs for the lower/upper halves.\n  unsigned EltSizeInBits = RootSizeInBits / Mask.size();\n  SmallVector<int, 16> TargetMask128, WideMask128;\n  if (isRepeatedTargetShuffleMask(128, EltSizeInBits, Mask, TargetMask128) &&\n      scaleShuffleElements(TargetMask128, 2, WideMask128)) {\n    assert(isUndefOrZeroOrInRange(WideMask128, 0, 4) && \"Illegal shuffle\");\n    bool SingleOp = (Ops.size() == 1);\n    if (!isHoriz || shouldUseHorizontalOp(SingleOp, DAG, Subtarget)) {\n      SDValue Lo = isInRange(WideMask128[0], 0, 2) ? BC0 : BC1;\n      SDValue Hi = isInRange(WideMask128[1], 0, 2) ? BC0 : BC1;\n      Lo = Lo.getOperand(WideMask128[0] & 1);\n      Hi = Hi.getOperand(WideMask128[1] & 1);\n      if (SingleOp) {\n        MVT SrcVT = BC0.getOperand(0).getSimpleValueType();\n        SDValue Undef = DAG.getUNDEF(SrcVT);\n        SDValue Zero = getZeroVector(SrcVT, Subtarget, DAG, DL);\n        Lo = (WideMask128[0] == SM_SentinelZero ? Zero : Lo);\n        Hi = (WideMask128[1] == SM_SentinelZero ? Zero : Hi);\n        Lo = (WideMask128[0] == SM_SentinelUndef ? Undef : Lo);\n        Hi = (WideMask128[1] == SM_SentinelUndef ? Undef : Hi);\n      }\n      return DAG.getNode(Opcode0, DL, VT0, Lo, Hi);\n    }\n  }\n\n  return SDValue();\n}\n\n// Attempt to constant fold all of the constant source ops.\n// Returns true if the entire shuffle is folded to a constant.\n// TODO: Extend this to merge multiple constant Ops and update the mask.\nstatic SDValue combineX86ShufflesConstants(ArrayRef<SDValue> Ops,\n                                           ArrayRef<int> Mask, SDValue Root,\n                                           bool HasVariableMask,\n                                           SelectionDAG &DAG,\n                                           const X86Subtarget &Subtarget) {\n  MVT VT = Root.getSimpleValueType();\n\n  unsigned SizeInBits = VT.getSizeInBits();\n  unsigned NumMaskElts = Mask.size();\n  unsigned MaskSizeInBits = SizeInBits / NumMaskElts;\n  unsigned NumOps = Ops.size();\n\n  // Extract constant bits from each source op.\n  bool OneUseConstantOp = false;\n  SmallVector<APInt, 16> UndefEltsOps(NumOps);\n  SmallVector<SmallVector<APInt, 16>, 16> RawBitsOps(NumOps);\n  for (unsigned i = 0; i != NumOps; ++i) {\n    SDValue SrcOp = Ops[i];\n    OneUseConstantOp |= SrcOp.hasOneUse();\n    if (!getTargetConstantBitsFromNode(SrcOp, MaskSizeInBits, UndefEltsOps[i],\n                                       RawBitsOps[i]))\n      return SDValue();\n  }\n\n  // Only fold if at least one of the constants is only used once or\n  // the combined shuffle has included a variable mask shuffle, this\n  // is to avoid constant pool bloat.\n  if (!OneUseConstantOp && !HasVariableMask)\n    return SDValue();\n\n  // Shuffle the constant bits according to the mask.\n  SDLoc DL(Root);\n  APInt UndefElts(NumMaskElts, 0);\n  APInt ZeroElts(NumMaskElts, 0);\n  APInt ConstantElts(NumMaskElts, 0);\n  SmallVector<APInt, 8> ConstantBitData(NumMaskElts,\n                                        APInt::getNullValue(MaskSizeInBits));\n  for (unsigned i = 0; i != NumMaskElts; ++i) {\n    int M = Mask[i];\n    if (M == SM_SentinelUndef) {\n      UndefElts.setBit(i);\n      continue;\n    } else if (M == SM_SentinelZero) {\n      ZeroElts.setBit(i);\n      continue;\n    }\n    assert(0 <= M && M < (int)(NumMaskElts * NumOps));\n\n    unsigned SrcOpIdx = (unsigned)M / NumMaskElts;\n    unsigned SrcMaskIdx = (unsigned)M % NumMaskElts;\n\n    auto &SrcUndefElts = UndefEltsOps[SrcOpIdx];\n    if (SrcUndefElts[SrcMaskIdx]) {\n      UndefElts.setBit(i);\n      continue;\n    }\n\n    auto &SrcEltBits = RawBitsOps[SrcOpIdx];\n    APInt &Bits = SrcEltBits[SrcMaskIdx];\n    if (!Bits) {\n      ZeroElts.setBit(i);\n      continue;\n    }\n\n    ConstantElts.setBit(i);\n    ConstantBitData[i] = Bits;\n  }\n  assert((UndefElts | ZeroElts | ConstantElts).isAllOnesValue());\n\n  // Attempt to create a zero vector.\n  if ((UndefElts | ZeroElts).isAllOnesValue())\n    return getZeroVector(Root.getSimpleValueType(), Subtarget, DAG, DL);\n\n  // Create the constant data.\n  MVT MaskSVT;\n  if (VT.isFloatingPoint() && (MaskSizeInBits == 32 || MaskSizeInBits == 64))\n    MaskSVT = MVT::getFloatingPointVT(MaskSizeInBits);\n  else\n    MaskSVT = MVT::getIntegerVT(MaskSizeInBits);\n\n  MVT MaskVT = MVT::getVectorVT(MaskSVT, NumMaskElts);\n  if (!DAG.getTargetLoweringInfo().isTypeLegal(MaskVT))\n    return SDValue();\n\n  SDValue CstOp = getConstVector(ConstantBitData, UndefElts, MaskVT, DAG, DL);\n  return DAG.getBitcast(VT, CstOp);\n}\n\nnamespace llvm {\n  namespace X86 {\n    enum {\n      MaxShuffleCombineDepth = 8\n    };\n  }\n} // namespace llvm\n\n/// Fully generic combining of x86 shuffle instructions.\n///\n/// This should be the last combine run over the x86 shuffle instructions. Once\n/// they have been fully optimized, this will recursively consider all chains\n/// of single-use shuffle instructions, build a generic model of the cumulative\n/// shuffle operation, and check for simpler instructions which implement this\n/// operation. We use this primarily for two purposes:\n///\n/// 1) Collapse generic shuffles to specialized single instructions when\n///    equivalent. In most cases, this is just an encoding size win, but\n///    sometimes we will collapse multiple generic shuffles into a single\n///    special-purpose shuffle.\n/// 2) Look for sequences of shuffle instructions with 3 or more total\n///    instructions, and replace them with the slightly more expensive SSSE3\n///    PSHUFB instruction if available. We do this as the last combining step\n///    to ensure we avoid using PSHUFB if we can implement the shuffle with\n///    a suitable short sequence of other instructions. The PSHUFB will either\n///    use a register or have to read from memory and so is slightly (but only\n///    slightly) more expensive than the other shuffle instructions.\n///\n/// Because this is inherently a quadratic operation (for each shuffle in\n/// a chain, we recurse up the chain), the depth is limited to 8 instructions.\n/// This should never be an issue in practice as the shuffle lowering doesn't\n/// produce sequences of more than 8 instructions.\n///\n/// FIXME: We will currently miss some cases where the redundant shuffling\n/// would simplify under the threshold for PSHUFB formation because of\n/// combine-ordering. To fix this, we should do the redundant instruction\n/// combining in this recursive walk.\nstatic SDValue combineX86ShufflesRecursively(\n    ArrayRef<SDValue> SrcOps, int SrcOpIndex, SDValue Root,\n    ArrayRef<int> RootMask, ArrayRef<const SDNode *> SrcNodes, unsigned Depth,\n    unsigned MaxDepth, bool HasVariableMask, bool AllowVariableMask,\n    SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  assert(RootMask.size() > 0 &&\n         (RootMask.size() > 1 || (RootMask[0] == 0 && SrcOpIndex == 0)) &&\n         \"Illegal shuffle root mask\");\n  assert(Root.getSimpleValueType().isVector() &&\n         \"Shuffles operate on vector types!\");\n  unsigned RootSizeInBits = Root.getSimpleValueType().getSizeInBits();\n\n  // Bound the depth of our recursive combine because this is ultimately\n  // quadratic in nature.\n  if (Depth >= MaxDepth)\n    return SDValue();\n\n  // Directly rip through bitcasts to find the underlying operand.\n  SDValue Op = SrcOps[SrcOpIndex];\n  Op = peekThroughOneUseBitcasts(Op);\n\n  EVT VT = Op.getValueType();\n  if (!VT.isVector() || !VT.isSimple())\n    return SDValue(); // Bail if we hit a non-simple non-vector.\n\n  assert((RootSizeInBits % VT.getSizeInBits()) == 0 &&\n         \"Can only combine shuffles upto size of the root op.\");\n\n  // Extract target shuffle mask and resolve sentinels and inputs.\n  // TODO - determine Op's demanded elts from RootMask.\n  SmallVector<int, 64> OpMask;\n  SmallVector<SDValue, 2> OpInputs;\n  APInt OpUndef, OpZero;\n  APInt OpDemandedElts = APInt::getAllOnesValue(VT.getVectorNumElements());\n  bool IsOpVariableMask = isTargetShuffleVariableMask(Op.getOpcode());\n  if (!getTargetShuffleInputs(Op, OpDemandedElts, OpInputs, OpMask, OpUndef,\n                              OpZero, DAG, Depth, false))\n    return SDValue();\n\n  // Shuffle inputs must not be larger than the shuffle result.\n  // TODO: Relax this for single input faux shuffles (trunc/extract_subvector).\n  if (llvm::any_of(OpInputs, [VT](SDValue OpInput) {\n        return OpInput.getValueSizeInBits() > VT.getSizeInBits();\n      }))\n    return SDValue();\n\n  // If the shuffle result was smaller than the root, we need to adjust the\n  // mask indices and pad the mask with undefs.\n  if (RootSizeInBits > VT.getSizeInBits()) {\n    unsigned NumSubVecs = RootSizeInBits / VT.getSizeInBits();\n    unsigned OpMaskSize = OpMask.size();\n    if (OpInputs.size() > 1) {\n      unsigned PaddedMaskSize = NumSubVecs * OpMaskSize;\n      for (int &M : OpMask) {\n        if (M < 0)\n          continue;\n        int EltIdx = M % OpMaskSize;\n        int OpIdx = M / OpMaskSize;\n        M = (PaddedMaskSize * OpIdx) + EltIdx;\n      }\n    }\n    OpZero = OpZero.zext(NumSubVecs * OpMaskSize);\n    OpUndef = OpUndef.zext(NumSubVecs * OpMaskSize);\n    OpMask.append((NumSubVecs - 1) * OpMaskSize, SM_SentinelUndef);\n  }\n\n  SmallVector<int, 64> Mask;\n  SmallVector<SDValue, 16> Ops;\n\n  // We don't need to merge masks if the root is empty.\n  bool EmptyRoot = (Depth == 0) && (RootMask.size() == 1);\n  if (EmptyRoot) {\n    // Only resolve zeros if it will remove an input, otherwise we might end\n    // up in an infinite loop.\n    bool ResolveKnownZeros = true;\n    if (!OpZero.isNullValue()) {\n      APInt UsedInputs = APInt::getNullValue(OpInputs.size());\n      for (int i = 0, e = OpMask.size(); i != e; ++i) {\n        int M = OpMask[i];\n        if (OpUndef[i] || OpZero[i] || isUndefOrZero(M))\n          continue;\n        UsedInputs.setBit(M / OpMask.size());\n        if (UsedInputs.isAllOnesValue()) {\n          ResolveKnownZeros = false;\n          break;\n        }\n      }\n    }\n    resolveTargetShuffleFromZeroables(OpMask, OpUndef, OpZero,\n                                      ResolveKnownZeros);\n\n    Mask = OpMask;\n    Ops.append(OpInputs.begin(), OpInputs.end());\n  } else {\n    resolveTargetShuffleFromZeroables(OpMask, OpUndef, OpZero);\n\n    // Add the inputs to the Ops list, avoiding duplicates.\n    Ops.append(SrcOps.begin(), SrcOps.end());\n\n    auto AddOp = [&Ops](SDValue Input, int InsertionPoint) -> int {\n      // Attempt to find an existing match.\n      SDValue InputBC = peekThroughBitcasts(Input);\n      for (int i = 0, e = Ops.size(); i < e; ++i)\n        if (InputBC == peekThroughBitcasts(Ops[i]))\n          return i;\n      // Match failed - should we replace an existing Op?\n      if (InsertionPoint >= 0) {\n        Ops[InsertionPoint] = Input;\n        return InsertionPoint;\n      }\n      // Add to the end of the Ops list.\n      Ops.push_back(Input);\n      return Ops.size() - 1;\n    };\n\n    SmallVector<int, 2> OpInputIdx;\n    for (SDValue OpInput : OpInputs)\n      OpInputIdx.push_back(\n          AddOp(OpInput, OpInputIdx.empty() ? SrcOpIndex : -1));\n\n    assert(((RootMask.size() > OpMask.size() &&\n             RootMask.size() % OpMask.size() == 0) ||\n            (OpMask.size() > RootMask.size() &&\n             OpMask.size() % RootMask.size() == 0) ||\n            OpMask.size() == RootMask.size()) &&\n           \"The smaller number of elements must divide the larger.\");\n\n    // This function can be performance-critical, so we rely on the power-of-2\n    // knowledge that we have about the mask sizes to replace div/rem ops with\n    // bit-masks and shifts.\n    assert(isPowerOf2_32(RootMask.size()) &&\n           \"Non-power-of-2 shuffle mask sizes\");\n    assert(isPowerOf2_32(OpMask.size()) && \"Non-power-of-2 shuffle mask sizes\");\n    unsigned RootMaskSizeLog2 = countTrailingZeros(RootMask.size());\n    unsigned OpMaskSizeLog2 = countTrailingZeros(OpMask.size());\n\n    unsigned MaskWidth = std::max<unsigned>(OpMask.size(), RootMask.size());\n    unsigned RootRatio =\n        std::max<unsigned>(1, OpMask.size() >> RootMaskSizeLog2);\n    unsigned OpRatio = std::max<unsigned>(1, RootMask.size() >> OpMaskSizeLog2);\n    assert((RootRatio == 1 || OpRatio == 1) &&\n           \"Must not have a ratio for both incoming and op masks!\");\n\n    assert(isPowerOf2_32(MaskWidth) && \"Non-power-of-2 shuffle mask sizes\");\n    assert(isPowerOf2_32(RootRatio) && \"Non-power-of-2 shuffle mask sizes\");\n    assert(isPowerOf2_32(OpRatio) && \"Non-power-of-2 shuffle mask sizes\");\n    unsigned RootRatioLog2 = countTrailingZeros(RootRatio);\n    unsigned OpRatioLog2 = countTrailingZeros(OpRatio);\n\n    Mask.resize(MaskWidth, SM_SentinelUndef);\n\n    // Merge this shuffle operation's mask into our accumulated mask. Note that\n    // this shuffle's mask will be the first applied to the input, followed by\n    // the root mask to get us all the way to the root value arrangement. The\n    // reason for this order is that we are recursing up the operation chain.\n    for (unsigned i = 0; i < MaskWidth; ++i) {\n      unsigned RootIdx = i >> RootRatioLog2;\n      if (RootMask[RootIdx] < 0) {\n        // This is a zero or undef lane, we're done.\n        Mask[i] = RootMask[RootIdx];\n        continue;\n      }\n\n      unsigned RootMaskedIdx =\n          RootRatio == 1\n              ? RootMask[RootIdx]\n              : (RootMask[RootIdx] << RootRatioLog2) + (i & (RootRatio - 1));\n\n      // Just insert the scaled root mask value if it references an input other\n      // than the SrcOp we're currently inserting.\n      if ((RootMaskedIdx < (SrcOpIndex * MaskWidth)) ||\n          (((SrcOpIndex + 1) * MaskWidth) <= RootMaskedIdx)) {\n        Mask[i] = RootMaskedIdx;\n        continue;\n      }\n\n      RootMaskedIdx = RootMaskedIdx & (MaskWidth - 1);\n      unsigned OpIdx = RootMaskedIdx >> OpRatioLog2;\n      if (OpMask[OpIdx] < 0) {\n        // The incoming lanes are zero or undef, it doesn't matter which ones we\n        // are using.\n        Mask[i] = OpMask[OpIdx];\n        continue;\n      }\n\n      // Ok, we have non-zero lanes, map them through to one of the Op's inputs.\n      unsigned OpMaskedIdx = OpRatio == 1 ? OpMask[OpIdx]\n                                          : (OpMask[OpIdx] << OpRatioLog2) +\n                                                (RootMaskedIdx & (OpRatio - 1));\n\n      OpMaskedIdx = OpMaskedIdx & (MaskWidth - 1);\n      int InputIdx = OpMask[OpIdx] / (int)OpMask.size();\n      assert(0 <= OpInputIdx[InputIdx] && \"Unknown target shuffle input\");\n      OpMaskedIdx += OpInputIdx[InputIdx] * MaskWidth;\n\n      Mask[i] = OpMaskedIdx;\n    }\n  }\n\n  // Remove unused/repeated shuffle source ops.\n  resolveTargetShuffleInputsAndMask(Ops, Mask);\n\n  // Handle the all undef/zero cases early.\n  if (all_of(Mask, [](int Idx) { return Idx == SM_SentinelUndef; }))\n    return DAG.getUNDEF(Root.getValueType());\n  if (all_of(Mask, [](int Idx) { return Idx < 0; }))\n    return getZeroVector(Root.getSimpleValueType(), Subtarget, DAG,\n                         SDLoc(Root));\n\n  assert(!Ops.empty() && \"Shuffle with no inputs detected\");\n  HasVariableMask |= IsOpVariableMask;\n\n  // Update the list of shuffle nodes that have been combined so far.\n  SmallVector<const SDNode *, 16> CombinedNodes(SrcNodes.begin(),\n                                                SrcNodes.end());\n  CombinedNodes.push_back(Op.getNode());\n\n  // See if we can recurse into each shuffle source op (if it's a target\n  // shuffle). The source op should only be generally combined if it either has\n  // a single use (i.e. current Op) or all its users have already been combined,\n  // if not then we can still combine but should prevent generation of variable\n  // shuffles to avoid constant pool bloat.\n  // Don't recurse if we already have more source ops than we can combine in\n  // the remaining recursion depth.\n  if (Ops.size() < (MaxDepth - Depth)) {\n    for (int i = 0, e = Ops.size(); i < e; ++i) {\n      // For empty roots, we need to resolve zeroable elements before combining\n      // them with other shuffles.\n      SmallVector<int, 64> ResolvedMask = Mask;\n      if (EmptyRoot)\n        resolveTargetShuffleFromZeroables(ResolvedMask, OpUndef, OpZero);\n      bool AllowVar = false;\n      if (Ops[i].getNode()->hasOneUse() ||\n          SDNode::areOnlyUsersOf(CombinedNodes, Ops[i].getNode()))\n        AllowVar = AllowVariableMask;\n      if (SDValue Res = combineX86ShufflesRecursively(\n              Ops, i, Root, ResolvedMask, CombinedNodes, Depth + 1, MaxDepth,\n              HasVariableMask, AllowVar, DAG, Subtarget))\n        return Res;\n    }\n  }\n\n  // Attempt to constant fold all of the constant source ops.\n  if (SDValue Cst = combineX86ShufflesConstants(\n          Ops, Mask, Root, HasVariableMask, DAG, Subtarget))\n    return Cst;\n\n  // Canonicalize the combined shuffle mask chain with horizontal ops.\n  // NOTE: This will update the Ops and Mask.\n  if (SDValue HOp = canonicalizeShuffleMaskWithHorizOp(\n          Ops, Mask, RootSizeInBits, SDLoc(Root), DAG, Subtarget))\n    return DAG.getBitcast(Root.getValueType(), HOp);\n\n  // Widen any subvector shuffle inputs we've collected.\n  if (any_of(Ops, [RootSizeInBits](SDValue Op) {\n        return Op.getValueSizeInBits() < RootSizeInBits;\n      })) {\n    for (SDValue &Op : Ops)\n      if (Op.getValueSizeInBits() < RootSizeInBits)\n        Op = widenSubVector(Op, false, Subtarget, DAG, SDLoc(Op),\n                            RootSizeInBits);\n    // Reresolve - we might have repeated subvector sources.\n    resolveTargetShuffleInputsAndMask(Ops, Mask);\n  }\n\n  // We can only combine unary and binary shuffle mask cases.\n  if (Ops.size() <= 2) {\n    // Minor canonicalization of the accumulated shuffle mask to make it easier\n    // to match below. All this does is detect masks with sequential pairs of\n    // elements, and shrink them to the half-width mask. It does this in a loop\n    // so it will reduce the size of the mask to the minimal width mask which\n    // performs an equivalent shuffle.\n    while (Mask.size() > 1) {\n      SmallVector<int, 64> WidenedMask;\n      if (!canWidenShuffleElements(Mask, WidenedMask))\n        break;\n      Mask = std::move(WidenedMask);\n    }\n\n    // Canonicalization of binary shuffle masks to improve pattern matching by\n    // commuting the inputs.\n    if (Ops.size() == 2 && canonicalizeShuffleMaskWithCommute(Mask)) {\n      ShuffleVectorSDNode::commuteMask(Mask);\n      std::swap(Ops[0], Ops[1]);\n    }\n\n    // Finally, try to combine into a single shuffle instruction.\n    return combineX86ShuffleChain(Ops, Root, Mask, Depth, HasVariableMask,\n                                  AllowVariableMask, DAG, Subtarget);\n  }\n\n  // If that failed and any input is extracted then try to combine as a\n  // shuffle with the larger type.\n  return combineX86ShuffleChainWithExtract(Ops, Root, Mask, Depth,\n                                           HasVariableMask, AllowVariableMask,\n                                           DAG, Subtarget);\n}\n\n/// Helper entry wrapper to combineX86ShufflesRecursively.\nstatic SDValue combineX86ShufflesRecursively(SDValue Op, SelectionDAG &DAG,\n                                             const X86Subtarget &Subtarget) {\n  return combineX86ShufflesRecursively({Op}, 0, Op, {0}, {}, /*Depth*/ 0,\n                                       X86::MaxShuffleCombineDepth,\n                                       /*HasVarMask*/ false,\n                                       /*AllowVarMask*/ true, DAG, Subtarget);\n}\n\n/// Get the PSHUF-style mask from PSHUF node.\n///\n/// This is a very minor wrapper around getTargetShuffleMask to easy forming v4\n/// PSHUF-style masks that can be reused with such instructions.\nstatic SmallVector<int, 4> getPSHUFShuffleMask(SDValue N) {\n  MVT VT = N.getSimpleValueType();\n  SmallVector<int, 4> Mask;\n  SmallVector<SDValue, 2> Ops;\n  bool IsUnary;\n  bool HaveMask =\n      getTargetShuffleMask(N.getNode(), VT, false, Ops, Mask, IsUnary);\n  (void)HaveMask;\n  assert(HaveMask);\n\n  // If we have more than 128-bits, only the low 128-bits of shuffle mask\n  // matter. Check that the upper masks are repeats and remove them.\n  if (VT.getSizeInBits() > 128) {\n    int LaneElts = 128 / VT.getScalarSizeInBits();\n#ifndef NDEBUG\n    for (int i = 1, NumLanes = VT.getSizeInBits() / 128; i < NumLanes; ++i)\n      for (int j = 0; j < LaneElts; ++j)\n        assert(Mask[j] == Mask[i * LaneElts + j] - (LaneElts * i) &&\n               \"Mask doesn't repeat in high 128-bit lanes!\");\n#endif\n    Mask.resize(LaneElts);\n  }\n\n  switch (N.getOpcode()) {\n  case X86ISD::PSHUFD:\n    return Mask;\n  case X86ISD::PSHUFLW:\n    Mask.resize(4);\n    return Mask;\n  case X86ISD::PSHUFHW:\n    Mask.erase(Mask.begin(), Mask.begin() + 4);\n    for (int &M : Mask)\n      M -= 4;\n    return Mask;\n  default:\n    llvm_unreachable(\"No valid shuffle instruction found!\");\n  }\n}\n\n/// Search for a combinable shuffle across a chain ending in pshufd.\n///\n/// We walk up the chain and look for a combinable shuffle, skipping over\n/// shuffles that we could hoist this shuffle's transformation past without\n/// altering anything.\nstatic SDValue\ncombineRedundantDWordShuffle(SDValue N, MutableArrayRef<int> Mask,\n                             SelectionDAG &DAG) {\n  assert(N.getOpcode() == X86ISD::PSHUFD &&\n         \"Called with something other than an x86 128-bit half shuffle!\");\n  SDLoc DL(N);\n\n  // Walk up a single-use chain looking for a combinable shuffle. Keep a stack\n  // of the shuffles in the chain so that we can form a fresh chain to replace\n  // this one.\n  SmallVector<SDValue, 8> Chain;\n  SDValue V = N.getOperand(0);\n  for (; V.hasOneUse(); V = V.getOperand(0)) {\n    switch (V.getOpcode()) {\n    default:\n      return SDValue(); // Nothing combined!\n\n    case ISD::BITCAST:\n      // Skip bitcasts as we always know the type for the target specific\n      // instructions.\n      continue;\n\n    case X86ISD::PSHUFD:\n      // Found another dword shuffle.\n      break;\n\n    case X86ISD::PSHUFLW:\n      // Check that the low words (being shuffled) are the identity in the\n      // dword shuffle, and the high words are self-contained.\n      if (Mask[0] != 0 || Mask[1] != 1 ||\n          !(Mask[2] >= 2 && Mask[2] < 4 && Mask[3] >= 2 && Mask[3] < 4))\n        return SDValue();\n\n      Chain.push_back(V);\n      continue;\n\n    case X86ISD::PSHUFHW:\n      // Check that the high words (being shuffled) are the identity in the\n      // dword shuffle, and the low words are self-contained.\n      if (Mask[2] != 2 || Mask[3] != 3 ||\n          !(Mask[0] >= 0 && Mask[0] < 2 && Mask[1] >= 0 && Mask[1] < 2))\n        return SDValue();\n\n      Chain.push_back(V);\n      continue;\n\n    case X86ISD::UNPCKL:\n    case X86ISD::UNPCKH:\n      // For either i8 -> i16 or i16 -> i32 unpacks, we can combine a dword\n      // shuffle into a preceding word shuffle.\n      if (V.getSimpleValueType().getVectorElementType() != MVT::i8 &&\n          V.getSimpleValueType().getVectorElementType() != MVT::i16)\n        return SDValue();\n\n      // Search for a half-shuffle which we can combine with.\n      unsigned CombineOp =\n          V.getOpcode() == X86ISD::UNPCKL ? X86ISD::PSHUFLW : X86ISD::PSHUFHW;\n      if (V.getOperand(0) != V.getOperand(1) ||\n          !V->isOnlyUserOf(V.getOperand(0).getNode()))\n        return SDValue();\n      Chain.push_back(V);\n      V = V.getOperand(0);\n      do {\n        switch (V.getOpcode()) {\n        default:\n          return SDValue(); // Nothing to combine.\n\n        case X86ISD::PSHUFLW:\n        case X86ISD::PSHUFHW:\n          if (V.getOpcode() == CombineOp)\n            break;\n\n          Chain.push_back(V);\n\n          LLVM_FALLTHROUGH;\n        case ISD::BITCAST:\n          V = V.getOperand(0);\n          continue;\n        }\n        break;\n      } while (V.hasOneUse());\n      break;\n    }\n    // Break out of the loop if we break out of the switch.\n    break;\n  }\n\n  if (!V.hasOneUse())\n    // We fell out of the loop without finding a viable combining instruction.\n    return SDValue();\n\n  // Merge this node's mask and our incoming mask.\n  SmallVector<int, 4> VMask = getPSHUFShuffleMask(V);\n  for (int &M : Mask)\n    M = VMask[M];\n  V = DAG.getNode(V.getOpcode(), DL, V.getValueType(), V.getOperand(0),\n                  getV4X86ShuffleImm8ForMask(Mask, DL, DAG));\n\n  // Rebuild the chain around this new shuffle.\n  while (!Chain.empty()) {\n    SDValue W = Chain.pop_back_val();\n\n    if (V.getValueType() != W.getOperand(0).getValueType())\n      V = DAG.getBitcast(W.getOperand(0).getValueType(), V);\n\n    switch (W.getOpcode()) {\n    default:\n      llvm_unreachable(\"Only PSHUF and UNPCK instructions get here!\");\n\n    case X86ISD::UNPCKL:\n    case X86ISD::UNPCKH:\n      V = DAG.getNode(W.getOpcode(), DL, W.getValueType(), V, V);\n      break;\n\n    case X86ISD::PSHUFD:\n    case X86ISD::PSHUFLW:\n    case X86ISD::PSHUFHW:\n      V = DAG.getNode(W.getOpcode(), DL, W.getValueType(), V, W.getOperand(1));\n      break;\n    }\n  }\n  if (V.getValueType() != N.getValueType())\n    V = DAG.getBitcast(N.getValueType(), V);\n\n  // Return the new chain to replace N.\n  return V;\n}\n\n// Attempt to commute shufps LHS loads:\n// permilps(shufps(load(),x)) --> permilps(shufps(x,load()))\nstatic SDValue combineCommutableSHUFP(SDValue N, MVT VT, const SDLoc &DL,\n                                      SelectionDAG &DAG) {\n  // TODO: Add vXf64 support.\n  if (VT != MVT::v4f32 && VT != MVT::v8f32 && VT != MVT::v16f32)\n    return SDValue();\n\n  // SHUFP(LHS, RHS) -> SHUFP(RHS, LHS) iff LHS is foldable + RHS is not.\n  auto commuteSHUFP = [&VT, &DL, &DAG](SDValue Parent, SDValue V) {\n    if (V.getOpcode() != X86ISD::SHUFP || !Parent->isOnlyUserOf(V.getNode()))\n      return SDValue();\n    SDValue N0 = V.getOperand(0);\n    SDValue N1 = V.getOperand(1);\n    unsigned Imm = V.getConstantOperandVal(2);\n    if (!MayFoldLoad(peekThroughOneUseBitcasts(N0)) ||\n        MayFoldLoad(peekThroughOneUseBitcasts(N1)))\n      return SDValue();\n    Imm = ((Imm & 0x0F) << 4) | ((Imm & 0xF0) >> 4);\n    return DAG.getNode(X86ISD::SHUFP, DL, VT, N1, N0,\n                       DAG.getTargetConstant(Imm, DL, MVT::i8));\n  };\n\n  switch (N.getOpcode()) {\n  case X86ISD::VPERMILPI:\n    if (SDValue NewSHUFP = commuteSHUFP(N, N.getOperand(0))) {\n      unsigned Imm = N.getConstantOperandVal(1);\n      return DAG.getNode(X86ISD::VPERMILPI, DL, VT, NewSHUFP,\n                         DAG.getTargetConstant(Imm ^ 0xAA, DL, MVT::i8));\n    }\n    break;\n  case X86ISD::SHUFP: {\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n    unsigned Imm = N.getConstantOperandVal(2);\n    if (N0 == N1) {\n      if (SDValue NewSHUFP = commuteSHUFP(N, N0))\n        return DAG.getNode(X86ISD::SHUFP, DL, VT, NewSHUFP, NewSHUFP,\n                           DAG.getTargetConstant(Imm ^ 0xAA, DL, MVT::i8));\n    } else if (SDValue NewSHUFP = commuteSHUFP(N, N0)) {\n      return DAG.getNode(X86ISD::SHUFP, DL, VT, NewSHUFP, N1,\n                         DAG.getTargetConstant(Imm ^ 0x0A, DL, MVT::i8));\n    } else if (SDValue NewSHUFP = commuteSHUFP(N, N1)) {\n      return DAG.getNode(X86ISD::SHUFP, DL, VT, N0, NewSHUFP,\n                         DAG.getTargetConstant(Imm ^ 0xA0, DL, MVT::i8));\n    }\n    break;\n  }\n  }\n\n  return SDValue();\n}\n\n/// Attempt to fold vpermf128(op(),op()) -> op(vpermf128(),vpermf128()).\nstatic SDValue canonicalizeLaneShuffleWithRepeatedOps(SDValue V,\n                                                      SelectionDAG &DAG,\n                                                      const SDLoc &DL) {\n  assert(V.getOpcode() == X86ISD::VPERM2X128 && \"Unknown lane shuffle\");\n\n  MVT VT = V.getSimpleValueType();\n  SDValue Src0 = peekThroughBitcasts(V.getOperand(0));\n  SDValue Src1 = peekThroughBitcasts(V.getOperand(1));\n  unsigned SrcOpc0 = Src0.getOpcode();\n  unsigned SrcOpc1 = Src1.getOpcode();\n  EVT SrcVT0 = Src0.getValueType();\n  EVT SrcVT1 = Src1.getValueType();\n\n  if (!Src1.isUndef() && (SrcVT0 != SrcVT1 || SrcOpc0 != SrcOpc1))\n    return SDValue();\n\n  switch (SrcOpc0) {\n  case X86ISD::MOVDDUP: {\n    SDValue LHS = DAG.getBitcast(VT, Src0.getOperand(0));\n    SDValue RHS =\n        DAG.getBitcast(VT, Src1.isUndef() ? Src1 : Src1.getOperand(0));\n    SDValue Res =\n        DAG.getNode(X86ISD::VPERM2X128, DL, VT, LHS, RHS, V.getOperand(2));\n    Res = DAG.getNode(SrcOpc0, DL, SrcVT0, DAG.getBitcast(SrcVT0, Res));\n    return DAG.getBitcast(VT, Res);\n  }\n  case X86ISD::VPERMILPI:\n    // TODO: Handle v4f64 permutes with different low/high lane masks.\n    if (SrcVT0 == MVT::v4f64) {\n      uint64_t Mask = Src0.getConstantOperandVal(1);\n      if ((Mask & 0x3) != ((Mask >> 2) & 0x3))\n        break;\n    }\n    LLVM_FALLTHROUGH;\n  case X86ISD::VSHLI:\n  case X86ISD::VSRLI:\n  case X86ISD::VSRAI:\n  case X86ISD::PSHUFD:\n    if (Src1.isUndef() || Src0.getOperand(1) == Src1.getOperand(1)) {\n      SDValue LHS = DAG.getBitcast(VT, Src0.getOperand(0));\n      SDValue RHS =\n          DAG.getBitcast(VT, Src1.isUndef() ? Src1 : Src1.getOperand(0));\n      SDValue Res =\n          DAG.getNode(X86ISD::VPERM2X128, DL, VT, LHS, RHS, V.getOperand(2));\n      Res = DAG.getNode(SrcOpc0, DL, SrcVT0, DAG.getBitcast(SrcVT0, Res),\n                        Src0.getOperand(1));\n      return DAG.getBitcast(VT, Res);\n    }\n    break;\n  }\n\n  return SDValue();\n}\n\n/// Try to combine x86 target specific shuffles.\nstatic SDValue combineTargetShuffle(SDValue N, SelectionDAG &DAG,\n                                    TargetLowering::DAGCombinerInfo &DCI,\n                                    const X86Subtarget &Subtarget) {\n  SDLoc DL(N);\n  MVT VT = N.getSimpleValueType();\n  SmallVector<int, 4> Mask;\n  unsigned Opcode = N.getOpcode();\n\n  if (SDValue R = combineCommutableSHUFP(N, VT, DL, DAG))\n    return R;\n\n  // Canonicalize UNARYSHUFFLE(XOR(X,-1) -> XOR(UNARYSHUFFLE(X),-1) to\n  // help expose the 'NOT' pattern further up the DAG.\n  // TODO: This might be beneficial for any binop with a 'splattable' operand.\n  switch (Opcode) {\n  case X86ISD::MOVDDUP:\n  case X86ISD::PSHUFD: {\n    SDValue Src = N.getOperand(0);\n    if (Src.hasOneUse() && Src.getValueType() == VT) {\n      if (SDValue Not = IsNOT(Src, DAG, /*OneUse*/ true)) {\n        Not = DAG.getBitcast(VT, Not);\n        Not = Opcode == X86ISD::MOVDDUP\n                  ? DAG.getNode(Opcode, DL, VT, Not)\n                  : DAG.getNode(Opcode, DL, VT, Not, N.getOperand(1));\n        EVT IntVT = Not.getValueType().changeTypeToInteger();\n        SDValue AllOnes = DAG.getConstant(-1, DL, IntVT);\n        Not = DAG.getBitcast(IntVT, Not);\n        Not = DAG.getNode(ISD::XOR, DL, IntVT, Not, AllOnes);\n        return DAG.getBitcast(VT, Not);\n      }\n    }\n    break;\n  }\n  }\n\n  // Handle specific target shuffles.\n  switch (Opcode) {\n  case X86ISD::MOVDDUP: {\n    SDValue Src = N.getOperand(0);\n    // Turn a 128-bit MOVDDUP of a full vector load into movddup+vzload.\n    if (VT == MVT::v2f64 && Src.hasOneUse() &&\n        ISD::isNormalLoad(Src.getNode())) {\n      LoadSDNode *LN = cast<LoadSDNode>(Src);\n      if (SDValue VZLoad = narrowLoadToVZLoad(LN, MVT::f64, MVT::v2f64, DAG)) {\n        SDValue Movddup = DAG.getNode(X86ISD::MOVDDUP, DL, MVT::v2f64, VZLoad);\n        DCI.CombineTo(N.getNode(), Movddup);\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return N; // Return N so it doesn't get rechecked!\n      }\n    }\n\n    return SDValue();\n  }\n  case X86ISD::VBROADCAST: {\n    SDValue Src = N.getOperand(0);\n    SDValue BC = peekThroughBitcasts(Src);\n    EVT SrcVT = Src.getValueType();\n    EVT BCVT = BC.getValueType();\n\n    // If broadcasting from another shuffle, attempt to simplify it.\n    // TODO - we really need a general SimplifyDemandedVectorElts mechanism.\n    if (isTargetShuffle(BC.getOpcode()) &&\n        VT.getScalarSizeInBits() % BCVT.getScalarSizeInBits() == 0) {\n      unsigned Scale = VT.getScalarSizeInBits() / BCVT.getScalarSizeInBits();\n      SmallVector<int, 16> DemandedMask(BCVT.getVectorNumElements(),\n                                        SM_SentinelUndef);\n      for (unsigned i = 0; i != Scale; ++i)\n        DemandedMask[i] = i;\n      if (SDValue Res = combineX86ShufflesRecursively(\n              {BC}, 0, BC, DemandedMask, {}, /*Depth*/ 0,\n              X86::MaxShuffleCombineDepth,\n              /*HasVarMask*/ false, /*AllowVarMask*/ true, DAG, Subtarget))\n        return DAG.getNode(X86ISD::VBROADCAST, DL, VT,\n                           DAG.getBitcast(SrcVT, Res));\n    }\n\n    // broadcast(bitcast(src)) -> bitcast(broadcast(src))\n    // 32-bit targets have to bitcast i64 to f64, so better to bitcast upward.\n    if (Src.getOpcode() == ISD::BITCAST &&\n        SrcVT.getScalarSizeInBits() == BCVT.getScalarSizeInBits() &&\n        DAG.getTargetLoweringInfo().isTypeLegal(BCVT)) {\n      EVT NewVT = EVT::getVectorVT(*DAG.getContext(), BCVT.getScalarType(),\n                                   VT.getVectorNumElements());\n      return DAG.getBitcast(VT, DAG.getNode(X86ISD::VBROADCAST, DL, NewVT, BC));\n    }\n\n    // Reduce broadcast source vector to lowest 128-bits.\n    if (SrcVT.getSizeInBits() > 128)\n      return DAG.getNode(X86ISD::VBROADCAST, DL, VT,\n                         extract128BitVector(Src, 0, DAG, DL));\n\n    // broadcast(scalar_to_vector(x)) -> broadcast(x).\n    if (Src.getOpcode() == ISD::SCALAR_TO_VECTOR)\n      return DAG.getNode(X86ISD::VBROADCAST, DL, VT, Src.getOperand(0));\n\n    // Share broadcast with the longest vector and extract low subvector (free).\n    // Ensure the same SDValue from the SDNode use is being used.\n    for (SDNode *User : Src->uses())\n      if (User != N.getNode() && User->getOpcode() == X86ISD::VBROADCAST &&\n          Src == User->getOperand(0) &&\n          User->getValueSizeInBits(0).getFixedSize() >\n              VT.getFixedSizeInBits()) {\n        return extractSubVector(SDValue(User, 0), 0, DAG, DL,\n                                VT.getSizeInBits());\n      }\n\n    // vbroadcast(scalarload X) -> vbroadcast_load X\n    // For float loads, extract other uses of the scalar from the broadcast.\n    if (!SrcVT.isVector() && (Src.hasOneUse() || VT.isFloatingPoint()) &&\n        ISD::isNormalLoad(Src.getNode())) {\n      LoadSDNode *LN = cast<LoadSDNode>(Src);\n      SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n      SDValue Ops[] = { LN->getChain(), LN->getBasePtr() };\n      SDValue BcastLd =\n          DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, DL, Tys, Ops,\n                                  LN->getMemoryVT(), LN->getMemOperand());\n      // If the load value is used only by N, replace it via CombineTo N.\n      bool NoReplaceExtract = Src.hasOneUse();\n      DCI.CombineTo(N.getNode(), BcastLd);\n      if (NoReplaceExtract) {\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n      } else {\n        SDValue Scl = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, SrcVT, BcastLd,\n                                  DAG.getIntPtrConstant(0, DL));\n        DCI.CombineTo(LN, Scl, BcastLd.getValue(1));\n      }\n      return N; // Return N so it doesn't get rechecked!\n    }\n\n    // Due to isTypeDesirableForOp, we won't always shrink a load truncated to\n    // i16. So shrink it ourselves if we can make a broadcast_load.\n    if (SrcVT == MVT::i16 && Src.getOpcode() == ISD::TRUNCATE &&\n        Src.hasOneUse() && Src.getOperand(0).hasOneUse()) {\n      assert(Subtarget.hasAVX2() && \"Expected AVX2\");\n      SDValue TruncIn = Src.getOperand(0);\n\n      // If this is a truncate of a non extending load we can just narrow it to\n      // use a broadcast_load.\n      if (ISD::isNormalLoad(TruncIn.getNode())) {\n        LoadSDNode *LN = cast<LoadSDNode>(TruncIn);\n        // Unless its volatile or atomic.\n        if (LN->isSimple()) {\n          SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n          SDValue Ops[] = { LN->getChain(), LN->getBasePtr() };\n          SDValue BcastLd = DAG.getMemIntrinsicNode(\n              X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, MVT::i16,\n              LN->getPointerInfo(), LN->getOriginalAlign(),\n              LN->getMemOperand()->getFlags());\n          DCI.CombineTo(N.getNode(), BcastLd);\n          DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n          DCI.recursivelyDeleteUnusedNodes(Src.getNode());\n          return N; // Return N so it doesn't get rechecked!\n        }\n      }\n\n      // If this is a truncate of an i16 extload, we can directly replace it.\n      if (ISD::isUNINDEXEDLoad(Src.getOperand(0).getNode()) &&\n          ISD::isEXTLoad(Src.getOperand(0).getNode())) {\n        LoadSDNode *LN = cast<LoadSDNode>(Src.getOperand(0));\n        if (LN->getMemoryVT().getSizeInBits() == 16) {\n          SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n          SDValue Ops[] = { LN->getChain(), LN->getBasePtr() };\n          SDValue BcastLd =\n              DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, DL, Tys, Ops,\n                                      LN->getMemoryVT(), LN->getMemOperand());\n          DCI.CombineTo(N.getNode(), BcastLd);\n          DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n          DCI.recursivelyDeleteUnusedNodes(Src.getNode());\n          return N; // Return N so it doesn't get rechecked!\n        }\n      }\n\n      // If this is a truncate of load that has been shifted right, we can\n      // offset the pointer and use a narrower load.\n      if (TruncIn.getOpcode() == ISD::SRL &&\n          TruncIn.getOperand(0).hasOneUse() &&\n          isa<ConstantSDNode>(TruncIn.getOperand(1)) &&\n          ISD::isNormalLoad(TruncIn.getOperand(0).getNode())) {\n        LoadSDNode *LN = cast<LoadSDNode>(TruncIn.getOperand(0));\n        unsigned ShiftAmt = TruncIn.getConstantOperandVal(1);\n        // Make sure the shift amount and the load size are divisible by 16.\n        // Don't do this if the load is volatile or atomic.\n        if (ShiftAmt % 16 == 0 && TruncIn.getValueSizeInBits() % 16 == 0 &&\n            LN->isSimple()) {\n          unsigned Offset = ShiftAmt / 8;\n          SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n          SDValue Ptr = DAG.getMemBasePlusOffset(LN->getBasePtr(),\n                                                 TypeSize::Fixed(Offset), DL);\n          SDValue Ops[] = { LN->getChain(), Ptr };\n          SDValue BcastLd = DAG.getMemIntrinsicNode(\n              X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, MVT::i16,\n              LN->getPointerInfo().getWithOffset(Offset),\n              LN->getOriginalAlign(),\n              LN->getMemOperand()->getFlags());\n          DCI.CombineTo(N.getNode(), BcastLd);\n          DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n          DCI.recursivelyDeleteUnusedNodes(Src.getNode());\n          return N; // Return N so it doesn't get rechecked!\n        }\n      }\n    }\n\n    // vbroadcast(vzload X) -> vbroadcast_load X\n    if (Src.getOpcode() == X86ISD::VZEXT_LOAD && Src.hasOneUse()) {\n      MemSDNode *LN = cast<MemIntrinsicSDNode>(Src);\n      if (LN->getMemoryVT().getSizeInBits() == VT.getScalarSizeInBits()) {\n        SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n        SDValue Ops[] = { LN->getChain(), LN->getBasePtr() };\n        SDValue BcastLd =\n            DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, DL, Tys, Ops,\n                                    LN->getMemoryVT(), LN->getMemOperand());\n        DCI.CombineTo(N.getNode(), BcastLd);\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return N; // Return N so it doesn't get rechecked!\n      }\n    }\n\n    // vbroadcast(vector load X) -> vbroadcast_load\n    if ((SrcVT == MVT::v2f64 || SrcVT == MVT::v4f32 || SrcVT == MVT::v2i64 ||\n         SrcVT == MVT::v4i32) &&\n        Src.hasOneUse() && ISD::isNormalLoad(Src.getNode())) {\n      LoadSDNode *LN = cast<LoadSDNode>(Src);\n      // Unless the load is volatile or atomic.\n      if (LN->isSimple()) {\n        SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n        SDValue Ops[] = {LN->getChain(), LN->getBasePtr()};\n        SDValue BcastLd = DAG.getMemIntrinsicNode(\n            X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, SrcVT.getScalarType(),\n            LN->getPointerInfo(), LN->getOriginalAlign(),\n            LN->getMemOperand()->getFlags());\n        DCI.CombineTo(N.getNode(), BcastLd);\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), BcastLd.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return N; // Return N so it doesn't get rechecked!\n      }\n    }\n\n    return SDValue();\n  }\n  case X86ISD::VZEXT_MOVL: {\n    SDValue N0 = N.getOperand(0);\n\n    // If this a vzmovl of a full vector load, replace it with a vzload, unless\n    // the load is volatile.\n    if (N0.hasOneUse() && ISD::isNormalLoad(N0.getNode())) {\n      auto *LN = cast<LoadSDNode>(N0);\n      if (SDValue VZLoad =\n              narrowLoadToVZLoad(LN, VT.getVectorElementType(), VT, DAG)) {\n        DCI.CombineTo(N.getNode(), VZLoad);\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return N;\n      }\n    }\n\n    // If this a VZEXT_MOVL of a VBROADCAST_LOAD, we don't need the broadcast\n    // and can just use a VZEXT_LOAD.\n    // FIXME: Is there some way to do this with SimplifyDemandedVectorElts?\n    if (N0.hasOneUse() && N0.getOpcode() == X86ISD::VBROADCAST_LOAD) {\n      auto *LN = cast<MemSDNode>(N0);\n      if (VT.getScalarSizeInBits() == LN->getMemoryVT().getSizeInBits()) {\n        SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n        SDValue Ops[] = {LN->getChain(), LN->getBasePtr()};\n        SDValue VZLoad =\n            DAG.getMemIntrinsicNode(X86ISD::VZEXT_LOAD, DL, Tys, Ops,\n                                    LN->getMemoryVT(), LN->getMemOperand());\n        DCI.CombineTo(N.getNode(), VZLoad);\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return N;\n      }\n    }\n\n    // Turn (v2i64 (vzext_movl (scalar_to_vector (i64 X)))) into\n    // (v2i64 (bitcast (v4i32 (vzext_movl (scalar_to_vector (i32 (trunc X)))))))\n    // if the upper bits of the i64 are zero.\n    if (N0.hasOneUse() && N0.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n        N0.getOperand(0).hasOneUse() &&\n        N0.getOperand(0).getValueType() == MVT::i64) {\n      SDValue In = N0.getOperand(0);\n      APInt Mask = APInt::getHighBitsSet(64, 32);\n      if (DAG.MaskedValueIsZero(In, Mask)) {\n        SDValue Trunc = DAG.getNode(ISD::TRUNCATE, DL, MVT::i32, In);\n        MVT VecVT = MVT::getVectorVT(MVT::i32, VT.getVectorNumElements() * 2);\n        SDValue SclVec = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VecVT, Trunc);\n        SDValue Movl = DAG.getNode(X86ISD::VZEXT_MOVL, DL, VecVT, SclVec);\n        return DAG.getBitcast(VT, Movl);\n      }\n    }\n\n    // Load a scalar integer constant directly to XMM instead of transferring an\n    // immediate value from GPR.\n    // vzext_movl (scalar_to_vector C) --> load [C,0...]\n    if (N0.getOpcode() == ISD::SCALAR_TO_VECTOR) {\n      if (auto *C = dyn_cast<ConstantSDNode>(N0.getOperand(0))) {\n        // Create a vector constant - scalar constant followed by zeros.\n        EVT ScalarVT = N0.getOperand(0).getValueType();\n        Type *ScalarTy = ScalarVT.getTypeForEVT(*DAG.getContext());\n        unsigned NumElts = VT.getVectorNumElements();\n        Constant *Zero = ConstantInt::getNullValue(ScalarTy);\n        SmallVector<Constant *, 32> ConstantVec(NumElts, Zero);\n        ConstantVec[0] = const_cast<ConstantInt *>(C->getConstantIntValue());\n\n        // Load the vector constant from constant pool.\n        MVT PVT = DAG.getTargetLoweringInfo().getPointerTy(DAG.getDataLayout());\n        SDValue CP = DAG.getConstantPool(ConstantVector::get(ConstantVec), PVT);\n        MachinePointerInfo MPI =\n            MachinePointerInfo::getConstantPool(DAG.getMachineFunction());\n        Align Alignment = cast<ConstantPoolSDNode>(CP)->getAlign();\n        return DAG.getLoad(VT, DL, DAG.getEntryNode(), CP, MPI, Alignment,\n                           MachineMemOperand::MOLoad);\n      }\n    }\n\n    // Pull subvector inserts into undef through VZEXT_MOVL by making it an\n    // insert into a zero vector. This helps get VZEXT_MOVL closer to\n    // scalar_to_vectors where 256/512 are canonicalized to an insert and a\n    // 128-bit scalar_to_vector. This reduces the number of isel patterns.\n    if (!DCI.isBeforeLegalizeOps() && N0.hasOneUse()) {\n      SDValue V = peekThroughOneUseBitcasts(N0);\n\n      if (V.getOpcode() == ISD::INSERT_SUBVECTOR && V.getOperand(0).isUndef() &&\n          isNullConstant(V.getOperand(2))) {\n        SDValue In = V.getOperand(1);\n        MVT SubVT = MVT::getVectorVT(VT.getVectorElementType(),\n                                     In.getValueSizeInBits() /\n                                         VT.getScalarSizeInBits());\n        In = DAG.getBitcast(SubVT, In);\n        SDValue Movl = DAG.getNode(X86ISD::VZEXT_MOVL, DL, SubVT, In);\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                           getZeroVector(VT, Subtarget, DAG, DL), Movl,\n                           V.getOperand(2));\n      }\n    }\n\n    return SDValue();\n  }\n  case X86ISD::BLENDI: {\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n\n    // blend(bitcast(x),bitcast(y)) -> bitcast(blend(x,y)) to narrower types.\n    // TODO: Handle MVT::v16i16 repeated blend mask.\n    if (N0.getOpcode() == ISD::BITCAST && N1.getOpcode() == ISD::BITCAST &&\n        N0.getOperand(0).getValueType() == N1.getOperand(0).getValueType()) {\n      MVT SrcVT = N0.getOperand(0).getSimpleValueType();\n      if ((VT.getScalarSizeInBits() % SrcVT.getScalarSizeInBits()) == 0 &&\n          SrcVT.getScalarSizeInBits() >= 32) {\n        unsigned BlendMask = N.getConstantOperandVal(2);\n        unsigned Size = VT.getVectorNumElements();\n        unsigned Scale = VT.getScalarSizeInBits() / SrcVT.getScalarSizeInBits();\n        BlendMask = scaleVectorShuffleBlendMask(BlendMask, Size, Scale);\n        return DAG.getBitcast(\n            VT, DAG.getNode(X86ISD::BLENDI, DL, SrcVT, N0.getOperand(0),\n                            N1.getOperand(0),\n                            DAG.getTargetConstant(BlendMask, DL, MVT::i8)));\n      }\n    }\n    return SDValue();\n  }\n  case X86ISD::VPERMI: {\n    // vpermi(bitcast(x)) -> bitcast(vpermi(x)) for same number of elements.\n    // TODO: Remove when we have preferred domains in combineX86ShuffleChain.\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n    unsigned EltSizeInBits = VT.getScalarSizeInBits();\n    if (N0.getOpcode() == ISD::BITCAST &&\n        N0.getOperand(0).getScalarValueSizeInBits() == EltSizeInBits) {\n      SDValue Src = N0.getOperand(0);\n      EVT SrcVT = Src.getValueType();\n      SDValue Res = DAG.getNode(X86ISD::VPERMI, DL, SrcVT, Src, N1);\n      return DAG.getBitcast(VT, Res);\n    }\n    return SDValue();\n  }\n  case X86ISD::VPERM2X128: {\n    // Fold vperm2x128(bitcast(x),bitcast(y),c) -> bitcast(vperm2x128(x,y,c)).\n    SDValue LHS = N->getOperand(0);\n    SDValue RHS = N->getOperand(1);\n    if (LHS.getOpcode() == ISD::BITCAST &&\n        (RHS.getOpcode() == ISD::BITCAST || RHS.isUndef())) {\n      EVT SrcVT = LHS.getOperand(0).getValueType();\n      if (RHS.isUndef() || SrcVT == RHS.getOperand(0).getValueType()) {\n        return DAG.getBitcast(VT, DAG.getNode(X86ISD::VPERM2X128, DL, SrcVT,\n                                              DAG.getBitcast(SrcVT, LHS),\n                                              DAG.getBitcast(SrcVT, RHS),\n                                              N->getOperand(2)));\n      }\n    }\n\n    // Fold vperm2x128(op(),op()) -> op(vperm2x128(),vperm2x128()).\n    if (SDValue Res = canonicalizeLaneShuffleWithRepeatedOps(N, DAG, DL))\n      return Res;\n\n    // Fold vperm2x128 subvector shuffle with an inner concat pattern.\n    // vperm2x128(concat(X,Y),concat(Z,W)) --> concat X,Y etc.  \n    auto FindSubVector128 = [&](unsigned Idx) {\n      if (Idx > 3)\n        return SDValue();\n      SDValue Src = peekThroughBitcasts(N.getOperand(Idx < 2 ? 0 : 1));\n      SmallVector<SDValue> SubOps;\n      if (collectConcatOps(Src.getNode(), SubOps) && SubOps.size() == 2)\n        return SubOps[Idx & 1];\n      unsigned NumElts = Src.getValueType().getVectorNumElements();\n      if ((Idx & 1) == 1 && Src.getOpcode() == ISD::INSERT_SUBVECTOR &&\n          Src.getOperand(1).getValueSizeInBits() == 128 &&\n          Src.getConstantOperandAPInt(2) == (NumElts / 2)) {\n        return Src.getOperand(1);\n      }\n      return SDValue();\n    };\n    unsigned Imm = N.getConstantOperandVal(2);\n    if (SDValue SubLo = FindSubVector128(Imm & 0x0F)) {\n      if (SDValue SubHi = FindSubVector128((Imm & 0xF0) >> 4)) {\n        MVT SubVT = VT.getHalfNumVectorElementsVT();\n        SubLo = DAG.getBitcast(SubVT, SubLo);\n        SubHi = DAG.getBitcast(SubVT, SubHi);\n        return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, SubLo, SubHi);\n      }\n    }\n    return SDValue();\n  }\n  case X86ISD::PSHUFD:\n  case X86ISD::PSHUFLW:\n  case X86ISD::PSHUFHW:\n    Mask = getPSHUFShuffleMask(N);\n    assert(Mask.size() == 4);\n    break;\n  case X86ISD::MOVSD:\n  case X86ISD::MOVSS: {\n    SDValue N0 = N.getOperand(0);\n    SDValue N1 = N.getOperand(1);\n\n    // Canonicalize scalar FPOps:\n    // MOVS*(N0, OP(N0, N1)) --> MOVS*(N0, SCALAR_TO_VECTOR(OP(N0[0], N1[0])))\n    // If commutable, allow OP(N1[0], N0[0]).\n    unsigned Opcode1 = N1.getOpcode();\n    if (Opcode1 == ISD::FADD || Opcode1 == ISD::FMUL || Opcode1 == ISD::FSUB ||\n        Opcode1 == ISD::FDIV) {\n      SDValue N10 = N1.getOperand(0);\n      SDValue N11 = N1.getOperand(1);\n      if (N10 == N0 ||\n          (N11 == N0 && (Opcode1 == ISD::FADD || Opcode1 == ISD::FMUL))) {\n        if (N10 != N0)\n          std::swap(N10, N11);\n        MVT SVT = VT.getVectorElementType();\n        SDValue ZeroIdx = DAG.getIntPtrConstant(0, DL);\n        N10 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, SVT, N10, ZeroIdx);\n        N11 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, SVT, N11, ZeroIdx);\n        SDValue Scl = DAG.getNode(Opcode1, DL, SVT, N10, N11);\n        SDValue SclVec = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VT, Scl);\n        return DAG.getNode(Opcode, DL, VT, N0, SclVec);\n      }\n    }\n\n    return SDValue();\n  }\n  case X86ISD::INSERTPS: {\n    assert(VT == MVT::v4f32 && \"INSERTPS ValueType must be MVT::v4f32\");\n    SDValue Op0 = N.getOperand(0);\n    SDValue Op1 = N.getOperand(1);\n    unsigned InsertPSMask = N.getConstantOperandVal(2);\n    unsigned SrcIdx = (InsertPSMask >> 6) & 0x3;\n    unsigned DstIdx = (InsertPSMask >> 4) & 0x3;\n    unsigned ZeroMask = InsertPSMask & 0xF;\n\n    // If we zero out all elements from Op0 then we don't need to reference it.\n    if (((ZeroMask | (1u << DstIdx)) == 0xF) && !Op0.isUndef())\n      return DAG.getNode(X86ISD::INSERTPS, DL, VT, DAG.getUNDEF(VT), Op1,\n                         DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n\n    // If we zero out the element from Op1 then we don't need to reference it.\n    if ((ZeroMask & (1u << DstIdx)) && !Op1.isUndef())\n      return DAG.getNode(X86ISD::INSERTPS, DL, VT, Op0, DAG.getUNDEF(VT),\n                         DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n\n    // Attempt to merge insertps Op1 with an inner target shuffle node.\n    SmallVector<int, 8> TargetMask1;\n    SmallVector<SDValue, 2> Ops1;\n    APInt KnownUndef1, KnownZero1;\n    if (getTargetShuffleAndZeroables(Op1, TargetMask1, Ops1, KnownUndef1,\n                                     KnownZero1)) {\n      if (KnownUndef1[SrcIdx] || KnownZero1[SrcIdx]) {\n        // Zero/UNDEF insertion - zero out element and remove dependency.\n        InsertPSMask |= (1u << DstIdx);\n        return DAG.getNode(X86ISD::INSERTPS, DL, VT, Op0, DAG.getUNDEF(VT),\n                           DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n      }\n      // Update insertps mask srcidx and reference the source input directly.\n      int M = TargetMask1[SrcIdx];\n      assert(0 <= M && M < 8 && \"Shuffle index out of range\");\n      InsertPSMask = (InsertPSMask & 0x3f) | ((M & 0x3) << 6);\n      Op1 = Ops1[M < 4 ? 0 : 1];\n      return DAG.getNode(X86ISD::INSERTPS, DL, VT, Op0, Op1,\n                         DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n    }\n\n    // Attempt to merge insertps Op0 with an inner target shuffle node.\n    SmallVector<int, 8> TargetMask0;\n    SmallVector<SDValue, 2> Ops0;\n    APInt KnownUndef0, KnownZero0;\n    if (getTargetShuffleAndZeroables(Op0, TargetMask0, Ops0, KnownUndef0,\n                                     KnownZero0)) {\n      bool Updated = false;\n      bool UseInput00 = false;\n      bool UseInput01 = false;\n      for (int i = 0; i != 4; ++i) {\n        if ((InsertPSMask & (1u << i)) || (i == (int)DstIdx)) {\n          // No change if element is already zero or the inserted element.\n          continue;\n        } else if (KnownUndef0[i] || KnownZero0[i]) {\n          // If the target mask is undef/zero then we must zero the element.\n          InsertPSMask |= (1u << i);\n          Updated = true;\n          continue;\n        }\n\n        // The input vector element must be inline.\n        int M = TargetMask0[i];\n        if (M != i && M != (i + 4))\n          return SDValue();\n\n        // Determine which inputs of the target shuffle we're using.\n        UseInput00 |= (0 <= M && M < 4);\n        UseInput01 |= (4 <= M);\n      }\n\n      // If we're not using both inputs of the target shuffle then use the\n      // referenced input directly.\n      if (UseInput00 && !UseInput01) {\n        Updated = true;\n        Op0 = Ops0[0];\n      } else if (!UseInput00 && UseInput01) {\n        Updated = true;\n        Op0 = Ops0[1];\n      }\n\n      if (Updated)\n        return DAG.getNode(X86ISD::INSERTPS, DL, VT, Op0, Op1,\n                           DAG.getTargetConstant(InsertPSMask, DL, MVT::i8));\n    }\n\n    // If we're inserting an element from a vbroadcast load, fold the\n    // load into the X86insertps instruction. We need to convert the scalar\n    // load to a vector and clear the source lane of the INSERTPS control.\n    if (Op1.getOpcode() == X86ISD::VBROADCAST_LOAD && Op1.hasOneUse()) {\n      auto *MemIntr = cast<MemIntrinsicSDNode>(Op1);\n      if (MemIntr->getMemoryVT().getScalarSizeInBits() == 32) {\n        SDValue Load = DAG.getLoad(MVT::f32, DL, MemIntr->getChain(),\n                                   MemIntr->getBasePtr(),\n                                   MemIntr->getMemOperand());\n        SDValue Insert = DAG.getNode(X86ISD::INSERTPS, DL, VT, Op0,\n                           DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VT,\n                                       Load),\n                           DAG.getTargetConstant(InsertPSMask & 0x3f, DL, MVT::i8));\n        DAG.ReplaceAllUsesOfValueWith(SDValue(MemIntr, 1), Load.getValue(1));\n        return Insert;\n      }\n    }\n\n    return SDValue();\n  }\n  default:\n    return SDValue();\n  }\n\n  // Nuke no-op shuffles that show up after combining.\n  if (isNoopShuffleMask(Mask))\n    return N.getOperand(0);\n\n  // Look for simplifications involving one or two shuffle instructions.\n  SDValue V = N.getOperand(0);\n  switch (N.getOpcode()) {\n  default:\n    break;\n  case X86ISD::PSHUFLW:\n  case X86ISD::PSHUFHW:\n    assert(VT.getVectorElementType() == MVT::i16 && \"Bad word shuffle type!\");\n\n    // See if this reduces to a PSHUFD which is no more expensive and can\n    // combine with more operations. Note that it has to at least flip the\n    // dwords as otherwise it would have been removed as a no-op.\n    if (makeArrayRef(Mask).equals({2, 3, 0, 1})) {\n      int DMask[] = {0, 1, 2, 3};\n      int DOffset = N.getOpcode() == X86ISD::PSHUFLW ? 0 : 2;\n      DMask[DOffset + 0] = DOffset + 1;\n      DMask[DOffset + 1] = DOffset + 0;\n      MVT DVT = MVT::getVectorVT(MVT::i32, VT.getVectorNumElements() / 2);\n      V = DAG.getBitcast(DVT, V);\n      V = DAG.getNode(X86ISD::PSHUFD, DL, DVT, V,\n                      getV4X86ShuffleImm8ForMask(DMask, DL, DAG));\n      return DAG.getBitcast(VT, V);\n    }\n\n    // Look for shuffle patterns which can be implemented as a single unpack.\n    // FIXME: This doesn't handle the location of the PSHUFD generically, and\n    // only works when we have a PSHUFD followed by two half-shuffles.\n    if (Mask[0] == Mask[1] && Mask[2] == Mask[3] &&\n        (V.getOpcode() == X86ISD::PSHUFLW ||\n         V.getOpcode() == X86ISD::PSHUFHW) &&\n        V.getOpcode() != N.getOpcode() &&\n        V.hasOneUse() && V.getOperand(0).hasOneUse()) {\n      SDValue D = peekThroughOneUseBitcasts(V.getOperand(0));\n      if (D.getOpcode() == X86ISD::PSHUFD) {\n        SmallVector<int, 4> VMask = getPSHUFShuffleMask(V);\n        SmallVector<int, 4> DMask = getPSHUFShuffleMask(D);\n        int NOffset = N.getOpcode() == X86ISD::PSHUFLW ? 0 : 4;\n        int VOffset = V.getOpcode() == X86ISD::PSHUFLW ? 0 : 4;\n        int WordMask[8];\n        for (int i = 0; i < 4; ++i) {\n          WordMask[i + NOffset] = Mask[i] + NOffset;\n          WordMask[i + VOffset] = VMask[i] + VOffset;\n        }\n        // Map the word mask through the DWord mask.\n        int MappedMask[8];\n        for (int i = 0; i < 8; ++i)\n          MappedMask[i] = 2 * DMask[WordMask[i] / 2] + WordMask[i] % 2;\n        if (makeArrayRef(MappedMask).equals({0, 0, 1, 1, 2, 2, 3, 3}) ||\n            makeArrayRef(MappedMask).equals({4, 4, 5, 5, 6, 6, 7, 7})) {\n          // We can replace all three shuffles with an unpack.\n          V = DAG.getBitcast(VT, D.getOperand(0));\n          return DAG.getNode(MappedMask[0] == 0 ? X86ISD::UNPCKL\n                                                : X86ISD::UNPCKH,\n                             DL, VT, V, V);\n        }\n      }\n    }\n\n    break;\n\n  case X86ISD::PSHUFD:\n    if (SDValue NewN = combineRedundantDWordShuffle(N, Mask, DAG))\n      return NewN;\n\n    break;\n  }\n\n  return SDValue();\n}\n\n/// Checks if the shuffle mask takes subsequent elements\n/// alternately from two vectors.\n/// For example <0, 5, 2, 7> or <8, 1, 10, 3, 12, 5, 14, 7> are both correct.\nstatic bool isAddSubOrSubAddMask(ArrayRef<int> Mask, bool &Op0Even) {\n\n  int ParitySrc[2] = {-1, -1};\n  unsigned Size = Mask.size();\n  for (unsigned i = 0; i != Size; ++i) {\n    int M = Mask[i];\n    if (M < 0)\n      continue;\n\n    // Make sure we are using the matching element from the input.\n    if ((M % Size) != i)\n      return false;\n\n    // Make sure we use the same input for all elements of the same parity.\n    int Src = M / Size;\n    if (ParitySrc[i % 2] >= 0 && ParitySrc[i % 2] != Src)\n      return false;\n    ParitySrc[i % 2] = Src;\n  }\n\n  // Make sure each input is used.\n  if (ParitySrc[0] < 0 || ParitySrc[1] < 0 || ParitySrc[0] == ParitySrc[1])\n    return false;\n\n  Op0Even = ParitySrc[0] == 0;\n  return true;\n}\n\n/// Returns true iff the shuffle node \\p N can be replaced with ADDSUB(SUBADD)\n/// operation. If true is returned then the operands of ADDSUB(SUBADD) operation\n/// are written to the parameters \\p Opnd0 and \\p Opnd1.\n///\n/// We combine shuffle to ADDSUB(SUBADD) directly on the abstract vector shuffle nodes\n/// so it is easier to generically match. We also insert dummy vector shuffle\n/// nodes for the operands which explicitly discard the lanes which are unused\n/// by this operation to try to flow through the rest of the combiner\n/// the fact that they're unused.\nstatic bool isAddSubOrSubAdd(SDNode *N, const X86Subtarget &Subtarget,\n                             SelectionDAG &DAG, SDValue &Opnd0, SDValue &Opnd1,\n                             bool &IsSubAdd) {\n\n  EVT VT = N->getValueType(0);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!Subtarget.hasSSE3() || !TLI.isTypeLegal(VT) ||\n      !VT.getSimpleVT().isFloatingPoint())\n    return false;\n\n  // We only handle target-independent shuffles.\n  // FIXME: It would be easy and harmless to use the target shuffle mask\n  // extraction tool to support more.\n  if (N->getOpcode() != ISD::VECTOR_SHUFFLE)\n    return false;\n\n  SDValue V1 = N->getOperand(0);\n  SDValue V2 = N->getOperand(1);\n\n  // Make sure we have an FADD and an FSUB.\n  if ((V1.getOpcode() != ISD::FADD && V1.getOpcode() != ISD::FSUB) ||\n      (V2.getOpcode() != ISD::FADD && V2.getOpcode() != ISD::FSUB) ||\n      V1.getOpcode() == V2.getOpcode())\n    return false;\n\n  // If there are other uses of these operations we can't fold them.\n  if (!V1->hasOneUse() || !V2->hasOneUse())\n    return false;\n\n  // Ensure that both operations have the same operands. Note that we can\n  // commute the FADD operands.\n  SDValue LHS, RHS;\n  if (V1.getOpcode() == ISD::FSUB) {\n    LHS = V1->getOperand(0); RHS = V1->getOperand(1);\n    if ((V2->getOperand(0) != LHS || V2->getOperand(1) != RHS) &&\n        (V2->getOperand(0) != RHS || V2->getOperand(1) != LHS))\n      return false;\n  } else {\n    assert(V2.getOpcode() == ISD::FSUB && \"Unexpected opcode\");\n    LHS = V2->getOperand(0); RHS = V2->getOperand(1);\n    if ((V1->getOperand(0) != LHS || V1->getOperand(1) != RHS) &&\n        (V1->getOperand(0) != RHS || V1->getOperand(1) != LHS))\n      return false;\n  }\n\n  ArrayRef<int> Mask = cast<ShuffleVectorSDNode>(N)->getMask();\n  bool Op0Even;\n  if (!isAddSubOrSubAddMask(Mask, Op0Even))\n    return false;\n\n  // It's a subadd if the vector in the even parity is an FADD.\n  IsSubAdd = Op0Even ? V1->getOpcode() == ISD::FADD\n                     : V2->getOpcode() == ISD::FADD;\n\n  Opnd0 = LHS;\n  Opnd1 = RHS;\n  return true;\n}\n\n/// Combine shuffle of two fma nodes into FMAddSub or FMSubAdd.\nstatic SDValue combineShuffleToFMAddSub(SDNode *N,\n                                        const X86Subtarget &Subtarget,\n                                        SelectionDAG &DAG) {\n  // We only handle target-independent shuffles.\n  // FIXME: It would be easy and harmless to use the target shuffle mask\n  // extraction tool to support more.\n  if (N->getOpcode() != ISD::VECTOR_SHUFFLE)\n    return SDValue();\n\n  MVT VT = N->getSimpleValueType(0);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!Subtarget.hasAnyFMA() || !TLI.isTypeLegal(VT))\n    return SDValue();\n\n  // We're trying to match (shuffle fma(a, b, c), X86Fmsub(a, b, c).\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue FMAdd = Op0, FMSub = Op1;\n  if (FMSub.getOpcode() != X86ISD::FMSUB)\n    std::swap(FMAdd, FMSub);\n\n  if (FMAdd.getOpcode() != ISD::FMA || FMSub.getOpcode() != X86ISD::FMSUB ||\n      FMAdd.getOperand(0) != FMSub.getOperand(0) || !FMAdd.hasOneUse() ||\n      FMAdd.getOperand(1) != FMSub.getOperand(1) || !FMSub.hasOneUse() ||\n      FMAdd.getOperand(2) != FMSub.getOperand(2))\n    return SDValue();\n\n  // Check for correct shuffle mask.\n  ArrayRef<int> Mask = cast<ShuffleVectorSDNode>(N)->getMask();\n  bool Op0Even;\n  if (!isAddSubOrSubAddMask(Mask, Op0Even))\n    return SDValue();\n\n  // FMAddSub takes zeroth operand from FMSub node.\n  SDLoc DL(N);\n  bool IsSubAdd = Op0Even ? Op0 == FMAdd : Op1 == FMAdd;\n  unsigned Opcode = IsSubAdd ? X86ISD::FMSUBADD : X86ISD::FMADDSUB;\n  return DAG.getNode(Opcode, DL, VT, FMAdd.getOperand(0), FMAdd.getOperand(1),\n                     FMAdd.getOperand(2));\n}\n\n/// Try to combine a shuffle into a target-specific add-sub or\n/// mul-add-sub node.\nstatic SDValue combineShuffleToAddSubOrFMAddSub(SDNode *N,\n                                                const X86Subtarget &Subtarget,\n                                                SelectionDAG &DAG) {\n  if (SDValue V = combineShuffleToFMAddSub(N, Subtarget, DAG))\n    return V;\n\n  SDValue Opnd0, Opnd1;\n  bool IsSubAdd;\n  if (!isAddSubOrSubAdd(N, Subtarget, DAG, Opnd0, Opnd1, IsSubAdd))\n    return SDValue();\n\n  MVT VT = N->getSimpleValueType(0);\n  SDLoc DL(N);\n\n  // Try to generate X86ISD::FMADDSUB node here.\n  SDValue Opnd2;\n  if (isFMAddSubOrFMSubAdd(Subtarget, DAG, Opnd0, Opnd1, Opnd2, 2)) {\n    unsigned Opc = IsSubAdd ? X86ISD::FMSUBADD : X86ISD::FMADDSUB;\n    return DAG.getNode(Opc, DL, VT, Opnd0, Opnd1, Opnd2);\n  }\n\n  if (IsSubAdd)\n    return SDValue();\n\n  // Do not generate X86ISD::ADDSUB node for 512-bit types even though\n  // the ADDSUB idiom has been successfully recognized. There are no known\n  // X86 targets with 512-bit ADDSUB instructions!\n  if (VT.is512BitVector())\n    return SDValue();\n\n  return DAG.getNode(X86ISD::ADDSUB, DL, VT, Opnd0, Opnd1);\n}\n\n// We are looking for a shuffle where both sources are concatenated with undef\n// and have a width that is half of the output's width. AVX2 has VPERMD/Q, so\n// if we can express this as a single-source shuffle, that's preferable.\nstatic SDValue combineShuffleOfConcatUndef(SDNode *N, SelectionDAG &DAG,\n                                           const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasAVX2() || !isa<ShuffleVectorSDNode>(N))\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n\n  // We only care about shuffles of 128/256-bit vectors of 32/64-bit values.\n  if (!VT.is128BitVector() && !VT.is256BitVector())\n    return SDValue();\n\n  if (VT.getVectorElementType() != MVT::i32 &&\n      VT.getVectorElementType() != MVT::i64 &&\n      VT.getVectorElementType() != MVT::f32 &&\n      VT.getVectorElementType() != MVT::f64)\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // Check that both sources are concats with undef.\n  if (N0.getOpcode() != ISD::CONCAT_VECTORS ||\n      N1.getOpcode() != ISD::CONCAT_VECTORS || N0.getNumOperands() != 2 ||\n      N1.getNumOperands() != 2 || !N0.getOperand(1).isUndef() ||\n      !N1.getOperand(1).isUndef())\n    return SDValue();\n\n  // Construct the new shuffle mask. Elements from the first source retain their\n  // index, but elements from the second source no longer need to skip an undef.\n  SmallVector<int, 8> Mask;\n  int NumElts = VT.getVectorNumElements();\n\n  ShuffleVectorSDNode *SVOp = cast<ShuffleVectorSDNode>(N);\n  for (int Elt : SVOp->getMask())\n    Mask.push_back(Elt < NumElts ? Elt : (Elt - NumElts / 2));\n\n  SDLoc DL(N);\n  SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, N0.getOperand(0),\n                               N1.getOperand(0));\n  return DAG.getVectorShuffle(VT, DL, Concat, DAG.getUNDEF(VT), Mask);\n}\n\n/// Eliminate a redundant shuffle of a horizontal math op.\nstatic SDValue foldShuffleOfHorizOp(SDNode *N, SelectionDAG &DAG) {\n  // TODO: Can we use getTargetShuffleInputs instead?\n  unsigned Opcode = N->getOpcode();\n  if (Opcode != X86ISD::MOVDDUP && Opcode != X86ISD::VBROADCAST)\n    if (Opcode != X86ISD::UNPCKL && Opcode != X86ISD::UNPCKH)\n      if (Opcode != ISD::VECTOR_SHUFFLE || !N->getOperand(1).isUndef())\n        return SDValue();\n\n  // For a broadcast, peek through an extract element of index 0 to find the\n  // horizontal op: broadcast (ext_vec_elt HOp, 0)\n  EVT VT = N->getValueType(0);\n  if (Opcode == X86ISD::VBROADCAST) {\n    SDValue SrcOp = N->getOperand(0);\n    if (SrcOp.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n        SrcOp.getValueType() == MVT::f64 &&\n        SrcOp.getOperand(0).getValueType() == VT &&\n        isNullConstant(SrcOp.getOperand(1)))\n      N = SrcOp.getNode();\n  }\n\n  SDValue HOp = N->getOperand(0);\n  if (HOp.getOpcode() != X86ISD::HADD && HOp.getOpcode() != X86ISD::FHADD &&\n      HOp.getOpcode() != X86ISD::HSUB && HOp.getOpcode() != X86ISD::FHSUB)\n    return SDValue();\n\n  // unpcklo(hop(x,y),hop(z,w)) -> permute(hop(x,z)).\n  // unpckhi(hop(x,y),hop(z,w)) -> permute(hop(y,w)).\n  // Don't fold if hop(x,y) == hop(z,w).\n  if (Opcode == X86ISD::UNPCKL || Opcode == X86ISD::UNPCKH) {\n    SDValue HOp2 = N->getOperand(1);\n    if (HOp.getOpcode() != HOp2.getOpcode() || VT.getScalarSizeInBits() != 32)\n      return SDValue();\n    if (HOp == HOp2)\n      return SDValue();\n    SDLoc DL(HOp);\n    unsigned LoHi = Opcode == X86ISD::UNPCKL ? 0 : 1;\n    SDValue Res = DAG.getNode(HOp.getOpcode(), DL, VT, HOp.getOperand(LoHi),\n                              HOp2.getOperand(LoHi));\n    // Use SHUFPS for the permute so this will work on SSE3 targets, shuffle\n    // combining and domain handling will simplify this later on.\n    EVT ShuffleVT = VT.changeVectorElementType(MVT::f32);\n    Res = DAG.getBitcast(ShuffleVT, Res);\n    Res = DAG.getNode(X86ISD::SHUFP, DL, ShuffleVT, Res, Res,\n                      getV4X86ShuffleImm8ForMask({0, 2, 1, 3}, DL, DAG));\n    return DAG.getBitcast(VT, Res);\n  }\n\n  // 128-bit horizontal math instructions are defined to operate on adjacent\n  // lanes of each operand as:\n  // v4X32: A[0] + A[1] , A[2] + A[3] , B[0] + B[1] , B[2] + B[3]\n  // ...similarly for v2f64 and v8i16.\n  if (!HOp.getOperand(0).isUndef() && !HOp.getOperand(1).isUndef() &&\n      HOp.getOperand(0) != HOp.getOperand(1))\n    return SDValue();\n\n  // The shuffle that we are eliminating may have allowed the horizontal op to\n  // have an undemanded (undefined) operand. Duplicate the other (defined)\n  // operand to ensure that the results are defined across all lanes without the\n  // shuffle.\n  auto updateHOp = [](SDValue HorizOp, SelectionDAG &DAG) {\n    SDValue X;\n    if (HorizOp.getOperand(0).isUndef()) {\n      assert(!HorizOp.getOperand(1).isUndef() && \"Not expecting foldable h-op\");\n      X = HorizOp.getOperand(1);\n    } else if (HorizOp.getOperand(1).isUndef()) {\n      assert(!HorizOp.getOperand(0).isUndef() && \"Not expecting foldable h-op\");\n      X = HorizOp.getOperand(0);\n    } else {\n      return HorizOp;\n    }\n    return DAG.getNode(HorizOp.getOpcode(), SDLoc(HorizOp),\n                       HorizOp.getValueType(), X, X);\n  };\n\n  // When the operands of a horizontal math op are identical, the low half of\n  // the result is the same as the high half. If a target shuffle is also\n  // replicating low and high halves (and without changing the type/length of\n  // the vector), we don't need the shuffle.\n  if (Opcode == X86ISD::MOVDDUP || Opcode == X86ISD::VBROADCAST) {\n    if (HOp.getScalarValueSizeInBits() == 64 && HOp.getValueType() == VT) {\n      // movddup (hadd X, X) --> hadd X, X\n      // broadcast (extract_vec_elt (hadd X, X), 0) --> hadd X, X\n      assert((HOp.getValueType() == MVT::v2f64 ||\n              HOp.getValueType() == MVT::v4f64) && \"Unexpected type for h-op\");\n      return updateHOp(HOp, DAG);\n    }\n    return SDValue();\n  }\n\n  // shuffle (hadd X, X), undef, [low half...high half] --> hadd X, X\n  ArrayRef<int> Mask = cast<ShuffleVectorSDNode>(N)->getMask();\n\n  // TODO: Other mask possibilities like {1,1} and {1,0} could be added here,\n  // but this should be tied to whatever horizontal op matching and shuffle\n  // canonicalization are producing.\n  if (HOp.getValueSizeInBits() == 128 &&\n      (isShuffleEquivalent(Mask, {0, 0}) ||\n       isShuffleEquivalent(Mask, {0, 1, 0, 1}) ||\n       isShuffleEquivalent(Mask, {0, 1, 2, 3, 0, 1, 2, 3})))\n    return updateHOp(HOp, DAG);\n\n  if (HOp.getValueSizeInBits() == 256 &&\n      (isShuffleEquivalent(Mask, {0, 0, 2, 2}) ||\n       isShuffleEquivalent(Mask, {0, 1, 0, 1, 4, 5, 4, 5}) ||\n       isShuffleEquivalent(\n           Mask, {0, 1, 2, 3, 0, 1, 2, 3, 8, 9, 10, 11, 8, 9, 10, 11})))\n    return updateHOp(HOp, DAG);\n\n  return SDValue();\n}\n\n/// If we have a shuffle of AVX/AVX512 (256/512 bit) vectors that only uses the\n/// low half of each source vector and does not set any high half elements in\n/// the destination vector, narrow the shuffle to half its original size.\nstatic SDValue narrowShuffle(ShuffleVectorSDNode *Shuf, SelectionDAG &DAG) {\n  if (!Shuf->getValueType(0).isSimple())\n    return SDValue();\n  MVT VT = Shuf->getSimpleValueType(0);\n  if (!VT.is256BitVector() && !VT.is512BitVector())\n    return SDValue();\n\n  // See if we can ignore all of the high elements of the shuffle.\n  ArrayRef<int> Mask = Shuf->getMask();\n  if (!isUndefUpperHalf(Mask))\n    return SDValue();\n\n  // Check if the shuffle mask accesses only the low half of each input vector\n  // (half-index output is 0 or 2).\n  int HalfIdx1, HalfIdx2;\n  SmallVector<int, 8> HalfMask(Mask.size() / 2);\n  if (!getHalfShuffleMask(Mask, HalfMask, HalfIdx1, HalfIdx2) ||\n      (HalfIdx1 % 2 == 1) || (HalfIdx2 % 2 == 1))\n    return SDValue();\n\n  // Create a half-width shuffle to replace the unnecessarily wide shuffle.\n  // The trick is knowing that all of the insert/extract are actually free\n  // subregister (zmm<->ymm or ymm<->xmm) ops. That leaves us with a shuffle\n  // of narrow inputs into a narrow output, and that is always cheaper than\n  // the wide shuffle that we started with.\n  return getShuffleHalfVectors(SDLoc(Shuf), Shuf->getOperand(0),\n                               Shuf->getOperand(1), HalfMask, HalfIdx1,\n                               HalfIdx2, false, DAG, /*UseConcat*/true);\n}\n\nstatic SDValue combineShuffle(SDNode *N, SelectionDAG &DAG,\n                              TargetLowering::DAGCombinerInfo &DCI,\n                              const X86Subtarget &Subtarget) {\n  if (auto *Shuf = dyn_cast<ShuffleVectorSDNode>(N))\n    if (SDValue V = narrowShuffle(Shuf, DAG))\n      return V;\n\n  // If we have legalized the vector types, look for blends of FADD and FSUB\n  // nodes that we can fuse into an ADDSUB, FMADDSUB, or FMSUBADD node.\n  SDLoc dl(N);\n  EVT VT = N->getValueType(0);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.isTypeLegal(VT)) {\n    if (SDValue AddSub = combineShuffleToAddSubOrFMAddSub(N, Subtarget, DAG))\n      return AddSub;\n\n    if (SDValue HAddSub = foldShuffleOfHorizOp(N, DAG))\n      return HAddSub;\n\n    // Merge shuffles through binops if its likely we'll be able to merge it\n    // with other shuffles (as long as they aren't splats).\n    // shuffle(bop(shuffle(x,y),shuffle(z,w)),bop(shuffle(a,b),shuffle(c,d)))\n    // TODO: We might be able to move this to DAGCombiner::visitVECTOR_SHUFFLE.\n    if (auto *SVN = dyn_cast<ShuffleVectorSDNode>(N)) {\n      unsigned SrcOpcode = N->getOperand(0).getOpcode();\n      if (SrcOpcode == N->getOperand(1).getOpcode() && TLI.isBinOp(SrcOpcode) &&\n          N->isOnlyUserOf(N->getOperand(0).getNode()) &&\n          N->isOnlyUserOf(N->getOperand(1).getNode())) {\n        SDValue Op00 = N->getOperand(0).getOperand(0);\n        SDValue Op10 = N->getOperand(1).getOperand(0);\n        SDValue Op01 = N->getOperand(0).getOperand(1);\n        SDValue Op11 = N->getOperand(1).getOperand(1);\n        auto *SVN00 = dyn_cast<ShuffleVectorSDNode>(Op00);\n        auto *SVN10 = dyn_cast<ShuffleVectorSDNode>(Op10);\n        auto *SVN01 = dyn_cast<ShuffleVectorSDNode>(Op01);\n        auto *SVN11 = dyn_cast<ShuffleVectorSDNode>(Op11);\n        if (((SVN00 && !SVN00->isSplat()) || (SVN10 && !SVN10->isSplat())) &&\n            ((SVN01 && !SVN01->isSplat()) || (SVN11 && !SVN11->isSplat()))) {\n          SDLoc DL(N);\n          ArrayRef<int> Mask = SVN->getMask();\n          SDValue LHS = DAG.getVectorShuffle(VT, DL, Op00, Op10, Mask);\n          SDValue RHS = DAG.getVectorShuffle(VT, DL, Op01, Op11, Mask);\n          return DAG.getNode(SrcOpcode, DL, VT, LHS, RHS);\n        }\n      }\n    }\n  }\n\n  // Attempt to combine into a vector load/broadcast.\n  if (SDValue LD = combineToConsecutiveLoads(VT, SDValue(N, 0), dl, DAG,\n                                             Subtarget, true))\n    return LD;\n\n  // For AVX2, we sometimes want to combine\n  // (vector_shuffle <mask> (concat_vectors t1, undef)\n  //                        (concat_vectors t2, undef))\n  // Into:\n  // (vector_shuffle <mask> (concat_vectors t1, t2), undef)\n  // Since the latter can be efficiently lowered with VPERMD/VPERMQ\n  if (SDValue ShufConcat = combineShuffleOfConcatUndef(N, DAG, Subtarget))\n    return ShufConcat;\n\n  if (isTargetShuffle(N->getOpcode())) {\n    SDValue Op(N, 0);\n    if (SDValue Shuffle = combineTargetShuffle(Op, DAG, DCI, Subtarget))\n      return Shuffle;\n\n    // Try recursively combining arbitrary sequences of x86 shuffle\n    // instructions into higher-order shuffles. We do this after combining\n    // specific PSHUF instruction sequences into their minimal form so that we\n    // can evaluate how many specialized shuffle instructions are involved in\n    // a particular chain.\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n\n    // Simplify source operands based on shuffle mask.\n    // TODO - merge this into combineX86ShufflesRecursively.\n    APInt KnownUndef, KnownZero;\n    APInt DemandedElts = APInt::getAllOnesValue(VT.getVectorNumElements());\n    if (TLI.SimplifyDemandedVectorElts(Op, DemandedElts, KnownUndef, KnownZero,\n                                       DCI))\n      return SDValue(N, 0);\n  }\n\n  return SDValue();\n}\n\n// Simplify variable target shuffle masks based on the demanded elements.\n// TODO: Handle DemandedBits in mask indices as well?\nbool X86TargetLowering::SimplifyDemandedVectorEltsForTargetShuffle(\n    SDValue Op, const APInt &DemandedElts, unsigned MaskIndex,\n    TargetLowering::TargetLoweringOpt &TLO, unsigned Depth) const {\n  // If we're demanding all elements don't bother trying to simplify the mask.\n  unsigned NumElts = DemandedElts.getBitWidth();\n  if (DemandedElts.isAllOnesValue())\n    return false;\n\n  SDValue Mask = Op.getOperand(MaskIndex);\n  if (!Mask.hasOneUse())\n    return false;\n\n  // Attempt to generically simplify the variable shuffle mask.\n  APInt MaskUndef, MaskZero;\n  if (SimplifyDemandedVectorElts(Mask, DemandedElts, MaskUndef, MaskZero, TLO,\n                                 Depth + 1))\n    return true;\n\n  // Attempt to extract+simplify a (constant pool load) shuffle mask.\n  // TODO: Support other types from getTargetShuffleMaskIndices?\n  SDValue BC = peekThroughOneUseBitcasts(Mask);\n  EVT BCVT = BC.getValueType();\n  auto *Load = dyn_cast<LoadSDNode>(BC);\n  if (!Load)\n    return false;\n\n  const Constant *C = getTargetConstantFromNode(Load);\n  if (!C)\n    return false;\n\n  Type *CTy = C->getType();\n  if (!CTy->isVectorTy() ||\n      CTy->getPrimitiveSizeInBits() != Mask.getValueSizeInBits())\n    return false;\n\n  // Handle scaling for i64 elements on 32-bit targets.\n  unsigned NumCstElts = cast<FixedVectorType>(CTy)->getNumElements();\n  if (NumCstElts != NumElts && NumCstElts != (NumElts * 2))\n    return false;\n  unsigned Scale = NumCstElts / NumElts;\n\n  // Simplify mask if we have an undemanded element that is not undef.\n  bool Simplified = false;\n  SmallVector<Constant *, 32> ConstVecOps;\n  for (unsigned i = 0; i != NumCstElts; ++i) {\n    Constant *Elt = C->getAggregateElement(i);\n    if (!DemandedElts[i / Scale] && !isa<UndefValue>(Elt)) {\n      ConstVecOps.push_back(UndefValue::get(Elt->getType()));\n      Simplified = true;\n      continue;\n    }\n    ConstVecOps.push_back(Elt);\n  }\n  if (!Simplified)\n    return false;\n\n  // Generate new constant pool entry + legalize immediately for the load.\n  SDLoc DL(Op);\n  SDValue CV = TLO.DAG.getConstantPool(ConstantVector::get(ConstVecOps), BCVT);\n  SDValue LegalCV = LowerConstantPool(CV, TLO.DAG);\n  SDValue NewMask = TLO.DAG.getLoad(\n      BCVT, DL, TLO.DAG.getEntryNode(), LegalCV,\n      MachinePointerInfo::getConstantPool(TLO.DAG.getMachineFunction()),\n      Load->getAlign());\n  return TLO.CombineTo(Mask, TLO.DAG.getBitcast(Mask.getValueType(), NewMask));\n}\n\nbool X86TargetLowering::SimplifyDemandedVectorEltsForTargetNode(\n    SDValue Op, const APInt &DemandedElts, APInt &KnownUndef, APInt &KnownZero,\n    TargetLoweringOpt &TLO, unsigned Depth) const {\n  int NumElts = DemandedElts.getBitWidth();\n  unsigned Opc = Op.getOpcode();\n  EVT VT = Op.getValueType();\n\n  // Handle special case opcodes.\n  switch (Opc) {\n  case X86ISD::PMULDQ:\n  case X86ISD::PMULUDQ: {\n    APInt LHSUndef, LHSZero;\n    APInt RHSUndef, RHSZero;\n    SDValue LHS = Op.getOperand(0);\n    SDValue RHS = Op.getOperand(1);\n    if (SimplifyDemandedVectorElts(LHS, DemandedElts, LHSUndef, LHSZero, TLO,\n                                   Depth + 1))\n      return true;\n    if (SimplifyDemandedVectorElts(RHS, DemandedElts, RHSUndef, RHSZero, TLO,\n                                   Depth + 1))\n      return true;\n    // Multiply by zero.\n    KnownZero = LHSZero | RHSZero;\n    break;\n  }\n  case X86ISD::VSHL:\n  case X86ISD::VSRL:\n  case X86ISD::VSRA: {\n    // We only need the bottom 64-bits of the (128-bit) shift amount.\n    SDValue Amt = Op.getOperand(1);\n    MVT AmtVT = Amt.getSimpleValueType();\n    assert(AmtVT.is128BitVector() && \"Unexpected value type\");\n\n    // If we reuse the shift amount just for sse shift amounts then we know that\n    // only the bottom 64-bits are only ever used.\n    bool AssumeSingleUse = llvm::all_of(Amt->uses(), [&Amt](SDNode *Use) {\n      unsigned UseOpc = Use->getOpcode();\n      return (UseOpc == X86ISD::VSHL || UseOpc == X86ISD::VSRL ||\n              UseOpc == X86ISD::VSRA) &&\n             Use->getOperand(0) != Amt;\n    });\n\n    APInt AmtUndef, AmtZero;\n    unsigned NumAmtElts = AmtVT.getVectorNumElements();\n    APInt AmtElts = APInt::getLowBitsSet(NumAmtElts, NumAmtElts / 2);\n    if (SimplifyDemandedVectorElts(Amt, AmtElts, AmtUndef, AmtZero, TLO,\n                                   Depth + 1, AssumeSingleUse))\n      return true;\n    LLVM_FALLTHROUGH;\n  }\n  case X86ISD::VSHLI:\n  case X86ISD::VSRLI:\n  case X86ISD::VSRAI: {\n    SDValue Src = Op.getOperand(0);\n    APInt SrcUndef;\n    if (SimplifyDemandedVectorElts(Src, DemandedElts, SrcUndef, KnownZero, TLO,\n                                   Depth + 1))\n      return true;\n\n    // Aggressively peek through ops to get at the demanded elts.\n    if (!DemandedElts.isAllOnesValue())\n      if (SDValue NewSrc = SimplifyMultipleUseDemandedVectorElts(\n              Src, DemandedElts, TLO.DAG, Depth + 1))\n        return TLO.CombineTo(\n            Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, NewSrc, Op.getOperand(1)));\n    break;\n  }\n  case X86ISD::KSHIFTL: {\n    SDValue Src = Op.getOperand(0);\n    auto *Amt = cast<ConstantSDNode>(Op.getOperand(1));\n    assert(Amt->getAPIntValue().ult(NumElts) && \"Out of range shift amount\");\n    unsigned ShiftAmt = Amt->getZExtValue();\n\n    if (ShiftAmt == 0)\n      return TLO.CombineTo(Op, Src);\n\n    // If this is ((X >>u C1) << ShAmt), see if we can simplify this into a\n    // single shift.  We can do this if the bottom bits (which are shifted\n    // out) are never demanded.\n    if (Src.getOpcode() == X86ISD::KSHIFTR) {\n      if (!DemandedElts.intersects(APInt::getLowBitsSet(NumElts, ShiftAmt))) {\n        unsigned C1 = Src.getConstantOperandVal(1);\n        unsigned NewOpc = X86ISD::KSHIFTL;\n        int Diff = ShiftAmt - C1;\n        if (Diff < 0) {\n          Diff = -Diff;\n          NewOpc = X86ISD::KSHIFTR;\n        }\n\n        SDLoc dl(Op);\n        SDValue NewSA = TLO.DAG.getTargetConstant(Diff, dl, MVT::i8);\n        return TLO.CombineTo(\n            Op, TLO.DAG.getNode(NewOpc, dl, VT, Src.getOperand(0), NewSA));\n      }\n    }\n\n    APInt DemandedSrc = DemandedElts.lshr(ShiftAmt);\n    if (SimplifyDemandedVectorElts(Src, DemandedSrc, KnownUndef, KnownZero, TLO,\n                                   Depth + 1))\n      return true;\n\n    KnownUndef <<= ShiftAmt;\n    KnownZero <<= ShiftAmt;\n    KnownZero.setLowBits(ShiftAmt);\n    break;\n  }\n  case X86ISD::KSHIFTR: {\n    SDValue Src = Op.getOperand(0);\n    auto *Amt = cast<ConstantSDNode>(Op.getOperand(1));\n    assert(Amt->getAPIntValue().ult(NumElts) && \"Out of range shift amount\");\n    unsigned ShiftAmt = Amt->getZExtValue();\n\n    if (ShiftAmt == 0)\n      return TLO.CombineTo(Op, Src);\n\n    // If this is ((X << C1) >>u ShAmt), see if we can simplify this into a\n    // single shift.  We can do this if the top bits (which are shifted\n    // out) are never demanded.\n    if (Src.getOpcode() == X86ISD::KSHIFTL) {\n      if (!DemandedElts.intersects(APInt::getHighBitsSet(NumElts, ShiftAmt))) {\n        unsigned C1 = Src.getConstantOperandVal(1);\n        unsigned NewOpc = X86ISD::KSHIFTR;\n        int Diff = ShiftAmt - C1;\n        if (Diff < 0) {\n          Diff = -Diff;\n          NewOpc = X86ISD::KSHIFTL;\n        }\n\n        SDLoc dl(Op);\n        SDValue NewSA = TLO.DAG.getTargetConstant(Diff, dl, MVT::i8);\n        return TLO.CombineTo(\n            Op, TLO.DAG.getNode(NewOpc, dl, VT, Src.getOperand(0), NewSA));\n      }\n    }\n\n    APInt DemandedSrc = DemandedElts.shl(ShiftAmt);\n    if (SimplifyDemandedVectorElts(Src, DemandedSrc, KnownUndef, KnownZero, TLO,\n                                   Depth + 1))\n      return true;\n\n    KnownUndef.lshrInPlace(ShiftAmt);\n    KnownZero.lshrInPlace(ShiftAmt);\n    KnownZero.setHighBits(ShiftAmt);\n    break;\n  }\n  case X86ISD::CVTSI2P:\n  case X86ISD::CVTUI2P: {\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n    APInt SrcUndef, SrcZero;\n    APInt SrcElts = DemandedElts.zextOrTrunc(SrcVT.getVectorNumElements());\n    if (SimplifyDemandedVectorElts(Src, SrcElts, SrcUndef, SrcZero, TLO,\n                                   Depth + 1))\n      return true;\n    break;\n  }\n  case X86ISD::PACKSS:\n  case X86ISD::PACKUS: {\n    SDValue N0 = Op.getOperand(0);\n    SDValue N1 = Op.getOperand(1);\n\n    APInt DemandedLHS, DemandedRHS;\n    getPackDemandedElts(VT, DemandedElts, DemandedLHS, DemandedRHS);\n\n    APInt SrcUndef, SrcZero;\n    if (SimplifyDemandedVectorElts(N0, DemandedLHS, SrcUndef, SrcZero, TLO,\n                                   Depth + 1))\n      return true;\n    if (SimplifyDemandedVectorElts(N1, DemandedRHS, SrcUndef, SrcZero, TLO,\n                                   Depth + 1))\n      return true;\n\n    // Aggressively peek through ops to get at the demanded elts.\n    // TODO - we should do this for all target/faux shuffles ops.\n    if (!DemandedElts.isAllOnesValue()) {\n      SDValue NewN0 = SimplifyMultipleUseDemandedVectorElts(N0, DemandedLHS,\n                                                            TLO.DAG, Depth + 1);\n      SDValue NewN1 = SimplifyMultipleUseDemandedVectorElts(N1, DemandedRHS,\n                                                            TLO.DAG, Depth + 1);\n      if (NewN0 || NewN1) {\n        NewN0 = NewN0 ? NewN0 : N0;\n        NewN1 = NewN1 ? NewN1 : N1;\n        return TLO.CombineTo(Op,\n                             TLO.DAG.getNode(Opc, SDLoc(Op), VT, NewN0, NewN1));\n      }\n    }\n    break;\n  }\n  case X86ISD::HADD:\n  case X86ISD::HSUB:\n  case X86ISD::FHADD:\n  case X86ISD::FHSUB: {\n    APInt DemandedLHS, DemandedRHS;\n    getHorizDemandedElts(VT, DemandedElts, DemandedLHS, DemandedRHS);\n\n    APInt LHSUndef, LHSZero;\n    if (SimplifyDemandedVectorElts(Op.getOperand(0), DemandedLHS, LHSUndef,\n                                   LHSZero, TLO, Depth + 1))\n      return true;\n    APInt RHSUndef, RHSZero;\n    if (SimplifyDemandedVectorElts(Op.getOperand(1), DemandedRHS, RHSUndef,\n                                   RHSZero, TLO, Depth + 1))\n      return true;\n    break;\n  }\n  case X86ISD::VTRUNC:\n  case X86ISD::VTRUNCS:\n  case X86ISD::VTRUNCUS: {\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n    APInt DemandedSrc = DemandedElts.zextOrTrunc(SrcVT.getVectorNumElements());\n    APInt SrcUndef, SrcZero;\n    if (SimplifyDemandedVectorElts(Src, DemandedSrc, SrcUndef, SrcZero, TLO,\n                                   Depth + 1))\n      return true;\n    KnownZero = SrcZero.zextOrTrunc(NumElts);\n    KnownUndef = SrcUndef.zextOrTrunc(NumElts);\n    break;\n  }\n  case X86ISD::BLENDV: {\n    APInt SelUndef, SelZero;\n    if (SimplifyDemandedVectorElts(Op.getOperand(0), DemandedElts, SelUndef,\n                                   SelZero, TLO, Depth + 1))\n      return true;\n\n    // TODO: Use SelZero to adjust LHS/RHS DemandedElts.\n    APInt LHSUndef, LHSZero;\n    if (SimplifyDemandedVectorElts(Op.getOperand(1), DemandedElts, LHSUndef,\n                                   LHSZero, TLO, Depth + 1))\n      return true;\n\n    APInt RHSUndef, RHSZero;\n    if (SimplifyDemandedVectorElts(Op.getOperand(2), DemandedElts, RHSUndef,\n                                   RHSZero, TLO, Depth + 1))\n      return true;\n\n    KnownZero = LHSZero & RHSZero;\n    KnownUndef = LHSUndef & RHSUndef;\n    break;\n  }\n  case X86ISD::VZEXT_MOVL: {\n    // If upper demanded elements are already zero then we have nothing to do.\n    SDValue Src = Op.getOperand(0);\n    APInt DemandedUpperElts = DemandedElts;\n    DemandedUpperElts.clearLowBits(1);\n    if (TLO.DAG.computeKnownBits(Src, DemandedUpperElts, Depth + 1).isZero())\n      return TLO.CombineTo(Op, Src);\n    break;\n  }\n  case X86ISD::VBROADCAST: {\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n    if (!SrcVT.isVector())\n      break;\n    // Don't bother broadcasting if we just need the 0'th element.\n    if (DemandedElts == 1) {\n      if (Src.getValueType() != VT)\n        Src = widenSubVector(VT.getSimpleVT(), Src, false, Subtarget, TLO.DAG,\n                             SDLoc(Op));\n      return TLO.CombineTo(Op, Src);\n    }\n    APInt SrcUndef, SrcZero;\n    APInt SrcElts = APInt::getOneBitSet(SrcVT.getVectorNumElements(), 0);\n    if (SimplifyDemandedVectorElts(Src, SrcElts, SrcUndef, SrcZero, TLO,\n                                   Depth + 1))\n      return true;\n    // Aggressively peek through src to get at the demanded elt.\n    // TODO - we should do this for all target/faux shuffles ops.\n    if (SDValue NewSrc = SimplifyMultipleUseDemandedVectorElts(\n            Src, SrcElts, TLO.DAG, Depth + 1))\n      return TLO.CombineTo(Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, NewSrc));\n    break;\n  }\n  case X86ISD::VPERMV:\n    if (SimplifyDemandedVectorEltsForTargetShuffle(Op, DemandedElts, 0, TLO,\n                                                   Depth))\n      return true;\n    break;\n  case X86ISD::PSHUFB:\n  case X86ISD::VPERMV3:\n  case X86ISD::VPERMILPV:\n    if (SimplifyDemandedVectorEltsForTargetShuffle(Op, DemandedElts, 1, TLO,\n                                                   Depth))\n      return true;\n    break;\n  case X86ISD::VPPERM:\n  case X86ISD::VPERMIL2:\n    if (SimplifyDemandedVectorEltsForTargetShuffle(Op, DemandedElts, 2, TLO,\n                                                   Depth))\n      return true;\n    break;\n  }\n\n  // For 256/512-bit ops that are 128/256-bit ops glued together, if we do not\n  // demand any of the high elements, then narrow the op to 128/256-bits: e.g.\n  // (op ymm0, ymm1) --> insert undef, (op xmm0, xmm1), 0\n  if ((VT.is256BitVector() || VT.is512BitVector()) &&\n      DemandedElts.lshr(NumElts / 2) == 0) {\n    unsigned SizeInBits = VT.getSizeInBits();\n    unsigned ExtSizeInBits = SizeInBits / 2;\n\n    // See if 512-bit ops only use the bottom 128-bits.\n    if (VT.is512BitVector() && DemandedElts.lshr(NumElts / 4) == 0)\n      ExtSizeInBits = SizeInBits / 4;\n\n    switch (Opc) {\n      // Scalar broadcast.\n    case X86ISD::VBROADCAST: {\n      SDLoc DL(Op);\n      SDValue Src = Op.getOperand(0);\n      if (Src.getValueSizeInBits() > ExtSizeInBits)\n        Src = extractSubVector(Src, 0, TLO.DAG, DL, ExtSizeInBits);\n      EVT BcstVT = EVT::getVectorVT(*TLO.DAG.getContext(), VT.getScalarType(),\n                                    ExtSizeInBits / VT.getScalarSizeInBits());\n      SDValue Bcst = TLO.DAG.getNode(X86ISD::VBROADCAST, DL, BcstVT, Src);\n      return TLO.CombineTo(Op, insertSubVector(TLO.DAG.getUNDEF(VT), Bcst, 0,\n                                               TLO.DAG, DL, ExtSizeInBits));\n    }\n    case X86ISD::VBROADCAST_LOAD: {\n      SDLoc DL(Op);\n      auto *MemIntr = cast<MemIntrinsicSDNode>(Op);\n      EVT BcstVT = EVT::getVectorVT(*TLO.DAG.getContext(), VT.getScalarType(),\n                                    ExtSizeInBits / VT.getScalarSizeInBits());\n      SDVTList Tys = TLO.DAG.getVTList(BcstVT, MVT::Other);\n      SDValue Ops[] = {MemIntr->getOperand(0), MemIntr->getOperand(1)};\n      SDValue Bcst = TLO.DAG.getMemIntrinsicNode(\n          X86ISD::VBROADCAST_LOAD, DL, Tys, Ops, MemIntr->getMemoryVT(),\n          MemIntr->getMemOperand());\n      TLO.DAG.makeEquivalentMemoryOrdering(SDValue(MemIntr, 1),\n                                           Bcst.getValue(1));\n      return TLO.CombineTo(Op, insertSubVector(TLO.DAG.getUNDEF(VT), Bcst, 0,\n                                               TLO.DAG, DL, ExtSizeInBits));\n    }\n      // Subvector broadcast.\n    case X86ISD::SUBV_BROADCAST_LOAD: {\n      auto *MemIntr = cast<MemIntrinsicSDNode>(Op);\n      EVT MemVT = MemIntr->getMemoryVT();\n      if (ExtSizeInBits == MemVT.getStoreSizeInBits()) {\n        SDLoc DL(Op);\n        SDValue Ld =\n            TLO.DAG.getLoad(MemVT, DL, MemIntr->getChain(),\n                            MemIntr->getBasePtr(), MemIntr->getMemOperand());\n        TLO.DAG.makeEquivalentMemoryOrdering(SDValue(MemIntr, 1),\n                                             Ld.getValue(1));\n        return TLO.CombineTo(Op, insertSubVector(TLO.DAG.getUNDEF(VT), Ld, 0,\n                                                 TLO.DAG, DL, ExtSizeInBits));\n      } else if ((ExtSizeInBits % MemVT.getStoreSizeInBits()) == 0) {\n        SDLoc DL(Op);\n        EVT BcstVT = EVT::getVectorVT(*TLO.DAG.getContext(), VT.getScalarType(),\n                                      ExtSizeInBits / VT.getScalarSizeInBits());\n        SDVTList Tys = TLO.DAG.getVTList(BcstVT, MVT::Other);\n        SDValue Ops[] = {MemIntr->getOperand(0), MemIntr->getOperand(1)};\n        SDValue Bcst =\n            TLO.DAG.getMemIntrinsicNode(X86ISD::SUBV_BROADCAST_LOAD, DL, Tys,\n                                        Ops, MemVT, MemIntr->getMemOperand());\n        TLO.DAG.makeEquivalentMemoryOrdering(SDValue(MemIntr, 1),\n                                             Bcst.getValue(1));\n        return TLO.CombineTo(Op, insertSubVector(TLO.DAG.getUNDEF(VT), Bcst, 0,\n                                                 TLO.DAG, DL, ExtSizeInBits));\n      }\n      break;\n    }\n      // Byte shifts by immediate.\n    case X86ISD::VSHLDQ:\n    case X86ISD::VSRLDQ:\n      // Shift by uniform.\n    case X86ISD::VSHL:\n    case X86ISD::VSRL:\n    case X86ISD::VSRA:\n      // Shift by immediate.\n    case X86ISD::VSHLI:\n    case X86ISD::VSRLI:\n    case X86ISD::VSRAI: {\n      SDLoc DL(Op);\n      SDValue Ext0 =\n          extractSubVector(Op.getOperand(0), 0, TLO.DAG, DL, ExtSizeInBits);\n      SDValue ExtOp =\n          TLO.DAG.getNode(Opc, DL, Ext0.getValueType(), Ext0, Op.getOperand(1));\n      SDValue UndefVec = TLO.DAG.getUNDEF(VT);\n      SDValue Insert =\n          insertSubVector(UndefVec, ExtOp, 0, TLO.DAG, DL, ExtSizeInBits);\n      return TLO.CombineTo(Op, Insert);\n    }\n    case X86ISD::VPERMI: {\n      // Simplify PERMPD/PERMQ to extract_subvector.\n      // TODO: This should be done in shuffle combining.\n      if (VT == MVT::v4f64 || VT == MVT::v4i64) {\n        SmallVector<int, 4> Mask;\n        DecodeVPERMMask(NumElts, Op.getConstantOperandVal(1), Mask);\n        if (isUndefOrEqual(Mask[0], 2) && isUndefOrEqual(Mask[1], 3)) {\n          SDLoc DL(Op);\n          SDValue Ext = extractSubVector(Op.getOperand(0), 2, TLO.DAG, DL, 128);\n          SDValue UndefVec = TLO.DAG.getUNDEF(VT);\n          SDValue Insert = insertSubVector(UndefVec, Ext, 0, TLO.DAG, DL, 128);\n          return TLO.CombineTo(Op, Insert);\n        }\n      }\n      break;\n    }\n      // Zero upper elements.\n    case X86ISD::VZEXT_MOVL:\n      // Target unary shuffles by immediate:\n    case X86ISD::PSHUFD:\n    case X86ISD::PSHUFLW:\n    case X86ISD::PSHUFHW:\n    case X86ISD::VPERMILPI:\n      // (Non-Lane Crossing) Target Shuffles.\n    case X86ISD::VPERMILPV:\n    case X86ISD::VPERMIL2:\n    case X86ISD::PSHUFB:\n    case X86ISD::UNPCKL:\n    case X86ISD::UNPCKH:\n    case X86ISD::BLENDI:\n      // Integer ops.\n    case X86ISD::AVG:\n    case X86ISD::PACKSS:\n    case X86ISD::PACKUS:\n      // Horizontal Ops.\n    case X86ISD::HADD:\n    case X86ISD::HSUB:\n    case X86ISD::FHADD:\n    case X86ISD::FHSUB: {\n      SDLoc DL(Op);\n      SmallVector<SDValue, 4> Ops;\n      for (unsigned i = 0, e = Op.getNumOperands(); i != e; ++i) {\n        SDValue SrcOp = Op.getOperand(i);\n        EVT SrcVT = SrcOp.getValueType();\n        assert((!SrcVT.isVector() || SrcVT.getSizeInBits() == SizeInBits) &&\n               \"Unsupported vector size\");\n        Ops.push_back(SrcVT.isVector() ? extractSubVector(SrcOp, 0, TLO.DAG, DL,\n                                                          ExtSizeInBits)\n                                       : SrcOp);\n      }\n      MVT ExtVT = VT.getSimpleVT();\n      ExtVT = MVT::getVectorVT(ExtVT.getScalarType(),\n                               ExtSizeInBits / ExtVT.getScalarSizeInBits());\n      SDValue ExtOp = TLO.DAG.getNode(Opc, DL, ExtVT, Ops);\n      SDValue UndefVec = TLO.DAG.getUNDEF(VT);\n      SDValue Insert =\n          insertSubVector(UndefVec, ExtOp, 0, TLO.DAG, DL, ExtSizeInBits);\n      return TLO.CombineTo(Op, Insert);\n    }\n    }\n  }\n\n  // Get target/faux shuffle mask.\n  APInt OpUndef, OpZero;\n  SmallVector<int, 64> OpMask;\n  SmallVector<SDValue, 2> OpInputs;\n  if (!getTargetShuffleInputs(Op, DemandedElts, OpInputs, OpMask, OpUndef,\n                              OpZero, TLO.DAG, Depth, false))\n    return false;\n\n  // Shuffle inputs must be the same size as the result.\n  if (OpMask.size() != (unsigned)NumElts ||\n      llvm::any_of(OpInputs, [VT](SDValue V) {\n        return VT.getSizeInBits() != V.getValueSizeInBits() ||\n               !V.getValueType().isVector();\n      }))\n    return false;\n\n  KnownZero = OpZero;\n  KnownUndef = OpUndef;\n\n  // Check if shuffle mask can be simplified to undef/zero/identity.\n  int NumSrcs = OpInputs.size();\n  for (int i = 0; i != NumElts; ++i)\n    if (!DemandedElts[i])\n      OpMask[i] = SM_SentinelUndef;\n\n  if (isUndefInRange(OpMask, 0, NumElts)) {\n    KnownUndef.setAllBits();\n    return TLO.CombineTo(Op, TLO.DAG.getUNDEF(VT));\n  }\n  if (isUndefOrZeroInRange(OpMask, 0, NumElts)) {\n    KnownZero.setAllBits();\n    return TLO.CombineTo(\n        Op, getZeroVector(VT.getSimpleVT(), Subtarget, TLO.DAG, SDLoc(Op)));\n  }\n  for (int Src = 0; Src != NumSrcs; ++Src)\n    if (isSequentialOrUndefInRange(OpMask, 0, NumElts, Src * NumElts))\n      return TLO.CombineTo(Op, TLO.DAG.getBitcast(VT, OpInputs[Src]));\n\n  // Attempt to simplify inputs.\n  for (int Src = 0; Src != NumSrcs; ++Src) {\n    // TODO: Support inputs of different types.\n    if (OpInputs[Src].getValueType() != VT)\n      continue;\n\n    int Lo = Src * NumElts;\n    APInt SrcElts = APInt::getNullValue(NumElts);\n    for (int i = 0; i != NumElts; ++i)\n      if (DemandedElts[i]) {\n        int M = OpMask[i] - Lo;\n        if (0 <= M && M < NumElts)\n          SrcElts.setBit(M);\n      }\n\n    // TODO - Propagate input undef/zero elts.\n    APInt SrcUndef, SrcZero;\n    if (SimplifyDemandedVectorElts(OpInputs[Src], SrcElts, SrcUndef, SrcZero,\n                                   TLO, Depth + 1))\n      return true;\n  }\n\n  // If we don't demand all elements, then attempt to combine to a simpler\n  // shuffle.\n  // We need to convert the depth to something combineX86ShufflesRecursively\n  // can handle - so pretend its Depth == 0 again, and reduce the max depth\n  // to match. This prevents combineX86ShuffleChain from returning a\n  // combined shuffle that's the same as the original root, causing an\n  // infinite loop.\n  if (!DemandedElts.isAllOnesValue()) {\n    assert(Depth < X86::MaxShuffleCombineDepth && \"Depth out of range\");\n\n    SmallVector<int, 64> DemandedMask(NumElts, SM_SentinelUndef);\n    for (int i = 0; i != NumElts; ++i)\n      if (DemandedElts[i])\n        DemandedMask[i] = i;\n\n    SDValue NewShuffle = combineX86ShufflesRecursively(\n        {Op}, 0, Op, DemandedMask, {}, 0, X86::MaxShuffleCombineDepth - Depth,\n        /*HasVarMask*/ false,\n        /*AllowVarMask*/ true, TLO.DAG, Subtarget);\n    if (NewShuffle)\n      return TLO.CombineTo(Op, NewShuffle);\n  }\n\n  return false;\n}\n\nbool X86TargetLowering::SimplifyDemandedBitsForTargetNode(\n    SDValue Op, const APInt &OriginalDemandedBits,\n    const APInt &OriginalDemandedElts, KnownBits &Known, TargetLoweringOpt &TLO,\n    unsigned Depth) const {\n  EVT VT = Op.getValueType();\n  unsigned BitWidth = OriginalDemandedBits.getBitWidth();\n  unsigned Opc = Op.getOpcode();\n  switch(Opc) {\n  case X86ISD::VTRUNC: {\n    KnownBits KnownOp;\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n\n    // Simplify the input, using demanded bit information.\n    APInt TruncMask = OriginalDemandedBits.zext(SrcVT.getScalarSizeInBits());\n    APInt DemandedElts = OriginalDemandedElts.trunc(SrcVT.getVectorNumElements());\n    if (SimplifyDemandedBits(Src, TruncMask, DemandedElts, KnownOp, TLO, Depth + 1))\n      return true;\n    break;\n  }\n  case X86ISD::PMULDQ:\n  case X86ISD::PMULUDQ: {\n    // PMULDQ/PMULUDQ only uses lower 32 bits from each vector element.\n    KnownBits KnownOp;\n    SDValue LHS = Op.getOperand(0);\n    SDValue RHS = Op.getOperand(1);\n    // FIXME: Can we bound this better?\n    APInt DemandedMask = APInt::getLowBitsSet(64, 32);\n    if (SimplifyDemandedBits(LHS, DemandedMask, OriginalDemandedElts, KnownOp,\n                             TLO, Depth + 1))\n      return true;\n    if (SimplifyDemandedBits(RHS, DemandedMask, OriginalDemandedElts, KnownOp,\n                             TLO, Depth + 1))\n      return true;\n\n    // Aggressively peek through ops to get at the demanded low bits.\n    SDValue DemandedLHS = SimplifyMultipleUseDemandedBits(\n        LHS, DemandedMask, OriginalDemandedElts, TLO.DAG, Depth + 1);\n    SDValue DemandedRHS = SimplifyMultipleUseDemandedBits(\n        RHS, DemandedMask, OriginalDemandedElts, TLO.DAG, Depth + 1);\n    if (DemandedLHS || DemandedRHS) {\n      DemandedLHS = DemandedLHS ? DemandedLHS : LHS;\n      DemandedRHS = DemandedRHS ? DemandedRHS : RHS;\n      return TLO.CombineTo(\n          Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, DemandedLHS, DemandedRHS));\n    }\n    break;\n  }\n  case X86ISD::VSHLI: {\n    SDValue Op0 = Op.getOperand(0);\n\n    unsigned ShAmt = Op.getConstantOperandVal(1);\n    if (ShAmt >= BitWidth)\n      break;\n\n    APInt DemandedMask = OriginalDemandedBits.lshr(ShAmt);\n\n    // If this is ((X >>u C1) << ShAmt), see if we can simplify this into a\n    // single shift.  We can do this if the bottom bits (which are shifted\n    // out) are never demanded.\n    if (Op0.getOpcode() == X86ISD::VSRLI &&\n        OriginalDemandedBits.countTrailingZeros() >= ShAmt) {\n      unsigned Shift2Amt = Op0.getConstantOperandVal(1);\n      if (Shift2Amt < BitWidth) {\n        int Diff = ShAmt - Shift2Amt;\n        if (Diff == 0)\n          return TLO.CombineTo(Op, Op0.getOperand(0));\n\n        unsigned NewOpc = Diff < 0 ? X86ISD::VSRLI : X86ISD::VSHLI;\n        SDValue NewShift = TLO.DAG.getNode(\n            NewOpc, SDLoc(Op), VT, Op0.getOperand(0),\n            TLO.DAG.getTargetConstant(std::abs(Diff), SDLoc(Op), MVT::i8));\n        return TLO.CombineTo(Op, NewShift);\n      }\n    }\n\n    // If we are only demanding sign bits then we can use the shift source directly.\n    unsigned NumSignBits =\n        TLO.DAG.ComputeNumSignBits(Op0, OriginalDemandedElts, Depth + 1);\n    unsigned UpperDemandedBits =\n        BitWidth - OriginalDemandedBits.countTrailingZeros();\n    if (NumSignBits > ShAmt && (NumSignBits - ShAmt) >= UpperDemandedBits)\n      return TLO.CombineTo(Op, Op0);\n\n    if (SimplifyDemandedBits(Op0, DemandedMask, OriginalDemandedElts, Known,\n                             TLO, Depth + 1))\n      return true;\n\n    assert(!Known.hasConflict() && \"Bits known to be one AND zero?\");\n    Known.Zero <<= ShAmt;\n    Known.One <<= ShAmt;\n\n    // Low bits known zero.\n    Known.Zero.setLowBits(ShAmt);\n    return false;\n  }\n  case X86ISD::VSRLI: {\n    unsigned ShAmt = Op.getConstantOperandVal(1);\n    if (ShAmt >= BitWidth)\n      break;\n\n    APInt DemandedMask = OriginalDemandedBits << ShAmt;\n\n    if (SimplifyDemandedBits(Op.getOperand(0), DemandedMask,\n                             OriginalDemandedElts, Known, TLO, Depth + 1))\n      return true;\n\n    assert(!Known.hasConflict() && \"Bits known to be one AND zero?\");\n    Known.Zero.lshrInPlace(ShAmt);\n    Known.One.lshrInPlace(ShAmt);\n\n    // High bits known zero.\n    Known.Zero.setHighBits(ShAmt);\n    return false;\n  }\n  case X86ISD::VSRAI: {\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    unsigned ShAmt = cast<ConstantSDNode>(Op1)->getZExtValue();\n    if (ShAmt >= BitWidth)\n      break;\n\n    APInt DemandedMask = OriginalDemandedBits << ShAmt;\n\n    // If we just want the sign bit then we don't need to shift it.\n    if (OriginalDemandedBits.isSignMask())\n      return TLO.CombineTo(Op, Op0);\n\n    // fold (VSRAI (VSHLI X, C1), C1) --> X iff NumSignBits(X) > C1\n    if (Op0.getOpcode() == X86ISD::VSHLI &&\n        Op.getOperand(1) == Op0.getOperand(1)) {\n      SDValue Op00 = Op0.getOperand(0);\n      unsigned NumSignBits =\n          TLO.DAG.ComputeNumSignBits(Op00, OriginalDemandedElts);\n      if (ShAmt < NumSignBits)\n        return TLO.CombineTo(Op, Op00);\n    }\n\n    // If any of the demanded bits are produced by the sign extension, we also\n    // demand the input sign bit.\n    if (OriginalDemandedBits.countLeadingZeros() < ShAmt)\n      DemandedMask.setSignBit();\n\n    if (SimplifyDemandedBits(Op0, DemandedMask, OriginalDemandedElts, Known,\n                             TLO, Depth + 1))\n      return true;\n\n    assert(!Known.hasConflict() && \"Bits known to be one AND zero?\");\n    Known.Zero.lshrInPlace(ShAmt);\n    Known.One.lshrInPlace(ShAmt);\n\n    // If the input sign bit is known to be zero, or if none of the top bits\n    // are demanded, turn this into an unsigned shift right.\n    if (Known.Zero[BitWidth - ShAmt - 1] ||\n        OriginalDemandedBits.countLeadingZeros() >= ShAmt)\n      return TLO.CombineTo(\n          Op, TLO.DAG.getNode(X86ISD::VSRLI, SDLoc(Op), VT, Op0, Op1));\n\n    // High bits are known one.\n    if (Known.One[BitWidth - ShAmt - 1])\n      Known.One.setHighBits(ShAmt);\n    return false;\n  }\n  case X86ISD::PEXTRB:\n  case X86ISD::PEXTRW: {\n    SDValue Vec = Op.getOperand(0);\n    auto *CIdx = dyn_cast<ConstantSDNode>(Op.getOperand(1));\n    MVT VecVT = Vec.getSimpleValueType();\n    unsigned NumVecElts = VecVT.getVectorNumElements();\n\n    if (CIdx && CIdx->getAPIntValue().ult(NumVecElts)) {\n      unsigned Idx = CIdx->getZExtValue();\n      unsigned VecBitWidth = VecVT.getScalarSizeInBits();\n\n      // If we demand no bits from the vector then we must have demanded\n      // bits from the implict zext - simplify to zero.\n      APInt DemandedVecBits = OriginalDemandedBits.trunc(VecBitWidth);\n      if (DemandedVecBits == 0)\n        return TLO.CombineTo(Op, TLO.DAG.getConstant(0, SDLoc(Op), VT));\n\n      APInt KnownUndef, KnownZero;\n      APInt DemandedVecElts = APInt::getOneBitSet(NumVecElts, Idx);\n      if (SimplifyDemandedVectorElts(Vec, DemandedVecElts, KnownUndef,\n                                     KnownZero, TLO, Depth + 1))\n        return true;\n\n      KnownBits KnownVec;\n      if (SimplifyDemandedBits(Vec, DemandedVecBits, DemandedVecElts,\n                               KnownVec, TLO, Depth + 1))\n        return true;\n\n      if (SDValue V = SimplifyMultipleUseDemandedBits(\n              Vec, DemandedVecBits, DemandedVecElts, TLO.DAG, Depth + 1))\n        return TLO.CombineTo(\n            Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, V, Op.getOperand(1)));\n\n      Known = KnownVec.zext(BitWidth);\n      return false;\n    }\n    break;\n  }\n  case X86ISD::PINSRB:\n  case X86ISD::PINSRW: {\n    SDValue Vec = Op.getOperand(0);\n    SDValue Scl = Op.getOperand(1);\n    auto *CIdx = dyn_cast<ConstantSDNode>(Op.getOperand(2));\n    MVT VecVT = Vec.getSimpleValueType();\n\n    if (CIdx && CIdx->getAPIntValue().ult(VecVT.getVectorNumElements())) {\n      unsigned Idx = CIdx->getZExtValue();\n      if (!OriginalDemandedElts[Idx])\n        return TLO.CombineTo(Op, Vec);\n\n      KnownBits KnownVec;\n      APInt DemandedVecElts(OriginalDemandedElts);\n      DemandedVecElts.clearBit(Idx);\n      if (SimplifyDemandedBits(Vec, OriginalDemandedBits, DemandedVecElts,\n                               KnownVec, TLO, Depth + 1))\n        return true;\n\n      KnownBits KnownScl;\n      unsigned NumSclBits = Scl.getScalarValueSizeInBits();\n      APInt DemandedSclBits = OriginalDemandedBits.zext(NumSclBits);\n      if (SimplifyDemandedBits(Scl, DemandedSclBits, KnownScl, TLO, Depth + 1))\n        return true;\n\n      KnownScl = KnownScl.trunc(VecVT.getScalarSizeInBits());\n      Known = KnownBits::commonBits(KnownVec, KnownScl);\n      return false;\n    }\n    break;\n  }\n  case X86ISD::PACKSS:\n    // PACKSS saturates to MIN/MAX integer values. So if we just want the\n    // sign bit then we can just ask for the source operands sign bit.\n    // TODO - add known bits handling.\n    if (OriginalDemandedBits.isSignMask()) {\n      APInt DemandedLHS, DemandedRHS;\n      getPackDemandedElts(VT, OriginalDemandedElts, DemandedLHS, DemandedRHS);\n\n      KnownBits KnownLHS, KnownRHS;\n      APInt SignMask = APInt::getSignMask(BitWidth * 2);\n      if (SimplifyDemandedBits(Op.getOperand(0), SignMask, DemandedLHS,\n                               KnownLHS, TLO, Depth + 1))\n        return true;\n      if (SimplifyDemandedBits(Op.getOperand(1), SignMask, DemandedRHS,\n                               KnownRHS, TLO, Depth + 1))\n        return true;\n\n      // Attempt to avoid multi-use ops if we don't need anything from them.\n      SDValue DemandedOp0 = SimplifyMultipleUseDemandedBits(\n          Op.getOperand(0), SignMask, DemandedLHS, TLO.DAG, Depth + 1);\n      SDValue DemandedOp1 = SimplifyMultipleUseDemandedBits(\n          Op.getOperand(1), SignMask, DemandedRHS, TLO.DAG, Depth + 1);\n      if (DemandedOp0 || DemandedOp1) {\n        SDValue Op0 = DemandedOp0 ? DemandedOp0 : Op.getOperand(0);\n        SDValue Op1 = DemandedOp1 ? DemandedOp1 : Op.getOperand(1);\n        return TLO.CombineTo(Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, Op0, Op1));\n      }\n    }\n    // TODO - add general PACKSS/PACKUS SimplifyDemandedBits support.\n    break;\n  case X86ISD::PCMPGT:\n    // icmp sgt(0, R) == ashr(R, BitWidth-1).\n    // iff we only need the sign bit then we can use R directly.\n    if (OriginalDemandedBits.isSignMask() &&\n        ISD::isBuildVectorAllZeros(Op.getOperand(0).getNode()))\n      return TLO.CombineTo(Op, Op.getOperand(1));\n    break;\n  case X86ISD::MOVMSK: {\n    SDValue Src = Op.getOperand(0);\n    MVT SrcVT = Src.getSimpleValueType();\n    unsigned SrcBits = SrcVT.getScalarSizeInBits();\n    unsigned NumElts = SrcVT.getVectorNumElements();\n\n    // If we don't need the sign bits at all just return zero.\n    if (OriginalDemandedBits.countTrailingZeros() >= NumElts)\n      return TLO.CombineTo(Op, TLO.DAG.getConstant(0, SDLoc(Op), VT));\n\n    // Only demand the vector elements of the sign bits we need.\n    APInt KnownUndef, KnownZero;\n    APInt DemandedElts = OriginalDemandedBits.zextOrTrunc(NumElts);\n    if (SimplifyDemandedVectorElts(Src, DemandedElts, KnownUndef, KnownZero,\n                                   TLO, Depth + 1))\n      return true;\n\n    Known.Zero = KnownZero.zextOrSelf(BitWidth);\n    Known.Zero.setHighBits(BitWidth - NumElts);\n\n    // MOVMSK only uses the MSB from each vector element.\n    KnownBits KnownSrc;\n    APInt DemandedSrcBits = APInt::getSignMask(SrcBits);\n    if (SimplifyDemandedBits(Src, DemandedSrcBits, DemandedElts, KnownSrc, TLO,\n                             Depth + 1))\n      return true;\n\n    if (KnownSrc.One[SrcBits - 1])\n      Known.One.setLowBits(NumElts);\n    else if (KnownSrc.Zero[SrcBits - 1])\n      Known.Zero.setLowBits(NumElts);\n\n    // Attempt to avoid multi-use os if we don't need anything from it.\n    if (SDValue NewSrc = SimplifyMultipleUseDemandedBits(\n            Src, DemandedSrcBits, DemandedElts, TLO.DAG, Depth + 1))\n      return TLO.CombineTo(Op, TLO.DAG.getNode(Opc, SDLoc(Op), VT, NewSrc));\n    return false;\n  }\n  case X86ISD::BEXTR:\n  case X86ISD::BEXTRI: {\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    // Only bottom 16-bits of the control bits are required.\n    if (auto *Cst1 = dyn_cast<ConstantSDNode>(Op1)) {\n      // NOTE: SimplifyDemandedBits won't do this for constants.\n      uint64_t Val1 = Cst1->getZExtValue();\n      uint64_t MaskedVal1 = Val1 & 0xFFFF;\n      if (Opc == X86ISD::BEXTR && MaskedVal1 != Val1) {\n        SDLoc DL(Op);\n        return TLO.CombineTo(\n            Op, TLO.DAG.getNode(X86ISD::BEXTR, DL, VT, Op0,\n                                TLO.DAG.getConstant(MaskedVal1, DL, VT)));\n      }\n\n      unsigned Shift = Cst1->getAPIntValue().extractBitsAsZExtValue(8, 0);\n      unsigned Length = Cst1->getAPIntValue().extractBitsAsZExtValue(8, 8);\n\n      // If the length is 0, the result is 0.\n      if (Length == 0) {\n        Known.setAllZero();\n        return false;\n      }\n\n      if ((Shift + Length) <= BitWidth) {\n        APInt DemandedMask = APInt::getBitsSet(BitWidth, Shift, Shift + Length);\n        if (SimplifyDemandedBits(Op0, DemandedMask, Known, TLO, Depth + 1))\n          return true;\n\n        Known = Known.extractBits(Length, Shift);\n        Known = Known.zextOrTrunc(BitWidth);\n        return false;\n      }\n    } else {\n      assert(Opc == X86ISD::BEXTR && \"Unexpected opcode!\");\n      KnownBits Known1;\n      APInt DemandedMask(APInt::getLowBitsSet(BitWidth, 16));\n      if (SimplifyDemandedBits(Op1, DemandedMask, Known1, TLO, Depth + 1))\n        return true;\n\n      // If the length is 0, replace with 0.\n      KnownBits LengthBits = Known1.extractBits(8, 8);\n      if (LengthBits.isZero())\n        return TLO.CombineTo(Op, TLO.DAG.getConstant(0, SDLoc(Op), VT));\n    }\n\n    break;\n  }\n  case X86ISD::PDEP: {\n    SDValue Op0 = Op.getOperand(0);\n    SDValue Op1 = Op.getOperand(1);\n\n    unsigned DemandedBitsLZ = OriginalDemandedBits.countLeadingZeros();\n    APInt LoMask = APInt::getLowBitsSet(BitWidth, BitWidth - DemandedBitsLZ);\n\n    // If the demanded bits has leading zeroes, we don't demand those from the\n    // mask.\n    if (SimplifyDemandedBits(Op1, LoMask, Known, TLO, Depth + 1))\n      return true;\n\n    // The number of possible 1s in the mask determines the number of LSBs of\n    // operand 0 used. Undemanded bits from the mask don't matter so filter\n    // them before counting.\n    KnownBits Known2;\n    uint64_t Count = (~Known.Zero & LoMask).countPopulation();\n    APInt DemandedMask(APInt::getLowBitsSet(BitWidth, Count));\n    if (SimplifyDemandedBits(Op0, DemandedMask, Known2, TLO, Depth + 1))\n      return true;\n\n    // Zeroes are retained from the mask, but not ones.\n    Known.One.clearAllBits();\n    // The result will have at least as many trailing zeros as the non-mask\n    // operand since bits can only map to the same or higher bit position.\n    Known.Zero.setLowBits(Known2.countMinTrailingZeros());\n    return false;\n  }\n  }\n\n  return TargetLowering::SimplifyDemandedBitsForTargetNode(\n      Op, OriginalDemandedBits, OriginalDemandedElts, Known, TLO, Depth);\n}\n\nSDValue X86TargetLowering::SimplifyMultipleUseDemandedBitsForTargetNode(\n    SDValue Op, const APInt &DemandedBits, const APInt &DemandedElts,\n    SelectionDAG &DAG, unsigned Depth) const {\n  int NumElts = DemandedElts.getBitWidth();\n  unsigned Opc = Op.getOpcode();\n  EVT VT = Op.getValueType();\n\n  switch (Opc) {\n  case X86ISD::PINSRB:\n  case X86ISD::PINSRW: {\n    // If we don't demand the inserted element, return the base vector.\n    SDValue Vec = Op.getOperand(0);\n    auto *CIdx = dyn_cast<ConstantSDNode>(Op.getOperand(2));\n    MVT VecVT = Vec.getSimpleValueType();\n    if (CIdx && CIdx->getAPIntValue().ult(VecVT.getVectorNumElements()) &&\n        !DemandedElts[CIdx->getZExtValue()])\n      return Vec;\n    break;\n  }\n  case X86ISD::VSHLI: {\n    // If we are only demanding sign bits then we can use the shift source\n    // directly.\n    SDValue Op0 = Op.getOperand(0);\n    unsigned ShAmt = Op.getConstantOperandVal(1);\n    unsigned BitWidth = DemandedBits.getBitWidth();\n    unsigned NumSignBits = DAG.ComputeNumSignBits(Op0, DemandedElts, Depth + 1);\n    unsigned UpperDemandedBits = BitWidth - DemandedBits.countTrailingZeros();\n    if (NumSignBits > ShAmt && (NumSignBits - ShAmt) >= UpperDemandedBits)\n      return Op0;\n    break;\n  }\n  case X86ISD::VSRAI:\n    // iff we only need the sign bit then we can use the source directly.\n    // TODO: generalize where we only demand extended signbits.\n    if (DemandedBits.isSignMask())\n      return Op.getOperand(0);\n    break;\n  case X86ISD::PCMPGT:\n    // icmp sgt(0, R) == ashr(R, BitWidth-1).\n    // iff we only need the sign bit then we can use R directly.\n    if (DemandedBits.isSignMask() &&\n        ISD::isBuildVectorAllZeros(Op.getOperand(0).getNode()))\n      return Op.getOperand(1);\n    break;\n  }\n\n  APInt ShuffleUndef, ShuffleZero;\n  SmallVector<int, 16> ShuffleMask;\n  SmallVector<SDValue, 2> ShuffleOps;\n  if (getTargetShuffleInputs(Op, DemandedElts, ShuffleOps, ShuffleMask,\n                             ShuffleUndef, ShuffleZero, DAG, Depth, false)) {\n    // If all the demanded elts are from one operand and are inline,\n    // then we can use the operand directly.\n    int NumOps = ShuffleOps.size();\n    if (ShuffleMask.size() == (unsigned)NumElts &&\n        llvm::all_of(ShuffleOps, [VT](SDValue V) {\n          return VT.getSizeInBits() == V.getValueSizeInBits();\n        })) {\n\n      if (DemandedElts.isSubsetOf(ShuffleUndef))\n        return DAG.getUNDEF(VT);\n      if (DemandedElts.isSubsetOf(ShuffleUndef | ShuffleZero))\n        return getZeroVector(VT.getSimpleVT(), Subtarget, DAG, SDLoc(Op));\n\n      // Bitmask that indicates which ops have only been accessed 'inline'.\n      APInt IdentityOp = APInt::getAllOnesValue(NumOps);\n      for (int i = 0; i != NumElts; ++i) {\n        int M = ShuffleMask[i];\n        if (!DemandedElts[i] || ShuffleUndef[i])\n          continue;\n        int OpIdx = M / NumElts;\n        int EltIdx = M % NumElts;\n        if (M < 0 || EltIdx != i) {\n          IdentityOp.clearAllBits();\n          break;\n        }\n        IdentityOp &= APInt::getOneBitSet(NumOps, OpIdx);\n        if (IdentityOp == 0)\n          break;\n      }\n      assert((IdentityOp == 0 || IdentityOp.countPopulation() == 1) &&\n             \"Multiple identity shuffles detected\");\n\n      if (IdentityOp != 0)\n        return DAG.getBitcast(VT, ShuffleOps[IdentityOp.countTrailingZeros()]);\n    }\n  }\n\n  return TargetLowering::SimplifyMultipleUseDemandedBitsForTargetNode(\n      Op, DemandedBits, DemandedElts, DAG, Depth);\n}\n\n// Helper to peek through bitops/setcc to determine size of source vector.\n// Allows combineBitcastvxi1 to determine what size vector generated a <X x i1>.\nstatic bool checkBitcastSrcVectorSize(SDValue Src, unsigned Size) {\n  switch (Src.getOpcode()) {\n  case ISD::SETCC:\n    return Src.getOperand(0).getValueSizeInBits() == Size;\n  case ISD::AND:\n  case ISD::XOR:\n  case ISD::OR:\n    return checkBitcastSrcVectorSize(Src.getOperand(0), Size) &&\n           checkBitcastSrcVectorSize(Src.getOperand(1), Size);\n  }\n  return false;\n}\n\n// Helper to flip between AND/OR/XOR opcodes and their X86ISD FP equivalents.\nstatic unsigned getAltBitOpcode(unsigned Opcode) {\n  switch(Opcode) {\n  case ISD::AND: return X86ISD::FAND;\n  case ISD::OR: return X86ISD::FOR;\n  case ISD::XOR: return X86ISD::FXOR;\n  case X86ISD::ANDNP: return X86ISD::FANDN;\n  }\n  llvm_unreachable(\"Unknown bitwise opcode\");\n}\n\n// Helper to adjust v4i32 MOVMSK expansion to work with SSE1-only targets.\nstatic SDValue adjustBitcastSrcVectorSSE1(SelectionDAG &DAG, SDValue Src,\n                                          const SDLoc &DL) {\n  EVT SrcVT = Src.getValueType();\n  if (SrcVT != MVT::v4i1)\n    return SDValue();\n\n  switch (Src.getOpcode()) {\n  case ISD::SETCC:\n    if (Src.getOperand(0).getValueType() == MVT::v4i32 &&\n        ISD::isBuildVectorAllZeros(Src.getOperand(1).getNode()) &&\n        cast<CondCodeSDNode>(Src.getOperand(2))->get() == ISD::SETLT) {\n      SDValue Op0 = Src.getOperand(0);\n      if (ISD::isNormalLoad(Op0.getNode()))\n        return DAG.getBitcast(MVT::v4f32, Op0);\n      if (Op0.getOpcode() == ISD::BITCAST &&\n          Op0.getOperand(0).getValueType() == MVT::v4f32)\n        return Op0.getOperand(0);\n    }\n    break;\n  case ISD::AND:\n  case ISD::XOR:\n  case ISD::OR: {\n    SDValue Op0 = adjustBitcastSrcVectorSSE1(DAG, Src.getOperand(0), DL);\n    SDValue Op1 = adjustBitcastSrcVectorSSE1(DAG, Src.getOperand(1), DL);\n    if (Op0 && Op1)\n      return DAG.getNode(getAltBitOpcode(Src.getOpcode()), DL, MVT::v4f32, Op0,\n                         Op1);\n    break;\n  }\n  }\n  return SDValue();\n}\n\n// Helper to push sign extension of vXi1 SETCC result through bitops.\nstatic SDValue signExtendBitcastSrcVector(SelectionDAG &DAG, EVT SExtVT,\n                                          SDValue Src, const SDLoc &DL) {\n  switch (Src.getOpcode()) {\n  case ISD::SETCC:\n    return DAG.getNode(ISD::SIGN_EXTEND, DL, SExtVT, Src);\n  case ISD::AND:\n  case ISD::XOR:\n  case ISD::OR:\n    return DAG.getNode(\n        Src.getOpcode(), DL, SExtVT,\n        signExtendBitcastSrcVector(DAG, SExtVT, Src.getOperand(0), DL),\n        signExtendBitcastSrcVector(DAG, SExtVT, Src.getOperand(1), DL));\n  }\n  llvm_unreachable(\"Unexpected node type for vXi1 sign extension\");\n}\n\n// Try to match patterns such as\n// (i16 bitcast (v16i1 x))\n// ->\n// (i16 movmsk (16i8 sext (v16i1 x)))\n// before the illegal vector is scalarized on subtargets that don't have legal\n// vxi1 types.\nstatic SDValue combineBitcastvxi1(SelectionDAG &DAG, EVT VT, SDValue Src,\n                                  const SDLoc &DL,\n                                  const X86Subtarget &Subtarget) {\n  EVT SrcVT = Src.getValueType();\n  if (!SrcVT.isSimple() || SrcVT.getScalarType() != MVT::i1)\n    return SDValue();\n\n  // Recognize the IR pattern for the movmsk intrinsic under SSE1 before type\n  // legalization destroys the v4i32 type.\n  if (Subtarget.hasSSE1() && !Subtarget.hasSSE2()) {\n    if (SDValue V = adjustBitcastSrcVectorSSE1(DAG, Src, DL)) {\n      V = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32,\n                      DAG.getBitcast(MVT::v4f32, V));\n      return DAG.getZExtOrTrunc(V, DL, VT);\n    }\n  }\n\n  // If the input is a truncate from v16i8 or v32i8 go ahead and use a\n  // movmskb even with avx512. This will be better than truncating to vXi1 and\n  // using a kmov. This can especially help KNL if the input is a v16i8/v32i8\n  // vpcmpeqb/vpcmpgtb.\n  bool PreferMovMsk = Src.getOpcode() == ISD::TRUNCATE && Src.hasOneUse() &&\n                      (Src.getOperand(0).getValueType() == MVT::v16i8 ||\n                       Src.getOperand(0).getValueType() == MVT::v32i8 ||\n                       Src.getOperand(0).getValueType() == MVT::v64i8);\n\n  // Prefer movmsk for AVX512 for (bitcast (setlt X, 0)) which can be handled\n  // directly with vpmovmskb/vmovmskps/vmovmskpd.\n  if (Src.getOpcode() == ISD::SETCC && Src.hasOneUse() &&\n      cast<CondCodeSDNode>(Src.getOperand(2))->get() == ISD::SETLT &&\n      ISD::isBuildVectorAllZeros(Src.getOperand(1).getNode())) {\n    EVT CmpVT = Src.getOperand(0).getValueType();\n    EVT EltVT = CmpVT.getVectorElementType();\n    if (CmpVT.getSizeInBits() <= 256 &&\n        (EltVT == MVT::i8 || EltVT == MVT::i32 || EltVT == MVT::i64))\n      PreferMovMsk = true;\n  }\n\n  // With AVX512 vxi1 types are legal and we prefer using k-regs.\n  // MOVMSK is supported in SSE2 or later.\n  if (!Subtarget.hasSSE2() || (Subtarget.hasAVX512() && !PreferMovMsk))\n    return SDValue();\n\n  // There are MOVMSK flavors for types v16i8, v32i8, v4f32, v8f32, v4f64 and\n  // v8f64. So all legal 128-bit and 256-bit vectors are covered except for\n  // v8i16 and v16i16.\n  // For these two cases, we can shuffle the upper element bytes to a\n  // consecutive sequence at the start of the vector and treat the results as\n  // v16i8 or v32i8, and for v16i8 this is the preferable solution. However,\n  // for v16i16 this is not the case, because the shuffle is expensive, so we\n  // avoid sign-extending to this type entirely.\n  // For example, t0 := (v8i16 sext(v8i1 x)) needs to be shuffled as:\n  // (v16i8 shuffle <0,2,4,6,8,10,12,14,u,u,...,u> (v16i8 bitcast t0), undef)\n  MVT SExtVT;\n  bool PropagateSExt = false;\n  switch (SrcVT.getSimpleVT().SimpleTy) {\n  default:\n    return SDValue();\n  case MVT::v2i1:\n    SExtVT = MVT::v2i64;\n    break;\n  case MVT::v4i1:\n    SExtVT = MVT::v4i32;\n    // For cases such as (i4 bitcast (v4i1 setcc v4i64 v1, v2))\n    // sign-extend to a 256-bit operation to avoid truncation.\n    if (Subtarget.hasAVX() && checkBitcastSrcVectorSize(Src, 256)) {\n      SExtVT = MVT::v4i64;\n      PropagateSExt = true;\n    }\n    break;\n  case MVT::v8i1:\n    SExtVT = MVT::v8i16;\n    // For cases such as (i8 bitcast (v8i1 setcc v8i32 v1, v2)),\n    // sign-extend to a 256-bit operation to match the compare.\n    // If the setcc operand is 128-bit, prefer sign-extending to 128-bit over\n    // 256-bit because the shuffle is cheaper than sign extending the result of\n    // the compare.\n    if (Subtarget.hasAVX() && (checkBitcastSrcVectorSize(Src, 256) ||\n                               checkBitcastSrcVectorSize(Src, 512))) {\n      SExtVT = MVT::v8i32;\n      PropagateSExt = true;\n    }\n    break;\n  case MVT::v16i1:\n    SExtVT = MVT::v16i8;\n    // For the case (i16 bitcast (v16i1 setcc v16i16 v1, v2)),\n    // it is not profitable to sign-extend to 256-bit because this will\n    // require an extra cross-lane shuffle which is more expensive than\n    // truncating the result of the compare to 128-bits.\n    break;\n  case MVT::v32i1:\n    SExtVT = MVT::v32i8;\n    break;\n  case MVT::v64i1:\n    // If we have AVX512F, but not AVX512BW and the input is truncated from\n    // v64i8 checked earlier. Then split the input and make two pmovmskbs.\n    if (Subtarget.hasAVX512()) {\n      if (Subtarget.hasBWI())\n        return SDValue();\n      SExtVT = MVT::v64i8;\n      break;\n    }\n    // Split if this is a <64 x i8> comparison result.\n    if (checkBitcastSrcVectorSize(Src, 512)) {\n      SExtVT = MVT::v64i8;\n      break;\n    }\n    return SDValue();\n  };\n\n  SDValue V = PropagateSExt ? signExtendBitcastSrcVector(DAG, SExtVT, Src, DL)\n                            : DAG.getNode(ISD::SIGN_EXTEND, DL, SExtVT, Src);\n\n  if (SExtVT == MVT::v16i8 || SExtVT == MVT::v32i8 || SExtVT == MVT::v64i8) {\n    V = getPMOVMSKB(DL, V, DAG, Subtarget);\n  } else {\n    if (SExtVT == MVT::v8i16)\n      V = DAG.getNode(X86ISD::PACKSS, DL, MVT::v16i8, V,\n                      DAG.getUNDEF(MVT::v8i16));\n    V = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, V);\n  }\n\n  EVT IntVT =\n      EVT::getIntegerVT(*DAG.getContext(), SrcVT.getVectorNumElements());\n  V = DAG.getZExtOrTrunc(V, DL, IntVT);\n  return DAG.getBitcast(VT, V);\n}\n\n// Convert a vXi1 constant build vector to the same width scalar integer.\nstatic SDValue combinevXi1ConstantToInteger(SDValue Op, SelectionDAG &DAG) {\n  EVT SrcVT = Op.getValueType();\n  assert(SrcVT.getVectorElementType() == MVT::i1 &&\n         \"Expected a vXi1 vector\");\n  assert(ISD::isBuildVectorOfConstantSDNodes(Op.getNode()) &&\n         \"Expected a constant build vector\");\n\n  APInt Imm(SrcVT.getVectorNumElements(), 0);\n  for (unsigned Idx = 0, e = Op.getNumOperands(); Idx < e; ++Idx) {\n    SDValue In = Op.getOperand(Idx);\n    if (!In.isUndef() && (cast<ConstantSDNode>(In)->getZExtValue() & 0x1))\n      Imm.setBit(Idx);\n  }\n  EVT IntVT = EVT::getIntegerVT(*DAG.getContext(), Imm.getBitWidth());\n  return DAG.getConstant(Imm, SDLoc(Op), IntVT);\n}\n\nstatic SDValue combineCastedMaskArithmetic(SDNode *N, SelectionDAG &DAG,\n                                           TargetLowering::DAGCombinerInfo &DCI,\n                                           const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == ISD::BITCAST && \"Expected a bitcast\");\n\n  if (!DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  // Only do this if we have k-registers.\n  if (!Subtarget.hasAVX512())\n    return SDValue();\n\n  EVT DstVT = N->getValueType(0);\n  SDValue Op = N->getOperand(0);\n  EVT SrcVT = Op.getValueType();\n\n  if (!Op.hasOneUse())\n    return SDValue();\n\n  // Look for logic ops.\n  if (Op.getOpcode() != ISD::AND &&\n      Op.getOpcode() != ISD::OR &&\n      Op.getOpcode() != ISD::XOR)\n    return SDValue();\n\n  // Make sure we have a bitcast between mask registers and a scalar type.\n  if (!(SrcVT.isVector() && SrcVT.getVectorElementType() == MVT::i1 &&\n        DstVT.isScalarInteger()) &&\n      !(DstVT.isVector() && DstVT.getVectorElementType() == MVT::i1 &&\n        SrcVT.isScalarInteger()))\n    return SDValue();\n\n  SDValue LHS = Op.getOperand(0);\n  SDValue RHS = Op.getOperand(1);\n\n  if (LHS.hasOneUse() && LHS.getOpcode() == ISD::BITCAST &&\n      LHS.getOperand(0).getValueType() == DstVT)\n    return DAG.getNode(Op.getOpcode(), SDLoc(N), DstVT, LHS.getOperand(0),\n                       DAG.getBitcast(DstVT, RHS));\n\n  if (RHS.hasOneUse() && RHS.getOpcode() == ISD::BITCAST &&\n      RHS.getOperand(0).getValueType() == DstVT)\n    return DAG.getNode(Op.getOpcode(), SDLoc(N), DstVT,\n                       DAG.getBitcast(DstVT, LHS), RHS.getOperand(0));\n\n  // If the RHS is a vXi1 build vector, this is a good reason to flip too.\n  // Most of these have to move a constant from the scalar domain anyway.\n  if (ISD::isBuildVectorOfConstantSDNodes(RHS.getNode())) {\n    RHS = combinevXi1ConstantToInteger(RHS, DAG);\n    return DAG.getNode(Op.getOpcode(), SDLoc(N), DstVT,\n                       DAG.getBitcast(DstVT, LHS), RHS);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue createMMXBuildVector(BuildVectorSDNode *BV, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  SDLoc DL(BV);\n  unsigned NumElts = BV->getNumOperands();\n  SDValue Splat = BV->getSplatValue();\n\n  // Build MMX element from integer GPR or SSE float values.\n  auto CreateMMXElement = [&](SDValue V) {\n    if (V.isUndef())\n      return DAG.getUNDEF(MVT::x86mmx);\n    if (V.getValueType().isFloatingPoint()) {\n      if (Subtarget.hasSSE1() && !isa<ConstantFPSDNode>(V)) {\n        V = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v4f32, V);\n        V = DAG.getBitcast(MVT::v2i64, V);\n        return DAG.getNode(X86ISD::MOVDQ2Q, DL, MVT::x86mmx, V);\n      }\n      V = DAG.getBitcast(MVT::i32, V);\n    } else {\n      V = DAG.getAnyExtOrTrunc(V, DL, MVT::i32);\n    }\n    return DAG.getNode(X86ISD::MMX_MOVW2D, DL, MVT::x86mmx, V);\n  };\n\n  // Convert build vector ops to MMX data in the bottom elements.\n  SmallVector<SDValue, 8> Ops;\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // Broadcast - use (PUNPCKL+)PSHUFW to broadcast single element.\n  if (Splat) {\n    if (Splat.isUndef())\n      return DAG.getUNDEF(MVT::x86mmx);\n\n    Splat = CreateMMXElement(Splat);\n\n    if (Subtarget.hasSSE1()) {\n      // Unpack v8i8 to splat i8 elements to lowest 16-bits.\n      if (NumElts == 8)\n        Splat = DAG.getNode(\n            ISD::INTRINSIC_WO_CHAIN, DL, MVT::x86mmx,\n            DAG.getTargetConstant(Intrinsic::x86_mmx_punpcklbw, DL,\n                                  TLI.getPointerTy(DAG.getDataLayout())),\n            Splat, Splat);\n\n      // Use PSHUFW to repeat 16-bit elements.\n      unsigned ShufMask = (NumElts > 2 ? 0 : 0x44);\n      return DAG.getNode(\n          ISD::INTRINSIC_WO_CHAIN, DL, MVT::x86mmx,\n          DAG.getTargetConstant(Intrinsic::x86_sse_pshuf_w, DL,\n                                TLI.getPointerTy(DAG.getDataLayout())),\n          Splat, DAG.getTargetConstant(ShufMask, DL, MVT::i8));\n    }\n    Ops.append(NumElts, Splat);\n  } else {\n    for (unsigned i = 0; i != NumElts; ++i)\n      Ops.push_back(CreateMMXElement(BV->getOperand(i)));\n  }\n\n  // Use tree of PUNPCKLs to build up general MMX vector.\n  while (Ops.size() > 1) {\n    unsigned NumOps = Ops.size();\n    unsigned IntrinOp =\n        (NumOps == 2 ? Intrinsic::x86_mmx_punpckldq\n                     : (NumOps == 4 ? Intrinsic::x86_mmx_punpcklwd\n                                    : Intrinsic::x86_mmx_punpcklbw));\n    SDValue Intrin = DAG.getTargetConstant(\n        IntrinOp, DL, TLI.getPointerTy(DAG.getDataLayout()));\n    for (unsigned i = 0; i != NumOps; i += 2)\n      Ops[i / 2] = DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, MVT::x86mmx, Intrin,\n                               Ops[i], Ops[i + 1]);\n    Ops.resize(NumOps / 2);\n  }\n\n  return Ops[0];\n}\n\n// Recursive function that attempts to find if a bool vector node was originally\n// a vector/float/double that got truncated/extended/bitcast to/from a scalar\n// integer. If so, replace the scalar ops with bool vector equivalents back down\n// the chain.\nstatic SDValue combineBitcastToBoolVector(EVT VT, SDValue V, const SDLoc &DL,\n                                          SelectionDAG &DAG,\n                                          const X86Subtarget &Subtarget) {\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  unsigned Opc = V.getOpcode();\n  switch (Opc) {\n  case ISD::BITCAST: {\n    // Bitcast from a vector/float/double, we can cheaply bitcast to VT.\n    SDValue Src = V.getOperand(0);\n    EVT SrcVT = Src.getValueType();\n    if (SrcVT.isVector() || SrcVT.isFloatingPoint())\n      return DAG.getBitcast(VT, Src);\n    break;\n  }\n  case ISD::TRUNCATE: {\n    // If we find a suitable source, a truncated scalar becomes a subvector.\n    SDValue Src = V.getOperand(0);\n    EVT NewSrcVT =\n        EVT::getVectorVT(*DAG.getContext(), MVT::i1, Src.getValueSizeInBits());\n    if (TLI.isTypeLegal(NewSrcVT))\n      if (SDValue N0 =\n              combineBitcastToBoolVector(NewSrcVT, Src, DL, DAG, Subtarget))\n        return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, N0,\n                           DAG.getIntPtrConstant(0, DL));\n    break;\n  }\n  case ISD::ANY_EXTEND:\n  case ISD::ZERO_EXTEND: {\n    // If we find a suitable source, an extended scalar becomes a subvector.\n    SDValue Src = V.getOperand(0);\n    EVT NewSrcVT = EVT::getVectorVT(*DAG.getContext(), MVT::i1,\n                                    Src.getScalarValueSizeInBits());\n    if (TLI.isTypeLegal(NewSrcVT))\n      if (SDValue N0 =\n              combineBitcastToBoolVector(NewSrcVT, Src, DL, DAG, Subtarget))\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                           Opc == ISD::ANY_EXTEND ? DAG.getUNDEF(VT)\n                                                  : DAG.getConstant(0, DL, VT),\n                           N0, DAG.getIntPtrConstant(0, DL));\n    break;\n  }\n  case ISD::OR: {\n    // If we find suitable sources, we can just move an OR to the vector domain.\n    SDValue Src0 = V.getOperand(0);\n    SDValue Src1 = V.getOperand(1);\n    if (SDValue N0 = combineBitcastToBoolVector(VT, Src0, DL, DAG, Subtarget))\n      if (SDValue N1 = combineBitcastToBoolVector(VT, Src1, DL, DAG, Subtarget))\n        return DAG.getNode(Opc, DL, VT, N0, N1);\n    break;\n  }\n  case ISD::SHL: {\n    // If we find a suitable source, a SHL becomes a KSHIFTL.\n    SDValue Src0 = V.getOperand(0);\n    if ((VT == MVT::v8i1 && !Subtarget.hasDQI()) ||\n        ((VT == MVT::v32i1 || VT == MVT::v64i1) && !Subtarget.hasBWI()))\n      break;\n\n    if (auto *Amt = dyn_cast<ConstantSDNode>(V.getOperand(1)))\n      if (SDValue N0 = combineBitcastToBoolVector(VT, Src0, DL, DAG, Subtarget))\n        return DAG.getNode(\n            X86ISD::KSHIFTL, DL, VT, N0,\n            DAG.getTargetConstant(Amt->getZExtValue(), DL, MVT::i8));\n    break;\n  }\n  }\n  return SDValue();\n}\n\nstatic SDValue combineBitcast(SDNode *N, SelectionDAG &DAG,\n                              TargetLowering::DAGCombinerInfo &DCI,\n                              const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n  EVT SrcVT = N0.getValueType();\n\n  // Try to match patterns such as\n  // (i16 bitcast (v16i1 x))\n  // ->\n  // (i16 movmsk (16i8 sext (v16i1 x)))\n  // before the setcc result is scalarized on subtargets that don't have legal\n  // vxi1 types.\n  if (DCI.isBeforeLegalize()) {\n    SDLoc dl(N);\n    if (SDValue V = combineBitcastvxi1(DAG, VT, N0, dl, Subtarget))\n      return V;\n\n    // If this is a bitcast between a MVT::v4i1/v2i1 and an illegal integer\n    // type, widen both sides to avoid a trip through memory.\n    if ((VT == MVT::v4i1 || VT == MVT::v2i1) && SrcVT.isScalarInteger() &&\n        Subtarget.hasAVX512()) {\n      N0 = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i8, N0);\n      N0 = DAG.getBitcast(MVT::v8i1, N0);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, VT, N0,\n                         DAG.getIntPtrConstant(0, dl));\n    }\n\n    // If this is a bitcast between a MVT::v4i1/v2i1 and an illegal integer\n    // type, widen both sides to avoid a trip through memory.\n    if ((SrcVT == MVT::v4i1 || SrcVT == MVT::v2i1) && VT.isScalarInteger() &&\n        Subtarget.hasAVX512()) {\n      // Use zeros for the widening if we already have some zeroes. This can\n      // allow SimplifyDemandedBits to remove scalar ANDs that may be down\n      // stream of this.\n      // FIXME: It might make sense to detect a concat_vectors with a mix of\n      // zeroes and undef and turn it into insert_subvector for i1 vectors as\n      // a separate combine. What we can't do is canonicalize the operands of\n      // such a concat or we'll get into a loop with SimplifyDemandedBits.\n      if (N0.getOpcode() == ISD::CONCAT_VECTORS) {\n        SDValue LastOp = N0.getOperand(N0.getNumOperands() - 1);\n        if (ISD::isBuildVectorAllZeros(LastOp.getNode())) {\n          SrcVT = LastOp.getValueType();\n          unsigned NumConcats = 8 / SrcVT.getVectorNumElements();\n          SmallVector<SDValue, 4> Ops(N0->op_begin(), N0->op_end());\n          Ops.resize(NumConcats, DAG.getConstant(0, dl, SrcVT));\n          N0 = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8i1, Ops);\n          N0 = DAG.getBitcast(MVT::i8, N0);\n          return DAG.getNode(ISD::TRUNCATE, dl, VT, N0);\n        }\n      }\n\n      unsigned NumConcats = 8 / SrcVT.getVectorNumElements();\n      SmallVector<SDValue, 4> Ops(NumConcats, DAG.getUNDEF(SrcVT));\n      Ops[0] = N0;\n      N0 = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8i1, Ops);\n      N0 = DAG.getBitcast(MVT::i8, N0);\n      return DAG.getNode(ISD::TRUNCATE, dl, VT, N0);\n    }\n  } else {\n    // If we're bitcasting from iX to vXi1, see if the integer originally\n    // began as a vXi1 and whether we can remove the bitcast entirely.\n    if (VT.isVector() && VT.getScalarType() == MVT::i1 &&\n        SrcVT.isScalarInteger() &&\n        DAG.getTargetLoweringInfo().isTypeLegal(VT)) {\n      if (SDValue V =\n              combineBitcastToBoolVector(VT, N0, SDLoc(N), DAG, Subtarget))\n        return V;\n    }\n  }\n\n  // Look for (i8 (bitcast (v8i1 (extract_subvector (v16i1 X), 0)))) and\n  // replace with (i8 (trunc (i16 (bitcast (v16i1 X))))). This can occur\n  // due to insert_subvector legalization on KNL. By promoting the copy to i16\n  // we can help with known bits propagation from the vXi1 domain to the\n  // scalar domain.\n  if (VT == MVT::i8 && SrcVT == MVT::v8i1 && Subtarget.hasAVX512() &&\n      !Subtarget.hasDQI() && N0.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      N0.getOperand(0).getValueType() == MVT::v16i1 &&\n      isNullConstant(N0.getOperand(1)))\n    return DAG.getNode(ISD::TRUNCATE, SDLoc(N), VT,\n                       DAG.getBitcast(MVT::i16, N0.getOperand(0)));\n\n  // Canonicalize (bitcast (vbroadcast_load)) so that the output of the bitcast\n  // and the vbroadcast_load are both integer or both fp. In some cases this\n  // will remove the bitcast entirely.\n  if (N0.getOpcode() == X86ISD::VBROADCAST_LOAD && N0.hasOneUse() &&\n       VT.isFloatingPoint() != SrcVT.isFloatingPoint() && VT.isVector()) {\n    auto *BCast = cast<MemIntrinsicSDNode>(N0);\n    unsigned SrcVTSize = SrcVT.getScalarSizeInBits();\n    unsigned MemSize = BCast->getMemoryVT().getScalarSizeInBits();\n    // Don't swap i8/i16 since don't have fp types that size.\n    if (MemSize >= 32) {\n      MVT MemVT = VT.isFloatingPoint() ? MVT::getFloatingPointVT(MemSize)\n                                       : MVT::getIntegerVT(MemSize);\n      MVT LoadVT = VT.isFloatingPoint() ? MVT::getFloatingPointVT(SrcVTSize)\n                                        : MVT::getIntegerVT(SrcVTSize);\n      LoadVT = MVT::getVectorVT(LoadVT, SrcVT.getVectorNumElements());\n\n      SDVTList Tys = DAG.getVTList(LoadVT, MVT::Other);\n      SDValue Ops[] = { BCast->getChain(), BCast->getBasePtr() };\n      SDValue ResNode =\n          DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, SDLoc(N), Tys, Ops,\n                                  MemVT, BCast->getMemOperand());\n      DAG.ReplaceAllUsesOfValueWith(SDValue(BCast, 1), ResNode.getValue(1));\n      return DAG.getBitcast(VT, ResNode);\n    }\n  }\n\n  // Since MMX types are special and don't usually play with other vector types,\n  // it's better to handle them early to be sure we emit efficient code by\n  // avoiding store-load conversions.\n  if (VT == MVT::x86mmx) {\n    // Detect MMX constant vectors.\n    APInt UndefElts;\n    SmallVector<APInt, 1> EltBits;\n    if (getTargetConstantBitsFromNode(N0, 64, UndefElts, EltBits)) {\n      SDLoc DL(N0);\n      // Handle zero-extension of i32 with MOVD.\n      if (EltBits[0].countLeadingZeros() >= 32)\n        return DAG.getNode(X86ISD::MMX_MOVW2D, DL, VT,\n                           DAG.getConstant(EltBits[0].trunc(32), DL, MVT::i32));\n      // Else, bitcast to a double.\n      // TODO - investigate supporting sext 32-bit immediates on x86_64.\n      APFloat F64(APFloat::IEEEdouble(), EltBits[0]);\n      return DAG.getBitcast(VT, DAG.getConstantFP(F64, DL, MVT::f64));\n    }\n\n    // Detect bitcasts to x86mmx low word.\n    if (N0.getOpcode() == ISD::BUILD_VECTOR &&\n        (SrcVT == MVT::v2i32 || SrcVT == MVT::v4i16 || SrcVT == MVT::v8i8) &&\n        N0.getOperand(0).getValueType() == SrcVT.getScalarType()) {\n      bool LowUndef = true, AllUndefOrZero = true;\n      for (unsigned i = 1, e = SrcVT.getVectorNumElements(); i != e; ++i) {\n        SDValue Op = N0.getOperand(i);\n        LowUndef &= Op.isUndef() || (i >= e/2);\n        AllUndefOrZero &= (Op.isUndef() || isNullConstant(Op));\n      }\n      if (AllUndefOrZero) {\n        SDValue N00 = N0.getOperand(0);\n        SDLoc dl(N00);\n        N00 = LowUndef ? DAG.getAnyExtOrTrunc(N00, dl, MVT::i32)\n                       : DAG.getZExtOrTrunc(N00, dl, MVT::i32);\n        return DAG.getNode(X86ISD::MMX_MOVW2D, dl, VT, N00);\n      }\n    }\n\n    // Detect bitcasts of 64-bit build vectors and convert to a\n    // MMX UNPCK/PSHUFW which takes MMX type inputs with the value in the\n    // lowest element.\n    if (N0.getOpcode() == ISD::BUILD_VECTOR &&\n        (SrcVT == MVT::v2f32 || SrcVT == MVT::v2i32 || SrcVT == MVT::v4i16 ||\n         SrcVT == MVT::v8i8))\n      return createMMXBuildVector(cast<BuildVectorSDNode>(N0), DAG, Subtarget);\n\n    // Detect bitcasts between element or subvector extraction to x86mmx.\n    if ((N0.getOpcode() == ISD::EXTRACT_VECTOR_ELT ||\n         N0.getOpcode() == ISD::EXTRACT_SUBVECTOR) &&\n        isNullConstant(N0.getOperand(1))) {\n      SDValue N00 = N0.getOperand(0);\n      if (N00.getValueType().is128BitVector())\n        return DAG.getNode(X86ISD::MOVDQ2Q, SDLoc(N00), VT,\n                           DAG.getBitcast(MVT::v2i64, N00));\n    }\n\n    // Detect bitcasts from FP_TO_SINT to x86mmx.\n    if (SrcVT == MVT::v2i32 && N0.getOpcode() == ISD::FP_TO_SINT) {\n      SDLoc DL(N0);\n      SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v4i32, N0,\n                                DAG.getUNDEF(MVT::v2i32));\n      return DAG.getNode(X86ISD::MOVDQ2Q, DL, VT,\n                         DAG.getBitcast(MVT::v2i64, Res));\n    }\n  }\n\n  // Try to remove a bitcast of constant vXi1 vector. We have to legalize\n  // most of these to scalar anyway.\n  if (Subtarget.hasAVX512() && VT.isScalarInteger() &&\n      SrcVT.isVector() && SrcVT.getVectorElementType() == MVT::i1 &&\n      ISD::isBuildVectorOfConstantSDNodes(N0.getNode())) {\n    return combinevXi1ConstantToInteger(N0, DAG);\n  }\n\n  if (Subtarget.hasAVX512() && SrcVT.isScalarInteger() &&\n      VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      isa<ConstantSDNode>(N0)) {\n    auto *C = cast<ConstantSDNode>(N0);\n    if (C->isAllOnesValue())\n      return DAG.getConstant(1, SDLoc(N0), VT);\n    if (C->isNullValue())\n      return DAG.getConstant(0, SDLoc(N0), VT);\n  }\n\n  // Look for MOVMSK that is maybe truncated and then bitcasted to vXi1.\n  // Turn it into a sign bit compare that produces a k-register. This avoids\n  // a trip through a GPR.\n  if (Subtarget.hasAVX512() && SrcVT.isScalarInteger() &&\n      VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      isPowerOf2_32(VT.getVectorNumElements())) {\n    unsigned NumElts = VT.getVectorNumElements();\n    SDValue Src = N0;\n\n    // Peek through truncate.\n    if (N0.getOpcode() == ISD::TRUNCATE && N0.hasOneUse())\n      Src = N0.getOperand(0);\n\n    if (Src.getOpcode() == X86ISD::MOVMSK && Src.hasOneUse()) {\n      SDValue MovmskIn = Src.getOperand(0);\n      MVT MovmskVT = MovmskIn.getSimpleValueType();\n      unsigned MovMskElts = MovmskVT.getVectorNumElements();\n\n      // We allow extra bits of the movmsk to be used since they are known zero.\n      // We can't convert a VPMOVMSKB without avx512bw.\n      if (MovMskElts <= NumElts &&\n          (Subtarget.hasBWI() || MovmskVT.getVectorElementType() != MVT::i8)) {\n        EVT IntVT = EVT(MovmskVT).changeVectorElementTypeToInteger();\n        MovmskIn = DAG.getBitcast(IntVT, MovmskIn);\n        SDLoc dl(N);\n        MVT CmpVT = MVT::getVectorVT(MVT::i1, MovMskElts);\n        SDValue Cmp = DAG.getSetCC(dl, CmpVT, MovmskIn,\n                                   DAG.getConstant(0, dl, IntVT), ISD::SETLT);\n        if (EVT(CmpVT) == VT)\n          return Cmp;\n\n        // Pad with zeroes up to original VT to replace the zeroes that were\n        // being used from the MOVMSK.\n        unsigned NumConcats = NumElts / MovMskElts;\n        SmallVector<SDValue, 4> Ops(NumConcats, DAG.getConstant(0, dl, CmpVT));\n        Ops[0] = Cmp;\n        return DAG.getNode(ISD::CONCAT_VECTORS, dl, VT, Ops);\n      }\n    }\n  }\n\n  // Try to remove bitcasts from input and output of mask arithmetic to\n  // remove GPR<->K-register crossings.\n  if (SDValue V = combineCastedMaskArithmetic(N, DAG, DCI, Subtarget))\n    return V;\n\n  // Convert a bitcasted integer logic operation that has one bitcasted\n  // floating-point operand into a floating-point logic operation. This may\n  // create a load of a constant, but that is cheaper than materializing the\n  // constant in an integer register and transferring it to an SSE register or\n  // transferring the SSE operand to integer register and back.\n  unsigned FPOpcode;\n  switch (N0.getOpcode()) {\n    case ISD::AND: FPOpcode = X86ISD::FAND; break;\n    case ISD::OR:  FPOpcode = X86ISD::FOR;  break;\n    case ISD::XOR: FPOpcode = X86ISD::FXOR; break;\n    default: return SDValue();\n  }\n\n  if (!((Subtarget.hasSSE1() && VT == MVT::f32) ||\n        (Subtarget.hasSSE2() && VT == MVT::f64)))\n    return SDValue();\n\n  SDValue LogicOp0 = N0.getOperand(0);\n  SDValue LogicOp1 = N0.getOperand(1);\n  SDLoc DL0(N0);\n\n  // bitcast(logic(bitcast(X), Y)) --> logic'(X, bitcast(Y))\n  if (N0.hasOneUse() && LogicOp0.getOpcode() == ISD::BITCAST &&\n      LogicOp0.hasOneUse() && LogicOp0.getOperand(0).getValueType() == VT &&\n      !isa<ConstantSDNode>(LogicOp0.getOperand(0))) {\n    SDValue CastedOp1 = DAG.getBitcast(VT, LogicOp1);\n    return DAG.getNode(FPOpcode, DL0, VT, LogicOp0.getOperand(0), CastedOp1);\n  }\n  // bitcast(logic(X, bitcast(Y))) --> logic'(bitcast(X), Y)\n  if (N0.hasOneUse() && LogicOp1.getOpcode() == ISD::BITCAST &&\n      LogicOp1.hasOneUse() && LogicOp1.getOperand(0).getValueType() == VT &&\n      !isa<ConstantSDNode>(LogicOp1.getOperand(0))) {\n    SDValue CastedOp0 = DAG.getBitcast(VT, LogicOp0);\n    return DAG.getNode(FPOpcode, DL0, VT, LogicOp1.getOperand(0), CastedOp0);\n  }\n\n  return SDValue();\n}\n\n// Given a ABS node, detect the following pattern:\n// (ABS (SUB (ZERO_EXTEND a), (ZERO_EXTEND b))).\n// This is useful as it is the input into a SAD pattern.\nstatic bool detectZextAbsDiff(const SDValue &Abs, SDValue &Op0, SDValue &Op1) {\n  SDValue AbsOp1 = Abs->getOperand(0);\n  if (AbsOp1.getOpcode() != ISD::SUB)\n    return false;\n\n  Op0 = AbsOp1.getOperand(0);\n  Op1 = AbsOp1.getOperand(1);\n\n  // Check if the operands of the sub are zero-extended from vectors of i8.\n  if (Op0.getOpcode() != ISD::ZERO_EXTEND ||\n      Op0.getOperand(0).getValueType().getVectorElementType() != MVT::i8 ||\n      Op1.getOpcode() != ISD::ZERO_EXTEND ||\n      Op1.getOperand(0).getValueType().getVectorElementType() != MVT::i8)\n    return false;\n\n  return true;\n}\n\n// Given two zexts of <k x i8> to <k x i32>, create a PSADBW of the inputs\n// to these zexts.\nstatic SDValue createPSADBW(SelectionDAG &DAG, const SDValue &Zext0,\n                            const SDValue &Zext1, const SDLoc &DL,\n                            const X86Subtarget &Subtarget) {\n  // Find the appropriate width for the PSADBW.\n  EVT InVT = Zext0.getOperand(0).getValueType();\n  unsigned RegSize = std::max(128u, (unsigned)InVT.getSizeInBits());\n\n  // \"Zero-extend\" the i8 vectors. This is not a per-element zext, rather we\n  // fill in the missing vector elements with 0.\n  unsigned NumConcat = RegSize / InVT.getSizeInBits();\n  SmallVector<SDValue, 16> Ops(NumConcat, DAG.getConstant(0, DL, InVT));\n  Ops[0] = Zext0.getOperand(0);\n  MVT ExtendedVT = MVT::getVectorVT(MVT::i8, RegSize / 8);\n  SDValue SadOp0 = DAG.getNode(ISD::CONCAT_VECTORS, DL, ExtendedVT, Ops);\n  Ops[0] = Zext1.getOperand(0);\n  SDValue SadOp1 = DAG.getNode(ISD::CONCAT_VECTORS, DL, ExtendedVT, Ops);\n\n  // Actually build the SAD, split as 128/256/512 bits for SSE/AVX2/AVX512BW.\n  auto PSADBWBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                          ArrayRef<SDValue> Ops) {\n    MVT VT = MVT::getVectorVT(MVT::i64, Ops[0].getValueSizeInBits() / 64);\n    return DAG.getNode(X86ISD::PSADBW, DL, VT, Ops);\n  };\n  MVT SadVT = MVT::getVectorVT(MVT::i64, RegSize / 64);\n  return SplitOpsAndApply(DAG, Subtarget, DL, SadVT, { SadOp0, SadOp1 },\n                          PSADBWBuilder);\n}\n\n// Attempt to replace an min/max v8i16/v16i8 horizontal reduction with\n// PHMINPOSUW.\nstatic SDValue combineMinMaxReduction(SDNode *Extract, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  // Bail without SSE41.\n  if (!Subtarget.hasSSE41())\n    return SDValue();\n\n  EVT ExtractVT = Extract->getValueType(0);\n  if (ExtractVT != MVT::i16 && ExtractVT != MVT::i8)\n    return SDValue();\n\n  // Check for SMAX/SMIN/UMAX/UMIN horizontal reduction patterns.\n  ISD::NodeType BinOp;\n  SDValue Src = DAG.matchBinOpReduction(\n      Extract, BinOp, {ISD::SMAX, ISD::SMIN, ISD::UMAX, ISD::UMIN}, true);\n  if (!Src)\n    return SDValue();\n\n  EVT SrcVT = Src.getValueType();\n  EVT SrcSVT = SrcVT.getScalarType();\n  if (SrcSVT != ExtractVT || (SrcVT.getSizeInBits() % 128) != 0)\n    return SDValue();\n\n  SDLoc DL(Extract);\n  SDValue MinPos = Src;\n\n  // First, reduce the source down to 128-bit, applying BinOp to lo/hi.\n  while (SrcVT.getSizeInBits() > 128) {\n    SDValue Lo, Hi;\n    std::tie(Lo, Hi) = splitVector(MinPos, DAG, DL);\n    SrcVT = Lo.getValueType();\n    MinPos = DAG.getNode(BinOp, DL, SrcVT, Lo, Hi);\n  }\n  assert(((SrcVT == MVT::v8i16 && ExtractVT == MVT::i16) ||\n          (SrcVT == MVT::v16i8 && ExtractVT == MVT::i8)) &&\n         \"Unexpected value type\");\n\n  // PHMINPOSUW applies to UMIN(v8i16), for SMIN/SMAX/UMAX we must apply a mask\n  // to flip the value accordingly.\n  SDValue Mask;\n  unsigned MaskEltsBits = ExtractVT.getSizeInBits();\n  if (BinOp == ISD::SMAX)\n    Mask = DAG.getConstant(APInt::getSignedMaxValue(MaskEltsBits), DL, SrcVT);\n  else if (BinOp == ISD::SMIN)\n    Mask = DAG.getConstant(APInt::getSignedMinValue(MaskEltsBits), DL, SrcVT);\n  else if (BinOp == ISD::UMAX)\n    Mask = DAG.getConstant(APInt::getAllOnesValue(MaskEltsBits), DL, SrcVT);\n\n  if (Mask)\n    MinPos = DAG.getNode(ISD::XOR, DL, SrcVT, Mask, MinPos);\n\n  // For v16i8 cases we need to perform UMIN on pairs of byte elements,\n  // shuffling each upper element down and insert zeros. This means that the\n  // v16i8 UMIN will leave the upper element as zero, performing zero-extension\n  // ready for the PHMINPOS.\n  if (ExtractVT == MVT::i8) {\n    SDValue Upper = DAG.getVectorShuffle(\n        SrcVT, DL, MinPos, DAG.getConstant(0, DL, MVT::v16i8),\n        {1, 16, 3, 16, 5, 16, 7, 16, 9, 16, 11, 16, 13, 16, 15, 16});\n    MinPos = DAG.getNode(ISD::UMIN, DL, SrcVT, MinPos, Upper);\n  }\n\n  // Perform the PHMINPOS on a v8i16 vector,\n  MinPos = DAG.getBitcast(MVT::v8i16, MinPos);\n  MinPos = DAG.getNode(X86ISD::PHMINPOS, DL, MVT::v8i16, MinPos);\n  MinPos = DAG.getBitcast(SrcVT, MinPos);\n\n  if (Mask)\n    MinPos = DAG.getNode(ISD::XOR, DL, SrcVT, Mask, MinPos);\n\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, ExtractVT, MinPos,\n                     DAG.getIntPtrConstant(0, DL));\n}\n\n// Attempt to replace an all_of/any_of/parity style horizontal reduction with a MOVMSK.\nstatic SDValue combinePredicateReduction(SDNode *Extract, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  // Bail without SSE2.\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  EVT ExtractVT = Extract->getValueType(0);\n  unsigned BitWidth = ExtractVT.getSizeInBits();\n  if (ExtractVT != MVT::i64 && ExtractVT != MVT::i32 && ExtractVT != MVT::i16 &&\n      ExtractVT != MVT::i8 && ExtractVT != MVT::i1)\n    return SDValue();\n\n  // Check for OR(any_of)/AND(all_of)/XOR(parity) horizontal reduction patterns.\n  ISD::NodeType BinOp;\n  SDValue Match = DAG.matchBinOpReduction(Extract, BinOp, {ISD::OR, ISD::AND});\n  if (!Match && ExtractVT == MVT::i1)\n    Match = DAG.matchBinOpReduction(Extract, BinOp, {ISD::XOR});\n  if (!Match)\n    return SDValue();\n\n  // EXTRACT_VECTOR_ELT can require implicit extension of the vector element\n  // which we can't support here for now.\n  if (Match.getScalarValueSizeInBits() != BitWidth)\n    return SDValue();\n\n  SDValue Movmsk;\n  SDLoc DL(Extract);\n  EVT MatchVT = Match.getValueType();\n  unsigned NumElts = MatchVT.getVectorNumElements();\n  unsigned MaxElts = Subtarget.hasInt256() ? 32 : 16;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  if (ExtractVT == MVT::i1) {\n    // Special case for (pre-legalization) vXi1 reductions.\n    if (NumElts > 64 || !isPowerOf2_32(NumElts))\n      return SDValue();\n    if (TLI.isTypeLegal(MatchVT)) {\n      // If this is a legal AVX512 predicate type then we can just bitcast.\n      EVT MovmskVT = EVT::getIntegerVT(*DAG.getContext(), NumElts);\n      Movmsk = DAG.getBitcast(MovmskVT, Match);\n    } else {\n      // For all_of(setcc(vec,0,eq)) - avoid vXi64 comparisons if we don't have\n      // PCMPEQQ (SSE41+), use PCMPEQD instead.\n      if (BinOp == ISD::AND && !Subtarget.hasSSE41() &&\n          Match.getOpcode() == ISD::SETCC &&\n          ISD::isBuildVectorAllZeros(Match.getOperand(1).getNode()) &&\n          cast<CondCodeSDNode>(Match.getOperand(2))->get() ==\n              ISD::CondCode::SETEQ) {\n        SDValue Vec = Match.getOperand(0);\n        if (Vec.getValueType().getScalarType() == MVT::i64 &&\n            (2 * NumElts) <= MaxElts) {\n          NumElts *= 2;\n          EVT CmpVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32, NumElts);\n          MatchVT = EVT::getVectorVT(*DAG.getContext(), MVT::i1, NumElts);\n          Match = DAG.getSetCC(\n              DL, MatchVT, DAG.getBitcast(CmpVT, Match.getOperand(0)),\n              DAG.getBitcast(CmpVT, Match.getOperand(1)), ISD::CondCode::SETEQ);\n        }\n      }\n\n      // Use combineBitcastvxi1 to create the MOVMSK.\n      while (NumElts > MaxElts) {\n        SDValue Lo, Hi;\n        std::tie(Lo, Hi) = DAG.SplitVector(Match, DL);\n        Match = DAG.getNode(BinOp, DL, Lo.getValueType(), Lo, Hi);\n        NumElts /= 2;\n      }\n      EVT MovmskVT = EVT::getIntegerVT(*DAG.getContext(), NumElts);\n      Movmsk = combineBitcastvxi1(DAG, MovmskVT, Match, DL, Subtarget);\n    }\n    if (!Movmsk)\n      return SDValue();\n    Movmsk = DAG.getZExtOrTrunc(Movmsk, DL, NumElts > 32 ? MVT::i64 : MVT::i32);\n  } else {\n    // FIXME: Better handling of k-registers or 512-bit vectors?\n    unsigned MatchSizeInBits = Match.getValueSizeInBits();\n    if (!(MatchSizeInBits == 128 ||\n          (MatchSizeInBits == 256 && Subtarget.hasAVX())))\n      return SDValue();\n\n    // Make sure this isn't a vector of 1 element. The perf win from using\n    // MOVMSK diminishes with less elements in the reduction, but it is\n    // generally better to get the comparison over to the GPRs as soon as\n    // possible to reduce the number of vector ops.\n    if (Match.getValueType().getVectorNumElements() < 2)\n      return SDValue();\n\n    // Check that we are extracting a reduction of all sign bits.\n    if (DAG.ComputeNumSignBits(Match) != BitWidth)\n      return SDValue();\n\n    if (MatchSizeInBits == 256 && BitWidth < 32 && !Subtarget.hasInt256()) {\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = DAG.SplitVector(Match, DL);\n      Match = DAG.getNode(BinOp, DL, Lo.getValueType(), Lo, Hi);\n      MatchSizeInBits = Match.getValueSizeInBits();\n    }\n\n    // For 32/64 bit comparisons use MOVMSKPS/MOVMSKPD, else PMOVMSKB.\n    MVT MaskSrcVT;\n    if (64 == BitWidth || 32 == BitWidth)\n      MaskSrcVT = MVT::getVectorVT(MVT::getFloatingPointVT(BitWidth),\n                                   MatchSizeInBits / BitWidth);\n    else\n      MaskSrcVT = MVT::getVectorVT(MVT::i8, MatchSizeInBits / 8);\n\n    SDValue BitcastLogicOp = DAG.getBitcast(MaskSrcVT, Match);\n    Movmsk = getPMOVMSKB(DL, BitcastLogicOp, DAG, Subtarget);\n    NumElts = MaskSrcVT.getVectorNumElements();\n  }\n  assert((NumElts <= 32 || NumElts == 64) &&\n         \"Not expecting more than 64 elements\");\n\n  MVT CmpVT = NumElts == 64 ? MVT::i64 : MVT::i32;\n  if (BinOp == ISD::XOR) {\n    // parity -> (PARITY(MOVMSK X))\n    SDValue Result = DAG.getNode(ISD::PARITY, DL, CmpVT, Movmsk);\n    return DAG.getZExtOrTrunc(Result, DL, ExtractVT);\n  }\n\n  SDValue CmpC;\n  ISD::CondCode CondCode;\n  if (BinOp == ISD::OR) {\n    // any_of -> MOVMSK != 0\n    CmpC = DAG.getConstant(0, DL, CmpVT);\n    CondCode = ISD::CondCode::SETNE;\n  } else {\n    // all_of -> MOVMSK == ((1 << NumElts) - 1)\n    CmpC = DAG.getConstant(APInt::getLowBitsSet(CmpVT.getSizeInBits(), NumElts),\n                           DL, CmpVT);\n    CondCode = ISD::CondCode::SETEQ;\n  }\n\n  // The setcc produces an i8 of 0/1, so extend that to the result width and\n  // negate to get the final 0/-1 mask value.\n  EVT SetccVT =\n      TLI.getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(), CmpVT);\n  SDValue Setcc = DAG.getSetCC(DL, SetccVT, Movmsk, CmpC, CondCode);\n  SDValue Zext = DAG.getZExtOrTrunc(Setcc, DL, ExtractVT);\n  SDValue Zero = DAG.getConstant(0, DL, ExtractVT);\n  return DAG.getNode(ISD::SUB, DL, ExtractVT, Zero, Zext);\n}\n\nstatic SDValue combineBasicSADPattern(SDNode *Extract, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  // PSADBW is only supported on SSE2 and up.\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  EVT ExtractVT = Extract->getValueType(0);\n  // Verify the type we're extracting is either i32 or i64.\n  // FIXME: Could support other types, but this is what we have coverage for.\n  if (ExtractVT != MVT::i32 && ExtractVT != MVT::i64)\n    return SDValue();\n\n  EVT VT = Extract->getOperand(0).getValueType();\n  if (!isPowerOf2_32(VT.getVectorNumElements()))\n    return SDValue();\n\n  // Match shuffle + add pyramid.\n  ISD::NodeType BinOp;\n  SDValue Root = DAG.matchBinOpReduction(Extract, BinOp, {ISD::ADD});\n\n  // The operand is expected to be zero extended from i8\n  // (verified in detectZextAbsDiff).\n  // In order to convert to i64 and above, additional any/zero/sign\n  // extend is expected.\n  // The zero extend from 32 bit has no mathematical effect on the result.\n  // Also the sign extend is basically zero extend\n  // (extends the sign bit which is zero).\n  // So it is correct to skip the sign/zero extend instruction.\n  if (Root && (Root.getOpcode() == ISD::SIGN_EXTEND ||\n               Root.getOpcode() == ISD::ZERO_EXTEND ||\n               Root.getOpcode() == ISD::ANY_EXTEND))\n    Root = Root.getOperand(0);\n\n  // If there was a match, we want Root to be a select that is the root of an\n  // abs-diff pattern.\n  if (!Root || Root.getOpcode() != ISD::ABS)\n    return SDValue();\n\n  // Check whether we have an abs-diff pattern feeding into the select.\n  SDValue Zext0, Zext1;\n  if (!detectZextAbsDiff(Root, Zext0, Zext1))\n    return SDValue();\n\n  // Create the SAD instruction.\n  SDLoc DL(Extract);\n  SDValue SAD = createPSADBW(DAG, Zext0, Zext1, DL, Subtarget);\n\n  // If the original vector was wider than 8 elements, sum over the results\n  // in the SAD vector.\n  unsigned Stages = Log2_32(VT.getVectorNumElements());\n  EVT SadVT = SAD.getValueType();\n  if (Stages > 3) {\n    unsigned SadElems = SadVT.getVectorNumElements();\n\n    for(unsigned i = Stages - 3; i > 0; --i) {\n      SmallVector<int, 16> Mask(SadElems, -1);\n      for(unsigned j = 0, MaskEnd = 1 << (i - 1); j < MaskEnd; ++j)\n        Mask[j] = MaskEnd + j;\n\n      SDValue Shuffle =\n          DAG.getVectorShuffle(SadVT, DL, SAD, DAG.getUNDEF(SadVT), Mask);\n      SAD = DAG.getNode(ISD::ADD, DL, SadVT, SAD, Shuffle);\n    }\n  }\n\n  unsigned ExtractSizeInBits = ExtractVT.getSizeInBits();\n  // Return the lowest ExtractSizeInBits bits.\n  EVT ResVT = EVT::getVectorVT(*DAG.getContext(), ExtractVT,\n                               SadVT.getSizeInBits() / ExtractSizeInBits);\n  SAD = DAG.getBitcast(ResVT, SAD);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, ExtractVT, SAD,\n                     Extract->getOperand(1));\n}\n\n// Attempt to peek through a target shuffle and extract the scalar from the\n// source.\nstatic SDValue combineExtractWithShuffle(SDNode *N, SelectionDAG &DAG,\n                                         TargetLowering::DAGCombinerInfo &DCI,\n                                         const X86Subtarget &Subtarget) {\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  SDLoc dl(N);\n  SDValue Src = N->getOperand(0);\n  SDValue Idx = N->getOperand(1);\n\n  EVT VT = N->getValueType(0);\n  EVT SrcVT = Src.getValueType();\n  EVT SrcSVT = SrcVT.getVectorElementType();\n  unsigned SrcEltBits = SrcSVT.getSizeInBits();\n  unsigned NumSrcElts = SrcVT.getVectorNumElements();\n\n  // Don't attempt this for boolean mask vectors or unknown extraction indices.\n  if (SrcSVT == MVT::i1 || !isa<ConstantSDNode>(Idx))\n    return SDValue();\n\n  const APInt &IdxC = N->getConstantOperandAPInt(1);\n  if (IdxC.uge(NumSrcElts))\n    return SDValue();\n\n  SDValue SrcBC = peekThroughBitcasts(Src);\n\n  // Handle extract(bitcast(broadcast(scalar_value))).\n  if (X86ISD::VBROADCAST == SrcBC.getOpcode()) {\n    SDValue SrcOp = SrcBC.getOperand(0);\n    EVT SrcOpVT = SrcOp.getValueType();\n    if (SrcOpVT.isScalarInteger() && VT.isInteger() &&\n        (SrcOpVT.getSizeInBits() % SrcEltBits) == 0) {\n      unsigned Scale = SrcOpVT.getSizeInBits() / SrcEltBits;\n      unsigned Offset = IdxC.urem(Scale) * SrcEltBits;\n      // TODO support non-zero offsets.\n      if (Offset == 0) {\n        SrcOp = DAG.getZExtOrTrunc(SrcOp, dl, SrcVT.getScalarType());\n        SrcOp = DAG.getZExtOrTrunc(SrcOp, dl, VT);\n        return SrcOp;\n      }\n    }\n  }\n\n  // If we're extracting a single element from a broadcast load and there are\n  // no other users, just create a single load.\n  if (SrcBC.getOpcode() == X86ISD::VBROADCAST_LOAD && SrcBC.hasOneUse()) {\n    auto *MemIntr = cast<MemIntrinsicSDNode>(SrcBC);\n    unsigned SrcBCWidth = SrcBC.getScalarValueSizeInBits();\n    if (MemIntr->getMemoryVT().getSizeInBits() == SrcBCWidth &&\n        VT.getSizeInBits() == SrcBCWidth && SrcEltBits == SrcBCWidth) {\n      SDValue Load = DAG.getLoad(VT, dl, MemIntr->getChain(),\n                                 MemIntr->getBasePtr(),\n                                 MemIntr->getPointerInfo(),\n                                 MemIntr->getOriginalAlign(),\n                                 MemIntr->getMemOperand()->getFlags());\n      DAG.ReplaceAllUsesOfValueWith(SDValue(MemIntr, 1), Load.getValue(1));\n      return Load;\n    }\n  }\n\n  // Handle extract(bitcast(scalar_to_vector(scalar_value))) for integers.\n  // TODO: Move to DAGCombine?\n  if (SrcBC.getOpcode() == ISD::SCALAR_TO_VECTOR && VT.isInteger() &&\n      SrcBC.getValueType().isInteger() &&\n      (SrcBC.getScalarValueSizeInBits() % SrcEltBits) == 0 &&\n      SrcBC.getScalarValueSizeInBits() ==\n          SrcBC.getOperand(0).getValueSizeInBits()) {\n    unsigned Scale = SrcBC.getScalarValueSizeInBits() / SrcEltBits;\n    if (IdxC.ult(Scale)) {\n      unsigned Offset = IdxC.getZExtValue() * SrcVT.getScalarSizeInBits();\n      SDValue Scl = SrcBC.getOperand(0);\n      EVT SclVT = Scl.getValueType();\n      if (Offset) {\n        Scl = DAG.getNode(ISD::SRL, dl, SclVT, Scl,\n                          DAG.getShiftAmountConstant(Offset, SclVT, dl));\n      }\n      Scl = DAG.getZExtOrTrunc(Scl, dl, SrcVT.getScalarType());\n      Scl = DAG.getZExtOrTrunc(Scl, dl, VT);\n      return Scl;\n    }\n  }\n\n  // Handle extract(truncate(x)) for 0'th index.\n  // TODO: Treat this as a faux shuffle?\n  // TODO: When can we use this for general indices?\n  if (ISD::TRUNCATE == Src.getOpcode() && IdxC == 0 &&\n      (SrcVT.getSizeInBits() % 128) == 0) {\n    Src = extract128BitVector(Src.getOperand(0), 0, DAG, dl);\n    MVT ExtractVT = MVT::getVectorVT(SrcSVT.getSimpleVT(), 128 / SrcEltBits);\n    return DAG.getNode(N->getOpcode(), dl, VT, DAG.getBitcast(ExtractVT, Src),\n                       Idx);\n  }\n\n  // We can only legally extract other elements from 128-bit vectors and in\n  // certain circumstances, depending on SSE-level.\n  // TODO: Investigate float/double extraction if it will be just stored.\n  auto GetLegalExtract = [&Subtarget, &DAG, &dl](SDValue Vec, EVT VecVT,\n                                                 unsigned Idx) {\n    EVT VecSVT = VecVT.getScalarType();\n    if ((VecVT.is256BitVector() || VecVT.is512BitVector()) &&\n        (VecSVT == MVT::i8 || VecSVT == MVT::i16 || VecSVT == MVT::i32 ||\n         VecSVT == MVT::i64)) {\n      unsigned EltSizeInBits = VecSVT.getSizeInBits();\n      unsigned NumEltsPerLane = 128 / EltSizeInBits;\n      unsigned LaneOffset = (Idx & ~(NumEltsPerLane - 1)) * EltSizeInBits;\n      unsigned LaneIdx = LaneOffset / Vec.getScalarValueSizeInBits();\n      VecVT = EVT::getVectorVT(*DAG.getContext(), VecSVT, NumEltsPerLane);\n      Vec = extract128BitVector(Vec, LaneIdx, DAG, dl);\n      Idx &= (NumEltsPerLane - 1);\n    }\n    if ((VecVT == MVT::v4i32 || VecVT == MVT::v2i64) &&\n        ((Idx == 0 && Subtarget.hasSSE2()) || Subtarget.hasSSE41())) {\n      return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, VecVT.getScalarType(),\n                         DAG.getBitcast(VecVT, Vec),\n                         DAG.getIntPtrConstant(Idx, dl));\n    }\n    if ((VecVT == MVT::v8i16 && Subtarget.hasSSE2()) ||\n        (VecVT == MVT::v16i8 && Subtarget.hasSSE41())) {\n      unsigned OpCode = (VecVT == MVT::v8i16 ? X86ISD::PEXTRW : X86ISD::PEXTRB);\n      return DAG.getNode(OpCode, dl, MVT::i32, DAG.getBitcast(VecVT, Vec),\n                         DAG.getTargetConstant(Idx, dl, MVT::i8));\n    }\n    return SDValue();\n  };\n\n  // Resolve the target shuffle inputs and mask.\n  SmallVector<int, 16> Mask;\n  SmallVector<SDValue, 2> Ops;\n  if (!getTargetShuffleInputs(SrcBC, Ops, Mask, DAG))\n    return SDValue();\n\n  // Shuffle inputs must be the same size as the result.\n  if (llvm::any_of(Ops, [SrcVT](SDValue Op) {\n        return SrcVT.getSizeInBits() != Op.getValueSizeInBits();\n      }))\n    return SDValue();\n\n  // Attempt to narrow/widen the shuffle mask to the correct size.\n  if (Mask.size() != NumSrcElts) {\n    if ((NumSrcElts % Mask.size()) == 0) {\n      SmallVector<int, 16> ScaledMask;\n      int Scale = NumSrcElts / Mask.size();\n      narrowShuffleMaskElts(Scale, Mask, ScaledMask);\n      Mask = std::move(ScaledMask);\n    } else if ((Mask.size() % NumSrcElts) == 0) {\n      // Simplify Mask based on demanded element.\n      int ExtractIdx = (int)IdxC.getZExtValue();\n      int Scale = Mask.size() / NumSrcElts;\n      int Lo = Scale * ExtractIdx;\n      int Hi = Scale * (ExtractIdx + 1);\n      for (int i = 0, e = (int)Mask.size(); i != e; ++i)\n        if (i < Lo || Hi <= i)\n          Mask[i] = SM_SentinelUndef;\n\n      SmallVector<int, 16> WidenedMask;\n      while (Mask.size() > NumSrcElts &&\n             canWidenShuffleElements(Mask, WidenedMask))\n        Mask = std::move(WidenedMask);\n      // TODO - investigate support for wider shuffle masks with known upper\n      // undef/zero elements for implicit zero-extension.\n    }\n  }\n\n  // If narrowing/widening failed, see if we can extract+zero-extend.\n  int ExtractIdx;\n  EVT ExtractVT;\n  if (Mask.size() == NumSrcElts) {\n    ExtractIdx = Mask[IdxC.getZExtValue()];\n    ExtractVT = SrcVT;\n  } else {\n    unsigned Scale = Mask.size() / NumSrcElts;\n    if ((Mask.size() % NumSrcElts) != 0 || SrcVT.isFloatingPoint())\n      return SDValue();\n    unsigned ScaledIdx = Scale * IdxC.getZExtValue();\n    if (!isUndefOrZeroInRange(Mask, ScaledIdx + 1, Scale - 1))\n      return SDValue();\n    ExtractIdx = Mask[ScaledIdx];\n    EVT ExtractSVT = EVT::getIntegerVT(*DAG.getContext(), SrcEltBits / Scale);\n    ExtractVT = EVT::getVectorVT(*DAG.getContext(), ExtractSVT, Mask.size());\n    assert(SrcVT.getSizeInBits() == ExtractVT.getSizeInBits() &&\n           \"Failed to widen vector type\");\n  }\n\n  // If the shuffle source element is undef/zero then we can just accept it.\n  if (ExtractIdx == SM_SentinelUndef)\n    return DAG.getUNDEF(VT);\n\n  if (ExtractIdx == SM_SentinelZero)\n    return VT.isFloatingPoint() ? DAG.getConstantFP(0.0, dl, VT)\n                                : DAG.getConstant(0, dl, VT);\n\n  SDValue SrcOp = Ops[ExtractIdx / Mask.size()];\n  ExtractIdx = ExtractIdx % Mask.size();\n  if (SDValue V = GetLegalExtract(SrcOp, ExtractVT, ExtractIdx))\n    return DAG.getZExtOrTrunc(V, dl, VT);\n\n  return SDValue();\n}\n\n/// Extracting a scalar FP value from vector element 0 is free, so extract each\n/// operand first, then perform the math as a scalar op.\nstatic SDValue scalarizeExtEltFP(SDNode *ExtElt, SelectionDAG &DAG) {\n  assert(ExtElt->getOpcode() == ISD::EXTRACT_VECTOR_ELT && \"Expected extract\");\n  SDValue Vec = ExtElt->getOperand(0);\n  SDValue Index = ExtElt->getOperand(1);\n  EVT VT = ExtElt->getValueType(0);\n  EVT VecVT = Vec.getValueType();\n\n  // TODO: If this is a unary/expensive/expand op, allow extraction from a\n  // non-zero element because the shuffle+scalar op will be cheaper?\n  if (!Vec.hasOneUse() || !isNullConstant(Index) || VecVT.getScalarType() != VT)\n    return SDValue();\n\n  // Vector FP compares don't fit the pattern of FP math ops (propagate, not\n  // extract, the condition code), so deal with those as a special-case.\n  if (Vec.getOpcode() == ISD::SETCC && VT == MVT::i1) {\n    EVT OpVT = Vec.getOperand(0).getValueType().getScalarType();\n    if (OpVT != MVT::f32 && OpVT != MVT::f64)\n      return SDValue();\n\n    // extract (setcc X, Y, CC), 0 --> setcc (extract X, 0), (extract Y, 0), CC\n    SDLoc DL(ExtElt);\n    SDValue Ext0 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, OpVT,\n                               Vec.getOperand(0), Index);\n    SDValue Ext1 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, OpVT,\n                               Vec.getOperand(1), Index);\n    return DAG.getNode(Vec.getOpcode(), DL, VT, Ext0, Ext1, Vec.getOperand(2));\n  }\n\n  if (VT != MVT::f32 && VT != MVT::f64)\n    return SDValue();\n\n  // Vector FP selects don't fit the pattern of FP math ops (because the\n  // condition has a different type and we have to change the opcode), so deal\n  // with those here.\n  // FIXME: This is restricted to pre type legalization by ensuring the setcc\n  // has i1 elements. If we loosen this we need to convert vector bool to a\n  // scalar bool.\n  if (Vec.getOpcode() == ISD::VSELECT &&\n      Vec.getOperand(0).getOpcode() == ISD::SETCC &&\n      Vec.getOperand(0).getValueType().getScalarType() == MVT::i1 &&\n      Vec.getOperand(0).getOperand(0).getValueType() == VecVT) {\n    // ext (sel Cond, X, Y), 0 --> sel (ext Cond, 0), (ext X, 0), (ext Y, 0)\n    SDLoc DL(ExtElt);\n    SDValue Ext0 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL,\n                               Vec.getOperand(0).getValueType().getScalarType(),\n                               Vec.getOperand(0), Index);\n    SDValue Ext1 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT,\n                               Vec.getOperand(1), Index);\n    SDValue Ext2 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT,\n                               Vec.getOperand(2), Index);\n    return DAG.getNode(ISD::SELECT, DL, VT, Ext0, Ext1, Ext2);\n  }\n\n  // TODO: This switch could include FNEG and the x86-specific FP logic ops\n  // (FAND, FANDN, FOR, FXOR). But that may require enhancements to avoid\n  // missed load folding and fma+fneg combining.\n  switch (Vec.getOpcode()) {\n  case ISD::FMA: // Begin 3 operands\n  case ISD::FMAD:\n  case ISD::FADD: // Begin 2 operands\n  case ISD::FSUB:\n  case ISD::FMUL:\n  case ISD::FDIV:\n  case ISD::FREM:\n  case ISD::FCOPYSIGN:\n  case ISD::FMINNUM:\n  case ISD::FMAXNUM:\n  case ISD::FMINNUM_IEEE:\n  case ISD::FMAXNUM_IEEE:\n  case ISD::FMAXIMUM:\n  case ISD::FMINIMUM:\n  case X86ISD::FMAX:\n  case X86ISD::FMIN:\n  case ISD::FABS: // Begin 1 operand\n  case ISD::FSQRT:\n  case ISD::FRINT:\n  case ISD::FCEIL:\n  case ISD::FTRUNC:\n  case ISD::FNEARBYINT:\n  case ISD::FROUND:\n  case ISD::FFLOOR:\n  case X86ISD::FRCP:\n  case X86ISD::FRSQRT: {\n    // extract (fp X, Y, ...), 0 --> fp (extract X, 0), (extract Y, 0), ...\n    SDLoc DL(ExtElt);\n    SmallVector<SDValue, 4> ExtOps;\n    for (SDValue Op : Vec->ops())\n      ExtOps.push_back(DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Op, Index));\n    return DAG.getNode(Vec.getOpcode(), DL, VT, ExtOps);\n  }\n  default:\n    return SDValue();\n  }\n  llvm_unreachable(\"All opcodes should return within switch\");\n}\n\n/// Try to convert a vector reduction sequence composed of binops and shuffles\n/// into horizontal ops.\nstatic SDValue combineArithReduction(SDNode *ExtElt, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  assert(ExtElt->getOpcode() == ISD::EXTRACT_VECTOR_ELT && \"Unexpected caller\");\n\n  // We need at least SSE2 to anything here.\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  ISD::NodeType Opc;\n  SDValue Rdx = DAG.matchBinOpReduction(ExtElt, Opc,\n                                        {ISD::ADD, ISD::MUL, ISD::FADD}, true);\n  if (!Rdx)\n    return SDValue();\n\n  SDValue Index = ExtElt->getOperand(1);\n  assert(isNullConstant(Index) &&\n         \"Reduction doesn't end in an extract from index 0\");\n\n  EVT VT = ExtElt->getValueType(0);\n  EVT VecVT = Rdx.getValueType();\n  if (VecVT.getScalarType() != VT)\n    return SDValue();\n\n  SDLoc DL(ExtElt);\n\n  // vXi8 mul reduction - promote to vXi16 mul reduction.\n  if (Opc == ISD::MUL) {\n    unsigned NumElts = VecVT.getVectorNumElements();\n    if (VT != MVT::i8 || NumElts < 4 || !isPowerOf2_32(NumElts))\n      return SDValue();\n    if (VecVT.getSizeInBits() >= 128) {\n      EVT WideVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16, NumElts / 2);\n      SDValue Lo = getUnpackl(DAG, DL, VecVT, Rdx, DAG.getUNDEF(VecVT));\n      SDValue Hi = getUnpackh(DAG, DL, VecVT, Rdx, DAG.getUNDEF(VecVT));\n      Lo = DAG.getBitcast(WideVT, Lo);\n      Hi = DAG.getBitcast(WideVT, Hi);\n      Rdx = DAG.getNode(Opc, DL, WideVT, Lo, Hi);\n      while (Rdx.getValueSizeInBits() > 128) {\n        std::tie(Lo, Hi) = splitVector(Rdx, DAG, DL);\n        Rdx = DAG.getNode(Opc, DL, Lo.getValueType(), Lo, Hi);\n      }\n    } else {\n      if (VecVT == MVT::v4i8)\n        Rdx = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v8i8, Rdx,\n                          DAG.getUNDEF(MVT::v4i8));\n      Rdx = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v16i8, Rdx,\n                        DAG.getUNDEF(MVT::v8i8));\n      Rdx = getUnpackl(DAG, DL, MVT::v16i8, Rdx, DAG.getUNDEF(MVT::v16i8));\n      Rdx = DAG.getBitcast(MVT::v8i16, Rdx);\n    }\n    if (NumElts >= 8)\n      Rdx = DAG.getNode(Opc, DL, MVT::v8i16, Rdx,\n                        DAG.getVectorShuffle(MVT::v8i16, DL, Rdx, Rdx,\n                                             {4, 5, 6, 7, -1, -1, -1, -1}));\n    Rdx = DAG.getNode(Opc, DL, MVT::v8i16, Rdx,\n                      DAG.getVectorShuffle(MVT::v8i16, DL, Rdx, Rdx,\n                                           {2, 3, -1, -1, -1, -1, -1, -1}));\n    Rdx = DAG.getNode(Opc, DL, MVT::v8i16, Rdx,\n                      DAG.getVectorShuffle(MVT::v8i16, DL, Rdx, Rdx,\n                                           {1, -1, -1, -1, -1, -1, -1, -1}));\n    Rdx = DAG.getBitcast(MVT::v16i8, Rdx);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Rdx, Index);\n  }\n\n  // vXi8 add reduction - sub 128-bit vector.\n  if (VecVT == MVT::v4i8 || VecVT == MVT::v8i8) {\n    if (VecVT == MVT::v4i8) {\n      // Pad with zero.\n      if (Subtarget.hasSSE41()) {\n        Rdx = DAG.getBitcast(MVT::i32, Rdx);\n        Rdx = DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, MVT::v4i32,\n                          DAG.getConstant(0, DL, MVT::v4i32), Rdx,\n                          DAG.getIntPtrConstant(0, DL));\n        Rdx = DAG.getBitcast(MVT::v16i8, Rdx);\n      } else {\n        Rdx = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v8i8, Rdx,\n                          DAG.getConstant(0, DL, VecVT));\n      }\n    }\n    if (Rdx.getValueType() == MVT::v8i8) {\n      // Pad with undef.\n      Rdx = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v16i8, Rdx,\n                        DAG.getUNDEF(MVT::v8i8));\n    }\n    Rdx = DAG.getNode(X86ISD::PSADBW, DL, MVT::v2i64, Rdx,\n                      DAG.getConstant(0, DL, MVT::v16i8));\n    Rdx = DAG.getBitcast(MVT::v16i8, Rdx);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Rdx, Index);\n  }\n\n  // Must be a >=128-bit vector with pow2 elements.\n  if ((VecVT.getSizeInBits() % 128) != 0 ||\n      !isPowerOf2_32(VecVT.getVectorNumElements()))\n    return SDValue();\n\n  // vXi8 add reduction - sum lo/hi halves then use PSADBW.\n  if (VT == MVT::i8) {\n    while (Rdx.getValueSizeInBits() > 128) {\n      SDValue Lo, Hi;\n      std::tie(Lo, Hi) = splitVector(Rdx, DAG, DL);\n      VecVT = Lo.getValueType();\n      Rdx = DAG.getNode(ISD::ADD, DL, VecVT, Lo, Hi);\n    }\n    assert(VecVT == MVT::v16i8 && \"v16i8 reduction expected\");\n\n    SDValue Hi = DAG.getVectorShuffle(\n        MVT::v16i8, DL, Rdx, Rdx,\n        {8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1});\n    Rdx = DAG.getNode(ISD::ADD, DL, MVT::v16i8, Rdx, Hi);\n    Rdx = DAG.getNode(X86ISD::PSADBW, DL, MVT::v2i64, Rdx,\n                      getZeroVector(MVT::v16i8, Subtarget, DAG, DL));\n    Rdx = DAG.getBitcast(MVT::v16i8, Rdx);\n    return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Rdx, Index);\n  }\n\n  // Only use (F)HADD opcodes if they aren't microcoded or minimizes codesize.\n  if (!shouldUseHorizontalOp(true, DAG, Subtarget))\n    return SDValue();\n\n  unsigned HorizOpcode = Opc == ISD::ADD ? X86ISD::HADD : X86ISD::FHADD;\n\n  // 256-bit horizontal instructions operate on 128-bit chunks rather than\n  // across the whole vector, so we need an extract + hop preliminary stage.\n  // This is the only step where the operands of the hop are not the same value.\n  // TODO: We could extend this to handle 512-bit or even longer vectors.\n  if (((VecVT == MVT::v16i16 || VecVT == MVT::v8i32) && Subtarget.hasSSSE3()) ||\n      ((VecVT == MVT::v8f32 || VecVT == MVT::v4f64) && Subtarget.hasSSE3())) {\n    unsigned NumElts = VecVT.getVectorNumElements();\n    SDValue Hi = extract128BitVector(Rdx, NumElts / 2, DAG, DL);\n    SDValue Lo = extract128BitVector(Rdx, 0, DAG, DL);\n    Rdx = DAG.getNode(HorizOpcode, DL, Lo.getValueType(), Hi, Lo);\n    VecVT = Rdx.getValueType();\n  }\n  if (!((VecVT == MVT::v8i16 || VecVT == MVT::v4i32) && Subtarget.hasSSSE3()) &&\n      !((VecVT == MVT::v4f32 || VecVT == MVT::v2f64) && Subtarget.hasSSE3()))\n    return SDValue();\n\n  // extract (add (shuf X), X), 0 --> extract (hadd X, X), 0\n  unsigned ReductionSteps = Log2_32(VecVT.getVectorNumElements());\n  for (unsigned i = 0; i != ReductionSteps; ++i)\n    Rdx = DAG.getNode(HorizOpcode, DL, VecVT, Rdx, Rdx);\n\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, VT, Rdx, Index);\n}\n\n/// Detect vector gather/scatter index generation and convert it from being a\n/// bunch of shuffles and extracts into a somewhat faster sequence.\n/// For i686, the best sequence is apparently storing the value and loading\n/// scalars back, while for x64 we should use 64-bit extracts and shifts.\nstatic SDValue combineExtractVectorElt(SDNode *N, SelectionDAG &DAG,\n                                       TargetLowering::DAGCombinerInfo &DCI,\n                                       const X86Subtarget &Subtarget) {\n  if (SDValue NewOp = combineExtractWithShuffle(N, DAG, DCI, Subtarget))\n    return NewOp;\n\n  SDValue InputVector = N->getOperand(0);\n  SDValue EltIdx = N->getOperand(1);\n  auto *CIdx = dyn_cast<ConstantSDNode>(EltIdx);\n\n  EVT SrcVT = InputVector.getValueType();\n  EVT VT = N->getValueType(0);\n  SDLoc dl(InputVector);\n  bool IsPextr = N->getOpcode() != ISD::EXTRACT_VECTOR_ELT;\n  unsigned NumSrcElts = SrcVT.getVectorNumElements();\n\n  if (CIdx && CIdx->getAPIntValue().uge(NumSrcElts))\n    return IsPextr ? DAG.getConstant(0, dl, VT) : DAG.getUNDEF(VT);\n\n  // Integer Constant Folding.\n  if (CIdx && VT.isInteger()) {\n    APInt UndefVecElts;\n    SmallVector<APInt, 16> EltBits;\n    unsigned VecEltBitWidth = SrcVT.getScalarSizeInBits();\n    if (getTargetConstantBitsFromNode(InputVector, VecEltBitWidth, UndefVecElts,\n                                      EltBits, true, false)) {\n      uint64_t Idx = CIdx->getZExtValue();\n      if (UndefVecElts[Idx])\n        return IsPextr ? DAG.getConstant(0, dl, VT) : DAG.getUNDEF(VT);\n      return DAG.getConstant(EltBits[Idx].zextOrSelf(VT.getScalarSizeInBits()),\n                             dl, VT);\n    }\n  }\n\n  if (IsPextr) {\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    if (TLI.SimplifyDemandedBits(\n            SDValue(N, 0), APInt::getAllOnesValue(VT.getSizeInBits()), DCI))\n      return SDValue(N, 0);\n\n    // PEXTR*(PINSR*(v, s, c), c) -> s (with implicit zext handling).\n    if ((InputVector.getOpcode() == X86ISD::PINSRB ||\n         InputVector.getOpcode() == X86ISD::PINSRW) &&\n        InputVector.getOperand(2) == EltIdx) {\n      assert(SrcVT == InputVector.getOperand(0).getValueType() &&\n             \"Vector type mismatch\");\n      SDValue Scl = InputVector.getOperand(1);\n      Scl = DAG.getNode(ISD::TRUNCATE, dl, SrcVT.getScalarType(), Scl);\n      return DAG.getZExtOrTrunc(Scl, dl, VT);\n    }\n\n    // TODO - Remove this once we can handle the implicit zero-extension of\n    // X86ISD::PEXTRW/X86ISD::PEXTRB in combinePredicateReduction and\n    // combineBasicSADPattern.\n    return SDValue();\n  }\n\n  // Detect mmx extraction of all bits as a i64. It works better as a bitcast.\n  if (InputVector.getOpcode() == ISD::BITCAST && InputVector.hasOneUse() &&\n      VT == MVT::i64 && SrcVT == MVT::v1i64 && isNullConstant(EltIdx)) {\n    SDValue MMXSrc = InputVector.getOperand(0);\n\n    // The bitcast source is a direct mmx result.\n    if (MMXSrc.getValueType() == MVT::x86mmx)\n      return DAG.getBitcast(VT, InputVector);\n  }\n\n  // Detect mmx to i32 conversion through a v2i32 elt extract.\n  if (InputVector.getOpcode() == ISD::BITCAST && InputVector.hasOneUse() &&\n      VT == MVT::i32 && SrcVT == MVT::v2i32 && isNullConstant(EltIdx)) {\n    SDValue MMXSrc = InputVector.getOperand(0);\n\n    // The bitcast source is a direct mmx result.\n    if (MMXSrc.getValueType() == MVT::x86mmx)\n      return DAG.getNode(X86ISD::MMX_MOVD2W, dl, MVT::i32, MMXSrc);\n  }\n\n  // Check whether this extract is the root of a sum of absolute differences\n  // pattern. This has to be done here because we really want it to happen\n  // pre-legalization,\n  if (SDValue SAD = combineBasicSADPattern(N, DAG, Subtarget))\n    return SAD;\n\n  // Attempt to replace an all_of/any_of horizontal reduction with a MOVMSK.\n  if (SDValue Cmp = combinePredicateReduction(N, DAG, Subtarget))\n    return Cmp;\n\n  // Attempt to replace min/max v8i16/v16i8 reductions with PHMINPOSUW.\n  if (SDValue MinMax = combineMinMaxReduction(N, DAG, Subtarget))\n    return MinMax;\n\n  // Attempt to optimize ADD/FADD/MUL reductions with HADD, promotion etc..\n  if (SDValue V = combineArithReduction(N, DAG, Subtarget))\n    return V;\n\n  if (SDValue V = scalarizeExtEltFP(N, DAG))\n    return V;\n\n  // Attempt to extract a i1 element by using MOVMSK to extract the signbits\n  // and then testing the relevant element.\n  //\n  // Note that we only combine extracts on the *same* result number, i.e.\n  //   t0 = merge_values a0, a1, a2, a3\n  //   i1 = extract_vector_elt t0, Constant:i64<2>\n  //   i1 = extract_vector_elt t0, Constant:i64<3>\n  // but not\n  //   i1 = extract_vector_elt t0:1, Constant:i64<2>\n  // since the latter would need its own MOVMSK.\n  if (CIdx && SrcVT.getScalarType() == MVT::i1) {\n    SmallVector<SDNode *, 16> BoolExtracts;\n    unsigned ResNo = InputVector.getResNo();\n    auto IsBoolExtract = [&BoolExtracts, &ResNo](SDNode *Use) {\n      if (Use->getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n          isa<ConstantSDNode>(Use->getOperand(1)) &&\n          Use->getOperand(0).getResNo() == ResNo &&\n          Use->getValueType(0) == MVT::i1) {\n        BoolExtracts.push_back(Use);\n        return true;\n      }\n      return false;\n    };\n    if (all_of(InputVector->uses(), IsBoolExtract) &&\n        BoolExtracts.size() > 1) {\n      EVT BCVT = EVT::getIntegerVT(*DAG.getContext(), NumSrcElts);\n      if (SDValue BC =\n              combineBitcastvxi1(DAG, BCVT, InputVector, dl, Subtarget)) {\n        for (SDNode *Use : BoolExtracts) {\n          // extractelement vXi1 X, MaskIdx --> ((movmsk X) & Mask) == Mask\n          unsigned MaskIdx = Use->getConstantOperandVal(1);\n          APInt MaskBit = APInt::getOneBitSet(NumSrcElts, MaskIdx);\n          SDValue Mask = DAG.getConstant(MaskBit, dl, BCVT);\n          SDValue Res = DAG.getNode(ISD::AND, dl, BCVT, BC, Mask);\n          Res = DAG.getSetCC(dl, MVT::i1, Res, Mask, ISD::SETEQ);\n          DCI.CombineTo(Use, Res);\n        }\n        return SDValue(N, 0);\n      }\n    }\n  }\n\n  return SDValue();\n}\n\n/// If a vector select has an operand that is -1 or 0, try to simplify the\n/// select to a bitwise logic operation.\n/// TODO: Move to DAGCombiner, possibly using TargetLowering::hasAndNot()?\nstatic SDValue\ncombineVSelectWithAllOnesOrZeros(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const X86Subtarget &Subtarget) {\n  SDValue Cond = N->getOperand(0);\n  SDValue LHS = N->getOperand(1);\n  SDValue RHS = N->getOperand(2);\n  EVT VT = LHS.getValueType();\n  EVT CondVT = Cond.getValueType();\n  SDLoc DL(N);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  if (N->getOpcode() != ISD::VSELECT)\n    return SDValue();\n\n  assert(CondVT.isVector() && \"Vector select expects a vector selector!\");\n\n  // TODO: Use isNullOrNullSplat() to distinguish constants with undefs?\n  // TODO: Can we assert that both operands are not zeros (because that should\n  //       get simplified at node creation time)?\n  bool TValIsAllZeros = ISD::isBuildVectorAllZeros(LHS.getNode());\n  bool FValIsAllZeros = ISD::isBuildVectorAllZeros(RHS.getNode());\n\n  // If both inputs are 0/undef, create a complete zero vector.\n  // FIXME: As noted above this should be handled by DAGCombiner/getNode.\n  if (TValIsAllZeros && FValIsAllZeros) {\n    if (VT.isFloatingPoint())\n      return DAG.getConstantFP(0.0, DL, VT);\n    return DAG.getConstant(0, DL, VT);\n  }\n\n  // To use the condition operand as a bitwise mask, it must have elements that\n  // are the same size as the select elements. Ie, the condition operand must\n  // have already been promoted from the IR select condition type <N x i1>.\n  // Don't check if the types themselves are equal because that excludes\n  // vector floating-point selects.\n  if (CondVT.getScalarSizeInBits() != VT.getScalarSizeInBits())\n    return SDValue();\n\n  // Try to invert the condition if true value is not all 1s and false value is\n  // not all 0s. Only do this if the condition has one use.\n  bool TValIsAllOnes = ISD::isBuildVectorAllOnes(LHS.getNode());\n  if (!TValIsAllOnes && !FValIsAllZeros && Cond.hasOneUse() &&\n      // Check if the selector will be produced by CMPP*/PCMP*.\n      Cond.getOpcode() == ISD::SETCC &&\n      // Check if SETCC has already been promoted.\n      TLI.getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(), VT) ==\n          CondVT) {\n    bool FValIsAllOnes = ISD::isBuildVectorAllOnes(RHS.getNode());\n\n    if (TValIsAllZeros || FValIsAllOnes) {\n      SDValue CC = Cond.getOperand(2);\n      ISD::CondCode NewCC = ISD::getSetCCInverse(\n          cast<CondCodeSDNode>(CC)->get(), Cond.getOperand(0).getValueType());\n      Cond = DAG.getSetCC(DL, CondVT, Cond.getOperand(0), Cond.getOperand(1),\n                          NewCC);\n      std::swap(LHS, RHS);\n      TValIsAllOnes = FValIsAllOnes;\n      FValIsAllZeros = TValIsAllZeros;\n    }\n  }\n\n  // Cond value must be 'sign splat' to be converted to a logical op.\n  if (DAG.ComputeNumSignBits(Cond) != CondVT.getScalarSizeInBits())\n    return SDValue();\n\n  // vselect Cond, 111..., 000... -> Cond\n  if (TValIsAllOnes && FValIsAllZeros)\n    return DAG.getBitcast(VT, Cond);\n\n  if (!TLI.isTypeLegal(CondVT))\n    return SDValue();\n\n  // vselect Cond, 111..., X -> or Cond, X\n  if (TValIsAllOnes) {\n    SDValue CastRHS = DAG.getBitcast(CondVT, RHS);\n    SDValue Or = DAG.getNode(ISD::OR, DL, CondVT, Cond, CastRHS);\n    return DAG.getBitcast(VT, Or);\n  }\n\n  // vselect Cond, X, 000... -> and Cond, X\n  if (FValIsAllZeros) {\n    SDValue CastLHS = DAG.getBitcast(CondVT, LHS);\n    SDValue And = DAG.getNode(ISD::AND, DL, CondVT, Cond, CastLHS);\n    return DAG.getBitcast(VT, And);\n  }\n\n  // vselect Cond, 000..., X -> andn Cond, X\n  if (TValIsAllZeros) {\n    SDValue CastRHS = DAG.getBitcast(CondVT, RHS);\n    SDValue AndN;\n    // The canonical form differs for i1 vectors - x86andnp is not used\n    if (CondVT.getScalarType() == MVT::i1)\n      AndN = DAG.getNode(ISD::AND, DL, CondVT, DAG.getNOT(DL, Cond, CondVT),\n                         CastRHS);\n    else\n      AndN = DAG.getNode(X86ISD::ANDNP, DL, CondVT, Cond, CastRHS);\n    return DAG.getBitcast(VT, AndN);\n  }\n\n  return SDValue();\n}\n\n/// If both arms of a vector select are concatenated vectors, split the select,\n/// and concatenate the result to eliminate a wide (256-bit) vector instruction:\n///   vselect Cond, (concat T0, T1), (concat F0, F1) -->\n///   concat (vselect (split Cond), T0, F0), (vselect (split Cond), T1, F1)\nstatic SDValue narrowVectorSelect(SDNode *N, SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  unsigned Opcode = N->getOpcode();\n  if (Opcode != X86ISD::BLENDV && Opcode != ISD::VSELECT)\n    return SDValue();\n\n  // TODO: Split 512-bit vectors too?\n  EVT VT = N->getValueType(0);\n  if (!VT.is256BitVector())\n    return SDValue();\n\n  // TODO: Split as long as any 2 of the 3 operands are concatenated?\n  SDValue Cond = N->getOperand(0);\n  SDValue TVal = N->getOperand(1);\n  SDValue FVal = N->getOperand(2);\n  SmallVector<SDValue, 4> CatOpsT, CatOpsF;\n  if (!TVal.hasOneUse() || !FVal.hasOneUse() ||\n      !collectConcatOps(TVal.getNode(), CatOpsT) ||\n      !collectConcatOps(FVal.getNode(), CatOpsF))\n    return SDValue();\n\n  auto makeBlend = [Opcode](SelectionDAG &DAG, const SDLoc &DL,\n                            ArrayRef<SDValue> Ops) {\n    return DAG.getNode(Opcode, DL, Ops[1].getValueType(), Ops);\n  };\n  return SplitOpsAndApply(DAG, Subtarget, SDLoc(N), VT, { Cond, TVal, FVal },\n                          makeBlend, /*CheckBWI*/ false);\n}\n\nstatic SDValue combineSelectOfTwoConstants(SDNode *N, SelectionDAG &DAG) {\n  SDValue Cond = N->getOperand(0);\n  SDValue LHS = N->getOperand(1);\n  SDValue RHS = N->getOperand(2);\n  SDLoc DL(N);\n\n  auto *TrueC = dyn_cast<ConstantSDNode>(LHS);\n  auto *FalseC = dyn_cast<ConstantSDNode>(RHS);\n  if (!TrueC || !FalseC)\n    return SDValue();\n\n  // Don't do this for crazy integer types.\n  EVT VT = N->getValueType(0);\n  if (!DAG.getTargetLoweringInfo().isTypeLegal(VT))\n    return SDValue();\n\n  // We're going to use the condition bit in math or logic ops. We could allow\n  // this with a wider condition value (post-legalization it becomes an i8),\n  // but if nothing is creating selects that late, it doesn't matter.\n  if (Cond.getValueType() != MVT::i1)\n    return SDValue();\n\n  // A power-of-2 multiply is just a shift. LEA also cheaply handles multiply by\n  // 3, 5, or 9 with i32/i64, so those get transformed too.\n  // TODO: For constants that overflow or do not differ by power-of-2 or small\n  // multiplier, convert to 'and' + 'add'.\n  const APInt &TrueVal = TrueC->getAPIntValue();\n  const APInt &FalseVal = FalseC->getAPIntValue();\n  bool OV;\n  APInt Diff = TrueVal.ssub_ov(FalseVal, OV);\n  if (OV)\n    return SDValue();\n\n  APInt AbsDiff = Diff.abs();\n  if (AbsDiff.isPowerOf2() ||\n      ((VT == MVT::i32 || VT == MVT::i64) &&\n       (AbsDiff == 3 || AbsDiff == 5 || AbsDiff == 9))) {\n\n    // We need a positive multiplier constant for shift/LEA codegen. The 'not'\n    // of the condition can usually be folded into a compare predicate, but even\n    // without that, the sequence should be cheaper than a CMOV alternative.\n    if (TrueVal.slt(FalseVal)) {\n      Cond = DAG.getNOT(DL, Cond, MVT::i1);\n      std::swap(TrueC, FalseC);\n    }\n\n    // select Cond, TC, FC --> (zext(Cond) * (TC - FC)) + FC\n    SDValue R = DAG.getNode(ISD::ZERO_EXTEND, DL, VT, Cond);\n\n    // Multiply condition by the difference if non-one.\n    if (!AbsDiff.isOneValue())\n      R = DAG.getNode(ISD::MUL, DL, VT, R, DAG.getConstant(AbsDiff, DL, VT));\n\n    // Add the base if non-zero.\n    if (!FalseC->isNullValue())\n      R = DAG.getNode(ISD::ADD, DL, VT, R, SDValue(FalseC, 0));\n\n    return R;\n  }\n\n  return SDValue();\n}\n\n/// If this is a *dynamic* select (non-constant condition) and we can match\n/// this node with one of the variable blend instructions, restructure the\n/// condition so that blends can use the high (sign) bit of each element.\n/// This function will also call SimplifyDemandedBits on already created\n/// BLENDV to perform additional simplifications.\nstatic SDValue combineVSelectToBLENDV(SDNode *N, SelectionDAG &DAG,\n                                           TargetLowering::DAGCombinerInfo &DCI,\n                                           const X86Subtarget &Subtarget) {\n  SDValue Cond = N->getOperand(0);\n  if ((N->getOpcode() != ISD::VSELECT &&\n       N->getOpcode() != X86ISD::BLENDV) ||\n      ISD::isBuildVectorOfConstantSDNodes(Cond.getNode()))\n    return SDValue();\n\n  // Don't optimize before the condition has been transformed to a legal type\n  // and don't ever optimize vector selects that map to AVX512 mask-registers.\n  unsigned BitWidth = Cond.getScalarValueSizeInBits();\n  if (BitWidth < 8 || BitWidth > 64)\n    return SDValue();\n\n  // We can only handle the cases where VSELECT is directly legal on the\n  // subtarget. We custom lower VSELECT nodes with constant conditions and\n  // this makes it hard to see whether a dynamic VSELECT will correctly\n  // lower, so we both check the operation's status and explicitly handle the\n  // cases where a *dynamic* blend will fail even though a constant-condition\n  // blend could be custom lowered.\n  // FIXME: We should find a better way to handle this class of problems.\n  // Potentially, we should combine constant-condition vselect nodes\n  // pre-legalization into shuffles and not mark as many types as custom\n  // lowered.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  EVT VT = N->getValueType(0);\n  if (!TLI.isOperationLegalOrCustom(ISD::VSELECT, VT))\n    return SDValue();\n  // FIXME: We don't support i16-element blends currently. We could and\n  // should support them by making *all* the bits in the condition be set\n  // rather than just the high bit and using an i8-element blend.\n  if (VT.getVectorElementType() == MVT::i16)\n    return SDValue();\n  // Dynamic blending was only available from SSE4.1 onward.\n  if (VT.is128BitVector() && !Subtarget.hasSSE41())\n    return SDValue();\n  // Byte blends are only available in AVX2\n  if (VT == MVT::v32i8 && !Subtarget.hasAVX2())\n    return SDValue();\n  // There are no 512-bit blend instructions that use sign bits.\n  if (VT.is512BitVector())\n    return SDValue();\n\n  auto OnlyUsedAsSelectCond = [](SDValue Cond) {\n    for (SDNode::use_iterator UI = Cond->use_begin(), UE = Cond->use_end();\n         UI != UE; ++UI)\n      if ((UI->getOpcode() != ISD::VSELECT &&\n           UI->getOpcode() != X86ISD::BLENDV) ||\n          UI.getOperandNo() != 0)\n        return false;\n\n    return true;\n  };\n\n  APInt DemandedBits(APInt::getSignMask(BitWidth));\n\n  if (OnlyUsedAsSelectCond(Cond)) {\n    KnownBits Known;\n    TargetLowering::TargetLoweringOpt TLO(DAG, !DCI.isBeforeLegalize(),\n                                          !DCI.isBeforeLegalizeOps());\n    if (!TLI.SimplifyDemandedBits(Cond, DemandedBits, Known, TLO, 0, true))\n      return SDValue();\n\n    // If we changed the computation somewhere in the DAG, this change will\n    // affect all users of Cond. Update all the nodes so that we do not use\n    // the generic VSELECT anymore. Otherwise, we may perform wrong\n    // optimizations as we messed with the actual expectation for the vector\n    // boolean values.\n    for (SDNode *U : Cond->uses()) {\n      if (U->getOpcode() == X86ISD::BLENDV)\n        continue;\n\n      SDValue SB = DAG.getNode(X86ISD::BLENDV, SDLoc(U), U->getValueType(0),\n                               Cond, U->getOperand(1), U->getOperand(2));\n      DAG.ReplaceAllUsesOfValueWith(SDValue(U, 0), SB);\n      DCI.AddToWorklist(U);\n    }\n    DCI.CommitTargetLoweringOpt(TLO);\n    return SDValue(N, 0);\n  }\n\n  // Otherwise we can still at least try to simplify multiple use bits.\n  if (SDValue V = TLI.SimplifyMultipleUseDemandedBits(Cond, DemandedBits, DAG))\n      return DAG.getNode(X86ISD::BLENDV, SDLoc(N), N->getValueType(0), V,\n                         N->getOperand(1), N->getOperand(2));\n\n  return SDValue();\n}\n\n// Try to match:\n//   (or (and (M, (sub 0, X)), (pandn M, X)))\n// which is a special case of:\n//   (select M, (sub 0, X), X)\n// Per:\n// http://graphics.stanford.edu/~seander/bithacks.html#ConditionalNegate\n// We know that, if fNegate is 0 or 1:\n//   (fNegate ? -v : v) == ((v ^ -fNegate) + fNegate)\n//\n// Here, we have a mask, M (all 1s or 0), and, similarly, we know that:\n//   ((M & 1) ? -X : X) == ((X ^ -(M & 1)) + (M & 1))\n//   ( M      ? -X : X) == ((X ^   M     ) + (M & 1))\n// This lets us transform our vselect to:\n//   (add (xor X, M), (and M, 1))\n// And further to:\n//   (sub (xor X, M), M)\nstatic SDValue combineLogicBlendIntoConditionalNegate(\n    EVT VT, SDValue Mask, SDValue X, SDValue Y, const SDLoc &DL,\n    SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  EVT MaskVT = Mask.getValueType();\n  assert(MaskVT.isInteger() &&\n         DAG.ComputeNumSignBits(Mask) == MaskVT.getScalarSizeInBits() &&\n         \"Mask must be zero/all-bits\");\n\n  if (X.getValueType() != MaskVT || Y.getValueType() != MaskVT)\n    return SDValue();\n  if (!DAG.getTargetLoweringInfo().isOperationLegal(ISD::SUB, MaskVT))\n    return SDValue();\n\n  auto IsNegV = [](SDNode *N, SDValue V) {\n    return N->getOpcode() == ISD::SUB && N->getOperand(1) == V &&\n           ISD::isBuildVectorAllZeros(N->getOperand(0).getNode());\n  };\n\n  SDValue V;\n  if (IsNegV(Y.getNode(), X))\n    V = X;\n  else if (IsNegV(X.getNode(), Y))\n    V = Y;\n  else\n    return SDValue();\n\n  SDValue SubOp1 = DAG.getNode(ISD::XOR, DL, MaskVT, V, Mask);\n  SDValue SubOp2 = Mask;\n\n  // If the negate was on the false side of the select, then\n  // the operands of the SUB need to be swapped. PR 27251.\n  // This is because the pattern being matched above is\n  // (vselect M, (sub (0, X), X)  -> (sub (xor X, M), M)\n  // but if the pattern matched was\n  // (vselect M, X, (sub (0, X))), that is really negation of the pattern\n  // above, -(vselect M, (sub 0, X), X), and therefore the replacement\n  // pattern also needs to be a negation of the replacement pattern above.\n  // And -(sub X, Y) is just sub (Y, X), so swapping the operands of the\n  // sub accomplishes the negation of the replacement pattern.\n  if (V == Y)\n    std::swap(SubOp1, SubOp2);\n\n  SDValue Res = DAG.getNode(ISD::SUB, DL, MaskVT, SubOp1, SubOp2);\n  return DAG.getBitcast(VT, Res);\n}\n\n/// Do target-specific dag combines on SELECT and VSELECT nodes.\nstatic SDValue combineSelect(SDNode *N, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI,\n                             const X86Subtarget &Subtarget) {\n  SDLoc DL(N);\n  SDValue Cond = N->getOperand(0);\n  SDValue LHS = N->getOperand(1);\n  SDValue RHS = N->getOperand(2);\n\n  // Try simplification again because we use this function to optimize\n  // BLENDV nodes that are not handled by the generic combiner.\n  if (SDValue V = DAG.simplifySelect(Cond, LHS, RHS))\n    return V;\n\n  EVT VT = LHS.getValueType();\n  EVT CondVT = Cond.getValueType();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  bool CondConstantVector = ISD::isBuildVectorOfConstantSDNodes(Cond.getNode());\n\n  // Attempt to combine (select M, (sub 0, X), X) -> (sub (xor X, M), M).\n  // Limit this to cases of non-constant masks that createShuffleMaskFromVSELECT\n  // can't catch, plus vXi8 cases where we'd likely end up with BLENDV.\n  if (CondVT.isVector() && CondVT.isInteger() &&\n      CondVT.getScalarSizeInBits() == VT.getScalarSizeInBits() &&\n      (!CondConstantVector || CondVT.getScalarType() == MVT::i8) &&\n      DAG.ComputeNumSignBits(Cond) == CondVT.getScalarSizeInBits())\n    if (SDValue V = combineLogicBlendIntoConditionalNegate(VT, Cond, RHS, LHS,\n                                                           DL, DAG, Subtarget))\n      return V;\n\n  // Convert vselects with constant condition into shuffles.\n  if (CondConstantVector && DCI.isBeforeLegalizeOps()) {\n    SmallVector<int, 64> Mask;\n    if (createShuffleMaskFromVSELECT(Mask, Cond))\n      return DAG.getVectorShuffle(VT, DL, LHS, RHS, Mask);\n  }\n\n  // fold vselect(cond, pshufb(x), pshufb(y)) -> or (pshufb(x), pshufb(y))\n  // by forcing the unselected elements to zero.\n  // TODO: Can we handle more shuffles with this?\n  if (N->getOpcode() == ISD::VSELECT && CondVT.isVector() &&\n      LHS.getOpcode() == X86ISD::PSHUFB && RHS.getOpcode() == X86ISD::PSHUFB &&\n      LHS.hasOneUse() && RHS.hasOneUse()) {\n    MVT SimpleVT = VT.getSimpleVT();\n    bool LHSUnary, RHSUnary;\n    SmallVector<SDValue, 1> LHSOps, RHSOps;\n    SmallVector<int, 64> LHSMask, RHSMask, CondMask;\n    if (createShuffleMaskFromVSELECT(CondMask, Cond) &&\n        getTargetShuffleMask(LHS.getNode(), SimpleVT, true, LHSOps, LHSMask,\n                             LHSUnary) &&\n        getTargetShuffleMask(RHS.getNode(), SimpleVT, true, RHSOps, RHSMask,\n                             RHSUnary)) {\n      int NumElts = VT.getVectorNumElements();\n      for (int i = 0; i != NumElts; ++i) {\n        if (CondMask[i] < NumElts)\n          RHSMask[i] = 0x80;\n        else\n          LHSMask[i] = 0x80;\n      }\n      LHS = DAG.getNode(X86ISD::PSHUFB, DL, VT, LHS.getOperand(0),\n                        getConstVector(LHSMask, SimpleVT, DAG, DL, true));\n      RHS = DAG.getNode(X86ISD::PSHUFB, DL, VT, RHS.getOperand(0),\n                        getConstVector(RHSMask, SimpleVT, DAG, DL, true));\n      return DAG.getNode(ISD::OR, DL, VT, LHS, RHS);\n    }\n  }\n\n  // If we have SSE[12] support, try to form min/max nodes. SSE min/max\n  // instructions match the semantics of the common C idiom x<y?x:y but not\n  // x<=y?x:y, because of how they handle negative zero (which can be\n  // ignored in unsafe-math mode).\n  // We also try to create v2f32 min/max nodes, which we later widen to v4f32.\n  if (Cond.getOpcode() == ISD::SETCC && VT.isFloatingPoint() &&\n      VT != MVT::f80 && VT != MVT::f128 &&\n      (TLI.isTypeLegal(VT) || VT == MVT::v2f32) &&\n      (Subtarget.hasSSE2() ||\n       (Subtarget.hasSSE1() && VT.getScalarType() == MVT::f32))) {\n    ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n\n    unsigned Opcode = 0;\n    // Check for x CC y ? x : y.\n    if (DAG.isEqualTo(LHS, Cond.getOperand(0)) &&\n        DAG.isEqualTo(RHS, Cond.getOperand(1))) {\n      switch (CC) {\n      default: break;\n      case ISD::SETULT:\n        // Converting this to a min would handle NaNs incorrectly, and swapping\n        // the operands would cause it to handle comparisons between positive\n        // and negative zero incorrectly.\n        if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS)) {\n          if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n              !(DAG.isKnownNeverZeroFloat(LHS) ||\n                DAG.isKnownNeverZeroFloat(RHS)))\n            break;\n          std::swap(LHS, RHS);\n        }\n        Opcode = X86ISD::FMIN;\n        break;\n      case ISD::SETOLE:\n        // Converting this to a min would handle comparisons between positive\n        // and negative zero incorrectly.\n        if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n            !DAG.isKnownNeverZeroFloat(LHS) && !DAG.isKnownNeverZeroFloat(RHS))\n          break;\n        Opcode = X86ISD::FMIN;\n        break;\n      case ISD::SETULE:\n        // Converting this to a min would handle both negative zeros and NaNs\n        // incorrectly, but we can swap the operands to fix both.\n        std::swap(LHS, RHS);\n        LLVM_FALLTHROUGH;\n      case ISD::SETOLT:\n      case ISD::SETLT:\n      case ISD::SETLE:\n        Opcode = X86ISD::FMIN;\n        break;\n\n      case ISD::SETOGE:\n        // Converting this to a max would handle comparisons between positive\n        // and negative zero incorrectly.\n        if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n            !DAG.isKnownNeverZeroFloat(LHS) && !DAG.isKnownNeverZeroFloat(RHS))\n          break;\n        Opcode = X86ISD::FMAX;\n        break;\n      case ISD::SETUGT:\n        // Converting this to a max would handle NaNs incorrectly, and swapping\n        // the operands would cause it to handle comparisons between positive\n        // and negative zero incorrectly.\n        if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS)) {\n          if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n              !(DAG.isKnownNeverZeroFloat(LHS) ||\n                DAG.isKnownNeverZeroFloat(RHS)))\n            break;\n          std::swap(LHS, RHS);\n        }\n        Opcode = X86ISD::FMAX;\n        break;\n      case ISD::SETUGE:\n        // Converting this to a max would handle both negative zeros and NaNs\n        // incorrectly, but we can swap the operands to fix both.\n        std::swap(LHS, RHS);\n        LLVM_FALLTHROUGH;\n      case ISD::SETOGT:\n      case ISD::SETGT:\n      case ISD::SETGE:\n        Opcode = X86ISD::FMAX;\n        break;\n      }\n    // Check for x CC y ? y : x -- a min/max with reversed arms.\n    } else if (DAG.isEqualTo(LHS, Cond.getOperand(1)) &&\n               DAG.isEqualTo(RHS, Cond.getOperand(0))) {\n      switch (CC) {\n      default: break;\n      case ISD::SETOGE:\n        // Converting this to a min would handle comparisons between positive\n        // and negative zero incorrectly, and swapping the operands would\n        // cause it to handle NaNs incorrectly.\n        if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n            !(DAG.isKnownNeverZeroFloat(LHS) ||\n              DAG.isKnownNeverZeroFloat(RHS))) {\n          if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS))\n            break;\n          std::swap(LHS, RHS);\n        }\n        Opcode = X86ISD::FMIN;\n        break;\n      case ISD::SETUGT:\n        // Converting this to a min would handle NaNs incorrectly.\n        if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS))\n          break;\n        Opcode = X86ISD::FMIN;\n        break;\n      case ISD::SETUGE:\n        // Converting this to a min would handle both negative zeros and NaNs\n        // incorrectly, but we can swap the operands to fix both.\n        std::swap(LHS, RHS);\n        LLVM_FALLTHROUGH;\n      case ISD::SETOGT:\n      case ISD::SETGT:\n      case ISD::SETGE:\n        Opcode = X86ISD::FMIN;\n        break;\n\n      case ISD::SETULT:\n        // Converting this to a max would handle NaNs incorrectly.\n        if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS))\n          break;\n        Opcode = X86ISD::FMAX;\n        break;\n      case ISD::SETOLE:\n        // Converting this to a max would handle comparisons between positive\n        // and negative zero incorrectly, and swapping the operands would\n        // cause it to handle NaNs incorrectly.\n        if (!DAG.getTarget().Options.NoSignedZerosFPMath &&\n            !DAG.isKnownNeverZeroFloat(LHS) &&\n            !DAG.isKnownNeverZeroFloat(RHS)) {\n          if (!DAG.isKnownNeverNaN(LHS) || !DAG.isKnownNeverNaN(RHS))\n            break;\n          std::swap(LHS, RHS);\n        }\n        Opcode = X86ISD::FMAX;\n        break;\n      case ISD::SETULE:\n        // Converting this to a max would handle both negative zeros and NaNs\n        // incorrectly, but we can swap the operands to fix both.\n        std::swap(LHS, RHS);\n        LLVM_FALLTHROUGH;\n      case ISD::SETOLT:\n      case ISD::SETLT:\n      case ISD::SETLE:\n        Opcode = X86ISD::FMAX;\n        break;\n      }\n    }\n\n    if (Opcode)\n      return DAG.getNode(Opcode, DL, N->getValueType(0), LHS, RHS);\n  }\n\n  // Some mask scalar intrinsics rely on checking if only one bit is set\n  // and implement it in C code like this:\n  // A[0] = (U & 1) ? A[0] : W[0];\n  // This creates some redundant instructions that break pattern matching.\n  // fold (select (setcc (and (X, 1), 0, seteq), Y, Z)) -> select(and(X, 1),Z,Y)\n  if (Subtarget.hasAVX512() && N->getOpcode() == ISD::SELECT &&\n      Cond.getOpcode() == ISD::SETCC && (VT == MVT::f32 || VT == MVT::f64)) {\n    ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n    SDValue AndNode = Cond.getOperand(0);\n    if (AndNode.getOpcode() == ISD::AND && CC == ISD::SETEQ &&\n        isNullConstant(Cond.getOperand(1)) &&\n        isOneConstant(AndNode.getOperand(1))) {\n      // LHS and RHS swapped due to\n      // setcc outputting 1 when AND resulted in 0 and vice versa.\n      AndNode = DAG.getZExtOrTrunc(AndNode, DL, MVT::i8);\n      return DAG.getNode(ISD::SELECT, DL, VT, AndNode, RHS, LHS);\n    }\n  }\n\n  // v16i8 (select v16i1, v16i8, v16i8) does not have a proper\n  // lowering on KNL. In this case we convert it to\n  // v16i8 (select v16i8, v16i8, v16i8) and use AVX instruction.\n  // The same situation all vectors of i8 and i16 without BWI.\n  // Make sure we extend these even before type legalization gets a chance to\n  // split wide vectors.\n  // Since SKX these selects have a proper lowering.\n  if (Subtarget.hasAVX512() && !Subtarget.hasBWI() && CondVT.isVector() &&\n      CondVT.getVectorElementType() == MVT::i1 &&\n      (VT.getVectorElementType() == MVT::i8 ||\n       VT.getVectorElementType() == MVT::i16)) {\n    Cond = DAG.getNode(ISD::SIGN_EXTEND, DL, VT, Cond);\n    return DAG.getNode(N->getOpcode(), DL, VT, Cond, LHS, RHS);\n  }\n\n  // AVX512 - Extend select with zero to merge with target shuffle.\n  // select(mask, extract_subvector(shuffle(x)), zero) -->\n  // extract_subvector(select(insert_subvector(mask), shuffle(x), zero))\n  // TODO - support non target shuffles as well.\n  if (Subtarget.hasAVX512() && CondVT.isVector() &&\n      CondVT.getVectorElementType() == MVT::i1) {\n    auto SelectableOp = [&TLI](SDValue Op) {\n      return Op.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n             isTargetShuffle(Op.getOperand(0).getOpcode()) &&\n             isNullConstant(Op.getOperand(1)) &&\n             TLI.isTypeLegal(Op.getOperand(0).getValueType()) &&\n             Op.hasOneUse() && Op.getOperand(0).hasOneUse();\n    };\n\n    bool SelectableLHS = SelectableOp(LHS);\n    bool SelectableRHS = SelectableOp(RHS);\n    bool ZeroLHS = ISD::isBuildVectorAllZeros(LHS.getNode());\n    bool ZeroRHS = ISD::isBuildVectorAllZeros(RHS.getNode());\n\n    if ((SelectableLHS && ZeroRHS) || (SelectableRHS && ZeroLHS)) {\n      EVT SrcVT = SelectableLHS ? LHS.getOperand(0).getValueType()\n                                : RHS.getOperand(0).getValueType();\n      unsigned NumSrcElts = SrcVT.getVectorNumElements();\n      EVT SrcCondVT = EVT::getVectorVT(*DAG.getContext(), MVT::i1, NumSrcElts);\n      LHS = insertSubVector(DAG.getUNDEF(SrcVT), LHS, 0, DAG, DL,\n                            VT.getSizeInBits());\n      RHS = insertSubVector(DAG.getUNDEF(SrcVT), RHS, 0, DAG, DL,\n                            VT.getSizeInBits());\n      Cond = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, SrcCondVT,\n                         DAG.getUNDEF(SrcCondVT), Cond,\n                         DAG.getIntPtrConstant(0, DL));\n      SDValue Res = DAG.getSelect(DL, SrcVT, Cond, LHS, RHS);\n      return extractSubVector(Res, 0, DAG, DL, VT.getSizeInBits());\n    }\n  }\n\n  if (SDValue V = combineSelectOfTwoConstants(N, DAG))\n    return V;\n\n  // Canonicalize min/max:\n  // (x > 0) ? x : 0 -> (x >= 0) ? x : 0\n  // (x < -1) ? x : -1 -> (x <= -1) ? x : -1\n  // This allows use of COND_S / COND_NS (see TranslateX86CC) which eliminates\n  // the need for an extra compare against zero. e.g.\n  // (a - b) > 0 : (a - b) ? 0 -> (a - b) >= 0 : (a - b) ? 0\n  // subl   %esi, %edi\n  // testl  %edi, %edi\n  // movl   $0, %eax\n  // cmovgl %edi, %eax\n  // =>\n  // xorl   %eax, %eax\n  // subl   %esi, $edi\n  // cmovsl %eax, %edi\n  //\n  // We can also canonicalize\n  //  (x s> 1) ? x : 1 -> (x s>= 1) ? x : 1 -> (x s> 0) ? x : 1\n  //  (x u> 1) ? x : 1 -> (x u>= 1) ? x : 1 -> (x != 0) ? x : 1\n  // This allows the use of a test instruction for the compare.\n  if (N->getOpcode() == ISD::SELECT && Cond.getOpcode() == ISD::SETCC &&\n      Cond.hasOneUse() &&\n      LHS == Cond.getOperand(0) && RHS == Cond.getOperand(1)) {\n    ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n    if ((CC == ISD::SETGT && (isNullConstant(RHS) || isOneConstant(RHS))) ||\n        (CC == ISD::SETLT && isAllOnesConstant(RHS))) {\n      ISD::CondCode NewCC = CC == ISD::SETGT ? ISD::SETGE : ISD::SETLE;\n      Cond = DAG.getSetCC(SDLoc(Cond), Cond.getValueType(),\n                          Cond.getOperand(0), Cond.getOperand(1), NewCC);\n      return DAG.getSelect(DL, VT, Cond, LHS, RHS);\n    }\n    if (CC == ISD::SETUGT && isOneConstant(RHS)) {\n      ISD::CondCode NewCC = ISD::SETUGE;\n      Cond = DAG.getSetCC(SDLoc(Cond), Cond.getValueType(),\n                          Cond.getOperand(0), Cond.getOperand(1), NewCC);\n      return DAG.getSelect(DL, VT, Cond, LHS, RHS);\n    }\n  }\n\n  // Check if the first operand is all zeros and Cond type is vXi1.\n  // If this an avx512 target we can improve the use of zero masking by\n  // swapping the operands and inverting the condition.\n  if (N->getOpcode() == ISD::VSELECT && Cond.hasOneUse() &&\n       Subtarget.hasAVX512() && CondVT.getVectorElementType() == MVT::i1 &&\n      ISD::isBuildVectorAllZeros(LHS.getNode()) &&\n      !ISD::isBuildVectorAllZeros(RHS.getNode())) {\n    // Invert the cond to not(cond) : xor(op,allones)=not(op)\n    SDValue CondNew = DAG.getNOT(DL, Cond, CondVT);\n    // Vselect cond, op1, op2 = Vselect not(cond), op2, op1\n    return DAG.getSelect(DL, VT, CondNew, RHS, LHS);\n  }\n\n  // Early exit check\n  if (!TLI.isTypeLegal(VT))\n    return SDValue();\n\n  if (SDValue V = combineVSelectWithAllOnesOrZeros(N, DAG, DCI, Subtarget))\n    return V;\n\n  if (SDValue V = combineVSelectToBLENDV(N, DAG, DCI, Subtarget))\n    return V;\n\n  if (SDValue V = narrowVectorSelect(N, DAG, Subtarget))\n    return V;\n\n  // select(~Cond, X, Y) -> select(Cond, Y, X)\n  if (CondVT.getScalarType() != MVT::i1) {\n    if (SDValue CondNot = IsNOT(Cond, DAG))\n      return DAG.getNode(N->getOpcode(), DL, VT,\n                         DAG.getBitcast(CondVT, CondNot), RHS, LHS);\n    // pcmpgt(X, -1) -> pcmpgt(0, X) to help select/blendv just use the signbit.\n    if (Cond.getOpcode() == X86ISD::PCMPGT && Cond.hasOneUse() &&\n        ISD::isBuildVectorAllOnes(Cond.getOperand(1).getNode())) {\n      Cond = DAG.getNode(X86ISD::PCMPGT, DL, CondVT,\n                         DAG.getConstant(0, DL, CondVT), Cond.getOperand(0));\n      return DAG.getNode(N->getOpcode(), DL, VT, Cond, RHS, LHS);\n    }\n  }\n\n  // Try to optimize vXi1 selects if both operands are either all constants or\n  // bitcasts from scalar integer type. In that case we can convert the operands\n  // to integer and use an integer select which will be converted to a CMOV.\n  // We need to take a little bit of care to avoid creating an i64 type after\n  // type legalization.\n  if (N->getOpcode() == ISD::SELECT && VT.isVector() &&\n      VT.getVectorElementType() == MVT::i1 &&\n      (DCI.isBeforeLegalize() || (VT != MVT::v64i1 || Subtarget.is64Bit()))) {\n    EVT IntVT = EVT::getIntegerVT(*DAG.getContext(), VT.getVectorNumElements());\n    bool LHSIsConst = ISD::isBuildVectorOfConstantSDNodes(LHS.getNode());\n    bool RHSIsConst = ISD::isBuildVectorOfConstantSDNodes(RHS.getNode());\n\n    if ((LHSIsConst ||\n         (LHS.getOpcode() == ISD::BITCAST &&\n          LHS.getOperand(0).getValueType() == IntVT)) &&\n        (RHSIsConst ||\n         (RHS.getOpcode() == ISD::BITCAST &&\n          RHS.getOperand(0).getValueType() == IntVT))) {\n      if (LHSIsConst)\n        LHS = combinevXi1ConstantToInteger(LHS, DAG);\n      else\n        LHS = LHS.getOperand(0);\n\n      if (RHSIsConst)\n        RHS = combinevXi1ConstantToInteger(RHS, DAG);\n      else\n        RHS = RHS.getOperand(0);\n\n      SDValue Select = DAG.getSelect(DL, IntVT, Cond, LHS, RHS);\n      return DAG.getBitcast(VT, Select);\n    }\n  }\n\n  // If this is \"((X & C) == 0) ? Y : Z\" and C is a constant mask vector of\n  // single bits, then invert the predicate and swap the select operands.\n  // This can lower using a vector shift bit-hack rather than mask and compare.\n  if (DCI.isBeforeLegalize() && !Subtarget.hasAVX512() &&\n      N->getOpcode() == ISD::VSELECT && Cond.getOpcode() == ISD::SETCC &&\n      Cond.hasOneUse() && CondVT.getVectorElementType() == MVT::i1 &&\n      Cond.getOperand(0).getOpcode() == ISD::AND &&\n      isNullOrNullSplat(Cond.getOperand(1)) &&\n      cast<CondCodeSDNode>(Cond.getOperand(2))->get() == ISD::SETEQ &&\n      Cond.getOperand(0).getValueType() == VT) {\n    // The 'and' mask must be composed of power-of-2 constants.\n    SDValue And = Cond.getOperand(0);\n    auto *C = isConstOrConstSplat(And.getOperand(1));\n    if (C && C->getAPIntValue().isPowerOf2()) {\n      // vselect (X & C == 0), LHS, RHS --> vselect (X & C != 0), RHS, LHS\n      SDValue NotCond =\n          DAG.getSetCC(DL, CondVT, And, Cond.getOperand(1), ISD::SETNE);\n      return DAG.getSelect(DL, VT, NotCond, RHS, LHS);\n    }\n\n    // If we have a non-splat but still powers-of-2 mask, AVX1 can use pmulld\n    // and AVX2 can use vpsllv{dq}. 8-bit lacks a proper shift or multiply.\n    // 16-bit lacks a proper blendv.\n    unsigned EltBitWidth = VT.getScalarSizeInBits();\n    bool CanShiftBlend =\n        TLI.isTypeLegal(VT) && ((Subtarget.hasAVX() && EltBitWidth == 32) ||\n                                (Subtarget.hasAVX2() && EltBitWidth == 64) ||\n                                (Subtarget.hasXOP()));\n    if (CanShiftBlend &&\n        ISD::matchUnaryPredicate(And.getOperand(1), [](ConstantSDNode *C) {\n          return C->getAPIntValue().isPowerOf2();\n        })) {\n      // Create a left-shift constant to get the mask bits over to the sign-bit.\n      SDValue Mask = And.getOperand(1);\n      SmallVector<int, 32> ShlVals;\n      for (unsigned i = 0, e = VT.getVectorNumElements(); i != e; ++i) {\n        auto *MaskVal = cast<ConstantSDNode>(Mask.getOperand(i));\n        ShlVals.push_back(EltBitWidth - 1 -\n                          MaskVal->getAPIntValue().exactLogBase2());\n      }\n      // vsel ((X & C) == 0), LHS, RHS --> vsel ((shl X, C') < 0), RHS, LHS\n      SDValue ShlAmt = getConstVector(ShlVals, VT.getSimpleVT(), DAG, DL);\n      SDValue Shl = DAG.getNode(ISD::SHL, DL, VT, And.getOperand(0), ShlAmt);\n      SDValue NewCond =\n          DAG.getSetCC(DL, CondVT, Shl, Cond.getOperand(1), ISD::SETLT);\n      return DAG.getSelect(DL, VT, NewCond, RHS, LHS);\n    }\n  }\n\n  return SDValue();\n}\n\n/// Combine:\n///   (brcond/cmov/setcc .., (cmp (atomic_load_add x, 1), 0), COND_S)\n/// to:\n///   (brcond/cmov/setcc .., (LADD x, 1), COND_LE)\n/// i.e., reusing the EFLAGS produced by the LOCKed instruction.\n/// Note that this is only legal for some op/cc combinations.\nstatic SDValue combineSetCCAtomicArith(SDValue Cmp, X86::CondCode &CC,\n                                       SelectionDAG &DAG,\n                                       const X86Subtarget &Subtarget) {\n  // This combine only operates on CMP-like nodes.\n  if (!(Cmp.getOpcode() == X86ISD::CMP ||\n        (Cmp.getOpcode() == X86ISD::SUB && !Cmp->hasAnyUseOfValue(0))))\n    return SDValue();\n\n  // Can't replace the cmp if it has more uses than the one we're looking at.\n  // FIXME: We would like to be able to handle this, but would need to make sure\n  // all uses were updated.\n  if (!Cmp.hasOneUse())\n    return SDValue();\n\n  // This only applies to variations of the common case:\n  //   (icmp slt x, 0) -> (icmp sle (add x, 1), 0)\n  //   (icmp sge x, 0) -> (icmp sgt (add x, 1), 0)\n  //   (icmp sle x, 0) -> (icmp slt (sub x, 1), 0)\n  //   (icmp sgt x, 0) -> (icmp sge (sub x, 1), 0)\n  // Using the proper condcodes (see below), overflow is checked for.\n\n  // FIXME: We can generalize both constraints:\n  // - XOR/OR/AND (if they were made to survive AtomicExpand)\n  // - LHS != 1\n  // if the result is compared.\n\n  SDValue CmpLHS = Cmp.getOperand(0);\n  SDValue CmpRHS = Cmp.getOperand(1);\n\n  if (!CmpLHS.hasOneUse())\n    return SDValue();\n\n  unsigned Opc = CmpLHS.getOpcode();\n  if (Opc != ISD::ATOMIC_LOAD_ADD && Opc != ISD::ATOMIC_LOAD_SUB)\n    return SDValue();\n\n  SDValue OpRHS = CmpLHS.getOperand(2);\n  auto *OpRHSC = dyn_cast<ConstantSDNode>(OpRHS);\n  if (!OpRHSC)\n    return SDValue();\n\n  APInt Addend = OpRHSC->getAPIntValue();\n  if (Opc == ISD::ATOMIC_LOAD_SUB)\n    Addend = -Addend;\n\n  auto *CmpRHSC = dyn_cast<ConstantSDNode>(CmpRHS);\n  if (!CmpRHSC)\n    return SDValue();\n\n  APInt Comparison = CmpRHSC->getAPIntValue();\n\n  // If the addend is the negation of the comparison value, then we can do\n  // a full comparison by emitting the atomic arithmetic as a locked sub.\n  if (Comparison == -Addend) {\n    // The CC is fine, but we need to rewrite the LHS of the comparison as an\n    // atomic sub.\n    auto *AN = cast<AtomicSDNode>(CmpLHS.getNode());\n    auto AtomicSub = DAG.getAtomic(\n        ISD::ATOMIC_LOAD_SUB, SDLoc(CmpLHS), CmpLHS.getValueType(),\n        /*Chain*/ CmpLHS.getOperand(0), /*LHS*/ CmpLHS.getOperand(1),\n        /*RHS*/ DAG.getConstant(-Addend, SDLoc(CmpRHS), CmpRHS.getValueType()),\n        AN->getMemOperand());\n    auto LockOp = lowerAtomicArithWithLOCK(AtomicSub, DAG, Subtarget);\n    DAG.ReplaceAllUsesOfValueWith(CmpLHS.getValue(0),\n                                  DAG.getUNDEF(CmpLHS.getValueType()));\n    DAG.ReplaceAllUsesOfValueWith(CmpLHS.getValue(1), LockOp.getValue(1));\n    return LockOp;\n  }\n\n  // We can handle comparisons with zero in a number of cases by manipulating\n  // the CC used.\n  if (!Comparison.isNullValue())\n    return SDValue();\n\n  if (CC == X86::COND_S && Addend == 1)\n    CC = X86::COND_LE;\n  else if (CC == X86::COND_NS && Addend == 1)\n    CC = X86::COND_G;\n  else if (CC == X86::COND_G && Addend == -1)\n    CC = X86::COND_GE;\n  else if (CC == X86::COND_LE && Addend == -1)\n    CC = X86::COND_L;\n  else\n    return SDValue();\n\n  SDValue LockOp = lowerAtomicArithWithLOCK(CmpLHS, DAG, Subtarget);\n  DAG.ReplaceAllUsesOfValueWith(CmpLHS.getValue(0),\n                                DAG.getUNDEF(CmpLHS.getValueType()));\n  DAG.ReplaceAllUsesOfValueWith(CmpLHS.getValue(1), LockOp.getValue(1));\n  return LockOp;\n}\n\n// Check whether a boolean test is testing a boolean value generated by\n// X86ISD::SETCC. If so, return the operand of that SETCC and proper condition\n// code.\n//\n// Simplify the following patterns:\n// (Op (CMP (SETCC Cond EFLAGS) 1) EQ) or\n// (Op (CMP (SETCC Cond EFLAGS) 0) NEQ)\n// to (Op EFLAGS Cond)\n//\n// (Op (CMP (SETCC Cond EFLAGS) 0) EQ) or\n// (Op (CMP (SETCC Cond EFLAGS) 1) NEQ)\n// to (Op EFLAGS !Cond)\n//\n// where Op could be BRCOND or CMOV.\n//\nstatic SDValue checkBoolTestSetCCCombine(SDValue Cmp, X86::CondCode &CC) {\n  // This combine only operates on CMP-like nodes.\n  if (!(Cmp.getOpcode() == X86ISD::CMP ||\n        (Cmp.getOpcode() == X86ISD::SUB && !Cmp->hasAnyUseOfValue(0))))\n    return SDValue();\n\n  // Quit if not used as a boolean value.\n  if (CC != X86::COND_E && CC != X86::COND_NE)\n    return SDValue();\n\n  // Check CMP operands. One of them should be 0 or 1 and the other should be\n  // an SetCC or extended from it.\n  SDValue Op1 = Cmp.getOperand(0);\n  SDValue Op2 = Cmp.getOperand(1);\n\n  SDValue SetCC;\n  const ConstantSDNode* C = nullptr;\n  bool needOppositeCond = (CC == X86::COND_E);\n  bool checkAgainstTrue = false; // Is it a comparison against 1?\n\n  if ((C = dyn_cast<ConstantSDNode>(Op1)))\n    SetCC = Op2;\n  else if ((C = dyn_cast<ConstantSDNode>(Op2)))\n    SetCC = Op1;\n  else // Quit if all operands are not constants.\n    return SDValue();\n\n  if (C->getZExtValue() == 1) {\n    needOppositeCond = !needOppositeCond;\n    checkAgainstTrue = true;\n  } else if (C->getZExtValue() != 0)\n    // Quit if the constant is neither 0 or 1.\n    return SDValue();\n\n  bool truncatedToBoolWithAnd = false;\n  // Skip (zext $x), (trunc $x), or (and $x, 1) node.\n  while (SetCC.getOpcode() == ISD::ZERO_EXTEND ||\n         SetCC.getOpcode() == ISD::TRUNCATE ||\n         SetCC.getOpcode() == ISD::AND) {\n    if (SetCC.getOpcode() == ISD::AND) {\n      int OpIdx = -1;\n      if (isOneConstant(SetCC.getOperand(0)))\n        OpIdx = 1;\n      if (isOneConstant(SetCC.getOperand(1)))\n        OpIdx = 0;\n      if (OpIdx < 0)\n        break;\n      SetCC = SetCC.getOperand(OpIdx);\n      truncatedToBoolWithAnd = true;\n    } else\n      SetCC = SetCC.getOperand(0);\n  }\n\n  switch (SetCC.getOpcode()) {\n  case X86ISD::SETCC_CARRY:\n    // Since SETCC_CARRY gives output based on R = CF ? ~0 : 0, it's unsafe to\n    // simplify it if the result of SETCC_CARRY is not canonicalized to 0 or 1,\n    // i.e. it's a comparison against true but the result of SETCC_CARRY is not\n    // truncated to i1 using 'and'.\n    if (checkAgainstTrue && !truncatedToBoolWithAnd)\n      break;\n    assert(X86::CondCode(SetCC.getConstantOperandVal(0)) == X86::COND_B &&\n           \"Invalid use of SETCC_CARRY!\");\n    LLVM_FALLTHROUGH;\n  case X86ISD::SETCC:\n    // Set the condition code or opposite one if necessary.\n    CC = X86::CondCode(SetCC.getConstantOperandVal(0));\n    if (needOppositeCond)\n      CC = X86::GetOppositeBranchCondition(CC);\n    return SetCC.getOperand(1);\n  case X86ISD::CMOV: {\n    // Check whether false/true value has canonical one, i.e. 0 or 1.\n    ConstantSDNode *FVal = dyn_cast<ConstantSDNode>(SetCC.getOperand(0));\n    ConstantSDNode *TVal = dyn_cast<ConstantSDNode>(SetCC.getOperand(1));\n    // Quit if true value is not a constant.\n    if (!TVal)\n      return SDValue();\n    // Quit if false value is not a constant.\n    if (!FVal) {\n      SDValue Op = SetCC.getOperand(0);\n      // Skip 'zext' or 'trunc' node.\n      if (Op.getOpcode() == ISD::ZERO_EXTEND ||\n          Op.getOpcode() == ISD::TRUNCATE)\n        Op = Op.getOperand(0);\n      // A special case for rdrand/rdseed, where 0 is set if false cond is\n      // found.\n      if ((Op.getOpcode() != X86ISD::RDRAND &&\n           Op.getOpcode() != X86ISD::RDSEED) || Op.getResNo() != 0)\n        return SDValue();\n    }\n    // Quit if false value is not the constant 0 or 1.\n    bool FValIsFalse = true;\n    if (FVal && FVal->getZExtValue() != 0) {\n      if (FVal->getZExtValue() != 1)\n        return SDValue();\n      // If FVal is 1, opposite cond is needed.\n      needOppositeCond = !needOppositeCond;\n      FValIsFalse = false;\n    }\n    // Quit if TVal is not the constant opposite of FVal.\n    if (FValIsFalse && TVal->getZExtValue() != 1)\n      return SDValue();\n    if (!FValIsFalse && TVal->getZExtValue() != 0)\n      return SDValue();\n    CC = X86::CondCode(SetCC.getConstantOperandVal(2));\n    if (needOppositeCond)\n      CC = X86::GetOppositeBranchCondition(CC);\n    return SetCC.getOperand(3);\n  }\n  }\n\n  return SDValue();\n}\n\n/// Check whether Cond is an AND/OR of SETCCs off of the same EFLAGS.\n/// Match:\n///   (X86or (X86setcc) (X86setcc))\n///   (X86cmp (and (X86setcc) (X86setcc)), 0)\nstatic bool checkBoolTestAndOrSetCCCombine(SDValue Cond, X86::CondCode &CC0,\n                                           X86::CondCode &CC1, SDValue &Flags,\n                                           bool &isAnd) {\n  if (Cond->getOpcode() == X86ISD::CMP) {\n    if (!isNullConstant(Cond->getOperand(1)))\n      return false;\n\n    Cond = Cond->getOperand(0);\n  }\n\n  isAnd = false;\n\n  SDValue SetCC0, SetCC1;\n  switch (Cond->getOpcode()) {\n  default: return false;\n  case ISD::AND:\n  case X86ISD::AND:\n    isAnd = true;\n    LLVM_FALLTHROUGH;\n  case ISD::OR:\n  case X86ISD::OR:\n    SetCC0 = Cond->getOperand(0);\n    SetCC1 = Cond->getOperand(1);\n    break;\n  };\n\n  // Make sure we have SETCC nodes, using the same flags value.\n  if (SetCC0.getOpcode() != X86ISD::SETCC ||\n      SetCC1.getOpcode() != X86ISD::SETCC ||\n      SetCC0->getOperand(1) != SetCC1->getOperand(1))\n    return false;\n\n  CC0 = (X86::CondCode)SetCC0->getConstantOperandVal(0);\n  CC1 = (X86::CondCode)SetCC1->getConstantOperandVal(0);\n  Flags = SetCC0->getOperand(1);\n  return true;\n}\n\n// When legalizing carry, we create carries via add X, -1\n// If that comes from an actual carry, via setcc, we use the\n// carry directly.\nstatic SDValue combineCarryThroughADD(SDValue EFLAGS, SelectionDAG &DAG) {\n  if (EFLAGS.getOpcode() == X86ISD::ADD) {\n    if (isAllOnesConstant(EFLAGS.getOperand(1))) {\n      SDValue Carry = EFLAGS.getOperand(0);\n      while (Carry.getOpcode() == ISD::TRUNCATE ||\n             Carry.getOpcode() == ISD::ZERO_EXTEND ||\n             Carry.getOpcode() == ISD::SIGN_EXTEND ||\n             Carry.getOpcode() == ISD::ANY_EXTEND ||\n             (Carry.getOpcode() == ISD::AND &&\n              isOneConstant(Carry.getOperand(1))))\n        Carry = Carry.getOperand(0);\n      if (Carry.getOpcode() == X86ISD::SETCC ||\n          Carry.getOpcode() == X86ISD::SETCC_CARRY) {\n        // TODO: Merge this code with equivalent in combineAddOrSubToADCOrSBB?\n        uint64_t CarryCC = Carry.getConstantOperandVal(0);\n        SDValue CarryOp1 = Carry.getOperand(1);\n        if (CarryCC == X86::COND_B)\n          return CarryOp1;\n        if (CarryCC == X86::COND_A) {\n          // Try to convert COND_A into COND_B in an attempt to facilitate\n          // materializing \"setb reg\".\n          //\n          // Do not flip \"e > c\", where \"c\" is a constant, because Cmp\n          // instruction cannot take an immediate as its first operand.\n          //\n          if (CarryOp1.getOpcode() == X86ISD::SUB &&\n              CarryOp1.getNode()->hasOneUse() &&\n              CarryOp1.getValueType().isInteger() &&\n              !isa<ConstantSDNode>(CarryOp1.getOperand(1))) {\n            SDValue SubCommute =\n                DAG.getNode(X86ISD::SUB, SDLoc(CarryOp1), CarryOp1->getVTList(),\n                            CarryOp1.getOperand(1), CarryOp1.getOperand(0));\n            return SDValue(SubCommute.getNode(), CarryOp1.getResNo());\n          }\n        }\n        // If this is a check of the z flag of an add with 1, switch to the\n        // C flag.\n        if (CarryCC == X86::COND_E &&\n            CarryOp1.getOpcode() == X86ISD::ADD &&\n            isOneConstant(CarryOp1.getOperand(1)))\n          return CarryOp1;\n      }\n    }\n  }\n\n  return SDValue();\n}\n\n/// If we are inverting an PTEST/TESTP operand, attempt to adjust the CC\n/// to avoid the inversion.\nstatic SDValue combinePTESTCC(SDValue EFLAGS, X86::CondCode &CC,\n                              SelectionDAG &DAG,\n                              const X86Subtarget &Subtarget) {\n  // TODO: Handle X86ISD::KTEST/X86ISD::KORTEST.\n  if (EFLAGS.getOpcode() != X86ISD::PTEST &&\n      EFLAGS.getOpcode() != X86ISD::TESTP)\n    return SDValue();\n\n  // PTEST/TESTP sets EFLAGS as:\n  // TESTZ: ZF = (Op0 & Op1) == 0\n  // TESTC: CF = (~Op0 & Op1) == 0\n  // TESTNZC: ZF == 0 && CF == 0\n  EVT VT = EFLAGS.getValueType();\n  SDValue Op0 = EFLAGS.getOperand(0);\n  SDValue Op1 = EFLAGS.getOperand(1);\n  EVT OpVT = Op0.getValueType();\n\n  // TEST*(~X,Y) == TEST*(X,Y)\n  if (SDValue NotOp0 = IsNOT(Op0, DAG)) {\n    X86::CondCode InvCC;\n    switch (CC) {\n    case X86::COND_B:\n      // testc -> testz.\n      InvCC = X86::COND_E;\n      break;\n    case X86::COND_AE:\n      // !testc -> !testz.\n      InvCC = X86::COND_NE;\n      break;\n    case X86::COND_E:\n      // testz -> testc.\n      InvCC = X86::COND_B;\n      break;\n    case X86::COND_NE:\n      // !testz -> !testc.\n      InvCC = X86::COND_AE;\n      break;\n    case X86::COND_A:\n    case X86::COND_BE:\n      // testnzc -> testnzc (no change).\n      InvCC = CC;\n      break;\n    default:\n      InvCC = X86::COND_INVALID;\n      break;\n    }\n\n    if (InvCC != X86::COND_INVALID) {\n      CC = InvCC;\n      return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT,\n                         DAG.getBitcast(OpVT, NotOp0), Op1);\n    }\n  }\n\n  if (CC == X86::COND_E || CC == X86::COND_NE) {\n    // TESTZ(X,~Y) == TESTC(Y,X)\n    if (SDValue NotOp1 = IsNOT(Op1, DAG)) {\n      CC = (CC == X86::COND_E ? X86::COND_B : X86::COND_AE);\n      return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT,\n                         DAG.getBitcast(OpVT, NotOp1), Op0);\n    }\n\n    if (Op0 == Op1) {\n      SDValue BC = peekThroughBitcasts(Op0);\n      EVT BCVT = BC.getValueType();\n      assert(BCVT.isVector() && DAG.getTargetLoweringInfo().isTypeLegal(BCVT) &&\n             \"Unexpected vector type\");\n\n      // TESTZ(AND(X,Y),AND(X,Y)) == TESTZ(X,Y)\n      if (BC.getOpcode() == ISD::AND || BC.getOpcode() == X86ISD::FAND) {\n        return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT,\n                           DAG.getBitcast(OpVT, BC.getOperand(0)),\n                           DAG.getBitcast(OpVT, BC.getOperand(1)));\n      }\n\n      // TESTZ(AND(~X,Y),AND(~X,Y)) == TESTC(X,Y)\n      if (BC.getOpcode() == X86ISD::ANDNP || BC.getOpcode() == X86ISD::FANDN) {\n        CC = (CC == X86::COND_E ? X86::COND_B : X86::COND_AE);\n        return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT,\n                           DAG.getBitcast(OpVT, BC.getOperand(0)),\n                           DAG.getBitcast(OpVT, BC.getOperand(1)));\n      }\n\n      // If every element is an all-sign value, see if we can use MOVMSK to\n      // more efficiently extract the sign bits and compare that.\n      // TODO: Handle TESTC with comparison inversion.\n      // TODO: Can we remove SimplifyMultipleUseDemandedBits and rely on\n      // MOVMSK combines to make sure its never worse than PTEST?\n      unsigned EltBits = BCVT.getScalarSizeInBits();\n      if (DAG.ComputeNumSignBits(BC) == EltBits) {\n        assert(VT == MVT::i32 && \"Expected i32 EFLAGS comparison result\");\n        APInt SignMask = APInt::getSignMask(EltBits);\n        const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n        if (SDValue Res =\n                TLI.SimplifyMultipleUseDemandedBits(BC, SignMask, DAG)) {\n          // For vXi16 cases we need to use pmovmksb and extract every other\n          // sign bit.\n          SDLoc DL(EFLAGS);\n          if (EltBits == 16) {\n            MVT MovmskVT = BCVT.is128BitVector() ? MVT::v16i8 : MVT::v32i8;\n            Res = DAG.getBitcast(MovmskVT, Res);\n            Res = getPMOVMSKB(DL, Res, DAG, Subtarget);\n            Res = DAG.getNode(ISD::AND, DL, MVT::i32, Res,\n                              DAG.getConstant(0xAAAAAAAA, DL, MVT::i32));\n          } else {\n            Res = getPMOVMSKB(DL, Res, DAG, Subtarget);\n          }\n          return DAG.getNode(X86ISD::CMP, DL, MVT::i32, Res,\n                             DAG.getConstant(0, DL, MVT::i32));\n        }\n      }\n    }\n\n    // TESTZ(-1,X) == TESTZ(X,X)\n    if (ISD::isBuildVectorAllOnes(Op0.getNode()))\n      return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT, Op1, Op1);\n\n    // TESTZ(X,-1) == TESTZ(X,X)\n    if (ISD::isBuildVectorAllOnes(Op1.getNode()))\n      return DAG.getNode(EFLAGS.getOpcode(), SDLoc(EFLAGS), VT, Op0, Op0);\n  }\n\n  return SDValue();\n}\n\n// Attempt to simplify the MOVMSK input based on the comparison type.\nstatic SDValue combineSetCCMOVMSK(SDValue EFLAGS, X86::CondCode &CC,\n                                  SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  // Handle eq/ne against zero (any_of).\n  // Handle eq/ne against -1 (all_of).\n  if (!(CC == X86::COND_E || CC == X86::COND_NE))\n    return SDValue();\n  if (EFLAGS.getValueType() != MVT::i32)\n    return SDValue();\n  unsigned CmpOpcode = EFLAGS.getOpcode();\n  if (CmpOpcode != X86ISD::CMP && CmpOpcode != X86ISD::SUB)\n    return SDValue();\n  auto *CmpConstant = dyn_cast<ConstantSDNode>(EFLAGS.getOperand(1));\n  if (!CmpConstant)\n    return SDValue();\n  const APInt &CmpVal = CmpConstant->getAPIntValue();\n\n  SDValue CmpOp = EFLAGS.getOperand(0);\n  unsigned CmpBits = CmpOp.getValueSizeInBits();\n  assert(CmpBits == CmpVal.getBitWidth() && \"Value size mismatch\");\n\n  // Peek through any truncate.\n  if (CmpOp.getOpcode() == ISD::TRUNCATE)\n    CmpOp = CmpOp.getOperand(0);\n\n  // Bail if we don't find a MOVMSK.\n  if (CmpOp.getOpcode() != X86ISD::MOVMSK)\n    return SDValue();\n\n  SDValue Vec = CmpOp.getOperand(0);\n  MVT VecVT = Vec.getSimpleValueType();\n  assert((VecVT.is128BitVector() || VecVT.is256BitVector()) &&\n         \"Unexpected MOVMSK operand\");\n  unsigned NumElts = VecVT.getVectorNumElements();\n  unsigned NumEltBits = VecVT.getScalarSizeInBits();\n\n  bool IsAnyOf = CmpOpcode == X86ISD::CMP && CmpVal.isNullValue();\n  bool IsAllOf = CmpOpcode == X86ISD::SUB && NumElts <= CmpBits &&\n                 CmpVal.isMask(NumElts);\n  if (!IsAnyOf && !IsAllOf)\n    return SDValue();\n\n  // See if we can peek through to a vector with a wider element type, if the\n  // signbits extend down to all the sub-elements as well.\n  // Calling MOVMSK with the wider type, avoiding the bitcast, helps expose\n  // potential SimplifyDemandedBits/Elts cases.\n  if (Vec.getOpcode() == ISD::BITCAST) {\n    SDValue BC = peekThroughBitcasts(Vec);\n    MVT BCVT = BC.getSimpleValueType();\n    unsigned BCNumElts = BCVT.getVectorNumElements();\n    unsigned BCNumEltBits = BCVT.getScalarSizeInBits();\n    if ((BCNumEltBits == 32 || BCNumEltBits == 64) &&\n        BCNumEltBits > NumEltBits &&\n        DAG.ComputeNumSignBits(BC) > (BCNumEltBits - NumEltBits)) {\n      SDLoc DL(EFLAGS);\n      unsigned CmpMask = IsAnyOf ? 0 : ((1 << BCNumElts) - 1);\n      return DAG.getNode(X86ISD::CMP, DL, MVT::i32,\n                         DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, BC),\n                         DAG.getConstant(CmpMask, DL, MVT::i32));\n    }\n  }\n\n  // MOVMSK(PCMPEQ(X,0)) == -1 -> PTESTZ(X,X).\n  // MOVMSK(PCMPEQ(X,0)) != -1 -> !PTESTZ(X,X).\n  if (IsAllOf && Subtarget.hasSSE41()) {\n    SDValue BC = peekThroughBitcasts(Vec);\n    if (BC.getOpcode() == X86ISD::PCMPEQ &&\n        ISD::isBuildVectorAllZeros(BC.getOperand(1).getNode())) {\n      MVT TestVT = VecVT.is128BitVector() ? MVT::v2i64 : MVT::v4i64;\n      SDValue V = DAG.getBitcast(TestVT, BC.getOperand(0));\n      return DAG.getNode(X86ISD::PTEST, SDLoc(EFLAGS), MVT::i32, V, V);\n    }\n  }\n\n  // See if we can avoid a PACKSS by calling MOVMSK on the sources.\n  // For vXi16 cases we can use a v2Xi8 PMOVMSKB. We must mask out\n  // sign bits prior to the comparison with zero unless we know that\n  // the vXi16 splats the sign bit down to the lower i8 half.\n  // TODO: Handle all_of patterns.\n  if (Vec.getOpcode() == X86ISD::PACKSS && VecVT == MVT::v16i8) {\n    SDValue VecOp0 = Vec.getOperand(0);\n    SDValue VecOp1 = Vec.getOperand(1);\n    bool SignExt0 = DAG.ComputeNumSignBits(VecOp0) > 8;\n    bool SignExt1 = DAG.ComputeNumSignBits(VecOp1) > 8;\n    // PMOVMSKB(PACKSSBW(X, undef)) -> PMOVMSKB(BITCAST_v16i8(X)) & 0xAAAA.\n    if (IsAnyOf && CmpBits == 8 && VecOp1.isUndef()) {\n      SDLoc DL(EFLAGS);\n      SDValue Result = DAG.getBitcast(MVT::v16i8, VecOp0);\n      Result = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Result);\n      Result = DAG.getZExtOrTrunc(Result, DL, MVT::i16);\n      if (!SignExt0) {\n        Result = DAG.getNode(ISD::AND, DL, MVT::i16, Result,\n                             DAG.getConstant(0xAAAA, DL, MVT::i16));\n      }\n      return DAG.getNode(X86ISD::CMP, DL, MVT::i32, Result,\n                         DAG.getConstant(0, DL, MVT::i16));\n    }\n    // PMOVMSKB(PACKSSBW(LO(X), HI(X)))\n    // -> PMOVMSKB(BITCAST_v32i8(X)) & 0xAAAAAAAA.\n    if (CmpBits == 16 && Subtarget.hasInt256() &&\n        VecOp0.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n        VecOp1.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n        VecOp0.getOperand(0) == VecOp1.getOperand(0) &&\n        VecOp0.getConstantOperandAPInt(1) == 0 &&\n        VecOp1.getConstantOperandAPInt(1) == 8 &&\n        (IsAnyOf || (SignExt0 && SignExt1))) {\n      SDLoc DL(EFLAGS);\n      SDValue Result = DAG.getBitcast(MVT::v32i8, VecOp0.getOperand(0));\n      Result = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Result);\n      unsigned CmpMask = IsAnyOf ? 0 : 0xFFFFFFFF;\n      if (!SignExt0 || !SignExt1) {\n        assert(IsAnyOf && \"Only perform v16i16 signmasks for any_of patterns\");\n        Result = DAG.getNode(ISD::AND, DL, MVT::i32, Result,\n                             DAG.getConstant(0xAAAAAAAA, DL, MVT::i32));\n      }\n      return DAG.getNode(X86ISD::CMP, DL, MVT::i32, Result,\n                         DAG.getConstant(CmpMask, DL, MVT::i32));\n    }\n  }\n\n  // MOVMSK(SHUFFLE(X,u)) -> MOVMSK(X) iff every element is referenced.\n  SmallVector<int, 32> ShuffleMask;\n  SmallVector<SDValue, 2> ShuffleInputs;\n  if (NumElts == CmpBits &&\n      getTargetShuffleInputs(peekThroughBitcasts(Vec), ShuffleInputs,\n                             ShuffleMask, DAG) &&\n      ShuffleInputs.size() == 1 && !isAnyZeroOrUndef(ShuffleMask) &&\n      ShuffleInputs[0].getValueSizeInBits() == VecVT.getSizeInBits()) {\n    unsigned NumShuffleElts = ShuffleMask.size();\n    APInt DemandedElts = APInt::getNullValue(NumShuffleElts);\n    for (int M : ShuffleMask) {\n      assert(0 <= M && M < (int)NumShuffleElts && \"Bad unary shuffle index\");\n      DemandedElts.setBit(M);\n    }\n    if (DemandedElts.isAllOnesValue()) {\n      SDLoc DL(EFLAGS);\n      SDValue Result = DAG.getBitcast(VecVT, ShuffleInputs[0]);\n      Result = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Result);\n      Result =\n          DAG.getZExtOrTrunc(Result, DL, EFLAGS.getOperand(0).getValueType());\n      return DAG.getNode(X86ISD::CMP, DL, MVT::i32, Result,\n                         EFLAGS.getOperand(1));\n    }\n  }\n\n  return SDValue();\n}\n\n/// Optimize an EFLAGS definition used according to the condition code \\p CC\n/// into a simpler EFLAGS value, potentially returning a new \\p CC and replacing\n/// uses of chain values.\nstatic SDValue combineSetCCEFLAGS(SDValue EFLAGS, X86::CondCode &CC,\n                                  SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  if (CC == X86::COND_B)\n    if (SDValue Flags = combineCarryThroughADD(EFLAGS, DAG))\n      return Flags;\n\n  if (SDValue R = checkBoolTestSetCCCombine(EFLAGS, CC))\n    return R;\n\n  if (SDValue R = combinePTESTCC(EFLAGS, CC, DAG, Subtarget))\n    return R;\n\n  if (SDValue R = combineSetCCMOVMSK(EFLAGS, CC, DAG, Subtarget))\n    return R;\n\n  return combineSetCCAtomicArith(EFLAGS, CC, DAG, Subtarget);\n}\n\n/// Optimize X86ISD::CMOV [LHS, RHS, CONDCODE (e.g. X86::COND_NE), CONDVAL]\nstatic SDValue combineCMov(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI,\n                           const X86Subtarget &Subtarget) {\n  SDLoc DL(N);\n\n  SDValue FalseOp = N->getOperand(0);\n  SDValue TrueOp = N->getOperand(1);\n  X86::CondCode CC = (X86::CondCode)N->getConstantOperandVal(2);\n  SDValue Cond = N->getOperand(3);\n\n  // cmov X, X, ?, ? --> X\n  if (TrueOp == FalseOp)\n    return TrueOp;\n\n  // Try to simplify the EFLAGS and condition code operands.\n  // We can't always do this as FCMOV only supports a subset of X86 cond.\n  if (SDValue Flags = combineSetCCEFLAGS(Cond, CC, DAG, Subtarget)) {\n    if (!(FalseOp.getValueType() == MVT::f80 ||\n          (FalseOp.getValueType() == MVT::f64 && !Subtarget.hasSSE2()) ||\n          (FalseOp.getValueType() == MVT::f32 && !Subtarget.hasSSE1())) ||\n        !Subtarget.hasCMov() || hasFPCMov(CC)) {\n      SDValue Ops[] = {FalseOp, TrueOp, DAG.getTargetConstant(CC, DL, MVT::i8),\n                       Flags};\n      return DAG.getNode(X86ISD::CMOV, DL, N->getValueType(0), Ops);\n    }\n  }\n\n  // If this is a select between two integer constants, try to do some\n  // optimizations.  Note that the operands are ordered the opposite of SELECT\n  // operands.\n  if (ConstantSDNode *TrueC = dyn_cast<ConstantSDNode>(TrueOp)) {\n    if (ConstantSDNode *FalseC = dyn_cast<ConstantSDNode>(FalseOp)) {\n      // Canonicalize the TrueC/FalseC values so that TrueC (the true value) is\n      // larger than FalseC (the false value).\n      if (TrueC->getAPIntValue().ult(FalseC->getAPIntValue())) {\n        CC = X86::GetOppositeBranchCondition(CC);\n        std::swap(TrueC, FalseC);\n        std::swap(TrueOp, FalseOp);\n      }\n\n      // Optimize C ? 8 : 0 -> zext(setcc(C)) << 3.  Likewise for any pow2/0.\n      // This is efficient for any integer data type (including i8/i16) and\n      // shift amount.\n      if (FalseC->getAPIntValue() == 0 && TrueC->getAPIntValue().isPowerOf2()) {\n        Cond = getSETCC(CC, Cond, DL, DAG);\n\n        // Zero extend the condition if needed.\n        Cond = DAG.getNode(ISD::ZERO_EXTEND, DL, TrueC->getValueType(0), Cond);\n\n        unsigned ShAmt = TrueC->getAPIntValue().logBase2();\n        Cond = DAG.getNode(ISD::SHL, DL, Cond.getValueType(), Cond,\n                           DAG.getConstant(ShAmt, DL, MVT::i8));\n        return Cond;\n      }\n\n      // Optimize Cond ? cst+1 : cst -> zext(setcc(C)+cst.  This is efficient\n      // for any integer data type, including i8/i16.\n      if (FalseC->getAPIntValue()+1 == TrueC->getAPIntValue()) {\n        Cond = getSETCC(CC, Cond, DL, DAG);\n\n        // Zero extend the condition if needed.\n        Cond = DAG.getNode(ISD::ZERO_EXTEND, DL,\n                           FalseC->getValueType(0), Cond);\n        Cond = DAG.getNode(ISD::ADD, DL, Cond.getValueType(), Cond,\n                           SDValue(FalseC, 0));\n        return Cond;\n      }\n\n      // Optimize cases that will turn into an LEA instruction.  This requires\n      // an i32 or i64 and an efficient multiplier (1, 2, 3, 4, 5, 8, 9).\n      if (N->getValueType(0) == MVT::i32 || N->getValueType(0) == MVT::i64) {\n        APInt Diff = TrueC->getAPIntValue() - FalseC->getAPIntValue();\n        assert(Diff.getBitWidth() == N->getValueType(0).getSizeInBits() &&\n               \"Implicit constant truncation\");\n\n        bool isFastMultiplier = false;\n        if (Diff.ult(10)) {\n          switch (Diff.getZExtValue()) {\n          default: break;\n          case 1:  // result = add base, cond\n          case 2:  // result = lea base(    , cond*2)\n          case 3:  // result = lea base(cond, cond*2)\n          case 4:  // result = lea base(    , cond*4)\n          case 5:  // result = lea base(cond, cond*4)\n          case 8:  // result = lea base(    , cond*8)\n          case 9:  // result = lea base(cond, cond*8)\n            isFastMultiplier = true;\n            break;\n          }\n        }\n\n        if (isFastMultiplier) {\n          Cond = getSETCC(CC, Cond, DL ,DAG);\n          // Zero extend the condition if needed.\n          Cond = DAG.getNode(ISD::ZERO_EXTEND, DL, FalseC->getValueType(0),\n                             Cond);\n          // Scale the condition by the difference.\n          if (Diff != 1)\n            Cond = DAG.getNode(ISD::MUL, DL, Cond.getValueType(), Cond,\n                               DAG.getConstant(Diff, DL, Cond.getValueType()));\n\n          // Add the base if non-zero.\n          if (FalseC->getAPIntValue() != 0)\n            Cond = DAG.getNode(ISD::ADD, DL, Cond.getValueType(), Cond,\n                               SDValue(FalseC, 0));\n          return Cond;\n        }\n      }\n    }\n  }\n\n  // Handle these cases:\n  //   (select (x != c), e, c) -> select (x != c), e, x),\n  //   (select (x == c), c, e) -> select (x == c), x, e)\n  // where the c is an integer constant, and the \"select\" is the combination\n  // of CMOV and CMP.\n  //\n  // The rationale for this change is that the conditional-move from a constant\n  // needs two instructions, however, conditional-move from a register needs\n  // only one instruction.\n  //\n  // CAVEAT: By replacing a constant with a symbolic value, it may obscure\n  //  some instruction-combining opportunities. This opt needs to be\n  //  postponed as late as possible.\n  //\n  if (!DCI.isBeforeLegalize() && !DCI.isBeforeLegalizeOps()) {\n    // the DCI.xxxx conditions are provided to postpone the optimization as\n    // late as possible.\n\n    ConstantSDNode *CmpAgainst = nullptr;\n    if ((Cond.getOpcode() == X86ISD::CMP || Cond.getOpcode() == X86ISD::SUB) &&\n        (CmpAgainst = dyn_cast<ConstantSDNode>(Cond.getOperand(1))) &&\n        !isa<ConstantSDNode>(Cond.getOperand(0))) {\n\n      if (CC == X86::COND_NE &&\n          CmpAgainst == dyn_cast<ConstantSDNode>(FalseOp)) {\n        CC = X86::GetOppositeBranchCondition(CC);\n        std::swap(TrueOp, FalseOp);\n      }\n\n      if (CC == X86::COND_E &&\n          CmpAgainst == dyn_cast<ConstantSDNode>(TrueOp)) {\n        SDValue Ops[] = {FalseOp, Cond.getOperand(0),\n                         DAG.getTargetConstant(CC, DL, MVT::i8), Cond};\n        return DAG.getNode(X86ISD::CMOV, DL, N->getValueType(0), Ops);\n      }\n    }\n  }\n\n  // Fold and/or of setcc's to double CMOV:\n  //   (CMOV F, T, ((cc1 | cc2) != 0)) -> (CMOV (CMOV F, T, cc1), T, cc2)\n  //   (CMOV F, T, ((cc1 & cc2) != 0)) -> (CMOV (CMOV T, F, !cc1), F, !cc2)\n  //\n  // This combine lets us generate:\n  //   cmovcc1 (jcc1 if we don't have CMOV)\n  //   cmovcc2 (same)\n  // instead of:\n  //   setcc1\n  //   setcc2\n  //   and/or\n  //   cmovne (jne if we don't have CMOV)\n  // When we can't use the CMOV instruction, it might increase branch\n  // mispredicts.\n  // When we can use CMOV, or when there is no mispredict, this improves\n  // throughput and reduces register pressure.\n  //\n  if (CC == X86::COND_NE) {\n    SDValue Flags;\n    X86::CondCode CC0, CC1;\n    bool isAndSetCC;\n    if (checkBoolTestAndOrSetCCCombine(Cond, CC0, CC1, Flags, isAndSetCC)) {\n      if (isAndSetCC) {\n        std::swap(FalseOp, TrueOp);\n        CC0 = X86::GetOppositeBranchCondition(CC0);\n        CC1 = X86::GetOppositeBranchCondition(CC1);\n      }\n\n      SDValue LOps[] = {FalseOp, TrueOp,\n                        DAG.getTargetConstant(CC0, DL, MVT::i8), Flags};\n      SDValue LCMOV = DAG.getNode(X86ISD::CMOV, DL, N->getValueType(0), LOps);\n      SDValue Ops[] = {LCMOV, TrueOp, DAG.getTargetConstant(CC1, DL, MVT::i8),\n                       Flags};\n      SDValue CMOV = DAG.getNode(X86ISD::CMOV, DL, N->getValueType(0), Ops);\n      return CMOV;\n    }\n  }\n\n  // Fold (CMOV C1, (ADD (CTTZ X), C2), (X != 0)) ->\n  //      (ADD (CMOV C1-C2, (CTTZ X), (X != 0)), C2)\n  // Or (CMOV (ADD (CTTZ X), C2), C1, (X == 0)) ->\n  //    (ADD (CMOV (CTTZ X), C1-C2, (X == 0)), C2)\n  if ((CC == X86::COND_NE || CC == X86::COND_E) &&\n      Cond.getOpcode() == X86ISD::CMP && isNullConstant(Cond.getOperand(1))) {\n    SDValue Add = TrueOp;\n    SDValue Const = FalseOp;\n    // Canonicalize the condition code for easier matching and output.\n    if (CC == X86::COND_E)\n      std::swap(Add, Const);\n\n    // We might have replaced the constant in the cmov with the LHS of the\n    // compare. If so change it to the RHS of the compare.\n    if (Const == Cond.getOperand(0))\n      Const = Cond.getOperand(1);\n\n    // Ok, now make sure that Add is (add (cttz X), C2) and Const is a constant.\n    if (isa<ConstantSDNode>(Const) && Add.getOpcode() == ISD::ADD &&\n        Add.hasOneUse() && isa<ConstantSDNode>(Add.getOperand(1)) &&\n        (Add.getOperand(0).getOpcode() == ISD::CTTZ_ZERO_UNDEF ||\n         Add.getOperand(0).getOpcode() == ISD::CTTZ) &&\n        Add.getOperand(0).getOperand(0) == Cond.getOperand(0)) {\n      EVT VT = N->getValueType(0);\n      // This should constant fold.\n      SDValue Diff = DAG.getNode(ISD::SUB, DL, VT, Const, Add.getOperand(1));\n      SDValue CMov =\n          DAG.getNode(X86ISD::CMOV, DL, VT, Diff, Add.getOperand(0),\n                      DAG.getTargetConstant(X86::COND_NE, DL, MVT::i8), Cond);\n      return DAG.getNode(ISD::ADD, DL, VT, CMov, Add.getOperand(1));\n    }\n  }\n\n  return SDValue();\n}\n\n/// Different mul shrinking modes.\nenum class ShrinkMode { MULS8, MULU8, MULS16, MULU16 };\n\nstatic bool canReduceVMulWidth(SDNode *N, SelectionDAG &DAG, ShrinkMode &Mode) {\n  EVT VT = N->getOperand(0).getValueType();\n  if (VT.getScalarSizeInBits() != 32)\n    return false;\n\n  assert(N->getNumOperands() == 2 && \"NumOperands of Mul are 2\");\n  unsigned SignBits[2] = {1, 1};\n  bool IsPositive[2] = {false, false};\n  for (unsigned i = 0; i < 2; i++) {\n    SDValue Opd = N->getOperand(i);\n\n    SignBits[i] = DAG.ComputeNumSignBits(Opd);\n    IsPositive[i] = DAG.SignBitIsZero(Opd);\n  }\n\n  bool AllPositive = IsPositive[0] && IsPositive[1];\n  unsigned MinSignBits = std::min(SignBits[0], SignBits[1]);\n  // When ranges are from -128 ~ 127, use MULS8 mode.\n  if (MinSignBits >= 25)\n    Mode = ShrinkMode::MULS8;\n  // When ranges are from 0 ~ 255, use MULU8 mode.\n  else if (AllPositive && MinSignBits >= 24)\n    Mode = ShrinkMode::MULU8;\n  // When ranges are from -32768 ~ 32767, use MULS16 mode.\n  else if (MinSignBits >= 17)\n    Mode = ShrinkMode::MULS16;\n  // When ranges are from 0 ~ 65535, use MULU16 mode.\n  else if (AllPositive && MinSignBits >= 16)\n    Mode = ShrinkMode::MULU16;\n  else\n    return false;\n  return true;\n}\n\n/// When the operands of vector mul are extended from smaller size values,\n/// like i8 and i16, the type of mul may be shrinked to generate more\n/// efficient code. Two typical patterns are handled:\n/// Pattern1:\n///     %2 = sext/zext <N x i8> %1 to <N x i32>\n///     %4 = sext/zext <N x i8> %3 to <N x i32>\n//   or %4 = build_vector <N x i32> %C1, ..., %CN (%C1..%CN are constants)\n///     %5 = mul <N x i32> %2, %4\n///\n/// Pattern2:\n///     %2 = zext/sext <N x i16> %1 to <N x i32>\n///     %4 = zext/sext <N x i16> %3 to <N x i32>\n///  or %4 = build_vector <N x i32> %C1, ..., %CN (%C1..%CN are constants)\n///     %5 = mul <N x i32> %2, %4\n///\n/// There are four mul shrinking modes:\n/// If %2 == sext32(trunc8(%2)), i.e., the scalar value range of %2 is\n/// -128 to 128, and the scalar value range of %4 is also -128 to 128,\n/// generate pmullw+sext32 for it (MULS8 mode).\n/// If %2 == zext32(trunc8(%2)), i.e., the scalar value range of %2 is\n/// 0 to 255, and the scalar value range of %4 is also 0 to 255,\n/// generate pmullw+zext32 for it (MULU8 mode).\n/// If %2 == sext32(trunc16(%2)), i.e., the scalar value range of %2 is\n/// -32768 to 32767, and the scalar value range of %4 is also -32768 to 32767,\n/// generate pmullw+pmulhw for it (MULS16 mode).\n/// If %2 == zext32(trunc16(%2)), i.e., the scalar value range of %2 is\n/// 0 to 65535, and the scalar value range of %4 is also 0 to 65535,\n/// generate pmullw+pmulhuw for it (MULU16 mode).\nstatic SDValue reduceVMULWidth(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  // Check for legality\n  // pmullw/pmulhw are not supported by SSE.\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  // Check for profitability\n  // pmulld is supported since SSE41. It is better to use pmulld\n  // instead of pmullw+pmulhw, except for subtargets where pmulld is slower than\n  // the expansion.\n  bool OptForMinSize = DAG.getMachineFunction().getFunction().hasMinSize();\n  if (Subtarget.hasSSE41() && (OptForMinSize || !Subtarget.isPMULLDSlow()))\n    return SDValue();\n\n  ShrinkMode Mode;\n  if (!canReduceVMulWidth(N, DAG, Mode))\n    return SDValue();\n\n  SDLoc DL(N);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getOperand(0).getValueType();\n  unsigned NumElts = VT.getVectorNumElements();\n  if ((NumElts % 2) != 0)\n    return SDValue();\n\n  EVT ReducedVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16, NumElts);\n\n  // Shrink the operands of mul.\n  SDValue NewN0 = DAG.getNode(ISD::TRUNCATE, DL, ReducedVT, N0);\n  SDValue NewN1 = DAG.getNode(ISD::TRUNCATE, DL, ReducedVT, N1);\n\n  // Generate the lower part of mul: pmullw. For MULU8/MULS8, only the\n  // lower part is needed.\n  SDValue MulLo = DAG.getNode(ISD::MUL, DL, ReducedVT, NewN0, NewN1);\n  if (Mode == ShrinkMode::MULU8 || Mode == ShrinkMode::MULS8)\n    return DAG.getNode((Mode == ShrinkMode::MULU8) ? ISD::ZERO_EXTEND\n                                                   : ISD::SIGN_EXTEND,\n                       DL, VT, MulLo);\n\n  EVT ResVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32, NumElts / 2);\n  // Generate the higher part of mul: pmulhw/pmulhuw. For MULU16/MULS16,\n  // the higher part is also needed.\n  SDValue MulHi =\n      DAG.getNode(Mode == ShrinkMode::MULS16 ? ISD::MULHS : ISD::MULHU, DL,\n                  ReducedVT, NewN0, NewN1);\n\n  // Repack the lower part and higher part result of mul into a wider\n  // result.\n  // Generate shuffle functioning as punpcklwd.\n  SmallVector<int, 16> ShuffleMask(NumElts);\n  for (unsigned i = 0, e = NumElts / 2; i < e; i++) {\n    ShuffleMask[2 * i] = i;\n    ShuffleMask[2 * i + 1] = i + NumElts;\n  }\n  SDValue ResLo =\n      DAG.getVectorShuffle(ReducedVT, DL, MulLo, MulHi, ShuffleMask);\n  ResLo = DAG.getBitcast(ResVT, ResLo);\n  // Generate shuffle functioning as punpckhwd.\n  for (unsigned i = 0, e = NumElts / 2; i < e; i++) {\n    ShuffleMask[2 * i] = i + NumElts / 2;\n    ShuffleMask[2 * i + 1] = i + NumElts * 3 / 2;\n  }\n  SDValue ResHi =\n      DAG.getVectorShuffle(ReducedVT, DL, MulLo, MulHi, ShuffleMask);\n  ResHi = DAG.getBitcast(ResVT, ResHi);\n  return DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, ResLo, ResHi);\n}\n\nstatic SDValue combineMulSpecial(uint64_t MulAmt, SDNode *N, SelectionDAG &DAG,\n                                 EVT VT, const SDLoc &DL) {\n\n  auto combineMulShlAddOrSub = [&](int Mult, int Shift, bool isAdd) {\n    SDValue Result = DAG.getNode(X86ISD::MUL_IMM, DL, VT, N->getOperand(0),\n                                 DAG.getConstant(Mult, DL, VT));\n    Result = DAG.getNode(ISD::SHL, DL, VT, Result,\n                         DAG.getConstant(Shift, DL, MVT::i8));\n    Result = DAG.getNode(isAdd ? ISD::ADD : ISD::SUB, DL, VT, Result,\n                         N->getOperand(0));\n    return Result;\n  };\n\n  auto combineMulMulAddOrSub = [&](int Mul1, int Mul2, bool isAdd) {\n    SDValue Result = DAG.getNode(X86ISD::MUL_IMM, DL, VT, N->getOperand(0),\n                                 DAG.getConstant(Mul1, DL, VT));\n    Result = DAG.getNode(X86ISD::MUL_IMM, DL, VT, Result,\n                         DAG.getConstant(Mul2, DL, VT));\n    Result = DAG.getNode(isAdd ? ISD::ADD : ISD::SUB, DL, VT, Result,\n                         N->getOperand(0));\n    return Result;\n  };\n\n  switch (MulAmt) {\n  default:\n    break;\n  case 11:\n    // mul x, 11 => add ((shl (mul x, 5), 1), x)\n    return combineMulShlAddOrSub(5, 1, /*isAdd*/ true);\n  case 21:\n    // mul x, 21 => add ((shl (mul x, 5), 2), x)\n    return combineMulShlAddOrSub(5, 2, /*isAdd*/ true);\n  case 41:\n    // mul x, 41 => add ((shl (mul x, 5), 3), x)\n    return combineMulShlAddOrSub(5, 3, /*isAdd*/ true);\n  case 22:\n    // mul x, 22 => add (add ((shl (mul x, 5), 2), x), x)\n    return DAG.getNode(ISD::ADD, DL, VT, N->getOperand(0),\n                       combineMulShlAddOrSub(5, 2, /*isAdd*/ true));\n  case 19:\n    // mul x, 19 => add ((shl (mul x, 9), 1), x)\n    return combineMulShlAddOrSub(9, 1, /*isAdd*/ true);\n  case 37:\n    // mul x, 37 => add ((shl (mul x, 9), 2), x)\n    return combineMulShlAddOrSub(9, 2, /*isAdd*/ true);\n  case 73:\n    // mul x, 73 => add ((shl (mul x, 9), 3), x)\n    return combineMulShlAddOrSub(9, 3, /*isAdd*/ true);\n  case 13:\n    // mul x, 13 => add ((shl (mul x, 3), 2), x)\n    return combineMulShlAddOrSub(3, 2, /*isAdd*/ true);\n  case 23:\n    // mul x, 23 => sub ((shl (mul x, 3), 3), x)\n    return combineMulShlAddOrSub(3, 3, /*isAdd*/ false);\n  case 26:\n    // mul x, 26 => add ((mul (mul x, 5), 5), x)\n    return combineMulMulAddOrSub(5, 5, /*isAdd*/ true);\n  case 28:\n    // mul x, 28 => add ((mul (mul x, 9), 3), x)\n    return combineMulMulAddOrSub(9, 3, /*isAdd*/ true);\n  case 29:\n    // mul x, 29 => add (add ((mul (mul x, 9), 3), x), x)\n    return DAG.getNode(ISD::ADD, DL, VT, N->getOperand(0),\n                       combineMulMulAddOrSub(9, 3, /*isAdd*/ true));\n  }\n\n  // Another trick. If this is a power 2 + 2/4/8, we can use a shift followed\n  // by a single LEA.\n  // First check if this a sum of two power of 2s because that's easy. Then\n  // count how many zeros are up to the first bit.\n  // TODO: We can do this even without LEA at a cost of two shifts and an add.\n  if (isPowerOf2_64(MulAmt & (MulAmt - 1))) {\n    unsigned ScaleShift = countTrailingZeros(MulAmt);\n    if (ScaleShift >= 1 && ScaleShift < 4) {\n      unsigned ShiftAmt = Log2_64((MulAmt & (MulAmt - 1)));\n      SDValue Shift1 = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                                   DAG.getConstant(ShiftAmt, DL, MVT::i8));\n      SDValue Shift2 = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                                   DAG.getConstant(ScaleShift, DL, MVT::i8));\n      return DAG.getNode(ISD::ADD, DL, VT, Shift1, Shift2);\n    }\n  }\n\n  return SDValue();\n}\n\n// If the upper 17 bits of each element are zero then we can use PMADDWD,\n// which is always at least as quick as PMULLD, except on KNL.\nstatic SDValue combineMulToPMADDWD(SDNode *N, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  if (Subtarget.isPMADDWDSlow())\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n\n  // Only support vXi32 vectors.\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::i32)\n    return SDValue();\n\n  // Make sure the type is legal or will be widened to a legal type.\n  if (VT != MVT::v2i32 && !DAG.getTargetLoweringInfo().isTypeLegal(VT))\n    return SDValue();\n\n  MVT WVT = MVT::getVectorVT(MVT::i16, 2 * VT.getVectorNumElements());\n\n  // Without BWI, we would need to split v32i16.\n  if (WVT == MVT::v32i16 && !Subtarget.hasBWI())\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // If we are zero extending two steps without SSE4.1, its better to reduce\n  // the vmul width instead.\n  if (!Subtarget.hasSSE41() &&\n      (N0.getOpcode() == ISD::ZERO_EXTEND &&\n       N0.getOperand(0).getScalarValueSizeInBits() <= 8) &&\n      (N1.getOpcode() == ISD::ZERO_EXTEND &&\n       N1.getOperand(0).getScalarValueSizeInBits() <= 8))\n    return SDValue();\n\n  APInt Mask17 = APInt::getHighBitsSet(32, 17);\n  if (!DAG.MaskedValueIsZero(N1, Mask17) ||\n      !DAG.MaskedValueIsZero(N0, Mask17))\n    return SDValue();\n\n  // Use SplitOpsAndApply to handle AVX splitting.\n  auto PMADDWDBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                           ArrayRef<SDValue> Ops) {\n    MVT OpVT = MVT::getVectorVT(MVT::i32, Ops[0].getValueSizeInBits() / 32);\n    return DAG.getNode(X86ISD::VPMADDWD, DL, OpVT, Ops);\n  };\n  return SplitOpsAndApply(DAG, Subtarget, SDLoc(N), VT,\n                          { DAG.getBitcast(WVT, N0), DAG.getBitcast(WVT, N1) },\n                          PMADDWDBuilder);\n}\n\nstatic SDValue combineMulToPMULDQ(SDNode *N, SelectionDAG &DAG,\n                                  const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n\n  // Only support vXi64 vectors.\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::i64 ||\n      VT.getVectorNumElements() < 2 ||\n      !isPowerOf2_32(VT.getVectorNumElements()))\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // MULDQ returns the 64-bit result of the signed multiplication of the lower\n  // 32-bits. We can lower with this if the sign bits stretch that far.\n  if (Subtarget.hasSSE41() && DAG.ComputeNumSignBits(N0) > 32 &&\n      DAG.ComputeNumSignBits(N1) > 32) {\n    auto PMULDQBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                            ArrayRef<SDValue> Ops) {\n      return DAG.getNode(X86ISD::PMULDQ, DL, Ops[0].getValueType(), Ops);\n    };\n    return SplitOpsAndApply(DAG, Subtarget, SDLoc(N), VT, { N0, N1 },\n                            PMULDQBuilder, /*CheckBWI*/false);\n  }\n\n  // If the upper bits are zero we can use a single pmuludq.\n  APInt Mask = APInt::getHighBitsSet(64, 32);\n  if (DAG.MaskedValueIsZero(N0, Mask) && DAG.MaskedValueIsZero(N1, Mask)) {\n    auto PMULUDQBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                             ArrayRef<SDValue> Ops) {\n      return DAG.getNode(X86ISD::PMULUDQ, DL, Ops[0].getValueType(), Ops);\n    };\n    return SplitOpsAndApply(DAG, Subtarget, SDLoc(N), VT, { N0, N1 },\n                            PMULUDQBuilder, /*CheckBWI*/false);\n  }\n\n  return SDValue();\n}\n\n/// Optimize a single multiply with constant into two operations in order to\n/// implement it with two cheaper instructions, e.g. LEA + SHL, LEA + LEA.\nstatic SDValue combineMul(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n\n  if (SDValue V = combineMulToPMADDWD(N, DAG, Subtarget))\n    return V;\n\n  if (SDValue V = combineMulToPMULDQ(N, DAG, Subtarget))\n    return V;\n\n  if (DCI.isBeforeLegalize() && VT.isVector())\n    return reduceVMULWidth(N, DAG, Subtarget);\n\n  if (!MulConstantOptimization)\n    return SDValue();\n  // An imul is usually smaller than the alternative sequence.\n  if (DAG.getMachineFunction().getFunction().hasMinSize())\n    return SDValue();\n\n  if (DCI.isBeforeLegalize() || DCI.isCalledByLegalizer())\n    return SDValue();\n\n  if (VT != MVT::i64 && VT != MVT::i32)\n    return SDValue();\n\n  ConstantSDNode *C = dyn_cast<ConstantSDNode>(N->getOperand(1));\n  if (!C)\n    return SDValue();\n  if (isPowerOf2_64(C->getZExtValue()))\n    return SDValue();\n\n  int64_t SignMulAmt = C->getSExtValue();\n  assert(SignMulAmt != INT64_MIN && \"Int min should have been handled!\");\n  uint64_t AbsMulAmt = SignMulAmt < 0 ? -SignMulAmt : SignMulAmt;\n\n  SDLoc DL(N);\n  if (AbsMulAmt == 3 || AbsMulAmt == 5 || AbsMulAmt == 9) {\n    SDValue NewMul = DAG.getNode(X86ISD::MUL_IMM, DL, VT, N->getOperand(0),\n                                 DAG.getConstant(AbsMulAmt, DL, VT));\n    if (SignMulAmt < 0)\n      NewMul = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT),\n                           NewMul);\n\n    return NewMul;\n  }\n\n  uint64_t MulAmt1 = 0;\n  uint64_t MulAmt2 = 0;\n  if ((AbsMulAmt % 9) == 0) {\n    MulAmt1 = 9;\n    MulAmt2 = AbsMulAmt / 9;\n  } else if ((AbsMulAmt % 5) == 0) {\n    MulAmt1 = 5;\n    MulAmt2 = AbsMulAmt / 5;\n  } else if ((AbsMulAmt % 3) == 0) {\n    MulAmt1 = 3;\n    MulAmt2 = AbsMulAmt / 3;\n  }\n\n  SDValue NewMul;\n  // For negative multiply amounts, only allow MulAmt2 to be a power of 2.\n  if (MulAmt2 &&\n      (isPowerOf2_64(MulAmt2) ||\n       (SignMulAmt >= 0 && (MulAmt2 == 3 || MulAmt2 == 5 || MulAmt2 == 9)))) {\n\n    if (isPowerOf2_64(MulAmt2) &&\n        !(SignMulAmt >= 0 && N->hasOneUse() &&\n          N->use_begin()->getOpcode() == ISD::ADD))\n      // If second multiplifer is pow2, issue it first. We want the multiply by\n      // 3, 5, or 9 to be folded into the addressing mode unless the lone use\n      // is an add. Only do this for positive multiply amounts since the\n      // negate would prevent it from being used as an address mode anyway.\n      std::swap(MulAmt1, MulAmt2);\n\n    if (isPowerOf2_64(MulAmt1))\n      NewMul = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                           DAG.getConstant(Log2_64(MulAmt1), DL, MVT::i8));\n    else\n      NewMul = DAG.getNode(X86ISD::MUL_IMM, DL, VT, N->getOperand(0),\n                           DAG.getConstant(MulAmt1, DL, VT));\n\n    if (isPowerOf2_64(MulAmt2))\n      NewMul = DAG.getNode(ISD::SHL, DL, VT, NewMul,\n                           DAG.getConstant(Log2_64(MulAmt2), DL, MVT::i8));\n    else\n      NewMul = DAG.getNode(X86ISD::MUL_IMM, DL, VT, NewMul,\n                           DAG.getConstant(MulAmt2, DL, VT));\n\n    // Negate the result.\n    if (SignMulAmt < 0)\n      NewMul = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT),\n                           NewMul);\n  } else if (!Subtarget.slowLEA())\n    NewMul = combineMulSpecial(C->getZExtValue(), N, DAG, VT, DL);\n\n  if (!NewMul) {\n    assert(C->getZExtValue() != 0 &&\n           C->getZExtValue() != (VT == MVT::i64 ? UINT64_MAX : UINT32_MAX) &&\n           \"Both cases that could cause potential overflows should have \"\n           \"already been handled.\");\n    if (isPowerOf2_64(AbsMulAmt - 1)) {\n      // (mul x, 2^N + 1) => (add (shl x, N), x)\n      NewMul = DAG.getNode(\n          ISD::ADD, DL, VT, N->getOperand(0),\n          DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                      DAG.getConstant(Log2_64(AbsMulAmt - 1), DL,\n                                      MVT::i8)));\n      // To negate, subtract the number from zero\n      if (SignMulAmt < 0)\n        NewMul = DAG.getNode(ISD::SUB, DL, VT,\n                             DAG.getConstant(0, DL, VT), NewMul);\n    } else if (isPowerOf2_64(AbsMulAmt + 1)) {\n      // (mul x, 2^N - 1) => (sub (shl x, N), x)\n      NewMul = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                           DAG.getConstant(Log2_64(AbsMulAmt + 1),\n                                           DL, MVT::i8));\n      // To negate, reverse the operands of the subtract.\n      if (SignMulAmt < 0)\n        NewMul = DAG.getNode(ISD::SUB, DL, VT, N->getOperand(0), NewMul);\n      else\n        NewMul = DAG.getNode(ISD::SUB, DL, VT, NewMul, N->getOperand(0));\n    } else if (SignMulAmt >= 0 && isPowerOf2_64(AbsMulAmt - 2)) {\n      // (mul x, 2^N + 2) => (add (add (shl x, N), x), x)\n      NewMul = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                           DAG.getConstant(Log2_64(AbsMulAmt - 2),\n                                           DL, MVT::i8));\n      NewMul = DAG.getNode(ISD::ADD, DL, VT, NewMul, N->getOperand(0));\n      NewMul = DAG.getNode(ISD::ADD, DL, VT, NewMul, N->getOperand(0));\n    } else if (SignMulAmt >= 0 && isPowerOf2_64(AbsMulAmt + 2)) {\n      // (mul x, 2^N - 2) => (sub (sub (shl x, N), x), x)\n      NewMul = DAG.getNode(ISD::SHL, DL, VT, N->getOperand(0),\n                           DAG.getConstant(Log2_64(AbsMulAmt + 2),\n                                           DL, MVT::i8));\n      NewMul = DAG.getNode(ISD::SUB, DL, VT, NewMul, N->getOperand(0));\n      NewMul = DAG.getNode(ISD::SUB, DL, VT, NewMul, N->getOperand(0));\n    }\n  }\n\n  return NewMul;\n}\n\n// Try to form a MULHU or MULHS node by looking for\n// (srl (mul ext, ext), 16)\n// TODO: This is X86 specific because we want to be able to handle wide types\n// before type legalization. But we can only do it if the vector will be\n// legalized via widening/splitting. Type legalization can't handle promotion\n// of a MULHU/MULHS. There isn't a way to convey this to the generic DAG\n// combiner.\nstatic SDValue combineShiftToPMULH(SDNode *N, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  assert((N->getOpcode() == ISD::SRL || N->getOpcode() == ISD::SRA) &&\n           \"SRL or SRA node is required here!\");\n  SDLoc DL(N);\n\n  // Only do this with SSE4.1. On earlier targets reduceVMULWidth will expand\n  // the multiply.\n  if (!Subtarget.hasSSE41())\n    return SDValue();\n\n  // The operation feeding into the shift must be a multiply.\n  SDValue ShiftOperand = N->getOperand(0);\n  if (ShiftOperand.getOpcode() != ISD::MUL || !ShiftOperand.hasOneUse())\n    return SDValue();\n\n  // Input type should be at least vXi32.\n  EVT VT = N->getValueType(0);\n  if (!VT.isVector() || VT.getVectorElementType().getSizeInBits() < 32)\n    return SDValue();\n\n  // Need a shift by 16.\n  APInt ShiftAmt;\n  if (!ISD::isConstantSplatVector(N->getOperand(1).getNode(), ShiftAmt) ||\n      ShiftAmt != 16)\n    return SDValue();\n\n  SDValue LHS = ShiftOperand.getOperand(0);\n  SDValue RHS = ShiftOperand.getOperand(1);\n\n  unsigned ExtOpc = LHS.getOpcode();\n  if ((ExtOpc != ISD::SIGN_EXTEND && ExtOpc != ISD::ZERO_EXTEND) ||\n      RHS.getOpcode() != ExtOpc)\n    return SDValue();\n\n  // Peek through the extends.\n  LHS = LHS.getOperand(0);\n  RHS = RHS.getOperand(0);\n\n  // Ensure the input types match.\n  EVT MulVT = LHS.getValueType();\n  if (MulVT.getVectorElementType() != MVT::i16 || RHS.getValueType() != MulVT)\n    return SDValue();\n\n  unsigned Opc = ExtOpc == ISD::SIGN_EXTEND ? ISD::MULHS : ISD::MULHU;\n  SDValue Mulh = DAG.getNode(Opc, DL, MulVT, LHS, RHS);\n\n  ExtOpc = N->getOpcode() == ISD::SRA ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;\n  return DAG.getNode(ExtOpc, DL, VT, Mulh);\n}\n\nstatic SDValue combineShiftLeft(SDNode *N, SelectionDAG &DAG) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  ConstantSDNode *N1C = dyn_cast<ConstantSDNode>(N1);\n  EVT VT = N0.getValueType();\n\n  // fold (shl (and (setcc_c), c1), c2) -> (and setcc_c, (c1 << c2))\n  // since the result of setcc_c is all zero's or all ones.\n  if (VT.isInteger() && !VT.isVector() &&\n      N1C && N0.getOpcode() == ISD::AND &&\n      N0.getOperand(1).getOpcode() == ISD::Constant) {\n    SDValue N00 = N0.getOperand(0);\n    APInt Mask = N0.getConstantOperandAPInt(1);\n    Mask <<= N1C->getAPIntValue();\n    bool MaskOK = false;\n    // We can handle cases concerning bit-widening nodes containing setcc_c if\n    // we carefully interrogate the mask to make sure we are semantics\n    // preserving.\n    // The transform is not safe if the result of C1 << C2 exceeds the bitwidth\n    // of the underlying setcc_c operation if the setcc_c was zero extended.\n    // Consider the following example:\n    //   zext(setcc_c)                 -> i32 0x0000FFFF\n    //   c1                            -> i32 0x0000FFFF\n    //   c2                            -> i32 0x00000001\n    //   (shl (and (setcc_c), c1), c2) -> i32 0x0001FFFE\n    //   (and setcc_c, (c1 << c2))     -> i32 0x0000FFFE\n    if (N00.getOpcode() == X86ISD::SETCC_CARRY) {\n      MaskOK = true;\n    } else if (N00.getOpcode() == ISD::SIGN_EXTEND &&\n               N00.getOperand(0).getOpcode() == X86ISD::SETCC_CARRY) {\n      MaskOK = true;\n    } else if ((N00.getOpcode() == ISD::ZERO_EXTEND ||\n                N00.getOpcode() == ISD::ANY_EXTEND) &&\n               N00.getOperand(0).getOpcode() == X86ISD::SETCC_CARRY) {\n      MaskOK = Mask.isIntN(N00.getOperand(0).getValueSizeInBits());\n    }\n    if (MaskOK && Mask != 0) {\n      SDLoc DL(N);\n      return DAG.getNode(ISD::AND, DL, VT, N00, DAG.getConstant(Mask, DL, VT));\n    }\n  }\n\n  // Hardware support for vector shifts is sparse which makes us scalarize the\n  // vector operations in many cases. Also, on sandybridge ADD is faster than\n  // shl.\n  // (shl V, 1) -> add V,V\n  if (auto *N1BV = dyn_cast<BuildVectorSDNode>(N1))\n    if (auto *N1SplatC = N1BV->getConstantSplatNode()) {\n      assert(N0.getValueType().isVector() && \"Invalid vector shift type\");\n      // We shift all of the values by one. In many cases we do not have\n      // hardware support for this operation. This is better expressed as an ADD\n      // of two values.\n      if (N1SplatC->isOne())\n        return DAG.getNode(ISD::ADD, SDLoc(N), VT, N0, N0);\n    }\n\n  return SDValue();\n}\n\nstatic SDValue combineShiftRightArithmetic(SDNode *N, SelectionDAG &DAG,\n                                           const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N0.getValueType();\n  unsigned Size = VT.getSizeInBits();\n\n  if (SDValue V = combineShiftToPMULH(N, DAG, Subtarget))\n    return V;\n\n  // fold (ashr (shl, a, [56,48,32,24,16]), SarConst)\n  // into (shl, (sext (a), [56,48,32,24,16] - SarConst)) or\n  // into (lshr, (sext (a), SarConst - [56,48,32,24,16]))\n  // depending on sign of (SarConst - [56,48,32,24,16])\n\n  // sexts in X86 are MOVs. The MOVs have the same code size\n  // as above SHIFTs (only SHIFT on 1 has lower code size).\n  // However the MOVs have 2 advantages to a SHIFT:\n  // 1. MOVs can write to a register that differs from source\n  // 2. MOVs accept memory operands\n\n  if (VT.isVector() || N1.getOpcode() != ISD::Constant ||\n      N0.getOpcode() != ISD::SHL || !N0.hasOneUse() ||\n      N0.getOperand(1).getOpcode() != ISD::Constant)\n    return SDValue();\n\n  SDValue N00 = N0.getOperand(0);\n  SDValue N01 = N0.getOperand(1);\n  APInt ShlConst = (cast<ConstantSDNode>(N01))->getAPIntValue();\n  APInt SarConst = (cast<ConstantSDNode>(N1))->getAPIntValue();\n  EVT CVT = N1.getValueType();\n\n  if (SarConst.isNegative())\n    return SDValue();\n\n  for (MVT SVT : { MVT::i8, MVT::i16, MVT::i32 }) {\n    unsigned ShiftSize = SVT.getSizeInBits();\n    // skipping types without corresponding sext/zext and\n    // ShlConst that is not one of [56,48,32,24,16]\n    if (ShiftSize >= Size || ShlConst != Size - ShiftSize)\n      continue;\n    SDLoc DL(N);\n    SDValue NN =\n        DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, VT, N00, DAG.getValueType(SVT));\n    SarConst = SarConst - (Size - ShiftSize);\n    if (SarConst == 0)\n      return NN;\n    else if (SarConst.isNegative())\n      return DAG.getNode(ISD::SHL, DL, VT, NN,\n                         DAG.getConstant(-SarConst, DL, CVT));\n    else\n      return DAG.getNode(ISD::SRA, DL, VT, NN,\n                         DAG.getConstant(SarConst, DL, CVT));\n  }\n  return SDValue();\n}\n\nstatic SDValue combineShiftRightLogical(SDNode *N, SelectionDAG &DAG,\n                                        TargetLowering::DAGCombinerInfo &DCI,\n                                        const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N0.getValueType();\n\n  if (SDValue V = combineShiftToPMULH(N, DAG, Subtarget))\n    return V;\n\n  // Only do this on the last DAG combine as it can interfere with other\n  // combines.\n  if (!DCI.isAfterLegalizeDAG())\n    return SDValue();\n\n  // Try to improve a sequence of srl (and X, C1), C2 by inverting the order.\n  // TODO: This is a generic DAG combine that became an x86-only combine to\n  // avoid shortcomings in other folds such as bswap, bit-test ('bt'), and\n  // and-not ('andn').\n  if (N0.getOpcode() != ISD::AND || !N0.hasOneUse())\n    return SDValue();\n\n  auto *ShiftC = dyn_cast<ConstantSDNode>(N1);\n  auto *AndC = dyn_cast<ConstantSDNode>(N0.getOperand(1));\n  if (!ShiftC || !AndC)\n    return SDValue();\n\n  // If we can shrink the constant mask below 8-bits or 32-bits, then this\n  // transform should reduce code size. It may also enable secondary transforms\n  // from improved known-bits analysis or instruction selection.\n  APInt MaskVal = AndC->getAPIntValue();\n\n  // If this can be matched by a zero extend, don't optimize.\n  if (MaskVal.isMask()) {\n    unsigned TO = MaskVal.countTrailingOnes();\n    if (TO >= 8 && isPowerOf2_32(TO))\n      return SDValue();\n  }\n\n  APInt NewMaskVal = MaskVal.lshr(ShiftC->getAPIntValue());\n  unsigned OldMaskSize = MaskVal.getMinSignedBits();\n  unsigned NewMaskSize = NewMaskVal.getMinSignedBits();\n  if ((OldMaskSize > 8 && NewMaskSize <= 8) ||\n      (OldMaskSize > 32 && NewMaskSize <= 32)) {\n    // srl (and X, AndC), ShiftC --> and (srl X, ShiftC), (AndC >> ShiftC)\n    SDLoc DL(N);\n    SDValue NewMask = DAG.getConstant(NewMaskVal, DL, VT);\n    SDValue NewShift = DAG.getNode(ISD::SRL, DL, VT, N0.getOperand(0), N1);\n    return DAG.getNode(ISD::AND, DL, VT, NewShift, NewMask);\n  }\n  return SDValue();\n}\n\nstatic SDValue combineHorizOpWithShuffle(SDNode *N, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  unsigned Opcode = N->getOpcode();\n  assert((X86ISD::HADD == Opcode || X86ISD::FHADD == Opcode ||\n          X86ISD::HSUB == Opcode || X86ISD::FHSUB == Opcode ||\n          X86ISD::PACKSS == Opcode || X86ISD::PACKUS == Opcode) &&\n         \"Unexpected hadd/hsub/pack opcode\");\n\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT SrcVT = N0.getValueType();\n\n  // Attempt to fold HOP(LOSUBVECTOR(SHUFFLE(X)),HISUBVECTOR(SHUFFLE(X)))\n  // to SHUFFLE(HOP(LOSUBVECTOR(X),HISUBVECTOR(X))), this is mainly for\n  // truncation trees that help us avoid lane crossing shuffles.\n  // TODO: There's a lot more we can do for PACK/HADD style shuffle combines.\n  // TODO: We don't handle vXf64 shuffles yet.\n  if (N0.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      N1.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      N0.getConstantOperandAPInt(1) == 0 &&\n      N1.getConstantOperandAPInt(1) == SrcVT.getVectorNumElements() &&\n      N0.getOperand(0) == N1.getOperand(0) && VT.is128BitVector() &&\n      N0.getOperand(0).getValueType().is256BitVector() &&\n      SrcVT.getScalarSizeInBits() <= 32) {\n    // TODO - support target/faux shuffles.\n    SDValue Vec = peekThroughBitcasts(N0.getOperand(0));\n    if (auto *SVN = dyn_cast<ShuffleVectorSDNode>(Vec)) {\n      // To keep the HOP LHS/RHS coherency, we must be able to scale the unary\n      // shuffle to a vXi64 width - we can probably relax this in the future.\n      SmallVector<int, 4> ShuffleMask;\n      if (SVN->getOperand(1).isUndef() &&\n          scaleShuffleElements(SVN->getMask(), 4, ShuffleMask)) {\n        SDLoc DL(N);\n        SDValue Lo, Hi;\n        MVT ShufVT = VT.isFloatingPoint() ? MVT::v4f32 : MVT::v4i32;\n        std::tie(Lo, Hi) = DAG.SplitVector(SVN->getOperand(0), DL);\n        Lo = DAG.getBitcast(N0.getValueType(), Lo);\n        Hi = DAG.getBitcast(N1.getValueType(), Hi);\n        SDValue Res = DAG.getNode(Opcode, DL, VT, Lo, Hi);\n        Res = DAG.getBitcast(ShufVT, Res);\n        Res = DAG.getVectorShuffle(ShufVT, DL, Res, Res, ShuffleMask);\n        return DAG.getBitcast(VT, Res);\n      }\n    }\n  }\n\n  // Attempt to fold HOP(SHUFFLE(X),SHUFFLE(Y)) -> SHUFFLE(HOP(X,Y)).\n  // TODO: Merge with binary shuffle folds below.\n  if (VT.is128BitVector() && SrcVT.getScalarSizeInBits() <= 32) {\n    int PostShuffle[4] = {0, 1, 2, 3};\n\n    // If the op is an unary shuffle that can scale to v2x64,\n    // then we can perform this as a v4x32 post shuffle.\n    auto AdjustOp = [&](SDValue V, int Offset) {\n      auto *SVN = dyn_cast<ShuffleVectorSDNode>(V);\n      SmallVector<int, 2> ScaledMask;\n      if (!SVN || !SVN->getOperand(1).isUndef() ||\n          !scaleShuffleElements(SVN->getMask(), 2, ScaledMask) ||\n          !N->isOnlyUserOf(V.getNode()))\n        return SDValue();\n      PostShuffle[Offset + 0] = ScaledMask[0] < 0 ? -1 : Offset + ScaledMask[0];\n      PostShuffle[Offset + 1] = ScaledMask[1] < 0 ? -1 : Offset + ScaledMask[1];\n      return SVN->getOperand(0);\n    };\n\n    SDValue Src0 = AdjustOp(N0, 0);\n    SDValue Src1 = AdjustOp(N1, 2);\n    if (Src0 || Src1) {\n      Src0 = Src0 ? Src0 : N0;\n      Src1 = Src1 ? Src1 : N1;\n      SDLoc DL(N);\n      MVT ShufVT = VT.isFloatingPoint() ? MVT::v4f32 : MVT::v4i32;\n      SDValue Res = DAG.getNode(Opcode, DL, VT, Src0, Src1);\n      Res = DAG.getBitcast(ShufVT, Res);\n      Res = DAG.getVectorShuffle(ShufVT, DL, Res, Res, PostShuffle);\n      return DAG.getBitcast(VT, Res);\n    }\n  }\n\n  // Attempt to fold HOP(SHUFFLE(X,Y),SHUFFLE(X,Y)) -> SHUFFLE(HOP(X,Y)).\n  // TODO: Relax shuffle scaling to support sub-128-bit subvector shuffles.\n  if (VT.is256BitVector() && Subtarget.hasInt256()) {\n    SmallVector<int> Mask0, Mask1;\n    SmallVector<SDValue> Ops0, Ops1;\n    if (getTargetShuffleInputs(N0, Ops0, Mask0, DAG) && !isAnyZero(Mask0) &&\n        getTargetShuffleInputs(N1, Ops1, Mask1, DAG) && !isAnyZero(Mask1) &&\n        !Ops0.empty() && !Ops1.empty()) {\n      SDValue Op00 = Ops0.front(), Op01 = Ops0.back();\n      SDValue Op10 = Ops1.front(), Op11 = Ops1.back();\n      SmallVector<int, 2> ShuffleMask0, ShuffleMask1;\n      if (Op00.getValueType() == SrcVT && Op01.getValueType() == SrcVT &&\n          Op10.getValueType() == SrcVT && Op11.getValueType() == SrcVT &&\n          scaleShuffleElements(Mask0, 2, ShuffleMask0) &&\n          scaleShuffleElements(Mask1, 2, ShuffleMask1)) {\n        if ((Op00 == Op11) && (Op01 == Op10)) {\n          std::swap(Op10, Op11);\n          ShuffleVectorSDNode::commuteMask(ShuffleMask1);\n        }\n        if ((Op00 == Op10) && (Op01 == Op11)) {\n          SmallVector<int, 4> ShuffleMask;\n          ShuffleMask.append(ShuffleMask0.begin(), ShuffleMask0.end());\n          ShuffleMask.append(ShuffleMask1.begin(), ShuffleMask1.end());\n          SDLoc DL(N);\n          MVT ShufVT = VT.isFloatingPoint() ? MVT::v4f64 : MVT::v4i64;\n          SDValue Res = DAG.getNode(Opcode, DL, VT, Op00, Op01);\n          Res = DAG.getBitcast(ShufVT, Res);\n          Res = DAG.getVectorShuffle(ShufVT, DL, Res, Res, ShuffleMask);\n          return DAG.getBitcast(VT, Res);\n        }\n      }\n    }\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineVectorPack(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const X86Subtarget &Subtarget) {\n  unsigned Opcode = N->getOpcode();\n  assert((X86ISD::PACKSS == Opcode || X86ISD::PACKUS == Opcode) &&\n         \"Unexpected pack opcode\");\n\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  unsigned NumDstElts = VT.getVectorNumElements();\n  unsigned DstBitsPerElt = VT.getScalarSizeInBits();\n  unsigned SrcBitsPerElt = 2 * DstBitsPerElt;\n  assert(N0.getScalarValueSizeInBits() == SrcBitsPerElt &&\n         N1.getScalarValueSizeInBits() == SrcBitsPerElt &&\n         \"Unexpected PACKSS/PACKUS input type\");\n\n  bool IsSigned = (X86ISD::PACKSS == Opcode);\n\n  // Constant Folding.\n  APInt UndefElts0, UndefElts1;\n  SmallVector<APInt, 32> EltBits0, EltBits1;\n  if ((N0.isUndef() || N->isOnlyUserOf(N0.getNode())) &&\n      (N1.isUndef() || N->isOnlyUserOf(N1.getNode())) &&\n      getTargetConstantBitsFromNode(N0, SrcBitsPerElt, UndefElts0, EltBits0) &&\n      getTargetConstantBitsFromNode(N1, SrcBitsPerElt, UndefElts1, EltBits1)) {\n    unsigned NumLanes = VT.getSizeInBits() / 128;\n    unsigned NumSrcElts = NumDstElts / 2;\n    unsigned NumDstEltsPerLane = NumDstElts / NumLanes;\n    unsigned NumSrcEltsPerLane = NumSrcElts / NumLanes;\n\n    APInt Undefs(NumDstElts, 0);\n    SmallVector<APInt, 32> Bits(NumDstElts, APInt::getNullValue(DstBitsPerElt));\n    for (unsigned Lane = 0; Lane != NumLanes; ++Lane) {\n      for (unsigned Elt = 0; Elt != NumDstEltsPerLane; ++Elt) {\n        unsigned SrcIdx = Lane * NumSrcEltsPerLane + Elt % NumSrcEltsPerLane;\n        auto &UndefElts = (Elt >= NumSrcEltsPerLane ? UndefElts1 : UndefElts0);\n        auto &EltBits = (Elt >= NumSrcEltsPerLane ? EltBits1 : EltBits0);\n\n        if (UndefElts[SrcIdx]) {\n          Undefs.setBit(Lane * NumDstEltsPerLane + Elt);\n          continue;\n        }\n\n        APInt &Val = EltBits[SrcIdx];\n        if (IsSigned) {\n          // PACKSS: Truncate signed value with signed saturation.\n          // Source values less than dst minint are saturated to minint.\n          // Source values greater than dst maxint are saturated to maxint.\n          if (Val.isSignedIntN(DstBitsPerElt))\n            Val = Val.trunc(DstBitsPerElt);\n          else if (Val.isNegative())\n            Val = APInt::getSignedMinValue(DstBitsPerElt);\n          else\n            Val = APInt::getSignedMaxValue(DstBitsPerElt);\n        } else {\n          // PACKUS: Truncate signed value with unsigned saturation.\n          // Source values less than zero are saturated to zero.\n          // Source values greater than dst maxuint are saturated to maxuint.\n          if (Val.isIntN(DstBitsPerElt))\n            Val = Val.trunc(DstBitsPerElt);\n          else if (Val.isNegative())\n            Val = APInt::getNullValue(DstBitsPerElt);\n          else\n            Val = APInt::getAllOnesValue(DstBitsPerElt);\n        }\n        Bits[Lane * NumDstEltsPerLane + Elt] = Val;\n      }\n    }\n\n    return getConstVector(Bits, Undefs, VT.getSimpleVT(), DAG, SDLoc(N));\n  }\n\n  // Try to fold PACK(SHUFFLE(),SHUFFLE()) -> SHUFFLE(PACK()).\n  if (SDValue V = combineHorizOpWithShuffle(N, DAG, Subtarget))\n    return V;\n\n  // Try to combine a PACKUSWB/PACKSSWB implemented truncate with a regular\n  // truncate to create a larger truncate.\n  if (Subtarget.hasAVX512() &&\n      N0.getOpcode() == ISD::TRUNCATE && N1.isUndef() && VT == MVT::v16i8 &&\n      N0.getOperand(0).getValueType() == MVT::v8i32) {\n    if ((IsSigned && DAG.ComputeNumSignBits(N0) > 8) ||\n        (!IsSigned &&\n         DAG.MaskedValueIsZero(N0, APInt::getHighBitsSet(16, 8)))) {\n      if (Subtarget.hasVLX())\n        return DAG.getNode(X86ISD::VTRUNC, SDLoc(N), VT, N0.getOperand(0));\n\n      // Widen input to v16i32 so we can truncate that.\n      SDLoc dl(N);\n      SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v16i32,\n                                   N0.getOperand(0), DAG.getUNDEF(MVT::v8i32));\n      return DAG.getNode(ISD::TRUNCATE, SDLoc(N), VT, Concat);\n    }\n  }\n\n  // Try to fold PACK(EXTEND(X),EXTEND(Y)) -> CONCAT(X,Y) subvectors.\n  if (VT.is128BitVector()) {\n    unsigned ExtOpc = IsSigned ? ISD::SIGN_EXTEND : ISD::ZERO_EXTEND;\n    SDValue Src0, Src1;\n    if (N0.getOpcode() == ExtOpc &&\n        N0.getOperand(0).getValueType().is64BitVector() &&\n        N0.getOperand(0).getScalarValueSizeInBits() == DstBitsPerElt) {\n      Src0 = N0.getOperand(0);\n    }\n    if (N1.getOpcode() == ExtOpc &&\n        N1.getOperand(0).getValueType().is64BitVector() &&\n        N1.getOperand(0).getScalarValueSizeInBits() == DstBitsPerElt) {\n      Src1 = N1.getOperand(0);\n    }\n    if ((Src0 || N0.isUndef()) && (Src1 || N1.isUndef())) {\n      assert((Src0 || Src1) && \"Found PACK(UNDEF,UNDEF)\");\n      Src0 = Src0 ? Src0 : DAG.getUNDEF(Src1.getValueType());\n      Src1 = Src1 ? Src1 : DAG.getUNDEF(Src0.getValueType());\n      return DAG.getNode(ISD::CONCAT_VECTORS, SDLoc(N), VT, Src0, Src1);\n    }\n  }\n\n  // Attempt to combine as shuffle.\n  SDValue Op(N, 0);\n  if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n    return Res;\n\n  return SDValue();\n}\n\nstatic SDValue combineVectorHADDSUB(SDNode *N, SelectionDAG &DAG,\n                                    TargetLowering::DAGCombinerInfo &DCI,\n                                    const X86Subtarget &Subtarget) {\n  assert((X86ISD::HADD == N->getOpcode() || X86ISD::FHADD == N->getOpcode() ||\n          X86ISD::HSUB == N->getOpcode() || X86ISD::FHSUB == N->getOpcode()) &&\n         \"Unexpected horizontal add/sub opcode\");\n\n  // Try to fold HOP(SHUFFLE(),SHUFFLE()) -> SHUFFLE(HOP()).\n  if (SDValue V = combineHorizOpWithShuffle(N, DAG, Subtarget))\n    return V;\n\n  return SDValue();\n}\n\nstatic SDValue combineVectorShiftVar(SDNode *N, SelectionDAG &DAG,\n                                     TargetLowering::DAGCombinerInfo &DCI,\n                                     const X86Subtarget &Subtarget) {\n  assert((X86ISD::VSHL == N->getOpcode() || X86ISD::VSRA == N->getOpcode() ||\n          X86ISD::VSRL == N->getOpcode()) &&\n         \"Unexpected shift opcode\");\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // Shift zero -> zero.\n  if (ISD::isBuildVectorAllZeros(N0.getNode()))\n    return DAG.getConstant(0, SDLoc(N), VT);\n\n  // Detect constant shift amounts.\n  APInt UndefElts;\n  SmallVector<APInt, 32> EltBits;\n  if (getTargetConstantBitsFromNode(N1, 64, UndefElts, EltBits, true, false)) {\n    unsigned X86Opc = getTargetVShiftUniformOpcode(N->getOpcode(), false);\n    return getTargetVShiftByConstNode(X86Opc, SDLoc(N), VT.getSimpleVT(), N0,\n                                      EltBits[0].getZExtValue(), DAG);\n  }\n\n  APInt KnownUndef, KnownZero;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  APInt DemandedElts = APInt::getAllOnesValue(VT.getVectorNumElements());\n  if (TLI.SimplifyDemandedVectorElts(SDValue(N, 0), DemandedElts, KnownUndef,\n                                     KnownZero, DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\nstatic SDValue combineVectorShiftImm(SDNode *N, SelectionDAG &DAG,\n                                     TargetLowering::DAGCombinerInfo &DCI,\n                                     const X86Subtarget &Subtarget) {\n  unsigned Opcode = N->getOpcode();\n  assert((X86ISD::VSHLI == Opcode || X86ISD::VSRAI == Opcode ||\n          X86ISD::VSRLI == Opcode) &&\n         \"Unexpected shift opcode\");\n  bool LogicalShift = X86ISD::VSHLI == Opcode || X86ISD::VSRLI == Opcode;\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  unsigned NumBitsPerElt = VT.getScalarSizeInBits();\n  assert(VT == N0.getValueType() && (NumBitsPerElt % 8) == 0 &&\n         \"Unexpected value type\");\n  assert(N->getOperand(1).getValueType() == MVT::i8 &&\n         \"Unexpected shift amount type\");\n\n  // Out of range logical bit shifts are guaranteed to be zero.\n  // Out of range arithmetic bit shifts splat the sign bit.\n  unsigned ShiftVal = N->getConstantOperandVal(1);\n  if (ShiftVal >= NumBitsPerElt) {\n    if (LogicalShift)\n      return DAG.getConstant(0, SDLoc(N), VT);\n    ShiftVal = NumBitsPerElt - 1;\n  }\n\n  // (shift X, 0) -> X\n  if (!ShiftVal)\n    return N0;\n\n  // (shift 0, C) -> 0\n  if (ISD::isBuildVectorAllZeros(N0.getNode()))\n    // N0 is all zeros or undef. We guarantee that the bits shifted into the\n    // result are all zeros, not undef.\n    return DAG.getConstant(0, SDLoc(N), VT);\n\n  // (VSRAI -1, C) -> -1\n  if (!LogicalShift && ISD::isBuildVectorAllOnes(N0.getNode()))\n    // N0 is all ones or undef. We guarantee that the bits shifted into the\n    // result are all ones, not undef.\n    return DAG.getConstant(-1, SDLoc(N), VT);\n\n  // (shift (shift X, C2), C1) -> (shift X, (C1 + C2))\n  if (Opcode == N0.getOpcode()) {\n    unsigned ShiftVal2 = cast<ConstantSDNode>(N0.getOperand(1))->getZExtValue();\n    unsigned NewShiftVal = ShiftVal + ShiftVal2;\n    if (NewShiftVal >= NumBitsPerElt) {\n      // Out of range logical bit shifts are guaranteed to be zero.\n      // Out of range arithmetic bit shifts splat the sign bit.\n      if (LogicalShift)\n        return DAG.getConstant(0, SDLoc(N), VT);\n      NewShiftVal = NumBitsPerElt - 1;\n    }\n    return DAG.getNode(Opcode, SDLoc(N), VT, N0.getOperand(0),\n                       DAG.getTargetConstant(NewShiftVal, SDLoc(N), MVT::i8));\n  }\n\n  // We can decode 'whole byte' logical bit shifts as shuffles.\n  if (LogicalShift && (ShiftVal % 8) == 0) {\n    SDValue Op(N, 0);\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n  }\n\n  // Constant Folding.\n  APInt UndefElts;\n  SmallVector<APInt, 32> EltBits;\n  if (N->isOnlyUserOf(N0.getNode()) &&\n      getTargetConstantBitsFromNode(N0, NumBitsPerElt, UndefElts, EltBits)) {\n    assert(EltBits.size() == VT.getVectorNumElements() &&\n           \"Unexpected shift value type\");\n    // Undef elements need to fold to 0. It's possible SimplifyDemandedBits\n    // created an undef input due to no input bits being demanded, but user\n    // still expects 0 in other bits.\n    for (unsigned i = 0, e = EltBits.size(); i != e; ++i) {\n      APInt &Elt = EltBits[i];\n      if (UndefElts[i])\n        Elt = 0;\n      else if (X86ISD::VSHLI == Opcode)\n        Elt <<= ShiftVal;\n      else if (X86ISD::VSRAI == Opcode)\n        Elt.ashrInPlace(ShiftVal);\n      else\n        Elt.lshrInPlace(ShiftVal);\n    }\n    // Reset undef elements since they were zeroed above.\n    UndefElts = 0;\n    return getConstVector(EltBits, UndefElts, VT.getSimpleVT(), DAG, SDLoc(N));\n  }\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0),\n                               APInt::getAllOnesValue(NumBitsPerElt), DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\nstatic SDValue combineVectorInsert(SDNode *N, SelectionDAG &DAG,\n                                   TargetLowering::DAGCombinerInfo &DCI,\n                                   const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  assert(((N->getOpcode() == X86ISD::PINSRB && VT == MVT::v16i8) ||\n          (N->getOpcode() == X86ISD::PINSRW && VT == MVT::v8i16) ||\n          N->getOpcode() == ISD::INSERT_VECTOR_ELT) &&\n         \"Unexpected vector insertion\");\n\n  if (N->getOpcode() == X86ISD::PINSRB || N->getOpcode() == X86ISD::PINSRW) {\n    unsigned NumBitsPerElt = VT.getScalarSizeInBits();\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    if (TLI.SimplifyDemandedBits(SDValue(N, 0),\n                                 APInt::getAllOnesValue(NumBitsPerElt), DCI))\n      return SDValue(N, 0);\n  }\n\n  // Attempt to combine insertion patterns to a shuffle.\n  if (VT.isSimple() && DCI.isAfterLegalizeDAG()) {\n    SDValue Op(N, 0);\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n  }\n\n  return SDValue();\n}\n\n/// Recognize the distinctive (AND (setcc ...) (setcc ..)) where both setccs\n/// reference the same FP CMP, and rewrite for CMPEQSS and friends. Likewise for\n/// OR -> CMPNEQSS.\nstatic SDValue combineCompareEqual(SDNode *N, SelectionDAG &DAG,\n                                   TargetLowering::DAGCombinerInfo &DCI,\n                                   const X86Subtarget &Subtarget) {\n  unsigned opcode;\n\n  // SSE1 supports CMP{eq|ne}SS, and SSE2 added CMP{eq|ne}SD, but\n  // we're requiring SSE2 for both.\n  if (Subtarget.hasSSE2() && isAndOrOfSetCCs(SDValue(N, 0U), opcode)) {\n    SDValue N0 = N->getOperand(0);\n    SDValue N1 = N->getOperand(1);\n    SDValue CMP0 = N0.getOperand(1);\n    SDValue CMP1 = N1.getOperand(1);\n    SDLoc DL(N);\n\n    // The SETCCs should both refer to the same CMP.\n    if (CMP0.getOpcode() != X86ISD::FCMP || CMP0 != CMP1)\n      return SDValue();\n\n    SDValue CMP00 = CMP0->getOperand(0);\n    SDValue CMP01 = CMP0->getOperand(1);\n    EVT     VT    = CMP00.getValueType();\n\n    if (VT == MVT::f32 || VT == MVT::f64) {\n      bool ExpectingFlags = false;\n      // Check for any users that want flags:\n      for (SDNode::use_iterator UI = N->use_begin(), UE = N->use_end();\n           !ExpectingFlags && UI != UE; ++UI)\n        switch (UI->getOpcode()) {\n        default:\n        case ISD::BR_CC:\n        case ISD::BRCOND:\n        case ISD::SELECT:\n          ExpectingFlags = true;\n          break;\n        case ISD::CopyToReg:\n        case ISD::SIGN_EXTEND:\n        case ISD::ZERO_EXTEND:\n        case ISD::ANY_EXTEND:\n          break;\n        }\n\n      if (!ExpectingFlags) {\n        enum X86::CondCode cc0 = (enum X86::CondCode)N0.getConstantOperandVal(0);\n        enum X86::CondCode cc1 = (enum X86::CondCode)N1.getConstantOperandVal(0);\n\n        if (cc1 == X86::COND_E || cc1 == X86::COND_NE) {\n          X86::CondCode tmp = cc0;\n          cc0 = cc1;\n          cc1 = tmp;\n        }\n\n        if ((cc0 == X86::COND_E  && cc1 == X86::COND_NP) ||\n            (cc0 == X86::COND_NE && cc1 == X86::COND_P)) {\n          // FIXME: need symbolic constants for these magic numbers.\n          // See X86ATTInstPrinter.cpp:printSSECC().\n          unsigned x86cc = (cc0 == X86::COND_E) ? 0 : 4;\n          if (Subtarget.hasAVX512()) {\n            SDValue FSetCC =\n                DAG.getNode(X86ISD::FSETCCM, DL, MVT::v1i1, CMP00, CMP01,\n                            DAG.getTargetConstant(x86cc, DL, MVT::i8));\n            // Need to fill with zeros to ensure the bitcast will produce zeroes\n            // for the upper bits. An EXTRACT_ELEMENT here wouldn't guarantee that.\n            SDValue Ins = DAG.getNode(ISD::INSERT_SUBVECTOR, DL, MVT::v16i1,\n                                      DAG.getConstant(0, DL, MVT::v16i1),\n                                      FSetCC, DAG.getIntPtrConstant(0, DL));\n            return DAG.getZExtOrTrunc(DAG.getBitcast(MVT::i16, Ins), DL,\n                                      N->getSimpleValueType(0));\n          }\n          SDValue OnesOrZeroesF =\n              DAG.getNode(X86ISD::FSETCC, DL, CMP00.getValueType(), CMP00,\n                          CMP01, DAG.getTargetConstant(x86cc, DL, MVT::i8));\n\n          bool is64BitFP = (CMP00.getValueType() == MVT::f64);\n          MVT IntVT = is64BitFP ? MVT::i64 : MVT::i32;\n\n          if (is64BitFP && !Subtarget.is64Bit()) {\n            // On a 32-bit target, we cannot bitcast the 64-bit float to a\n            // 64-bit integer, since that's not a legal type. Since\n            // OnesOrZeroesF is all ones of all zeroes, we don't need all the\n            // bits, but can do this little dance to extract the lowest 32 bits\n            // and work with those going forward.\n            SDValue Vector64 = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v2f64,\n                                           OnesOrZeroesF);\n            SDValue Vector32 = DAG.getBitcast(MVT::v4f32, Vector64);\n            OnesOrZeroesF = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, MVT::f32,\n                                        Vector32, DAG.getIntPtrConstant(0, DL));\n            IntVT = MVT::i32;\n          }\n\n          SDValue OnesOrZeroesI = DAG.getBitcast(IntVT, OnesOrZeroesF);\n          SDValue ANDed = DAG.getNode(ISD::AND, DL, IntVT, OnesOrZeroesI,\n                                      DAG.getConstant(1, DL, IntVT));\n          SDValue OneBitOfTruth = DAG.getNode(ISD::TRUNCATE, DL, MVT::i8,\n                                              ANDed);\n          return OneBitOfTruth;\n        }\n      }\n    }\n  }\n  return SDValue();\n}\n\n/// Try to fold: (and (xor X, -1), Y) -> (andnp X, Y).\nstatic SDValue combineANDXORWithAllOnesIntoANDNP(SDNode *N, SelectionDAG &DAG) {\n  assert(N->getOpcode() == ISD::AND);\n\n  MVT VT = N->getSimpleValueType(0);\n  if (!VT.is128BitVector() && !VT.is256BitVector() && !VT.is512BitVector())\n    return SDValue();\n\n  SDValue X, Y;\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  auto GetNot = [&VT, &DAG](SDValue V) {\n    // Basic X = NOT(Y) detection.\n    if (SDValue Not = IsNOT(V, DAG))\n      return Not;\n    // Fold BROADCAST(NOT(Y)) -> BROADCAST(Y).\n    if (V.getOpcode() == X86ISD::VBROADCAST) {\n      SDValue Src = V.getOperand(0);\n      EVT SrcVT = Src.getValueType();\n      if (!SrcVT.isVector())\n        return SDValue();\n      if (SDValue Not = IsNOT(Src, DAG))\n        return DAG.getNode(X86ISD::VBROADCAST, SDLoc(V), VT,\n                           DAG.getBitcast(SrcVT, Not));\n    }\n    return SDValue();\n  };\n\n  if (SDValue Not = GetNot(N0)) {\n    X = Not;\n    Y = N1;\n  } else if (SDValue Not = GetNot(N1)) {\n    X = Not;\n    Y = N0;\n  } else\n    return SDValue();\n\n  X = DAG.getBitcast(VT, X);\n  Y = DAG.getBitcast(VT, Y);\n  return DAG.getNode(X86ISD::ANDNP, SDLoc(N), VT, X, Y);\n}\n\n// Try to widen AND, OR and XOR nodes to VT in order to remove casts around\n// logical operations, like in the example below.\n//   or (and (truncate x, truncate y)),\n//      (xor (truncate z, build_vector (constants)))\n// Given a target type \\p VT, we generate\n//   or (and x, y), (xor z, zext(build_vector (constants)))\n// given x, y and z are of type \\p VT. We can do so, if operands are either\n// truncates from VT types, the second operand is a vector of constants or can\n// be recursively promoted.\nstatic SDValue PromoteMaskArithmetic(SDNode *N, EVT VT, SelectionDAG &DAG,\n                                     unsigned Depth) {\n  // Limit recursion to avoid excessive compile times.\n  if (Depth >= SelectionDAG::MaxRecursionDepth)\n    return SDValue();\n\n  if (N->getOpcode() != ISD::XOR && N->getOpcode() != ISD::AND &&\n      N->getOpcode() != ISD::OR)\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  SDLoc DL(N);\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!TLI.isOperationLegalOrPromote(N->getOpcode(), VT))\n    return SDValue();\n\n  if (SDValue NN0 = PromoteMaskArithmetic(N0.getNode(), VT, DAG, Depth + 1))\n    N0 = NN0;\n  else {\n    // The Left side has to be a trunc.\n    if (N0.getOpcode() != ISD::TRUNCATE)\n      return SDValue();\n\n    // The type of the truncated inputs.\n    if (N0.getOperand(0).getValueType() != VT)\n      return SDValue();\n\n    N0 = N0.getOperand(0);\n  }\n\n  if (SDValue NN1 = PromoteMaskArithmetic(N1.getNode(), VT, DAG, Depth + 1))\n    N1 = NN1;\n  else {\n    // The right side has to be a 'trunc' or a constant vector.\n    bool RHSTrunc = N1.getOpcode() == ISD::TRUNCATE &&\n                    N1.getOperand(0).getValueType() == VT;\n    if (!RHSTrunc && !ISD::isBuildVectorOfConstantSDNodes(N1.getNode()))\n      return SDValue();\n\n    if (RHSTrunc)\n      N1 = N1.getOperand(0);\n    else\n      N1 = DAG.getNode(ISD::ZERO_EXTEND, DL, VT, N1);\n  }\n\n  return DAG.getNode(N->getOpcode(), DL, VT, N0, N1);\n}\n\n// On AVX/AVX2 the type v8i1 is legalized to v8i16, which is an XMM sized\n// register. In most cases we actually compare or select YMM-sized registers\n// and mixing the two types creates horrible code. This method optimizes\n// some of the transition sequences.\n// Even with AVX-512 this is still useful for removing casts around logical\n// operations on vXi1 mask types.\nstatic SDValue PromoteMaskArithmetic(SDNode *N, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  assert(VT.isVector() && \"Expected vector type\");\n\n  SDLoc DL(N);\n  assert((N->getOpcode() == ISD::ANY_EXTEND ||\n          N->getOpcode() == ISD::ZERO_EXTEND ||\n          N->getOpcode() == ISD::SIGN_EXTEND) && \"Invalid Node\");\n\n  SDValue Narrow = N->getOperand(0);\n  EVT NarrowVT = Narrow.getValueType();\n\n  // Generate the wide operation.\n  SDValue Op = PromoteMaskArithmetic(Narrow.getNode(), VT, DAG, 0);\n  if (!Op)\n    return SDValue();\n  switch (N->getOpcode()) {\n  default: llvm_unreachable(\"Unexpected opcode\");\n  case ISD::ANY_EXTEND:\n    return Op;\n  case ISD::ZERO_EXTEND:\n    return DAG.getZeroExtendInReg(Op, DL, NarrowVT);\n  case ISD::SIGN_EXTEND:\n    return DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, VT,\n                       Op, DAG.getValueType(NarrowVT));\n  }\n}\n\nstatic unsigned convertIntLogicToFPLogicOpcode(unsigned Opcode) {\n  unsigned FPOpcode;\n  switch (Opcode) {\n  default: llvm_unreachable(\"Unexpected input node for FP logic conversion\");\n  case ISD::AND: FPOpcode = X86ISD::FAND; break;\n  case ISD::OR:  FPOpcode = X86ISD::FOR;  break;\n  case ISD::XOR: FPOpcode = X86ISD::FXOR; break;\n  }\n  return FPOpcode;\n}\n\n/// If both input operands of a logic op are being cast from floating point\n/// types, try to convert this into a floating point logic node to avoid\n/// unnecessary moves from SSE to integer registers.\nstatic SDValue convertIntLogicToFPLogic(SDNode *N, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  SDLoc DL(N);\n\n  if (N0.getOpcode() != ISD::BITCAST || N1.getOpcode() != ISD::BITCAST)\n    return SDValue();\n\n  SDValue N00 = N0.getOperand(0);\n  SDValue N10 = N1.getOperand(0);\n  EVT N00Type = N00.getValueType();\n  EVT N10Type = N10.getValueType();\n\n  // Ensure that both types are the same and are legal scalar fp types.\n  if (N00Type != N10Type ||\n      !((Subtarget.hasSSE1() && N00Type == MVT::f32) ||\n        (Subtarget.hasSSE2() && N00Type == MVT::f64)))\n    return SDValue();\n\n  unsigned FPOpcode = convertIntLogicToFPLogicOpcode(N->getOpcode());\n  SDValue FPLogic = DAG.getNode(FPOpcode, DL, N00Type, N00, N10);\n  return DAG.getBitcast(VT, FPLogic);\n}\n\n// Attempt to fold BITOP(MOVMSK(X),MOVMSK(Y)) -> MOVMSK(BITOP(X,Y))\n// to reduce XMM->GPR traffic.\nstatic SDValue combineBitOpWithMOVMSK(SDNode *N, SelectionDAG &DAG) {\n  unsigned Opc = N->getOpcode();\n  assert((Opc == ISD::OR || Opc == ISD::AND || Opc == ISD::XOR) &&\n         \"Unexpected bit opcode\");\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // Both operands must be single use MOVMSK.\n  if (N0.getOpcode() != X86ISD::MOVMSK || !N0.hasOneUse() ||\n      N1.getOpcode() != X86ISD::MOVMSK || !N1.hasOneUse())\n    return SDValue();\n\n  SDValue Vec0 = N0.getOperand(0);\n  SDValue Vec1 = N1.getOperand(0);\n  EVT VecVT0 = Vec0.getValueType();\n  EVT VecVT1 = Vec1.getValueType();\n\n  // Both MOVMSK operands must be from vectors of the same size and same element\n  // size, but its OK for a fp/int diff.\n  if (VecVT0.getSizeInBits() != VecVT1.getSizeInBits() ||\n      VecVT0.getScalarSizeInBits() != VecVT1.getScalarSizeInBits())\n    return SDValue();\n\n  SDLoc DL(N);\n  unsigned VecOpc =\n      VecVT0.isFloatingPoint() ? convertIntLogicToFPLogicOpcode(Opc) : Opc;\n  SDValue Result =\n      DAG.getNode(VecOpc, DL, VecVT0, Vec0, DAG.getBitcast(VecVT0, Vec1));\n  return DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Result);\n}\n\n/// If this is a zero/all-bits result that is bitwise-anded with a low bits\n/// mask. (Mask == 1 for the x86 lowering of a SETCC + ZEXT), replace the 'and'\n/// with a shift-right to eliminate loading the vector constant mask value.\nstatic SDValue combineAndMaskToShift(SDNode *N, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  SDValue Op0 = peekThroughBitcasts(N->getOperand(0));\n  SDValue Op1 = peekThroughBitcasts(N->getOperand(1));\n  EVT VT0 = Op0.getValueType();\n  EVT VT1 = Op1.getValueType();\n\n  if (VT0 != VT1 || !VT0.isSimple() || !VT0.isInteger())\n    return SDValue();\n\n  APInt SplatVal;\n  if (!ISD::isConstantSplatVector(Op1.getNode(), SplatVal) ||\n      !SplatVal.isMask())\n    return SDValue();\n\n  // Don't prevent creation of ANDN.\n  if (isBitwiseNot(Op0))\n    return SDValue();\n\n  if (!SupportedVectorShiftWithImm(VT0.getSimpleVT(), Subtarget, ISD::SRL))\n    return SDValue();\n\n  unsigned EltBitWidth = VT0.getScalarSizeInBits();\n  if (EltBitWidth != DAG.ComputeNumSignBits(Op0))\n    return SDValue();\n\n  SDLoc DL(N);\n  unsigned ShiftVal = SplatVal.countTrailingOnes();\n  SDValue ShAmt = DAG.getTargetConstant(EltBitWidth - ShiftVal, DL, MVT::i8);\n  SDValue Shift = DAG.getNode(X86ISD::VSRLI, DL, VT0, Op0, ShAmt);\n  return DAG.getBitcast(N->getValueType(0), Shift);\n}\n\n// Get the index node from the lowered DAG of a GEP IR instruction with one\n// indexing dimension.\nstatic SDValue getIndexFromUnindexedLoad(LoadSDNode *Ld) {\n  if (Ld->isIndexed())\n    return SDValue();\n\n  SDValue Base = Ld->getBasePtr();\n\n  if (Base.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  SDValue ShiftedIndex = Base.getOperand(0);\n\n  if (ShiftedIndex.getOpcode() != ISD::SHL)\n    return SDValue();\n\n  return ShiftedIndex.getOperand(0);\n\n}\n\nstatic bool hasBZHI(const X86Subtarget &Subtarget, MVT VT) {\n  if (Subtarget.hasBMI2() && VT.isScalarInteger()) {\n    switch (VT.getSizeInBits()) {\n    default: return false;\n    case 64: return Subtarget.is64Bit() ? true : false;\n    case 32: return true;\n    }\n  }\n  return false;\n}\n\n// This function recognizes cases where X86 bzhi instruction can replace and\n// 'and-load' sequence.\n// In case of loading integer value from an array of constants which is defined\n// as follows:\n//\n//   int array[SIZE] = {0x0, 0x1, 0x3, 0x7, 0xF ..., 2^(SIZE-1) - 1}\n//\n// then applying a bitwise and on the result with another input.\n// It's equivalent to performing bzhi (zero high bits) on the input, with the\n// same index of the load.\nstatic SDValue combineAndLoadToBZHI(SDNode *Node, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  MVT VT = Node->getSimpleValueType(0);\n  SDLoc dl(Node);\n\n  // Check if subtarget has BZHI instruction for the node's type\n  if (!hasBZHI(Subtarget, VT))\n    return SDValue();\n\n  // Try matching the pattern for both operands.\n  for (unsigned i = 0; i < 2; i++) {\n    SDValue N = Node->getOperand(i);\n    LoadSDNode *Ld = dyn_cast<LoadSDNode>(N.getNode());\n\n     // continue if the operand is not a load instruction\n    if (!Ld)\n      return SDValue();\n\n    const Value *MemOp = Ld->getMemOperand()->getValue();\n\n    if (!MemOp)\n      return SDValue();\n\n    if (const GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(MemOp)) {\n      if (GlobalVariable *GV = dyn_cast<GlobalVariable>(GEP->getOperand(0))) {\n        if (GV->isConstant() && GV->hasDefinitiveInitializer()) {\n\n          Constant *Init = GV->getInitializer();\n          Type *Ty = Init->getType();\n          if (!isa<ConstantDataArray>(Init) ||\n              !Ty->getArrayElementType()->isIntegerTy() ||\n              Ty->getArrayElementType()->getScalarSizeInBits() !=\n                  VT.getSizeInBits() ||\n              Ty->getArrayNumElements() >\n                  Ty->getArrayElementType()->getScalarSizeInBits())\n            continue;\n\n          // Check if the array's constant elements are suitable to our case.\n          uint64_t ArrayElementCount = Init->getType()->getArrayNumElements();\n          bool ConstantsMatch = true;\n          for (uint64_t j = 0; j < ArrayElementCount; j++) {\n            ConstantInt *Elem =\n                dyn_cast<ConstantInt>(Init->getAggregateElement(j));\n            if (Elem->getZExtValue() != (((uint64_t)1 << j) - 1)) {\n              ConstantsMatch = false;\n              break;\n            }\n          }\n          if (!ConstantsMatch)\n            continue;\n\n          // Do the transformation (For 32-bit type):\n          // -> (and (load arr[idx]), inp)\n          // <- (and (srl 0xFFFFFFFF, (sub 32, idx)))\n          //    that will be replaced with one bzhi instruction.\n          SDValue Inp = (i == 0) ? Node->getOperand(1) : Node->getOperand(0);\n          SDValue SizeC = DAG.getConstant(VT.getSizeInBits(), dl, MVT::i32);\n\n          // Get the Node which indexes into the array.\n          SDValue Index = getIndexFromUnindexedLoad(Ld);\n          if (!Index)\n            return SDValue();\n          Index = DAG.getZExtOrTrunc(Index, dl, MVT::i32);\n\n          SDValue Sub = DAG.getNode(ISD::SUB, dl, MVT::i32, SizeC, Index);\n          Sub = DAG.getNode(ISD::TRUNCATE, dl, MVT::i8, Sub);\n\n          SDValue AllOnes = DAG.getAllOnesConstant(dl, VT);\n          SDValue LShr = DAG.getNode(ISD::SRL, dl, VT, AllOnes, Sub);\n\n          return DAG.getNode(ISD::AND, dl, VT, Inp, LShr);\n        }\n      }\n    }\n  }\n  return SDValue();\n}\n\n// Look for (and (bitcast (vXi1 (concat_vectors (vYi1 setcc), undef,))), C)\n// Where C is a mask containing the same number of bits as the setcc and\n// where the setcc will freely 0 upper bits of k-register. We can replace the\n// undef in the concat with 0s and remove the AND. This mainly helps with\n// v2i1/v4i1 setcc being casted to scalar.\nstatic SDValue combineScalarAndWithMaskSetcc(SDNode *N, SelectionDAG &DAG,\n                                             const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == ISD::AND && \"Unexpected opcode!\");\n\n  EVT VT = N->getValueType(0);\n\n  // Make sure this is an AND with constant. We will check the value of the\n  // constant later.\n  if (!isa<ConstantSDNode>(N->getOperand(1)))\n    return SDValue();\n\n  // This is implied by the ConstantSDNode.\n  assert(!VT.isVector() && \"Expected scalar VT!\");\n\n  if (N->getOperand(0).getOpcode() != ISD::BITCAST ||\n      !N->getOperand(0).hasOneUse() ||\n      !N->getOperand(0).getOperand(0).hasOneUse())\n    return SDValue();\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  SDValue Src = N->getOperand(0).getOperand(0);\n  EVT SrcVT = Src.getValueType();\n  if (!SrcVT.isVector() || SrcVT.getVectorElementType() != MVT::i1 ||\n      !TLI.isTypeLegal(SrcVT))\n    return SDValue();\n\n  if (Src.getOpcode() != ISD::CONCAT_VECTORS)\n    return SDValue();\n\n  // We only care about the first subvector of the concat, we expect the\n  // other subvectors to be ignored due to the AND if we make the change.\n  SDValue SubVec = Src.getOperand(0);\n  EVT SubVecVT = SubVec.getValueType();\n\n  // First subvector should be a setcc with a legal result type. The RHS of the\n  // AND should be a mask with this many bits.\n  if (SubVec.getOpcode() != ISD::SETCC || !TLI.isTypeLegal(SubVecVT) ||\n      !N->getConstantOperandAPInt(1).isMask(SubVecVT.getVectorNumElements()))\n    return SDValue();\n\n  EVT SetccVT = SubVec.getOperand(0).getValueType();\n  if (!TLI.isTypeLegal(SetccVT) ||\n      !(Subtarget.hasVLX() || SetccVT.is512BitVector()))\n    return SDValue();\n\n  if (!(Subtarget.hasBWI() || SetccVT.getScalarSizeInBits() >= 32))\n    return SDValue();\n\n  // We passed all the checks. Rebuild the concat_vectors with zeroes\n  // and cast it back to VT.\n  SDLoc dl(N);\n  SmallVector<SDValue, 4> Ops(Src.getNumOperands(),\n                              DAG.getConstant(0, dl, SubVecVT));\n  Ops[0] = SubVec;\n  SDValue Concat = DAG.getNode(ISD::CONCAT_VECTORS, dl, SrcVT,\n                               Ops);\n  return DAG.getBitcast(VT, Concat);\n}\n\nstatic SDValue combineAnd(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n\n  // If this is SSE1 only convert to FAND to avoid scalarization.\n  if (Subtarget.hasSSE1() && !Subtarget.hasSSE2() && VT == MVT::v4i32) {\n    return DAG.getBitcast(\n        MVT::v4i32, DAG.getNode(X86ISD::FAND, SDLoc(N), MVT::v4f32,\n                                DAG.getBitcast(MVT::v4f32, N->getOperand(0)),\n                                DAG.getBitcast(MVT::v4f32, N->getOperand(1))));\n  }\n\n  // Use a 32-bit and+zext if upper bits known zero.\n  if (VT == MVT::i64 && Subtarget.is64Bit() &&\n      !isa<ConstantSDNode>(N->getOperand(1))) {\n    APInt HiMask = APInt::getHighBitsSet(64, 32);\n    if (DAG.MaskedValueIsZero(N->getOperand(1), HiMask) ||\n        DAG.MaskedValueIsZero(N->getOperand(0), HiMask)) {\n      SDLoc dl(N);\n      SDValue LHS = DAG.getNode(ISD::TRUNCATE, dl, MVT::i32, N->getOperand(0));\n      SDValue RHS = DAG.getNode(ISD::TRUNCATE, dl, MVT::i32, N->getOperand(1));\n      return DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i64,\n                         DAG.getNode(ISD::AND, dl, MVT::i32, LHS, RHS));\n    }\n  }\n\n  // Match all-of bool scalar reductions into a bitcast/movmsk + cmp.\n  // TODO: Support multiple SrcOps.\n  if (VT == MVT::i1) {\n    SmallVector<SDValue, 2> SrcOps;\n    SmallVector<APInt, 2> SrcPartials;\n    if (matchScalarReduction(SDValue(N, 0), ISD::AND, SrcOps, &SrcPartials) &&\n        SrcOps.size() == 1) {\n      SDLoc dl(N);\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      unsigned NumElts = SrcOps[0].getValueType().getVectorNumElements();\n      EVT MaskVT = EVT::getIntegerVT(*DAG.getContext(), NumElts);\n      SDValue Mask = combineBitcastvxi1(DAG, MaskVT, SrcOps[0], dl, Subtarget);\n      if (!Mask && TLI.isTypeLegal(SrcOps[0].getValueType()))\n        Mask = DAG.getBitcast(MaskVT, SrcOps[0]);\n      if (Mask) {\n        assert(SrcPartials[0].getBitWidth() == NumElts &&\n               \"Unexpected partial reduction mask\");\n        SDValue PartialBits = DAG.getConstant(SrcPartials[0], dl, MaskVT);\n        Mask = DAG.getNode(ISD::AND, dl, MaskVT, Mask, PartialBits);\n        return DAG.getSetCC(dl, MVT::i1, Mask, PartialBits, ISD::SETEQ);\n      }\n    }\n  }\n\n  if (SDValue V = combineScalarAndWithMaskSetcc(N, DAG, Subtarget))\n    return V;\n\n  if (SDValue R = combineBitOpWithMOVMSK(N, DAG))\n    return R;\n\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  if (SDValue R = combineCompareEqual(N, DAG, DCI, Subtarget))\n    return R;\n\n  if (SDValue FPLogic = convertIntLogicToFPLogic(N, DAG, Subtarget))\n    return FPLogic;\n\n  if (SDValue R = combineANDXORWithAllOnesIntoANDNP(N, DAG))\n    return R;\n\n  if (SDValue ShiftRight = combineAndMaskToShift(N, DAG, Subtarget))\n    return ShiftRight;\n\n  if (SDValue R = combineAndLoadToBZHI(N, DAG, Subtarget))\n    return R;\n\n  // Attempt to recursively combine a bitmask AND with shuffles.\n  if (VT.isVector() && (VT.getScalarSizeInBits() % 8) == 0) {\n    SDValue Op(N, 0);\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n  }\n\n  // Attempt to combine a scalar bitmask AND with an extracted shuffle.\n  if ((VT.getScalarSizeInBits() % 8) == 0 &&\n      N->getOperand(0).getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n      isa<ConstantSDNode>(N->getOperand(0).getOperand(1))) {\n    SDValue BitMask = N->getOperand(1);\n    SDValue SrcVec = N->getOperand(0).getOperand(0);\n    EVT SrcVecVT = SrcVec.getValueType();\n\n    // Check that the constant bitmask masks whole bytes.\n    APInt UndefElts;\n    SmallVector<APInt, 64> EltBits;\n    if (VT == SrcVecVT.getScalarType() &&\n        N->getOperand(0)->isOnlyUserOf(SrcVec.getNode()) &&\n        getTargetConstantBitsFromNode(BitMask, 8, UndefElts, EltBits) &&\n        llvm::all_of(EltBits, [](const APInt &M) {\n          return M.isNullValue() || M.isAllOnesValue();\n        })) {\n      unsigned NumElts = SrcVecVT.getVectorNumElements();\n      unsigned Scale = SrcVecVT.getScalarSizeInBits() / 8;\n      unsigned Idx = N->getOperand(0).getConstantOperandVal(1);\n\n      // Create a root shuffle mask from the byte mask and the extracted index.\n      SmallVector<int, 16> ShuffleMask(NumElts * Scale, SM_SentinelUndef);\n      for (unsigned i = 0; i != Scale; ++i) {\n        if (UndefElts[i])\n          continue;\n        int VecIdx = Scale * Idx + i;\n        ShuffleMask[VecIdx] =\n            EltBits[i].isNullValue() ? SM_SentinelZero : VecIdx;\n      }\n\n      if (SDValue Shuffle = combineX86ShufflesRecursively(\n              {SrcVec}, 0, SrcVec, ShuffleMask, {}, /*Depth*/ 1,\n              X86::MaxShuffleCombineDepth,\n              /*HasVarMask*/ false, /*AllowVarMask*/ true, DAG, Subtarget))\n        return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, SDLoc(N), VT, Shuffle,\n                           N->getOperand(0).getOperand(1));\n    }\n  }\n\n  return SDValue();\n}\n\n// Canonicalize OR(AND(X,C),AND(Y,~C)) -> OR(AND(X,C),ANDNP(C,Y))\nstatic SDValue canonicalizeBitSelect(SDNode *N, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == ISD::OR && \"Unexpected Opcode\");\n\n  MVT VT = N->getSimpleValueType(0);\n  if (!VT.isVector() || (VT.getScalarSizeInBits() % 8) != 0)\n    return SDValue();\n\n  SDValue N0 = peekThroughBitcasts(N->getOperand(0));\n  SDValue N1 = peekThroughBitcasts(N->getOperand(1));\n  if (N0.getOpcode() != ISD::AND || N1.getOpcode() != ISD::AND)\n    return SDValue();\n\n  // On XOP we'll lower to PCMOV so accept one use. With AVX512, we can use\n  // VPTERNLOG. Otherwise only do this if either mask has multiple uses already.\n  bool UseVPTERNLOG = (Subtarget.hasAVX512() && VT.is512BitVector()) ||\n                      Subtarget.hasVLX();\n  if (!(Subtarget.hasXOP() || UseVPTERNLOG ||\n        !N0.getOperand(1).hasOneUse() || !N1.getOperand(1).hasOneUse()))\n    return SDValue();\n\n  // Attempt to extract constant byte masks.\n  APInt UndefElts0, UndefElts1;\n  SmallVector<APInt, 32> EltBits0, EltBits1;\n  if (!getTargetConstantBitsFromNode(N0.getOperand(1), 8, UndefElts0, EltBits0,\n                                     false, false))\n    return SDValue();\n  if (!getTargetConstantBitsFromNode(N1.getOperand(1), 8, UndefElts1, EltBits1,\n                                     false, false))\n    return SDValue();\n\n  for (unsigned i = 0, e = EltBits0.size(); i != e; ++i) {\n    // TODO - add UNDEF elts support.\n    if (UndefElts0[i] || UndefElts1[i])\n      return SDValue();\n    if (EltBits0[i] != ~EltBits1[i])\n      return SDValue();\n  }\n\n  SDLoc DL(N);\n\n  if (UseVPTERNLOG) {\n    // Emit a VPTERNLOG node directly.\n    SDValue A = DAG.getBitcast(VT, N0.getOperand(1));\n    SDValue B = DAG.getBitcast(VT, N0.getOperand(0));\n    SDValue C = DAG.getBitcast(VT, N1.getOperand(0));\n    SDValue Imm = DAG.getTargetConstant(0xCA, DL, MVT::i8);\n    return DAG.getNode(X86ISD::VPTERNLOG, DL, VT, A, B, C, Imm);\n  }\n\n  SDValue X = N->getOperand(0);\n  SDValue Y =\n      DAG.getNode(X86ISD::ANDNP, DL, VT, DAG.getBitcast(VT, N0.getOperand(1)),\n                  DAG.getBitcast(VT, N1.getOperand(0)));\n  return DAG.getNode(ISD::OR, DL, VT, X, Y);\n}\n\n// Try to match OR(AND(~MASK,X),AND(MASK,Y)) logic pattern.\nstatic bool matchLogicBlend(SDNode *N, SDValue &X, SDValue &Y, SDValue &Mask) {\n  if (N->getOpcode() != ISD::OR)\n    return false;\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // Canonicalize AND to LHS.\n  if (N1.getOpcode() == ISD::AND)\n    std::swap(N0, N1);\n\n  // Attempt to match OR(AND(M,Y),ANDNP(M,X)).\n  if (N0.getOpcode() != ISD::AND || N1.getOpcode() != X86ISD::ANDNP)\n    return false;\n\n  Mask = N1.getOperand(0);\n  X = N1.getOperand(1);\n\n  // Check to see if the mask appeared in both the AND and ANDNP.\n  if (N0.getOperand(0) == Mask)\n    Y = N0.getOperand(1);\n  else if (N0.getOperand(1) == Mask)\n    Y = N0.getOperand(0);\n  else\n    return false;\n\n  // TODO: Attempt to match against AND(XOR(-1,M),Y) as well, waiting for\n  // ANDNP combine allows other combines to happen that prevent matching.\n  return true;\n}\n\n// Try to fold:\n//   (or (and (m, y), (pandn m, x)))\n// into:\n//   (vselect m, x, y)\n// As a special case, try to fold:\n//   (or (and (m, (sub 0, x)), (pandn m, x)))\n// into:\n//   (sub (xor X, M), M)\nstatic SDValue combineLogicBlendIntoPBLENDV(SDNode *N, SelectionDAG &DAG,\n                                            const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == ISD::OR && \"Unexpected Opcode\");\n\n  EVT VT = N->getValueType(0);\n  if (!((VT.is128BitVector() && Subtarget.hasSSE2()) ||\n        (VT.is256BitVector() && Subtarget.hasInt256())))\n    return SDValue();\n\n  SDValue X, Y, Mask;\n  if (!matchLogicBlend(N, X, Y, Mask))\n    return SDValue();\n\n  // Validate that X, Y, and Mask are bitcasts, and see through them.\n  Mask = peekThroughBitcasts(Mask);\n  X = peekThroughBitcasts(X);\n  Y = peekThroughBitcasts(Y);\n\n  EVT MaskVT = Mask.getValueType();\n  unsigned EltBits = MaskVT.getScalarSizeInBits();\n\n  // TODO: Attempt to handle floating point cases as well?\n  if (!MaskVT.isInteger() || DAG.ComputeNumSignBits(Mask) != EltBits)\n    return SDValue();\n\n  SDLoc DL(N);\n\n  // Attempt to combine to conditional negate: (sub (xor X, M), M)\n  if (SDValue Res = combineLogicBlendIntoConditionalNegate(VT, Mask, X, Y, DL,\n                                                           DAG, Subtarget))\n    return Res;\n\n  // PBLENDVB is only available on SSE 4.1.\n  if (!Subtarget.hasSSE41())\n    return SDValue();\n\n  // If we have VPTERNLOG we should prefer that since PBLENDVB is multiple uops.\n  if (Subtarget.hasVLX())\n    return SDValue();\n\n  MVT BlendVT = VT.is256BitVector() ? MVT::v32i8 : MVT::v16i8;\n\n  X = DAG.getBitcast(BlendVT, X);\n  Y = DAG.getBitcast(BlendVT, Y);\n  Mask = DAG.getBitcast(BlendVT, Mask);\n  Mask = DAG.getSelect(DL, BlendVT, Mask, Y, X);\n  return DAG.getBitcast(VT, Mask);\n}\n\n// Helper function for combineOrCmpEqZeroToCtlzSrl\n// Transforms:\n//   seteq(cmp x, 0)\n//   into:\n//   srl(ctlz x), log2(bitsize(x))\n// Input pattern is checked by caller.\nstatic SDValue lowerX86CmpEqZeroToCtlzSrl(SDValue Op, EVT ExtTy,\n                                          SelectionDAG &DAG) {\n  SDValue Cmp = Op.getOperand(1);\n  EVT VT = Cmp.getOperand(0).getValueType();\n  unsigned Log2b = Log2_32(VT.getSizeInBits());\n  SDLoc dl(Op);\n  SDValue Clz = DAG.getNode(ISD::CTLZ, dl, VT, Cmp->getOperand(0));\n  // The result of the shift is true or false, and on X86, the 32-bit\n  // encoding of shr and lzcnt is more desirable.\n  SDValue Trunc = DAG.getZExtOrTrunc(Clz, dl, MVT::i32);\n  SDValue Scc = DAG.getNode(ISD::SRL, dl, MVT::i32, Trunc,\n                            DAG.getConstant(Log2b, dl, MVT::i8));\n  return DAG.getZExtOrTrunc(Scc, dl, ExtTy);\n}\n\n// Try to transform:\n//   zext(or(setcc(eq, (cmp x, 0)), setcc(eq, (cmp y, 0))))\n//   into:\n//   srl(or(ctlz(x), ctlz(y)), log2(bitsize(x))\n// Will also attempt to match more generic cases, eg:\n//   zext(or(or(setcc(eq, cmp 0), setcc(eq, cmp 0)), setcc(eq, cmp 0)))\n// Only applies if the target supports the FastLZCNT feature.\nstatic SDValue combineOrCmpEqZeroToCtlzSrl(SDNode *N, SelectionDAG &DAG,\n                                           TargetLowering::DAGCombinerInfo &DCI,\n                                           const X86Subtarget &Subtarget) {\n  if (DCI.isBeforeLegalize() || !Subtarget.getTargetLowering()->isCtlzFast())\n    return SDValue();\n\n  auto isORCandidate = [](SDValue N) {\n    return (N->getOpcode() == ISD::OR && N->hasOneUse());\n  };\n\n  // Check the zero extend is extending to 32-bit or more. The code generated by\n  // srl(ctlz) for 16-bit or less variants of the pattern would require extra\n  // instructions to clear the upper bits.\n  if (!N->hasOneUse() || !N->getSimpleValueType(0).bitsGE(MVT::i32) ||\n      !isORCandidate(N->getOperand(0)))\n    return SDValue();\n\n  // Check the node matches: setcc(eq, cmp 0)\n  auto isSetCCCandidate = [](SDValue N) {\n    return N->getOpcode() == X86ISD::SETCC && N->hasOneUse() &&\n           X86::CondCode(N->getConstantOperandVal(0)) == X86::COND_E &&\n           N->getOperand(1).getOpcode() == X86ISD::CMP &&\n           isNullConstant(N->getOperand(1).getOperand(1)) &&\n           N->getOperand(1).getValueType().bitsGE(MVT::i32);\n  };\n\n  SDNode *OR = N->getOperand(0).getNode();\n  SDValue LHS = OR->getOperand(0);\n  SDValue RHS = OR->getOperand(1);\n\n  // Save nodes matching or(or, setcc(eq, cmp 0)).\n  SmallVector<SDNode *, 2> ORNodes;\n  while (((isORCandidate(LHS) && isSetCCCandidate(RHS)) ||\n          (isORCandidate(RHS) && isSetCCCandidate(LHS)))) {\n    ORNodes.push_back(OR);\n    OR = (LHS->getOpcode() == ISD::OR) ? LHS.getNode() : RHS.getNode();\n    LHS = OR->getOperand(0);\n    RHS = OR->getOperand(1);\n  }\n\n  // The last OR node should match or(setcc(eq, cmp 0), setcc(eq, cmp 0)).\n  if (!(isSetCCCandidate(LHS) && isSetCCCandidate(RHS)) ||\n      !isORCandidate(SDValue(OR, 0)))\n    return SDValue();\n\n  // We have a or(setcc(eq, cmp 0), setcc(eq, cmp 0)) pattern, try to lower it\n  // to\n  // or(srl(ctlz),srl(ctlz)).\n  // The dag combiner can then fold it into:\n  // srl(or(ctlz, ctlz)).\n  EVT VT = OR->getValueType(0);\n  SDValue NewLHS = lowerX86CmpEqZeroToCtlzSrl(LHS, VT, DAG);\n  SDValue Ret, NewRHS;\n  if (NewLHS && (NewRHS = lowerX86CmpEqZeroToCtlzSrl(RHS, VT, DAG)))\n    Ret = DAG.getNode(ISD::OR, SDLoc(OR), VT, NewLHS, NewRHS);\n\n  if (!Ret)\n    return SDValue();\n\n  // Try to lower nodes matching the or(or, setcc(eq, cmp 0)) pattern.\n  while (ORNodes.size() > 0) {\n    OR = ORNodes.pop_back_val();\n    LHS = OR->getOperand(0);\n    RHS = OR->getOperand(1);\n    // Swap rhs with lhs to match or(setcc(eq, cmp, 0), or).\n    if (RHS->getOpcode() == ISD::OR)\n      std::swap(LHS, RHS);\n    NewRHS = lowerX86CmpEqZeroToCtlzSrl(RHS, VT, DAG);\n    if (!NewRHS)\n      return SDValue();\n    Ret = DAG.getNode(ISD::OR, SDLoc(OR), VT, Ret, NewRHS);\n  }\n\n  if (Ret)\n    Ret = DAG.getNode(ISD::ZERO_EXTEND, SDLoc(N), N->getValueType(0), Ret);\n\n  return Ret;\n}\n\nstatic SDValue combineOr(SDNode *N, SelectionDAG &DAG,\n                         TargetLowering::DAGCombinerInfo &DCI,\n                         const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n\n  // If this is SSE1 only convert to FOR to avoid scalarization.\n  if (Subtarget.hasSSE1() && !Subtarget.hasSSE2() && VT == MVT::v4i32) {\n    return DAG.getBitcast(MVT::v4i32,\n                          DAG.getNode(X86ISD::FOR, SDLoc(N), MVT::v4f32,\n                                      DAG.getBitcast(MVT::v4f32, N0),\n                                      DAG.getBitcast(MVT::v4f32, N1)));\n  }\n\n  // Match any-of bool scalar reductions into a bitcast/movmsk + cmp.\n  // TODO: Support multiple SrcOps.\n  if (VT == MVT::i1) {\n    SmallVector<SDValue, 2> SrcOps;\n    SmallVector<APInt, 2> SrcPartials;\n    if (matchScalarReduction(SDValue(N, 0), ISD::OR, SrcOps, &SrcPartials) &&\n        SrcOps.size() == 1) {\n      SDLoc dl(N);\n      const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n      unsigned NumElts = SrcOps[0].getValueType().getVectorNumElements();\n      EVT MaskVT = EVT::getIntegerVT(*DAG.getContext(), NumElts);\n      SDValue Mask = combineBitcastvxi1(DAG, MaskVT, SrcOps[0], dl, Subtarget);\n      if (!Mask && TLI.isTypeLegal(SrcOps[0].getValueType()))\n        Mask = DAG.getBitcast(MaskVT, SrcOps[0]);\n      if (Mask) {\n        assert(SrcPartials[0].getBitWidth() == NumElts &&\n               \"Unexpected partial reduction mask\");\n        SDValue ZeroBits = DAG.getConstant(0, dl, MaskVT);\n        SDValue PartialBits = DAG.getConstant(SrcPartials[0], dl, MaskVT);\n        Mask = DAG.getNode(ISD::AND, dl, MaskVT, Mask, PartialBits);\n        return DAG.getSetCC(dl, MVT::i1, Mask, ZeroBits, ISD::SETNE);\n      }\n    }\n  }\n\n  if (SDValue R = combineBitOpWithMOVMSK(N, DAG))\n    return R;\n\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  if (SDValue R = combineCompareEqual(N, DAG, DCI, Subtarget))\n    return R;\n\n  if (SDValue FPLogic = convertIntLogicToFPLogic(N, DAG, Subtarget))\n    return FPLogic;\n\n  if (SDValue R = canonicalizeBitSelect(N, DAG, Subtarget))\n    return R;\n\n  if (SDValue R = combineLogicBlendIntoPBLENDV(N, DAG, Subtarget))\n    return R;\n\n  // Combine OR(X,KSHIFTL(Y,Elts/2)) -> CONCAT_VECTORS(X,Y) == KUNPCK(X,Y).\n  // Combine OR(KSHIFTL(X,Elts/2),Y) -> CONCAT_VECTORS(Y,X) == KUNPCK(Y,X).\n  // iff the upper elements of the non-shifted arg are zero.\n  // KUNPCK require 16+ bool vector elements.\n  if (N0.getOpcode() == X86ISD::KSHIFTL || N1.getOpcode() == X86ISD::KSHIFTL) {\n    unsigned NumElts = VT.getVectorNumElements();\n    unsigned HalfElts = NumElts / 2;\n    APInt UpperElts = APInt::getHighBitsSet(NumElts, HalfElts);\n    if (NumElts >= 16 && N1.getOpcode() == X86ISD::KSHIFTL &&\n        N1.getConstantOperandAPInt(1) == HalfElts &&\n        DAG.MaskedValueIsZero(N0, APInt(1, 1), UpperElts)) {\n      SDLoc dl(N);\n      return DAG.getNode(\n          ISD::CONCAT_VECTORS, dl, VT,\n          extractSubVector(N0, 0, DAG, dl, HalfElts),\n          extractSubVector(N1.getOperand(0), 0, DAG, dl, HalfElts));\n    }\n    if (NumElts >= 16 && N0.getOpcode() == X86ISD::KSHIFTL &&\n        N0.getConstantOperandAPInt(1) == HalfElts &&\n        DAG.MaskedValueIsZero(N1, APInt(1, 1), UpperElts)) {\n      SDLoc dl(N);\n      return DAG.getNode(\n          ISD::CONCAT_VECTORS, dl, VT,\n          extractSubVector(N1, 0, DAG, dl, HalfElts),\n          extractSubVector(N0.getOperand(0), 0, DAG, dl, HalfElts));\n    }\n  }\n\n  // Attempt to recursively combine an OR of shuffles.\n  if (VT.isVector() && (VT.getScalarSizeInBits() % 8) == 0) {\n    SDValue Op(N, 0);\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n  }\n\n  return SDValue();\n}\n\n/// Try to turn tests against the signbit in the form of:\n///   XOR(TRUNCATE(SRL(X, size(X)-1)), 1)\n/// into:\n///   SETGT(X, -1)\nstatic SDValue foldXorTruncShiftIntoCmp(SDNode *N, SelectionDAG &DAG) {\n  // This is only worth doing if the output type is i8 or i1.\n  EVT ResultType = N->getValueType(0);\n  if (ResultType != MVT::i8 && ResultType != MVT::i1)\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // We should be performing an xor against a truncated shift.\n  if (N0.getOpcode() != ISD::TRUNCATE || !N0.hasOneUse())\n    return SDValue();\n\n  // Make sure we are performing an xor against one.\n  if (!isOneConstant(N1))\n    return SDValue();\n\n  // SetCC on x86 zero extends so only act on this if it's a logical shift.\n  SDValue Shift = N0.getOperand(0);\n  if (Shift.getOpcode() != ISD::SRL || !Shift.hasOneUse())\n    return SDValue();\n\n  // Make sure we are truncating from one of i16, i32 or i64.\n  EVT ShiftTy = Shift.getValueType();\n  if (ShiftTy != MVT::i16 && ShiftTy != MVT::i32 && ShiftTy != MVT::i64)\n    return SDValue();\n\n  // Make sure the shift amount extracts the sign bit.\n  if (!isa<ConstantSDNode>(Shift.getOperand(1)) ||\n      Shift.getConstantOperandAPInt(1) != (ShiftTy.getSizeInBits() - 1))\n    return SDValue();\n\n  // Create a greater-than comparison against -1.\n  // N.B. Using SETGE against 0 works but we want a canonical looking\n  // comparison, using SETGT matches up with what TranslateX86CC.\n  SDLoc DL(N);\n  SDValue ShiftOp = Shift.getOperand(0);\n  EVT ShiftOpTy = ShiftOp.getValueType();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  EVT SetCCResultType = TLI.getSetCCResultType(DAG.getDataLayout(),\n                                               *DAG.getContext(), ResultType);\n  SDValue Cond = DAG.getSetCC(DL, SetCCResultType, ShiftOp,\n                              DAG.getConstant(-1, DL, ShiftOpTy), ISD::SETGT);\n  if (SetCCResultType != ResultType)\n    Cond = DAG.getNode(ISD::ZERO_EXTEND, DL, ResultType, Cond);\n  return Cond;\n}\n\n/// Turn vector tests of the signbit in the form of:\n///   xor (sra X, elt_size(X)-1), -1\n/// into:\n///   pcmpgt X, -1\n///\n/// This should be called before type legalization because the pattern may not\n/// persist after that.\nstatic SDValue foldVectorXorShiftIntoCmp(SDNode *N, SelectionDAG &DAG,\n                                         const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  if (!VT.isSimple())\n    return SDValue();\n\n  switch (VT.getSimpleVT().SimpleTy) {\n  default: return SDValue();\n  case MVT::v16i8:\n  case MVT::v8i16:\n  case MVT::v4i32:\n  case MVT::v2i64: if (!Subtarget.hasSSE2()) return SDValue(); break;\n  case MVT::v32i8:\n  case MVT::v16i16:\n  case MVT::v8i32:\n  case MVT::v4i64: if (!Subtarget.hasAVX2()) return SDValue(); break;\n  }\n\n  // There must be a shift right algebraic before the xor, and the xor must be a\n  // 'not' operation.\n  SDValue Shift = N->getOperand(0);\n  SDValue Ones = N->getOperand(1);\n  if (Shift.getOpcode() != ISD::SRA || !Shift.hasOneUse() ||\n      !ISD::isBuildVectorAllOnes(Ones.getNode()))\n    return SDValue();\n\n  // The shift should be smearing the sign bit across each vector element.\n  auto *ShiftAmt =\n      isConstOrConstSplat(Shift.getOperand(1), /*AllowUndefs*/ true);\n  if (!ShiftAmt ||\n      ShiftAmt->getAPIntValue() != (Shift.getScalarValueSizeInBits() - 1))\n    return SDValue();\n\n  // Create a greater-than comparison against -1. We don't use the more obvious\n  // greater-than-or-equal-to-zero because SSE/AVX don't have that instruction.\n  return DAG.getSetCC(SDLoc(N), VT, Shift.getOperand(0), Ones, ISD::SETGT);\n}\n\n/// Detect patterns of truncation with unsigned saturation:\n///\n/// 1. (truncate (umin (x, unsigned_max_of_dest_type)) to dest_type).\n///   Return the source value x to be truncated or SDValue() if the pattern was\n///   not matched.\n///\n/// 2. (truncate (smin (smax (x, C1), C2)) to dest_type),\n///   where C1 >= 0 and C2 is unsigned max of destination type.\n///\n///    (truncate (smax (smin (x, C2), C1)) to dest_type)\n///   where C1 >= 0, C2 is unsigned max of destination type and C1 <= C2.\n///\n///   These two patterns are equivalent to:\n///   (truncate (umin (smax(x, C1), unsigned_max_of_dest_type)) to dest_type)\n///   So return the smax(x, C1) value to be truncated or SDValue() if the\n///   pattern was not matched.\nstatic SDValue detectUSatPattern(SDValue In, EVT VT, SelectionDAG &DAG,\n                                 const SDLoc &DL) {\n  EVT InVT = In.getValueType();\n\n  // Saturation with truncation. We truncate from InVT to VT.\n  assert(InVT.getScalarSizeInBits() > VT.getScalarSizeInBits() &&\n         \"Unexpected types for truncate operation\");\n\n  // Match min/max and return limit value as a parameter.\n  auto MatchMinMax = [](SDValue V, unsigned Opcode, APInt &Limit) -> SDValue {\n    if (V.getOpcode() == Opcode &&\n        ISD::isConstantSplatVector(V.getOperand(1).getNode(), Limit))\n      return V.getOperand(0);\n    return SDValue();\n  };\n\n  APInt C1, C2;\n  if (SDValue UMin = MatchMinMax(In, ISD::UMIN, C2))\n    // C2 should be equal to UINT32_MAX / UINT16_MAX / UINT8_MAX according\n    // the element size of the destination type.\n    if (C2.isMask(VT.getScalarSizeInBits()))\n      return UMin;\n\n  if (SDValue SMin = MatchMinMax(In, ISD::SMIN, C2))\n    if (MatchMinMax(SMin, ISD::SMAX, C1))\n      if (C1.isNonNegative() && C2.isMask(VT.getScalarSizeInBits()))\n        return SMin;\n\n  if (SDValue SMax = MatchMinMax(In, ISD::SMAX, C1))\n    if (SDValue SMin = MatchMinMax(SMax, ISD::SMIN, C2))\n      if (C1.isNonNegative() && C2.isMask(VT.getScalarSizeInBits()) &&\n          C2.uge(C1)) {\n        return DAG.getNode(ISD::SMAX, DL, InVT, SMin, In.getOperand(1));\n      }\n\n  return SDValue();\n}\n\n/// Detect patterns of truncation with signed saturation:\n/// (truncate (smin ((smax (x, signed_min_of_dest_type)),\n///                  signed_max_of_dest_type)) to dest_type)\n/// or:\n/// (truncate (smax ((smin (x, signed_max_of_dest_type)),\n///                  signed_min_of_dest_type)) to dest_type).\n/// With MatchPackUS, the smax/smin range is [0, unsigned_max_of_dest_type].\n/// Return the source value to be truncated or SDValue() if the pattern was not\n/// matched.\nstatic SDValue detectSSatPattern(SDValue In, EVT VT, bool MatchPackUS = false) {\n  unsigned NumDstBits = VT.getScalarSizeInBits();\n  unsigned NumSrcBits = In.getScalarValueSizeInBits();\n  assert(NumSrcBits > NumDstBits && \"Unexpected types for truncate operation\");\n\n  auto MatchMinMax = [](SDValue V, unsigned Opcode,\n                        const APInt &Limit) -> SDValue {\n    APInt C;\n    if (V.getOpcode() == Opcode &&\n        ISD::isConstantSplatVector(V.getOperand(1).getNode(), C) && C == Limit)\n      return V.getOperand(0);\n    return SDValue();\n  };\n\n  APInt SignedMax, SignedMin;\n  if (MatchPackUS) {\n    SignedMax = APInt::getAllOnesValue(NumDstBits).zext(NumSrcBits);\n    SignedMin = APInt(NumSrcBits, 0);\n  } else {\n    SignedMax = APInt::getSignedMaxValue(NumDstBits).sext(NumSrcBits);\n    SignedMin = APInt::getSignedMinValue(NumDstBits).sext(NumSrcBits);\n  }\n\n  if (SDValue SMin = MatchMinMax(In, ISD::SMIN, SignedMax))\n    if (SDValue SMax = MatchMinMax(SMin, ISD::SMAX, SignedMin))\n      return SMax;\n\n  if (SDValue SMax = MatchMinMax(In, ISD::SMAX, SignedMin))\n    if (SDValue SMin = MatchMinMax(SMax, ISD::SMIN, SignedMax))\n      return SMin;\n\n  return SDValue();\n}\n\nstatic SDValue combineTruncateWithSat(SDValue In, EVT VT, const SDLoc &DL,\n                                      SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasSSE2() || !VT.isVector())\n    return SDValue();\n\n  EVT SVT = VT.getVectorElementType();\n  EVT InVT = In.getValueType();\n  EVT InSVT = InVT.getVectorElementType();\n\n  // If we're clamping a signed 32-bit vector to 0-255 and the 32-bit vector is\n  // split across two registers. We can use a packusdw+perm to clamp to 0-65535\n  // and concatenate at the same time. Then we can use a final vpmovuswb to\n  // clip to 0-255.\n  if (Subtarget.hasBWI() && !Subtarget.useAVX512Regs() &&\n      InVT == MVT::v16i32 && VT == MVT::v16i8) {\n    if (auto USatVal = detectSSatPattern(In, VT, true)) {\n      // Emit a VPACKUSDW+VPERMQ followed by a VPMOVUSWB.\n      SDValue Mid = truncateVectorWithPACK(X86ISD::PACKUS, MVT::v16i16, USatVal,\n                                           DL, DAG, Subtarget);\n      assert(Mid && \"Failed to pack!\");\n      return DAG.getNode(X86ISD::VTRUNCUS, DL, VT, Mid);\n    }\n  }\n\n  // vXi32 truncate instructions are available with AVX512F.\n  // vXi16 truncate instructions are only available with AVX512BW.\n  // For 256-bit or smaller vectors, we require VLX.\n  // FIXME: We could widen truncates to 512 to remove the VLX restriction.\n  // If the result type is 256-bits or larger and we have disable 512-bit\n  // registers, we should go ahead and use the pack instructions if possible.\n  bool PreferAVX512 = ((Subtarget.hasAVX512() && InSVT == MVT::i32) ||\n                       (Subtarget.hasBWI() && InSVT == MVT::i16)) &&\n                      (InVT.getSizeInBits() > 128) &&\n                      (Subtarget.hasVLX() || InVT.getSizeInBits() > 256) &&\n                      !(!Subtarget.useAVX512Regs() && VT.getSizeInBits() >= 256);\n\n  if (isPowerOf2_32(VT.getVectorNumElements()) && !PreferAVX512 &&\n      VT.getSizeInBits() >= 64 &&\n      (SVT == MVT::i8 || SVT == MVT::i16) &&\n      (InSVT == MVT::i16 || InSVT == MVT::i32)) {\n    if (auto USatVal = detectSSatPattern(In, VT, true)) {\n      // vXi32 -> vXi8 must be performed as PACKUSWB(PACKSSDW,PACKSSDW).\n      // Only do this when the result is at least 64 bits or we'll leaving\n      // dangling PACKSSDW nodes.\n      if (SVT == MVT::i8 && InSVT == MVT::i32) {\n        EVT MidVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16,\n                                     VT.getVectorNumElements());\n        SDValue Mid = truncateVectorWithPACK(X86ISD::PACKSS, MidVT, USatVal, DL,\n                                             DAG, Subtarget);\n        assert(Mid && \"Failed to pack!\");\n        SDValue V = truncateVectorWithPACK(X86ISD::PACKUS, VT, Mid, DL, DAG,\n                                           Subtarget);\n        assert(V && \"Failed to pack!\");\n        return V;\n      } else if (SVT == MVT::i8 || Subtarget.hasSSE41())\n        return truncateVectorWithPACK(X86ISD::PACKUS, VT, USatVal, DL, DAG,\n                                      Subtarget);\n    }\n    if (auto SSatVal = detectSSatPattern(In, VT))\n      return truncateVectorWithPACK(X86ISD::PACKSS, VT, SSatVal, DL, DAG,\n                                    Subtarget);\n  }\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.isTypeLegal(InVT) && InVT.isVector() && SVT != MVT::i1 &&\n      Subtarget.hasAVX512() && (InSVT != MVT::i16 || Subtarget.hasBWI())) {\n    unsigned TruncOpc = 0;\n    SDValue SatVal;\n    if (auto SSatVal = detectSSatPattern(In, VT)) {\n      SatVal = SSatVal;\n      TruncOpc = X86ISD::VTRUNCS;\n    } else if (auto USatVal = detectUSatPattern(In, VT, DAG, DL)) {\n      SatVal = USatVal;\n      TruncOpc = X86ISD::VTRUNCUS;\n    }\n    if (SatVal) {\n      unsigned ResElts = VT.getVectorNumElements();\n      // If the input type is less than 512 bits and we don't have VLX, we need\n      // to widen to 512 bits.\n      if (!Subtarget.hasVLX() && !InVT.is512BitVector()) {\n        unsigned NumConcats = 512 / InVT.getSizeInBits();\n        ResElts *= NumConcats;\n        SmallVector<SDValue, 4> ConcatOps(NumConcats, DAG.getUNDEF(InVT));\n        ConcatOps[0] = SatVal;\n        InVT = EVT::getVectorVT(*DAG.getContext(), InSVT,\n                                NumConcats * InVT.getVectorNumElements());\n        SatVal = DAG.getNode(ISD::CONCAT_VECTORS, DL, InVT, ConcatOps);\n      }\n      // Widen the result if its narrower than 128 bits.\n      if (ResElts * SVT.getSizeInBits() < 128)\n        ResElts = 128 / SVT.getSizeInBits();\n      EVT TruncVT = EVT::getVectorVT(*DAG.getContext(), SVT, ResElts);\n      SDValue Res = DAG.getNode(TruncOpc, DL, TruncVT, SatVal);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                         DAG.getIntPtrConstant(0, DL));\n    }\n  }\n\n  return SDValue();\n}\n\n/// This function detects the AVG pattern between vectors of unsigned i8/i16,\n/// which is c = (a + b + 1) / 2, and replace this operation with the efficient\n/// X86ISD::AVG instruction.\nstatic SDValue detectAVGPattern(SDValue In, EVT VT, SelectionDAG &DAG,\n                                const X86Subtarget &Subtarget,\n                                const SDLoc &DL) {\n  if (!VT.isVector())\n    return SDValue();\n  EVT InVT = In.getValueType();\n  unsigned NumElems = VT.getVectorNumElements();\n\n  EVT ScalarVT = VT.getVectorElementType();\n  if (!((ScalarVT == MVT::i8 || ScalarVT == MVT::i16) && NumElems >= 2))\n    return SDValue();\n\n  // InScalarVT is the intermediate type in AVG pattern and it should be greater\n  // than the original input type (i8/i16).\n  EVT InScalarVT = InVT.getVectorElementType();\n  if (InScalarVT.getFixedSizeInBits() <= ScalarVT.getFixedSizeInBits())\n    return SDValue();\n\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  // Detect the following pattern:\n  //\n  //   %1 = zext <N x i8> %a to <N x i32>\n  //   %2 = zext <N x i8> %b to <N x i32>\n  //   %3 = add nuw nsw <N x i32> %1, <i32 1 x N>\n  //   %4 = add nuw nsw <N x i32> %3, %2\n  //   %5 = lshr <N x i32> %N, <i32 1 x N>\n  //   %6 = trunc <N x i32> %5 to <N x i8>\n  //\n  // In AVX512, the last instruction can also be a trunc store.\n  if (In.getOpcode() != ISD::SRL)\n    return SDValue();\n\n  // A lambda checking the given SDValue is a constant vector and each element\n  // is in the range [Min, Max].\n  auto IsConstVectorInRange = [](SDValue V, unsigned Min, unsigned Max) {\n    return ISD::matchUnaryPredicate(V, [Min, Max](ConstantSDNode *C) {\n      return !(C->getAPIntValue().ult(Min) || C->getAPIntValue().ugt(Max));\n    });\n  };\n\n  // Check if each element of the vector is right-shifted by one.\n  SDValue LHS = In.getOperand(0);\n  SDValue RHS = In.getOperand(1);\n  if (!IsConstVectorInRange(RHS, 1, 1))\n    return SDValue();\n  if (LHS.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  // Detect a pattern of a + b + 1 where the order doesn't matter.\n  SDValue Operands[3];\n  Operands[0] = LHS.getOperand(0);\n  Operands[1] = LHS.getOperand(1);\n\n  auto AVGBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                       ArrayRef<SDValue> Ops) {\n    return DAG.getNode(X86ISD::AVG, DL, Ops[0].getValueType(), Ops);\n  };\n\n  auto AVGSplitter = [&](SDValue Op0, SDValue Op1) {\n    // Pad to a power-of-2 vector, split+apply and extract the original vector.\n    unsigned NumElemsPow2 = PowerOf2Ceil(NumElems);\n    EVT Pow2VT = EVT::getVectorVT(*DAG.getContext(), ScalarVT, NumElemsPow2);\n    if (NumElemsPow2 != NumElems) {\n      SmallVector<SDValue, 32> Ops0(NumElemsPow2, DAG.getUNDEF(ScalarVT));\n      SmallVector<SDValue, 32> Ops1(NumElemsPow2, DAG.getUNDEF(ScalarVT));\n      for (unsigned i = 0; i != NumElems; ++i) {\n        SDValue Idx = DAG.getIntPtrConstant(i, DL);\n        Ops0[i] = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, ScalarVT, Op0, Idx);\n        Ops1[i] = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, ScalarVT, Op1, Idx);\n      }\n      Op0 = DAG.getBuildVector(Pow2VT, DL, Ops0);\n      Op1 = DAG.getBuildVector(Pow2VT, DL, Ops1);\n    }\n    SDValue Res =\n        SplitOpsAndApply(DAG, Subtarget, DL, Pow2VT, {Op0, Op1}, AVGBuilder);\n    if (NumElemsPow2 == NumElems)\n      return Res;\n    return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Res,\n                       DAG.getIntPtrConstant(0, DL));\n  };\n\n  // Take care of the case when one of the operands is a constant vector whose\n  // element is in the range [1, 256].\n  if (IsConstVectorInRange(Operands[1], 1, ScalarVT == MVT::i8 ? 256 : 65536) &&\n      Operands[0].getOpcode() == ISD::ZERO_EXTEND &&\n      Operands[0].getOperand(0).getValueType() == VT) {\n    // The pattern is detected. Subtract one from the constant vector, then\n    // demote it and emit X86ISD::AVG instruction.\n    SDValue VecOnes = DAG.getConstant(1, DL, InVT);\n    Operands[1] = DAG.getNode(ISD::SUB, DL, InVT, Operands[1], VecOnes);\n    Operands[1] = DAG.getNode(ISD::TRUNCATE, DL, VT, Operands[1]);\n    return AVGSplitter(Operands[0].getOperand(0), Operands[1]);\n  }\n\n  // Matches 'add like' patterns: add(Op0,Op1) + zext(or(Op0,Op1)).\n  // Match the or case only if its 'add-like' - can be replaced by an add.\n  auto FindAddLike = [&](SDValue V, SDValue &Op0, SDValue &Op1) {\n    if (ISD::ADD == V.getOpcode()) {\n      Op0 = V.getOperand(0);\n      Op1 = V.getOperand(1);\n      return true;\n    }\n    if (ISD::ZERO_EXTEND != V.getOpcode())\n      return false;\n    V = V.getOperand(0);\n    if (V.getValueType() != VT || ISD::OR != V.getOpcode() ||\n        !DAG.haveNoCommonBitsSet(V.getOperand(0), V.getOperand(1)))\n      return false;\n    Op0 = V.getOperand(0);\n    Op1 = V.getOperand(1);\n    return true;\n  };\n\n  SDValue Op0, Op1;\n  if (FindAddLike(Operands[0], Op0, Op1))\n    std::swap(Operands[0], Operands[1]);\n  else if (!FindAddLike(Operands[1], Op0, Op1))\n    return SDValue();\n  Operands[2] = Op0;\n  Operands[1] = Op1;\n\n  // Now we have three operands of two additions. Check that one of them is a\n  // constant vector with ones, and the other two can be promoted from i8/i16.\n  for (int i = 0; i < 3; ++i) {\n    if (!IsConstVectorInRange(Operands[i], 1, 1))\n      continue;\n    std::swap(Operands[i], Operands[2]);\n\n    // Check if Operands[0] and Operands[1] are results of type promotion.\n    for (int j = 0; j < 2; ++j)\n      if (Operands[j].getValueType() != VT) {\n        if (Operands[j].getOpcode() != ISD::ZERO_EXTEND ||\n            Operands[j].getOperand(0).getValueType() != VT)\n          return SDValue();\n        Operands[j] = Operands[j].getOperand(0);\n      }\n\n    // The pattern is detected, emit X86ISD::AVG instruction(s).\n    return AVGSplitter(Operands[0], Operands[1]);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineLoad(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI,\n                           const X86Subtarget &Subtarget) {\n  LoadSDNode *Ld = cast<LoadSDNode>(N);\n  EVT RegVT = Ld->getValueType(0);\n  EVT MemVT = Ld->getMemoryVT();\n  SDLoc dl(Ld);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // For chips with slow 32-byte unaligned loads, break the 32-byte operation\n  // into two 16-byte operations. Also split non-temporal aligned loads on\n  // pre-AVX2 targets as 32-byte loads will lower to regular temporal loads.\n  ISD::LoadExtType Ext = Ld->getExtensionType();\n  bool Fast;\n  if (RegVT.is256BitVector() && !DCI.isBeforeLegalizeOps() &&\n      Ext == ISD::NON_EXTLOAD &&\n      ((Ld->isNonTemporal() && !Subtarget.hasInt256() &&\n        Ld->getAlignment() >= 16) ||\n       (TLI.allowsMemoryAccess(*DAG.getContext(), DAG.getDataLayout(), RegVT,\n                               *Ld->getMemOperand(), &Fast) &&\n        !Fast))) {\n    unsigned NumElems = RegVT.getVectorNumElements();\n    if (NumElems < 2)\n      return SDValue();\n\n    unsigned HalfOffset = 16;\n    SDValue Ptr1 = Ld->getBasePtr();\n    SDValue Ptr2 =\n        DAG.getMemBasePlusOffset(Ptr1, TypeSize::Fixed(HalfOffset), dl);\n    EVT HalfVT = EVT::getVectorVT(*DAG.getContext(), MemVT.getScalarType(),\n                                  NumElems / 2);\n    SDValue Load1 =\n        DAG.getLoad(HalfVT, dl, Ld->getChain(), Ptr1, Ld->getPointerInfo(),\n                    Ld->getOriginalAlign(),\n                    Ld->getMemOperand()->getFlags());\n    SDValue Load2 = DAG.getLoad(HalfVT, dl, Ld->getChain(), Ptr2,\n                                Ld->getPointerInfo().getWithOffset(HalfOffset),\n                                Ld->getOriginalAlign(),\n                                Ld->getMemOperand()->getFlags());\n    SDValue TF = DAG.getNode(ISD::TokenFactor, dl, MVT::Other,\n                             Load1.getValue(1), Load2.getValue(1));\n\n    SDValue NewVec = DAG.getNode(ISD::CONCAT_VECTORS, dl, RegVT, Load1, Load2);\n    return DCI.CombineTo(N, NewVec, TF, true);\n  }\n\n  // Bool vector load - attempt to cast to an integer, as we have good\n  // (vXiY *ext(vXi1 bitcast(iX))) handling.\n  if (Ext == ISD::NON_EXTLOAD && !Subtarget.hasAVX512() && RegVT.isVector() &&\n      RegVT.getScalarType() == MVT::i1 && DCI.isBeforeLegalize()) {\n    unsigned NumElts = RegVT.getVectorNumElements();\n    EVT IntVT = EVT::getIntegerVT(*DAG.getContext(), NumElts);\n    if (TLI.isTypeLegal(IntVT)) {\n      SDValue IntLoad = DAG.getLoad(IntVT, dl, Ld->getChain(), Ld->getBasePtr(),\n                                    Ld->getPointerInfo(),\n                                    Ld->getOriginalAlign(),\n                                    Ld->getMemOperand()->getFlags());\n      SDValue BoolVec = DAG.getBitcast(RegVT, IntLoad);\n      return DCI.CombineTo(N, BoolVec, IntLoad.getValue(1), true);\n    }\n  }\n\n  // If we also broadcast this as a subvector to a wider type, then just extract\n  // the lowest subvector.\n  if (Ext == ISD::NON_EXTLOAD && Subtarget.hasAVX() && Ld->isSimple() &&\n      (RegVT.is128BitVector() || RegVT.is256BitVector())) {\n    SDValue Ptr = Ld->getBasePtr();\n    SDValue Chain = Ld->getChain();\n    for (SDNode *User : Ptr->uses()) {\n      if (User != N && User->getOpcode() == X86ISD::SUBV_BROADCAST_LOAD &&\n          cast<MemIntrinsicSDNode>(User)->getBasePtr() == Ptr &&\n          cast<MemIntrinsicSDNode>(User)->getChain() == Chain &&\n          cast<MemIntrinsicSDNode>(User)->getMemoryVT().getSizeInBits() ==\n              MemVT.getSizeInBits() &&\n          !User->hasAnyUseOfValue(1) &&\n          User->getValueSizeInBits(0).getFixedSize() >\n              RegVT.getFixedSizeInBits()) {\n        SDValue Extract = extractSubVector(SDValue(User, 0), 0, DAG, SDLoc(N),\n                                           RegVT.getSizeInBits());\n        Extract = DAG.getBitcast(RegVT, Extract);\n        return DCI.CombineTo(N, Extract, SDValue(User, 1));\n      }\n    }\n  }\n\n  // Cast ptr32 and ptr64 pointers to the default address space before a load.\n  unsigned AddrSpace = Ld->getAddressSpace();\n  if (AddrSpace == X86AS::PTR64 || AddrSpace == X86AS::PTR32_SPTR ||\n      AddrSpace == X86AS::PTR32_UPTR) {\n    MVT PtrVT = TLI.getPointerTy(DAG.getDataLayout());\n    if (PtrVT != Ld->getBasePtr().getSimpleValueType()) {\n      SDValue Cast =\n          DAG.getAddrSpaceCast(dl, PtrVT, Ld->getBasePtr(), AddrSpace, 0);\n      return DAG.getLoad(RegVT, dl, Ld->getChain(), Cast, Ld->getPointerInfo(),\n                         Ld->getOriginalAlign(),\n                         Ld->getMemOperand()->getFlags());\n    }\n  }\n\n  return SDValue();\n}\n\n/// If V is a build vector of boolean constants and exactly one of those\n/// constants is true, return the operand index of that true element.\n/// Otherwise, return -1.\nstatic int getOneTrueElt(SDValue V) {\n  // This needs to be a build vector of booleans.\n  // TODO: Checking for the i1 type matches the IR definition for the mask,\n  // but the mask check could be loosened to i8 or other types. That might\n  // also require checking more than 'allOnesValue'; eg, the x86 HW\n  // instructions only require that the MSB is set for each mask element.\n  // The ISD::MSTORE comments/definition do not specify how the mask operand\n  // is formatted.\n  auto *BV = dyn_cast<BuildVectorSDNode>(V);\n  if (!BV || BV->getValueType(0).getVectorElementType() != MVT::i1)\n    return -1;\n\n  int TrueIndex = -1;\n  unsigned NumElts = BV->getValueType(0).getVectorNumElements();\n  for (unsigned i = 0; i < NumElts; ++i) {\n    const SDValue &Op = BV->getOperand(i);\n    if (Op.isUndef())\n      continue;\n    auto *ConstNode = dyn_cast<ConstantSDNode>(Op);\n    if (!ConstNode)\n      return -1;\n    if (ConstNode->getAPIntValue().countTrailingOnes() >= 1) {\n      // If we already found a one, this is too many.\n      if (TrueIndex >= 0)\n        return -1;\n      TrueIndex = i;\n    }\n  }\n  return TrueIndex;\n}\n\n/// Given a masked memory load/store operation, return true if it has one mask\n/// bit set. If it has one mask bit set, then also return the memory address of\n/// the scalar element to load/store, the vector index to insert/extract that\n/// scalar element, and the alignment for the scalar memory access.\nstatic bool getParamsForOneTrueMaskedElt(MaskedLoadStoreSDNode *MaskedOp,\n                                         SelectionDAG &DAG, SDValue &Addr,\n                                         SDValue &Index, Align &Alignment,\n                                         unsigned &Offset) {\n  int TrueMaskElt = getOneTrueElt(MaskedOp->getMask());\n  if (TrueMaskElt < 0)\n    return false;\n\n  // Get the address of the one scalar element that is specified by the mask\n  // using the appropriate offset from the base pointer.\n  EVT EltVT = MaskedOp->getMemoryVT().getVectorElementType();\n  Offset = 0;\n  Addr = MaskedOp->getBasePtr();\n  if (TrueMaskElt != 0) {\n    Offset = TrueMaskElt * EltVT.getStoreSize();\n    Addr = DAG.getMemBasePlusOffset(Addr, TypeSize::Fixed(Offset),\n                                    SDLoc(MaskedOp));\n  }\n\n  Index = DAG.getIntPtrConstant(TrueMaskElt, SDLoc(MaskedOp));\n  Alignment = commonAlignment(MaskedOp->getOriginalAlign(),\n                              EltVT.getStoreSize());\n  return true;\n}\n\n/// If exactly one element of the mask is set for a non-extending masked load,\n/// it is a scalar load and vector insert.\n/// Note: It is expected that the degenerate cases of an all-zeros or all-ones\n/// mask have already been optimized in IR, so we don't bother with those here.\nstatic SDValue\nreduceMaskedLoadToScalarLoad(MaskedLoadSDNode *ML, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI,\n                             const X86Subtarget &Subtarget) {\n  assert(ML->isUnindexed() && \"Unexpected indexed masked load!\");\n  // TODO: This is not x86-specific, so it could be lifted to DAGCombiner.\n  // However, some target hooks may need to be added to know when the transform\n  // is profitable. Endianness would also have to be considered.\n\n  SDValue Addr, VecIndex;\n  Align Alignment;\n  unsigned Offset;\n  if (!getParamsForOneTrueMaskedElt(ML, DAG, Addr, VecIndex, Alignment, Offset))\n    return SDValue();\n\n  // Load the one scalar element that is specified by the mask using the\n  // appropriate offset from the base pointer.\n  SDLoc DL(ML);\n  EVT VT = ML->getValueType(0);\n  EVT EltVT = VT.getVectorElementType();\n\n  EVT CastVT = VT;\n  if (EltVT == MVT::i64 && !Subtarget.is64Bit()) {\n    EltVT = MVT::f64;\n    CastVT =\n        EVT::getVectorVT(*DAG.getContext(), EltVT, VT.getVectorNumElements());\n  }\n\n  SDValue Load =\n      DAG.getLoad(EltVT, DL, ML->getChain(), Addr,\n                  ML->getPointerInfo().getWithOffset(Offset),\n                  Alignment, ML->getMemOperand()->getFlags());\n\n  SDValue PassThru = DAG.getBitcast(CastVT, ML->getPassThru());\n\n  // Insert the loaded element into the appropriate place in the vector.\n  SDValue Insert =\n      DAG.getNode(ISD::INSERT_VECTOR_ELT, DL, CastVT, PassThru, Load, VecIndex);\n  Insert = DAG.getBitcast(VT, Insert);\n  return DCI.CombineTo(ML, Insert, Load.getValue(1), true);\n}\n\nstatic SDValue\ncombineMaskedLoadConstantMask(MaskedLoadSDNode *ML, SelectionDAG &DAG,\n                              TargetLowering::DAGCombinerInfo &DCI) {\n  assert(ML->isUnindexed() && \"Unexpected indexed masked load!\");\n  if (!ISD::isBuildVectorOfConstantSDNodes(ML->getMask().getNode()))\n    return SDValue();\n\n  SDLoc DL(ML);\n  EVT VT = ML->getValueType(0);\n\n  // If we are loading the first and last elements of a vector, it is safe and\n  // always faster to load the whole vector. Replace the masked load with a\n  // vector load and select.\n  unsigned NumElts = VT.getVectorNumElements();\n  BuildVectorSDNode *MaskBV = cast<BuildVectorSDNode>(ML->getMask());\n  bool LoadFirstElt = !isNullConstant(MaskBV->getOperand(0));\n  bool LoadLastElt = !isNullConstant(MaskBV->getOperand(NumElts - 1));\n  if (LoadFirstElt && LoadLastElt) {\n    SDValue VecLd = DAG.getLoad(VT, DL, ML->getChain(), ML->getBasePtr(),\n                                ML->getMemOperand());\n    SDValue Blend = DAG.getSelect(DL, VT, ML->getMask(), VecLd,\n                                  ML->getPassThru());\n    return DCI.CombineTo(ML, Blend, VecLd.getValue(1), true);\n  }\n\n  // Convert a masked load with a constant mask into a masked load and a select.\n  // This allows the select operation to use a faster kind of select instruction\n  // (for example, vblendvps -> vblendps).\n\n  // Don't try this if the pass-through operand is already undefined. That would\n  // cause an infinite loop because that's what we're about to create.\n  if (ML->getPassThru().isUndef())\n    return SDValue();\n\n  if (ISD::isBuildVectorAllZeros(ML->getPassThru().getNode()))\n    return SDValue();\n\n  // The new masked load has an undef pass-through operand. The select uses the\n  // original pass-through operand.\n  SDValue NewML = DAG.getMaskedLoad(\n      VT, DL, ML->getChain(), ML->getBasePtr(), ML->getOffset(), ML->getMask(),\n      DAG.getUNDEF(VT), ML->getMemoryVT(), ML->getMemOperand(),\n      ML->getAddressingMode(), ML->getExtensionType());\n  SDValue Blend = DAG.getSelect(DL, VT, ML->getMask(), NewML,\n                                ML->getPassThru());\n\n  return DCI.CombineTo(ML, Blend, NewML.getValue(1), true);\n}\n\nstatic SDValue combineMaskedLoad(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const X86Subtarget &Subtarget) {\n  auto *Mld = cast<MaskedLoadSDNode>(N);\n\n  // TODO: Expanding load with constant mask may be optimized as well.\n  if (Mld->isExpandingLoad())\n    return SDValue();\n\n  if (Mld->getExtensionType() == ISD::NON_EXTLOAD) {\n    if (SDValue ScalarLoad =\n            reduceMaskedLoadToScalarLoad(Mld, DAG, DCI, Subtarget))\n      return ScalarLoad;\n\n    // TODO: Do some AVX512 subsets benefit from this transform?\n    if (!Subtarget.hasAVX512())\n      if (SDValue Blend = combineMaskedLoadConstantMask(Mld, DAG, DCI))\n        return Blend;\n  }\n\n  // If the mask value has been legalized to a non-boolean vector, try to\n  // simplify ops leading up to it. We only demand the MSB of each lane.\n  SDValue Mask = Mld->getMask();\n  if (Mask.getScalarValueSizeInBits() != 1) {\n    EVT VT = Mld->getValueType(0);\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    APInt DemandedBits(APInt::getSignMask(VT.getScalarSizeInBits()));\n    if (TLI.SimplifyDemandedBits(Mask, DemandedBits, DCI)) {\n      if (N->getOpcode() != ISD::DELETED_NODE)\n        DCI.AddToWorklist(N);\n      return SDValue(N, 0);\n    }\n    if (SDValue NewMask =\n            TLI.SimplifyMultipleUseDemandedBits(Mask, DemandedBits, DAG))\n      return DAG.getMaskedLoad(\n          VT, SDLoc(N), Mld->getChain(), Mld->getBasePtr(), Mld->getOffset(),\n          NewMask, Mld->getPassThru(), Mld->getMemoryVT(), Mld->getMemOperand(),\n          Mld->getAddressingMode(), Mld->getExtensionType());\n  }\n\n  return SDValue();\n}\n\n/// If exactly one element of the mask is set for a non-truncating masked store,\n/// it is a vector extract and scalar store.\n/// Note: It is expected that the degenerate cases of an all-zeros or all-ones\n/// mask have already been optimized in IR, so we don't bother with those here.\nstatic SDValue reduceMaskedStoreToScalarStore(MaskedStoreSDNode *MS,\n                                              SelectionDAG &DAG,\n                                              const X86Subtarget &Subtarget) {\n  // TODO: This is not x86-specific, so it could be lifted to DAGCombiner.\n  // However, some target hooks may need to be added to know when the transform\n  // is profitable. Endianness would also have to be considered.\n\n  SDValue Addr, VecIndex;\n  Align Alignment;\n  unsigned Offset;\n  if (!getParamsForOneTrueMaskedElt(MS, DAG, Addr, VecIndex, Alignment, Offset))\n    return SDValue();\n\n  // Extract the one scalar element that is actually being stored.\n  SDLoc DL(MS);\n  SDValue Value = MS->getValue();\n  EVT VT = Value.getValueType();\n  EVT EltVT = VT.getVectorElementType();\n  if (EltVT == MVT::i64 && !Subtarget.is64Bit()) {\n    EltVT = MVT::f64;\n    EVT CastVT =\n        EVT::getVectorVT(*DAG.getContext(), EltVT, VT.getVectorNumElements());\n    Value = DAG.getBitcast(CastVT, Value);\n  }\n  SDValue Extract =\n      DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, EltVT, Value, VecIndex);\n\n  // Store that element at the appropriate offset from the base pointer.\n  return DAG.getStore(MS->getChain(), DL, Extract, Addr,\n                      MS->getPointerInfo().getWithOffset(Offset),\n                      Alignment, MS->getMemOperand()->getFlags());\n}\n\nstatic SDValue combineMaskedStore(SDNode *N, SelectionDAG &DAG,\n                                  TargetLowering::DAGCombinerInfo &DCI,\n                                  const X86Subtarget &Subtarget) {\n  MaskedStoreSDNode *Mst = cast<MaskedStoreSDNode>(N);\n  if (Mst->isCompressingStore())\n    return SDValue();\n\n  EVT VT = Mst->getValue().getValueType();\n  SDLoc dl(Mst);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  if (Mst->isTruncatingStore())\n    return SDValue();\n\n  if (SDValue ScalarStore = reduceMaskedStoreToScalarStore(Mst, DAG, Subtarget))\n    return ScalarStore;\n\n  // If the mask value has been legalized to a non-boolean vector, try to\n  // simplify ops leading up to it. We only demand the MSB of each lane.\n  SDValue Mask = Mst->getMask();\n  if (Mask.getScalarValueSizeInBits() != 1) {\n    APInt DemandedBits(APInt::getSignMask(VT.getScalarSizeInBits()));\n    if (TLI.SimplifyDemandedBits(Mask, DemandedBits, DCI)) {\n      if (N->getOpcode() != ISD::DELETED_NODE)\n        DCI.AddToWorklist(N);\n      return SDValue(N, 0);\n    }\n    if (SDValue NewMask =\n            TLI.SimplifyMultipleUseDemandedBits(Mask, DemandedBits, DAG))\n      return DAG.getMaskedStore(Mst->getChain(), SDLoc(N), Mst->getValue(),\n                                Mst->getBasePtr(), Mst->getOffset(), NewMask,\n                                Mst->getMemoryVT(), Mst->getMemOperand(),\n                                Mst->getAddressingMode());\n  }\n\n  SDValue Value = Mst->getValue();\n  if (Value.getOpcode() == ISD::TRUNCATE && Value.getNode()->hasOneUse() &&\n      TLI.isTruncStoreLegal(Value.getOperand(0).getValueType(),\n                            Mst->getMemoryVT())) {\n    return DAG.getMaskedStore(Mst->getChain(), SDLoc(N), Value.getOperand(0),\n                              Mst->getBasePtr(), Mst->getOffset(), Mask,\n                              Mst->getMemoryVT(), Mst->getMemOperand(),\n                              Mst->getAddressingMode(), true);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineStore(SDNode *N, SelectionDAG &DAG,\n                            TargetLowering::DAGCombinerInfo &DCI,\n                            const X86Subtarget &Subtarget) {\n  StoreSDNode *St = cast<StoreSDNode>(N);\n  EVT StVT = St->getMemoryVT();\n  SDLoc dl(St);\n  SDValue StoredVal = St->getValue();\n  EVT VT = StoredVal.getValueType();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // Convert a store of vXi1 into a store of iX and a bitcast.\n  if (!Subtarget.hasAVX512() && VT == StVT && VT.isVector() &&\n      VT.getVectorElementType() == MVT::i1) {\n\n    EVT NewVT = EVT::getIntegerVT(*DAG.getContext(), VT.getVectorNumElements());\n    StoredVal = DAG.getBitcast(NewVT, StoredVal);\n\n    return DAG.getStore(St->getChain(), dl, StoredVal, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  // If this is a store of a scalar_to_vector to v1i1, just use a scalar store.\n  // This will avoid a copy to k-register.\n  if (VT == MVT::v1i1 && VT == StVT && Subtarget.hasAVX512() &&\n      StoredVal.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n      StoredVal.getOperand(0).getValueType() == MVT::i8) {\n    SDValue Val = StoredVal.getOperand(0);\n    // We must store zeros to the unused bits.\n    Val = DAG.getZeroExtendInReg(Val, dl, MVT::i1);\n    return DAG.getStore(St->getChain(), dl, Val,\n                        St->getBasePtr(), St->getPointerInfo(),\n                        St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  // Widen v2i1/v4i1 stores to v8i1.\n  if ((VT == MVT::v1i1 || VT == MVT::v2i1 || VT == MVT::v4i1) && VT == StVT &&\n      Subtarget.hasAVX512()) {\n    unsigned NumConcats = 8 / VT.getVectorNumElements();\n    // We must store zeros to the unused bits.\n    SmallVector<SDValue, 4> Ops(NumConcats, DAG.getConstant(0, dl, VT));\n    Ops[0] = StoredVal;\n    StoredVal = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8i1, Ops);\n    return DAG.getStore(St->getChain(), dl, StoredVal, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  // Turn vXi1 stores of constants into a scalar store.\n  if ((VT == MVT::v8i1 || VT == MVT::v16i1 || VT == MVT::v32i1 ||\n       VT == MVT::v64i1) && VT == StVT && TLI.isTypeLegal(VT) &&\n      ISD::isBuildVectorOfConstantSDNodes(StoredVal.getNode())) {\n    // If its a v64i1 store without 64-bit support, we need two stores.\n    if (!DCI.isBeforeLegalize() && VT == MVT::v64i1 && !Subtarget.is64Bit()) {\n      SDValue Lo = DAG.getBuildVector(MVT::v32i1, dl,\n                                      StoredVal->ops().slice(0, 32));\n      Lo = combinevXi1ConstantToInteger(Lo, DAG);\n      SDValue Hi = DAG.getBuildVector(MVT::v32i1, dl,\n                                      StoredVal->ops().slice(32, 32));\n      Hi = combinevXi1ConstantToInteger(Hi, DAG);\n\n      SDValue Ptr0 = St->getBasePtr();\n      SDValue Ptr1 = DAG.getMemBasePlusOffset(Ptr0, TypeSize::Fixed(4), dl);\n\n      SDValue Ch0 =\n          DAG.getStore(St->getChain(), dl, Lo, Ptr0, St->getPointerInfo(),\n                       St->getOriginalAlign(),\n                       St->getMemOperand()->getFlags());\n      SDValue Ch1 =\n          DAG.getStore(St->getChain(), dl, Hi, Ptr1,\n                       St->getPointerInfo().getWithOffset(4),\n                       St->getOriginalAlign(),\n                       St->getMemOperand()->getFlags());\n      return DAG.getNode(ISD::TokenFactor, dl, MVT::Other, Ch0, Ch1);\n    }\n\n    StoredVal = combinevXi1ConstantToInteger(StoredVal, DAG);\n    return DAG.getStore(St->getChain(), dl, StoredVal, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  // If we are saving a 32-byte vector and 32-byte stores are slow, such as on\n  // Sandy Bridge, perform two 16-byte stores.\n  bool Fast;\n  if (VT.is256BitVector() && StVT == VT &&\n      TLI.allowsMemoryAccess(*DAG.getContext(), DAG.getDataLayout(), VT,\n                             *St->getMemOperand(), &Fast) &&\n      !Fast) {\n    unsigned NumElems = VT.getVectorNumElements();\n    if (NumElems < 2)\n      return SDValue();\n\n    return splitVectorStore(St, DAG);\n  }\n\n  // Split under-aligned vector non-temporal stores.\n  if (St->isNonTemporal() && StVT == VT &&\n      St->getAlignment() < VT.getStoreSize()) {\n    // ZMM/YMM nt-stores - either it can be stored as a series of shorter\n    // vectors or the legalizer can scalarize it to use MOVNTI.\n    if (VT.is256BitVector() || VT.is512BitVector()) {\n      unsigned NumElems = VT.getVectorNumElements();\n      if (NumElems < 2)\n        return SDValue();\n      return splitVectorStore(St, DAG);\n    }\n\n    // XMM nt-stores - scalarize this to f64 nt-stores on SSE4A, else i32/i64\n    // to use MOVNTI.\n    if (VT.is128BitVector() && Subtarget.hasSSE2()) {\n      MVT NTVT = Subtarget.hasSSE4A()\n                     ? MVT::v2f64\n                     : (TLI.isTypeLegal(MVT::i64) ? MVT::v2i64 : MVT::v4i32);\n      return scalarizeVectorStore(St, NTVT, DAG);\n    }\n  }\n\n  // Try to optimize v16i16->v16i8 truncating stores when BWI is not\n  // supported, but avx512f is by extending to v16i32 and truncating.\n  if (!St->isTruncatingStore() && VT == MVT::v16i8 && !Subtarget.hasBWI() &&\n      St->getValue().getOpcode() == ISD::TRUNCATE &&\n      St->getValue().getOperand(0).getValueType() == MVT::v16i16 &&\n      TLI.isTruncStoreLegal(MVT::v16i32, MVT::v16i8) &&\n      St->getValue().hasOneUse() && !DCI.isBeforeLegalizeOps()) {\n    SDValue Ext = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::v16i32, St->getValue());\n    return DAG.getTruncStore(St->getChain(), dl, Ext, St->getBasePtr(),\n                             MVT::v16i8, St->getMemOperand());\n  }\n\n  // Try to fold a VTRUNCUS or VTRUNCS into a truncating store.\n  if (!St->isTruncatingStore() && StoredVal.hasOneUse() &&\n      (StoredVal.getOpcode() == X86ISD::VTRUNCUS ||\n       StoredVal.getOpcode() == X86ISD::VTRUNCS) &&\n      TLI.isTruncStoreLegal(StoredVal.getOperand(0).getValueType(), VT)) {\n    bool IsSigned = StoredVal.getOpcode() == X86ISD::VTRUNCS;\n    return EmitTruncSStore(IsSigned, St->getChain(),\n                           dl, StoredVal.getOperand(0), St->getBasePtr(),\n                           VT, St->getMemOperand(), DAG);\n  }\n\n  // Try to fold a extract_element(VTRUNC) pattern into a truncating store.\n  if (!St->isTruncatingStore() && StoredVal.hasOneUse()) {\n    auto IsExtractedElement = [](SDValue V) {\n      if (V.getOpcode() == ISD::TRUNCATE && V.getOperand(0).hasOneUse())\n        V = V.getOperand(0);\n      unsigned Opc = V.getOpcode();\n      if (Opc == ISD::EXTRACT_VECTOR_ELT || Opc == X86ISD::PEXTRW) {\n        if (V.getOperand(0).hasOneUse() && isNullConstant(V.getOperand(1)))\n          return V.getOperand(0);\n      }\n      return SDValue();\n    };\n    if (SDValue Extract = IsExtractedElement(StoredVal)) {\n      SDValue Trunc = peekThroughOneUseBitcasts(Extract);\n      if (Trunc.getOpcode() == X86ISD::VTRUNC) {\n        SDValue Src = Trunc.getOperand(0);\n        MVT DstVT = Trunc.getSimpleValueType();\n        MVT SrcVT = Src.getSimpleValueType();\n        unsigned NumSrcElts = SrcVT.getVectorNumElements();\n        unsigned NumTruncBits = DstVT.getScalarSizeInBits() * NumSrcElts;\n        MVT TruncVT = MVT::getVectorVT(DstVT.getScalarType(), NumSrcElts);\n        if (NumTruncBits == VT.getSizeInBits() &&\n            TLI.isTruncStoreLegal(SrcVT, TruncVT)) {\n          return DAG.getTruncStore(St->getChain(), dl, Src, St->getBasePtr(),\n                                   TruncVT, St->getMemOperand());\n        }\n      }\n    }\n  }\n\n  // Optimize trunc store (of multiple scalars) to shuffle and store.\n  // First, pack all of the elements in one place. Next, store to memory\n  // in fewer chunks.\n  if (St->isTruncatingStore() && VT.isVector()) {\n    // Check if we can detect an AVG pattern from the truncation. If yes,\n    // replace the trunc store by a normal store with the result of X86ISD::AVG\n    // instruction.\n    if (DCI.isBeforeLegalize() || TLI.isTypeLegal(St->getMemoryVT()))\n      if (SDValue Avg = detectAVGPattern(St->getValue(), St->getMemoryVT(), DAG,\n                                         Subtarget, dl))\n        return DAG.getStore(St->getChain(), dl, Avg, St->getBasePtr(),\n                            St->getPointerInfo(), St->getOriginalAlign(),\n                            St->getMemOperand()->getFlags());\n\n    if (TLI.isTruncStoreLegal(VT, StVT)) {\n      if (SDValue Val = detectSSatPattern(St->getValue(), St->getMemoryVT()))\n        return EmitTruncSStore(true /* Signed saturation */, St->getChain(),\n                               dl, Val, St->getBasePtr(),\n                               St->getMemoryVT(), St->getMemOperand(), DAG);\n      if (SDValue Val = detectUSatPattern(St->getValue(), St->getMemoryVT(),\n                                          DAG, dl))\n        return EmitTruncSStore(false /* Unsigned saturation */, St->getChain(),\n                               dl, Val, St->getBasePtr(),\n                               St->getMemoryVT(), St->getMemOperand(), DAG);\n    }\n\n    return SDValue();\n  }\n\n  // Cast ptr32 and ptr64 pointers to the default address space before a store.\n  unsigned AddrSpace = St->getAddressSpace();\n  if (AddrSpace == X86AS::PTR64 || AddrSpace == X86AS::PTR32_SPTR ||\n      AddrSpace == X86AS::PTR32_UPTR) {\n    MVT PtrVT = TLI.getPointerTy(DAG.getDataLayout());\n    if (PtrVT != St->getBasePtr().getSimpleValueType()) {\n      SDValue Cast =\n          DAG.getAddrSpaceCast(dl, PtrVT, St->getBasePtr(), AddrSpace, 0);\n      return DAG.getStore(St->getChain(), dl, StoredVal, Cast,\n                          St->getPointerInfo(), St->getOriginalAlign(),\n                          St->getMemOperand()->getFlags(), St->getAAInfo());\n    }\n  }\n\n  // Turn load->store of MMX types into GPR load/stores.  This avoids clobbering\n  // the FP state in cases where an emms may be missing.\n  // A preferable solution to the general problem is to figure out the right\n  // places to insert EMMS.  This qualifies as a quick hack.\n\n  // Similarly, turn load->store of i64 into double load/stores in 32-bit mode.\n  if (VT.getSizeInBits() != 64)\n    return SDValue();\n\n  const Function &F = DAG.getMachineFunction().getFunction();\n  bool NoImplicitFloatOps = F.hasFnAttribute(Attribute::NoImplicitFloat);\n  bool F64IsLegal =\n      !Subtarget.useSoftFloat() && !NoImplicitFloatOps && Subtarget.hasSSE2();\n  if ((VT == MVT::i64 && F64IsLegal && !Subtarget.is64Bit()) &&\n      isa<LoadSDNode>(St->getValue()) &&\n      cast<LoadSDNode>(St->getValue())->isSimple() &&\n      St->getChain().hasOneUse() && St->isSimple()) {\n    LoadSDNode *Ld = cast<LoadSDNode>(St->getValue().getNode());\n\n    if (!ISD::isNormalLoad(Ld))\n      return SDValue();\n\n    // Avoid the transformation if there are multiple uses of the loaded value.\n    if (!Ld->hasNUsesOfValue(1, 0))\n      return SDValue();\n\n    SDLoc LdDL(Ld);\n    SDLoc StDL(N);\n    // Lower to a single movq load/store pair.\n    SDValue NewLd = DAG.getLoad(MVT::f64, LdDL, Ld->getChain(),\n                                Ld->getBasePtr(), Ld->getMemOperand());\n\n    // Make sure new load is placed in same chain order.\n    DAG.makeEquivalentMemoryOrdering(Ld, NewLd);\n    return DAG.getStore(St->getChain(), StDL, NewLd, St->getBasePtr(),\n                        St->getMemOperand());\n  }\n\n  // This is similar to the above case, but here we handle a scalar 64-bit\n  // integer store that is extracted from a vector on a 32-bit target.\n  // If we have SSE2, then we can treat it like a floating-point double\n  // to get past legalization. The execution dependencies fixup pass will\n  // choose the optimal machine instruction for the store if this really is\n  // an integer or v2f32 rather than an f64.\n  if (VT == MVT::i64 && F64IsLegal && !Subtarget.is64Bit() &&\n      St->getOperand(1).getOpcode() == ISD::EXTRACT_VECTOR_ELT) {\n    SDValue OldExtract = St->getOperand(1);\n    SDValue ExtOp0 = OldExtract.getOperand(0);\n    unsigned VecSize = ExtOp0.getValueSizeInBits();\n    EVT VecVT = EVT::getVectorVT(*DAG.getContext(), MVT::f64, VecSize / 64);\n    SDValue BitCast = DAG.getBitcast(VecVT, ExtOp0);\n    SDValue NewExtract = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::f64,\n                                     BitCast, OldExtract.getOperand(1));\n    return DAG.getStore(St->getChain(), dl, NewExtract, St->getBasePtr(),\n                        St->getPointerInfo(), St->getOriginalAlign(),\n                        St->getMemOperand()->getFlags());\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineVEXTRACT_STORE(SDNode *N, SelectionDAG &DAG,\n                                     TargetLowering::DAGCombinerInfo &DCI,\n                                     const X86Subtarget &Subtarget) {\n  auto *St = cast<MemIntrinsicSDNode>(N);\n\n  SDValue StoredVal = N->getOperand(1);\n  MVT VT = StoredVal.getSimpleValueType();\n  EVT MemVT = St->getMemoryVT();\n\n  // Figure out which elements we demand.\n  unsigned StElts = MemVT.getSizeInBits() / VT.getScalarSizeInBits();\n  APInt DemandedElts = APInt::getLowBitsSet(VT.getVectorNumElements(), StElts);\n\n  APInt KnownUndef, KnownZero;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.SimplifyDemandedVectorElts(StoredVal, DemandedElts, KnownUndef,\n                                     KnownZero, DCI)) {\n    if (N->getOpcode() != ISD::DELETED_NODE)\n      DCI.AddToWorklist(N);\n    return SDValue(N, 0);\n  }\n\n  return SDValue();\n}\n\n/// Return 'true' if this vector operation is \"horizontal\"\n/// and return the operands for the horizontal operation in LHS and RHS.  A\n/// horizontal operation performs the binary operation on successive elements\n/// of its first operand, then on successive elements of its second operand,\n/// returning the resulting values in a vector.  For example, if\n///   A = < float a0, float a1, float a2, float a3 >\n/// and\n///   B = < float b0, float b1, float b2, float b3 >\n/// then the result of doing a horizontal operation on A and B is\n///   A horizontal-op B = < a0 op a1, a2 op a3, b0 op b1, b2 op b3 >.\n/// In short, LHS and RHS are inspected to see if LHS op RHS is of the form\n/// A horizontal-op B, for some already available A and B, and if so then LHS is\n/// set to A, RHS to B, and the routine returns 'true'.\nstatic bool isHorizontalBinOp(unsigned HOpcode, SDValue &LHS, SDValue &RHS,\n                              SelectionDAG &DAG, const X86Subtarget &Subtarget,\n                              bool IsCommutative,\n                              SmallVectorImpl<int> &PostShuffleMask) {\n  // If either operand is undef, bail out. The binop should be simplified.\n  if (LHS.isUndef() || RHS.isUndef())\n    return false;\n\n  // Look for the following pattern:\n  //   A = < float a0, float a1, float a2, float a3 >\n  //   B = < float b0, float b1, float b2, float b3 >\n  // and\n  //   LHS = VECTOR_SHUFFLE A, B, <0, 2, 4, 6>\n  //   RHS = VECTOR_SHUFFLE A, B, <1, 3, 5, 7>\n  // then LHS op RHS = < a0 op a1, a2 op a3, b0 op b1, b2 op b3 >\n  // which is A horizontal-op B.\n\n  MVT VT = LHS.getSimpleValueType();\n  assert((VT.is128BitVector() || VT.is256BitVector()) &&\n         \"Unsupported vector type for horizontal add/sub\");\n  unsigned NumElts = VT.getVectorNumElements();\n\n  // TODO - can we make a general helper method that does all of this for us?\n  auto GetShuffle = [&](SDValue Op, SDValue &N0, SDValue &N1,\n                        SmallVectorImpl<int> &ShuffleMask) {\n    if (Op.getOpcode() == ISD::VECTOR_SHUFFLE) {\n      if (!Op.getOperand(0).isUndef())\n        N0 = Op.getOperand(0);\n      if (!Op.getOperand(1).isUndef())\n        N1 = Op.getOperand(1);\n      ArrayRef<int> Mask = cast<ShuffleVectorSDNode>(Op)->getMask();\n      ShuffleMask.append(Mask.begin(), Mask.end());\n      return;\n    }\n    bool UseSubVector = false;\n    if (Op.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n        Op.getOperand(0).getValueType().is256BitVector() &&\n        llvm::isNullConstant(Op.getOperand(1))) {\n      Op = Op.getOperand(0);\n      UseSubVector = true;\n    }\n    bool IsUnary;\n    SmallVector<SDValue, 2> SrcOps;\n    SmallVector<int, 16> SrcShuffleMask;\n    SDValue BC = peekThroughBitcasts(Op);\n    if (isTargetShuffle(BC.getOpcode()) &&\n        getTargetShuffleMask(BC.getNode(), BC.getSimpleValueType(), false,\n                             SrcOps, SrcShuffleMask, IsUnary)) {\n      if (!UseSubVector && SrcShuffleMask.size() == NumElts &&\n          SrcOps.size() <= 2) {\n        N0 = SrcOps.size() > 0 ? SrcOps[0] : SDValue();\n        N1 = SrcOps.size() > 1 ? SrcOps[1] : SDValue();\n        ShuffleMask.append(SrcShuffleMask.begin(), SrcShuffleMask.end());\n      }\n      if (UseSubVector && (SrcShuffleMask.size() == (NumElts * 2)) &&\n          SrcOps.size() == 1) {\n        N0 = extract128BitVector(SrcOps[0], 0, DAG, SDLoc(Op));\n        N1 = extract128BitVector(SrcOps[0], NumElts, DAG, SDLoc(Op));\n        ArrayRef<int> Mask = ArrayRef<int>(SrcShuffleMask).slice(0, NumElts);\n        ShuffleMask.append(Mask.begin(), Mask.end());\n      }\n    }\n  };\n\n  // View LHS in the form\n  //   LHS = VECTOR_SHUFFLE A, B, LMask\n  // If LHS is not a shuffle, then pretend it is the identity shuffle:\n  //   LHS = VECTOR_SHUFFLE LHS, undef, <0, 1, ..., N-1>\n  // NOTE: A default initialized SDValue represents an UNDEF of type VT.\n  SDValue A, B;\n  SmallVector<int, 16> LMask;\n  GetShuffle(LHS, A, B, LMask);\n\n  // Likewise, view RHS in the form\n  //   RHS = VECTOR_SHUFFLE C, D, RMask\n  SDValue C, D;\n  SmallVector<int, 16> RMask;\n  GetShuffle(RHS, C, D, RMask);\n\n  // At least one of the operands should be a vector shuffle.\n  unsigned NumShuffles = (LMask.empty() ? 0 : 1) + (RMask.empty() ? 0 : 1);\n  if (NumShuffles == 0)\n    return false;\n\n  if (LMask.empty()) {\n    A = LHS;\n    for (unsigned i = 0; i != NumElts; ++i)\n      LMask.push_back(i);\n  }\n\n  if (RMask.empty()) {\n    C = RHS;\n    for (unsigned i = 0; i != NumElts; ++i)\n      RMask.push_back(i);\n  }\n\n  // If A and B occur in reverse order in RHS, then canonicalize by commuting\n  // RHS operands and shuffle mask.\n  if (A != C) {\n    std::swap(C, D);\n    ShuffleVectorSDNode::commuteMask(RMask);\n  }\n  // Check that the shuffles are both shuffling the same vectors.\n  if (!(A == C && B == D))\n    return false;\n\n  PostShuffleMask.clear();\n  PostShuffleMask.append(NumElts, SM_SentinelUndef);\n\n  // LHS and RHS are now:\n  //   LHS = shuffle A, B, LMask\n  //   RHS = shuffle A, B, RMask\n  // Check that the masks correspond to performing a horizontal operation.\n  // AVX defines horizontal add/sub to operate independently on 128-bit lanes,\n  // so we just repeat the inner loop if this is a 256-bit op.\n  unsigned Num128BitChunks = VT.getSizeInBits() / 128;\n  unsigned NumEltsPer128BitChunk = NumElts / Num128BitChunks;\n  unsigned NumEltsPer64BitChunk = NumEltsPer128BitChunk / 2;\n  assert((NumEltsPer128BitChunk % 2 == 0) &&\n         \"Vector type should have an even number of elements in each lane\");\n  for (unsigned j = 0; j != NumElts; j += NumEltsPer128BitChunk) {\n    for (unsigned i = 0; i != NumEltsPer128BitChunk; ++i) {\n      // Ignore undefined components.\n      int LIdx = LMask[i + j], RIdx = RMask[i + j];\n      if (LIdx < 0 || RIdx < 0 ||\n          (!A.getNode() && (LIdx < (int)NumElts || RIdx < (int)NumElts)) ||\n          (!B.getNode() && (LIdx >= (int)NumElts || RIdx >= (int)NumElts)))\n        continue;\n\n      // Check that successive odd/even elements are being operated on. If not,\n      // this is not a horizontal operation.\n      if (!((RIdx & 1) == 1 && (LIdx + 1) == RIdx) &&\n          !((LIdx & 1) == 1 && (RIdx + 1) == LIdx && IsCommutative))\n        return false;\n\n      // Compute the post-shuffle mask index based on where the element\n      // is stored in the HOP result, and where it needs to be moved to.\n      int Base = LIdx & ~1u;\n      int Index = ((Base % NumEltsPer128BitChunk) / 2) +\n                  ((Base % NumElts) & ~(NumEltsPer128BitChunk - 1));\n\n      // The  low half of the 128-bit result must choose from A.\n      // The high half of the 128-bit result must choose from B,\n      // unless B is undef. In that case, we are always choosing from A.\n      if ((B && Base >= (int)NumElts) || (!B && i >= NumEltsPer64BitChunk))\n        Index += NumEltsPer64BitChunk;\n      PostShuffleMask[i + j] = Index;\n    }\n  }\n\n  SDValue NewLHS = A.getNode() ? A : B; // If A is 'UNDEF', use B for it.\n  SDValue NewRHS = B.getNode() ? B : A; // If B is 'UNDEF', use A for it.\n\n  bool IsIdentityPostShuffle =\n      isSequentialOrUndefInRange(PostShuffleMask, 0, NumElts, 0);\n  if (IsIdentityPostShuffle)\n    PostShuffleMask.clear();\n\n  // Avoid 128-bit multi lane shuffles if pre-AVX2 and FP (integer will split).\n  if (!IsIdentityPostShuffle && !Subtarget.hasAVX2() && VT.isFloatingPoint() &&\n      isMultiLaneShuffleMask(128, VT.getScalarSizeInBits(), PostShuffleMask))\n    return false;\n\n  // If the source nodes are already used in HorizOps then always accept this.\n  // Shuffle folding should merge these back together.\n  bool FoundHorizLHS = llvm::any_of(NewLHS->uses(), [&](SDNode *User) {\n    return User->getOpcode() == HOpcode && User->getValueType(0) == VT;\n  });\n  bool FoundHorizRHS = llvm::any_of(NewRHS->uses(), [&](SDNode *User) {\n    return User->getOpcode() == HOpcode && User->getValueType(0) == VT;\n  });\n  bool ForceHorizOp = FoundHorizLHS && FoundHorizRHS;\n\n  // Assume a SingleSource HOP if we only shuffle one input and don't need to\n  // shuffle the result.\n  if (!ForceHorizOp &&\n      !shouldUseHorizontalOp(NewLHS == NewRHS &&\n                                 (NumShuffles < 2 || !IsIdentityPostShuffle),\n                             DAG, Subtarget))\n    return false;\n\n  LHS = DAG.getBitcast(VT, NewLHS);\n  RHS = DAG.getBitcast(VT, NewRHS);\n  return true;\n}\n\n/// Do target-specific dag combines on floating-point adds/subs.\nstatic SDValue combineFaddFsub(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n  bool IsFadd = N->getOpcode() == ISD::FADD;\n  auto HorizOpcode = IsFadd ? X86ISD::FHADD : X86ISD::FHSUB;\n  assert((IsFadd || N->getOpcode() == ISD::FSUB) && \"Wrong opcode\");\n\n  // Try to synthesize horizontal add/sub from adds/subs of shuffles.\n  SmallVector<int, 8> PostShuffleMask;\n  if (((Subtarget.hasSSE3() && (VT == MVT::v4f32 || VT == MVT::v2f64)) ||\n       (Subtarget.hasAVX() && (VT == MVT::v8f32 || VT == MVT::v4f64))) &&\n      isHorizontalBinOp(HorizOpcode, LHS, RHS, DAG, Subtarget, IsFadd,\n                        PostShuffleMask)) {\n    SDValue HorizBinOp = DAG.getNode(HorizOpcode, SDLoc(N), VT, LHS, RHS);\n    if (!PostShuffleMask.empty())\n      HorizBinOp = DAG.getVectorShuffle(VT, SDLoc(HorizBinOp), HorizBinOp,\n                                        DAG.getUNDEF(VT), PostShuffleMask);\n    return HorizBinOp;\n  }\n\n  return SDValue();\n}\n\n/// Attempt to pre-truncate inputs to arithmetic ops if it will simplify\n/// the codegen.\n/// e.g. TRUNC( BINOP( X, Y ) ) --> BINOP( TRUNC( X ), TRUNC( Y ) )\n/// TODO: This overlaps with the generic combiner's visitTRUNCATE. Remove\n///       anything that is guaranteed to be transformed by DAGCombiner.\nstatic SDValue combineTruncatedArithmetic(SDNode *N, SelectionDAG &DAG,\n                                          const X86Subtarget &Subtarget,\n                                          const SDLoc &DL) {\n  assert(N->getOpcode() == ISD::TRUNCATE && \"Wrong opcode\");\n  SDValue Src = N->getOperand(0);\n  unsigned SrcOpcode = Src.getOpcode();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  EVT VT = N->getValueType(0);\n  EVT SrcVT = Src.getValueType();\n\n  auto IsFreeTruncation = [VT](SDValue Op) {\n    unsigned TruncSizeInBits = VT.getScalarSizeInBits();\n\n    // See if this has been extended from a smaller/equal size to\n    // the truncation size, allowing a truncation to combine with the extend.\n    unsigned Opcode = Op.getOpcode();\n    if ((Opcode == ISD::ANY_EXTEND || Opcode == ISD::SIGN_EXTEND ||\n         Opcode == ISD::ZERO_EXTEND) &&\n        Op.getOperand(0).getScalarValueSizeInBits() <= TruncSizeInBits)\n      return true;\n\n    // See if this is a single use constant which can be constant folded.\n    // NOTE: We don't peek throught bitcasts here because there is currently\n    // no support for constant folding truncate+bitcast+vector_of_constants. So\n    // we'll just send up with a truncate on both operands which will\n    // get turned back into (truncate (binop)) causing an infinite loop.\n    return ISD::isBuildVectorOfConstantSDNodes(Op.getNode());\n  };\n\n  auto TruncateArithmetic = [&](SDValue N0, SDValue N1) {\n    SDValue Trunc0 = DAG.getNode(ISD::TRUNCATE, DL, VT, N0);\n    SDValue Trunc1 = DAG.getNode(ISD::TRUNCATE, DL, VT, N1);\n    return DAG.getNode(SrcOpcode, DL, VT, Trunc0, Trunc1);\n  };\n\n  // Don't combine if the operation has other uses.\n  if (!Src.hasOneUse())\n    return SDValue();\n\n  // Only support vector truncation for now.\n  // TODO: i64 scalar math would benefit as well.\n  if (!VT.isVector())\n    return SDValue();\n\n  // In most cases its only worth pre-truncating if we're only facing the cost\n  // of one truncation.\n  // i.e. if one of the inputs will constant fold or the input is repeated.\n  switch (SrcOpcode) {\n  case ISD::MUL:\n    // X86 is rubbish at scalar and vector i64 multiplies (until AVX512DQ) - its\n    // better to truncate if we have the chance.\n    if (SrcVT.getScalarType() == MVT::i64 &&\n        TLI.isOperationLegal(SrcOpcode, VT) &&\n        !TLI.isOperationLegal(SrcOpcode, SrcVT))\n      return TruncateArithmetic(Src.getOperand(0), Src.getOperand(1));\n    LLVM_FALLTHROUGH;\n  case ISD::AND:\n  case ISD::XOR:\n  case ISD::OR:\n  case ISD::ADD:\n  case ISD::SUB: {\n    SDValue Op0 = Src.getOperand(0);\n    SDValue Op1 = Src.getOperand(1);\n    if (TLI.isOperationLegal(SrcOpcode, VT) &&\n        (Op0 == Op1 || IsFreeTruncation(Op0) || IsFreeTruncation(Op1)))\n      return TruncateArithmetic(Op0, Op1);\n    break;\n  }\n  }\n\n  return SDValue();\n}\n\n/// Truncate using ISD::AND mask and X86ISD::PACKUS.\n/// e.g. trunc <8 x i32> X to <8 x i16> -->\n/// MaskX = X & 0xffff (clear high bits to prevent saturation)\n/// packus (extract_subv MaskX, 0), (extract_subv MaskX, 1)\nstatic SDValue combineVectorTruncationWithPACKUS(SDNode *N, const SDLoc &DL,\n                                                 const X86Subtarget &Subtarget,\n                                                 SelectionDAG &DAG) {\n  SDValue In = N->getOperand(0);\n  EVT InVT = In.getValueType();\n  EVT OutVT = N->getValueType(0);\n\n  APInt Mask = APInt::getLowBitsSet(InVT.getScalarSizeInBits(),\n                                    OutVT.getScalarSizeInBits());\n  In = DAG.getNode(ISD::AND, DL, InVT, In, DAG.getConstant(Mask, DL, InVT));\n  return truncateVectorWithPACK(X86ISD::PACKUS, OutVT, In, DL, DAG, Subtarget);\n}\n\n/// Truncate a group of v4i32 into v8i16 using X86ISD::PACKSS.\nstatic SDValue combineVectorTruncationWithPACKSS(SDNode *N, const SDLoc &DL,\n                                                 const X86Subtarget &Subtarget,\n                                                 SelectionDAG &DAG) {\n  SDValue In = N->getOperand(0);\n  EVT InVT = In.getValueType();\n  EVT OutVT = N->getValueType(0);\n  In = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, InVT, In,\n                   DAG.getValueType(OutVT));\n  return truncateVectorWithPACK(X86ISD::PACKSS, OutVT, In, DL, DAG, Subtarget);\n}\n\n/// This function transforms truncation from vXi32/vXi64 to vXi8/vXi16 into\n/// X86ISD::PACKUS/X86ISD::PACKSS operations. We do it here because after type\n/// legalization the truncation will be translated into a BUILD_VECTOR with each\n/// element that is extracted from a vector and then truncated, and it is\n/// difficult to do this optimization based on them.\nstatic SDValue combineVectorTruncation(SDNode *N, SelectionDAG &DAG,\n                                       const X86Subtarget &Subtarget) {\n  EVT OutVT = N->getValueType(0);\n  if (!OutVT.isVector())\n    return SDValue();\n\n  SDValue In = N->getOperand(0);\n  if (!In.getValueType().isSimple())\n    return SDValue();\n\n  EVT InVT = In.getValueType();\n  unsigned NumElems = OutVT.getVectorNumElements();\n\n  // TODO: On AVX2, the behavior of X86ISD::PACKUS is different from that on\n  // SSE2, and we need to take care of it specially.\n  // AVX512 provides vpmovdb.\n  if (!Subtarget.hasSSE2() || Subtarget.hasAVX2())\n    return SDValue();\n\n  EVT OutSVT = OutVT.getVectorElementType();\n  EVT InSVT = InVT.getVectorElementType();\n  if (!((InSVT == MVT::i16 || InSVT == MVT::i32 || InSVT == MVT::i64) &&\n        (OutSVT == MVT::i8 || OutSVT == MVT::i16) && isPowerOf2_32(NumElems) &&\n        NumElems >= 8))\n    return SDValue();\n\n  // SSSE3's pshufb results in less instructions in the cases below.\n  if (Subtarget.hasSSSE3() && NumElems == 8 &&\n      ((OutSVT == MVT::i8 && InSVT != MVT::i64) ||\n       (InSVT == MVT::i32 && OutSVT == MVT::i16)))\n    return SDValue();\n\n  SDLoc DL(N);\n  // SSE2 provides PACKUS for only 2 x v8i16 -> v16i8 and SSE4.1 provides PACKUS\n  // for 2 x v4i32 -> v8i16. For SSSE3 and below, we need to use PACKSS to\n  // truncate 2 x v4i32 to v8i16.\n  if (Subtarget.hasSSE41() || OutSVT == MVT::i8)\n    return combineVectorTruncationWithPACKUS(N, DL, Subtarget, DAG);\n  if (InSVT == MVT::i32)\n    return combineVectorTruncationWithPACKSS(N, DL, Subtarget, DAG);\n\n  return SDValue();\n}\n\n/// This function transforms vector truncation of 'extended sign-bits' or\n/// 'extended zero-bits' values.\n/// vXi16/vXi32/vXi64 to vXi8/vXi16/vXi32 into X86ISD::PACKSS/PACKUS operations.\nstatic SDValue combineVectorSignBitsTruncation(SDNode *N, const SDLoc &DL,\n                                               SelectionDAG &DAG,\n                                               const X86Subtarget &Subtarget) {\n  // Requires SSE2.\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  if (!N->getValueType(0).isVector() || !N->getValueType(0).isSimple())\n    return SDValue();\n\n  SDValue In = N->getOperand(0);\n  if (!In.getValueType().isSimple())\n    return SDValue();\n\n  MVT VT = N->getValueType(0).getSimpleVT();\n  MVT SVT = VT.getScalarType();\n\n  MVT InVT = In.getValueType().getSimpleVT();\n  MVT InSVT = InVT.getScalarType();\n\n  // Check we have a truncation suited for PACKSS/PACKUS.\n  if (!isPowerOf2_32(VT.getVectorNumElements()))\n    return SDValue();\n  if (SVT != MVT::i8 && SVT != MVT::i16 && SVT != MVT::i32)\n    return SDValue();\n  if (InSVT != MVT::i16 && InSVT != MVT::i32 && InSVT != MVT::i64)\n    return SDValue();\n\n  // Truncation to sub-128bit vXi32 can be better handled with shuffles.\n  if (SVT == MVT::i32 && VT.getSizeInBits() < 128)\n    return SDValue();\n\n  // AVX512 has fast truncate, but if the input is already going to be split,\n  // there's no harm in trying pack.\n  if (Subtarget.hasAVX512() &&\n      !(!Subtarget.useAVX512Regs() && VT.is256BitVector() &&\n        InVT.is512BitVector())) {\n    // PACK should still be worth it for 128-bit vectors if the sources were\n    // originally concatenated from subvectors.\n    SmallVector<SDValue> ConcatOps;\n    if (VT.getSizeInBits() > 128 || !collectConcatOps(In.getNode(), ConcatOps))\n    return SDValue();\n  }\n\n  unsigned NumPackedSignBits = std::min<unsigned>(SVT.getSizeInBits(), 16);\n  unsigned NumPackedZeroBits = Subtarget.hasSSE41() ? NumPackedSignBits : 8;\n\n  // Use PACKUS if the input has zero-bits that extend all the way to the\n  // packed/truncated value. e.g. masks, zext_in_reg, etc.\n  KnownBits Known = DAG.computeKnownBits(In);\n  unsigned NumLeadingZeroBits = Known.countMinLeadingZeros();\n  if (NumLeadingZeroBits >= (InSVT.getSizeInBits() - NumPackedZeroBits))\n    return truncateVectorWithPACK(X86ISD::PACKUS, VT, In, DL, DAG, Subtarget);\n\n  // Use PACKSS if the input has sign-bits that extend all the way to the\n  // packed/truncated value. e.g. Comparison result, sext_in_reg, etc.\n  unsigned NumSignBits = DAG.ComputeNumSignBits(In);\n\n  // Don't use PACKSS for vXi64 -> vXi32 truncations unless we're dealing with\n  // a sign splat. ComputeNumSignBits struggles to see through BITCASTs later\n  // on and combines/simplifications can't then use it.\n  if (SVT == MVT::i32 && NumSignBits != InSVT.getSizeInBits())\n    return SDValue();\n\n  unsigned MinSignBits = InSVT.getSizeInBits() - NumPackedSignBits;\n  if (NumSignBits > MinSignBits)\n    return truncateVectorWithPACK(X86ISD::PACKSS, VT, In, DL, DAG, Subtarget);\n\n  // If we have a srl that only generates signbits that we will discard in\n  // the truncation then we can use PACKSS by converting the srl to a sra.\n  // SimplifyDemandedBits often relaxes sra to srl so we need to reverse it.\n  if (In.getOpcode() == ISD::SRL && N->isOnlyUserOf(In.getNode()))\n    if (const APInt *ShAmt = DAG.getValidShiftAmountConstant(\n            In, APInt::getAllOnesValue(VT.getVectorNumElements()))) {\n      if (*ShAmt == MinSignBits) {\n        SDValue NewIn = DAG.getNode(ISD::SRA, DL, InVT, In->ops());\n        return truncateVectorWithPACK(X86ISD::PACKSS, VT, NewIn, DL, DAG,\n                                      Subtarget);\n      }\n    }\n\n  return SDValue();\n}\n\n// Try to form a MULHU or MULHS node by looking for\n// (trunc (srl (mul ext, ext), 16))\n// TODO: This is X86 specific because we want to be able to handle wide types\n// before type legalization. But we can only do it if the vector will be\n// legalized via widening/splitting. Type legalization can't handle promotion\n// of a MULHU/MULHS. There isn't a way to convey this to the generic DAG\n// combiner.\nstatic SDValue combinePMULH(SDValue Src, EVT VT, const SDLoc &DL,\n                            SelectionDAG &DAG, const X86Subtarget &Subtarget) {\n  // First instruction should be a right shift of a multiply.\n  if (Src.getOpcode() != ISD::SRL ||\n      Src.getOperand(0).getOpcode() != ISD::MUL)\n    return SDValue();\n\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  // Only handle vXi16 types that are at least 128-bits unless they will be\n  // widened.\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::i16)\n    return SDValue();\n\n  // Input type should be at least vXi32.\n  EVT InVT = Src.getValueType();\n  if (InVT.getVectorElementType().getSizeInBits() < 32)\n    return SDValue();\n\n  // Need a shift by 16.\n  APInt ShiftAmt;\n  if (!ISD::isConstantSplatVector(Src.getOperand(1).getNode(), ShiftAmt) ||\n      ShiftAmt != 16)\n    return SDValue();\n\n  SDValue LHS = Src.getOperand(0).getOperand(0);\n  SDValue RHS = Src.getOperand(0).getOperand(1);\n\n  unsigned ExtOpc = LHS.getOpcode();\n  if ((ExtOpc != ISD::SIGN_EXTEND && ExtOpc != ISD::ZERO_EXTEND) ||\n      RHS.getOpcode() != ExtOpc)\n    return SDValue();\n\n  // Peek through the extends.\n  LHS = LHS.getOperand(0);\n  RHS = RHS.getOperand(0);\n\n  // Ensure the input types match.\n  if (LHS.getValueType() != VT || RHS.getValueType() != VT)\n    return SDValue();\n\n  unsigned Opc = ExtOpc == ISD::SIGN_EXTEND ? ISD::MULHS : ISD::MULHU;\n  return DAG.getNode(Opc, DL, VT, LHS, RHS);\n}\n\n// Attempt to match PMADDUBSW, which multiplies corresponding unsigned bytes\n// from one vector with signed bytes from another vector, adds together\n// adjacent pairs of 16-bit products, and saturates the result before\n// truncating to 16-bits.\n//\n// Which looks something like this:\n// (i16 (ssat (add (mul (zext (even elts (i8 A))), (sext (even elts (i8 B)))),\n//                 (mul (zext (odd elts (i8 A)), (sext (odd elts (i8 B))))))))\nstatic SDValue detectPMADDUBSW(SDValue In, EVT VT, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget,\n                               const SDLoc &DL) {\n  if (!VT.isVector() || !Subtarget.hasSSSE3())\n    return SDValue();\n\n  unsigned NumElems = VT.getVectorNumElements();\n  EVT ScalarVT = VT.getVectorElementType();\n  if (ScalarVT != MVT::i16 || NumElems < 8 || !isPowerOf2_32(NumElems))\n    return SDValue();\n\n  SDValue SSatVal = detectSSatPattern(In, VT);\n  if (!SSatVal || SSatVal.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  // Ok this is a signed saturation of an ADD. See if this ADD is adding pairs\n  // of multiplies from even/odd elements.\n  SDValue N0 = SSatVal.getOperand(0);\n  SDValue N1 = SSatVal.getOperand(1);\n\n  if (N0.getOpcode() != ISD::MUL || N1.getOpcode() != ISD::MUL)\n    return SDValue();\n\n  SDValue N00 = N0.getOperand(0);\n  SDValue N01 = N0.getOperand(1);\n  SDValue N10 = N1.getOperand(0);\n  SDValue N11 = N1.getOperand(1);\n\n  // TODO: Handle constant vectors and use knownbits/computenumsignbits?\n  // Canonicalize zero_extend to LHS.\n  if (N01.getOpcode() == ISD::ZERO_EXTEND)\n    std::swap(N00, N01);\n  if (N11.getOpcode() == ISD::ZERO_EXTEND)\n    std::swap(N10, N11);\n\n  // Ensure we have a zero_extend and a sign_extend.\n  if (N00.getOpcode() != ISD::ZERO_EXTEND ||\n      N01.getOpcode() != ISD::SIGN_EXTEND ||\n      N10.getOpcode() != ISD::ZERO_EXTEND ||\n      N11.getOpcode() != ISD::SIGN_EXTEND)\n    return SDValue();\n\n  // Peek through the extends.\n  N00 = N00.getOperand(0);\n  N01 = N01.getOperand(0);\n  N10 = N10.getOperand(0);\n  N11 = N11.getOperand(0);\n\n  // Ensure the extend is from vXi8.\n  if (N00.getValueType().getVectorElementType() != MVT::i8 ||\n      N01.getValueType().getVectorElementType() != MVT::i8 ||\n      N10.getValueType().getVectorElementType() != MVT::i8 ||\n      N11.getValueType().getVectorElementType() != MVT::i8)\n    return SDValue();\n\n  // All inputs should be build_vectors.\n  if (N00.getOpcode() != ISD::BUILD_VECTOR ||\n      N01.getOpcode() != ISD::BUILD_VECTOR ||\n      N10.getOpcode() != ISD::BUILD_VECTOR ||\n      N11.getOpcode() != ISD::BUILD_VECTOR)\n    return SDValue();\n\n  // N00/N10 are zero extended. N01/N11 are sign extended.\n\n  // For each element, we need to ensure we have an odd element from one vector\n  // multiplied by the odd element of another vector and the even element from\n  // one of the same vectors being multiplied by the even element from the\n  // other vector. So we need to make sure for each element i, this operator\n  // is being performed:\n  //  A[2 * i] * B[2 * i] + A[2 * i + 1] * B[2 * i + 1]\n  SDValue ZExtIn, SExtIn;\n  for (unsigned i = 0; i != NumElems; ++i) {\n    SDValue N00Elt = N00.getOperand(i);\n    SDValue N01Elt = N01.getOperand(i);\n    SDValue N10Elt = N10.getOperand(i);\n    SDValue N11Elt = N11.getOperand(i);\n    // TODO: Be more tolerant to undefs.\n    if (N00Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N01Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N10Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N11Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return SDValue();\n    auto *ConstN00Elt = dyn_cast<ConstantSDNode>(N00Elt.getOperand(1));\n    auto *ConstN01Elt = dyn_cast<ConstantSDNode>(N01Elt.getOperand(1));\n    auto *ConstN10Elt = dyn_cast<ConstantSDNode>(N10Elt.getOperand(1));\n    auto *ConstN11Elt = dyn_cast<ConstantSDNode>(N11Elt.getOperand(1));\n    if (!ConstN00Elt || !ConstN01Elt || !ConstN10Elt || !ConstN11Elt)\n      return SDValue();\n    unsigned IdxN00 = ConstN00Elt->getZExtValue();\n    unsigned IdxN01 = ConstN01Elt->getZExtValue();\n    unsigned IdxN10 = ConstN10Elt->getZExtValue();\n    unsigned IdxN11 = ConstN11Elt->getZExtValue();\n    // Add is commutative so indices can be reordered.\n    if (IdxN00 > IdxN10) {\n      std::swap(IdxN00, IdxN10);\n      std::swap(IdxN01, IdxN11);\n    }\n    // N0 indices be the even element. N1 indices must be the next odd element.\n    if (IdxN00 != 2 * i || IdxN10 != 2 * i + 1 ||\n        IdxN01 != 2 * i || IdxN11 != 2 * i + 1)\n      return SDValue();\n    SDValue N00In = N00Elt.getOperand(0);\n    SDValue N01In = N01Elt.getOperand(0);\n    SDValue N10In = N10Elt.getOperand(0);\n    SDValue N11In = N11Elt.getOperand(0);\n    // First time we find an input capture it.\n    if (!ZExtIn) {\n      ZExtIn = N00In;\n      SExtIn = N01In;\n    }\n    if (ZExtIn != N00In || SExtIn != N01In ||\n        ZExtIn != N10In || SExtIn != N11In)\n      return SDValue();\n  }\n\n  auto PMADDBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                         ArrayRef<SDValue> Ops) {\n    // Shrink by adding truncate nodes and let DAGCombine fold with the\n    // sources.\n    EVT InVT = Ops[0].getValueType();\n    assert(InVT.getScalarType() == MVT::i8 &&\n           \"Unexpected scalar element type\");\n    assert(InVT == Ops[1].getValueType() && \"Operands' types mismatch\");\n    EVT ResVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16,\n                                 InVT.getVectorNumElements() / 2);\n    return DAG.getNode(X86ISD::VPMADDUBSW, DL, ResVT, Ops[0], Ops[1]);\n  };\n  return SplitOpsAndApply(DAG, Subtarget, DL, VT, { ZExtIn, SExtIn },\n                          PMADDBuilder);\n}\n\nstatic SDValue combineTruncate(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue Src = N->getOperand(0);\n  SDLoc DL(N);\n\n  // Attempt to pre-truncate inputs to arithmetic ops instead.\n  if (SDValue V = combineTruncatedArithmetic(N, DAG, Subtarget, DL))\n    return V;\n\n  // Try to detect AVG pattern first.\n  if (SDValue Avg = detectAVGPattern(Src, VT, DAG, Subtarget, DL))\n    return Avg;\n\n  // Try to detect PMADD\n  if (SDValue PMAdd = detectPMADDUBSW(Src, VT, DAG, Subtarget, DL))\n    return PMAdd;\n\n  // Try to combine truncation with signed/unsigned saturation.\n  if (SDValue Val = combineTruncateWithSat(Src, VT, DL, DAG, Subtarget))\n    return Val;\n\n  // Try to combine PMULHUW/PMULHW for vXi16.\n  if (SDValue V = combinePMULH(Src, VT, DL, DAG, Subtarget))\n    return V;\n\n  // The bitcast source is a direct mmx result.\n  // Detect bitcasts between i32 to x86mmx\n  if (Src.getOpcode() == ISD::BITCAST && VT == MVT::i32) {\n    SDValue BCSrc = Src.getOperand(0);\n    if (BCSrc.getValueType() == MVT::x86mmx)\n      return DAG.getNode(X86ISD::MMX_MOVD2W, DL, MVT::i32, BCSrc);\n  }\n\n  // Try to truncate extended sign/zero bits with PACKSS/PACKUS.\n  if (SDValue V = combineVectorSignBitsTruncation(N, DL, DAG, Subtarget))\n    return V;\n\n  return combineVectorTruncation(N, DAG, Subtarget);\n}\n\nstatic SDValue combineVTRUNC(SDNode *N, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI) {\n  EVT VT = N->getValueType(0);\n  SDValue In = N->getOperand(0);\n  SDLoc DL(N);\n\n  if (auto SSatVal = detectSSatPattern(In, VT))\n    return DAG.getNode(X86ISD::VTRUNCS, DL, VT, SSatVal);\n  if (auto USatVal = detectUSatPattern(In, VT, DAG, DL))\n    return DAG.getNode(X86ISD::VTRUNCUS, DL, VT, USatVal);\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  APInt DemandedMask(APInt::getAllOnesValue(VT.getScalarSizeInBits()));\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0), DemandedMask, DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\n/// Returns the negated value if the node \\p N flips sign of FP value.\n///\n/// FP-negation node may have different forms: FNEG(x), FXOR (x, 0x80000000)\n/// or FSUB(0, x)\n/// AVX512F does not have FXOR, so FNEG is lowered as\n/// (bitcast (xor (bitcast x), (bitcast ConstantFP(0x80000000)))).\n/// In this case we go though all bitcasts.\n/// This also recognizes splat of a negated value and returns the splat of that\n/// value.\nstatic SDValue isFNEG(SelectionDAG &DAG, SDNode *N, unsigned Depth = 0) {\n  if (N->getOpcode() == ISD::FNEG)\n    return N->getOperand(0);\n\n  // Don't recurse exponentially.\n  if (Depth > SelectionDAG::MaxRecursionDepth)\n    return SDValue();\n\n  unsigned ScalarSize = N->getValueType(0).getScalarSizeInBits();\n\n  SDValue Op = peekThroughBitcasts(SDValue(N, 0));\n  EVT VT = Op->getValueType(0);\n\n  // Make sure the element size doesn't change.\n  if (VT.getScalarSizeInBits() != ScalarSize)\n    return SDValue();\n\n  unsigned Opc = Op.getOpcode();\n  switch (Opc) {\n  case ISD::VECTOR_SHUFFLE: {\n    // For a VECTOR_SHUFFLE(VEC1, VEC2), if the VEC2 is undef, then the negate\n    // of this is VECTOR_SHUFFLE(-VEC1, UNDEF).  The mask can be anything here.\n    if (!Op.getOperand(1).isUndef())\n      return SDValue();\n    if (SDValue NegOp0 = isFNEG(DAG, Op.getOperand(0).getNode(), Depth + 1))\n      if (NegOp0.getValueType() == VT) // FIXME: Can we do better?\n        return DAG.getVectorShuffle(VT, SDLoc(Op), NegOp0, DAG.getUNDEF(VT),\n                                    cast<ShuffleVectorSDNode>(Op)->getMask());\n    break;\n  }\n  case ISD::INSERT_VECTOR_ELT: {\n    // Negate of INSERT_VECTOR_ELT(UNDEF, V, INDEX) is INSERT_VECTOR_ELT(UNDEF,\n    // -V, INDEX).\n    SDValue InsVector = Op.getOperand(0);\n    SDValue InsVal = Op.getOperand(1);\n    if (!InsVector.isUndef())\n      return SDValue();\n    if (SDValue NegInsVal = isFNEG(DAG, InsVal.getNode(), Depth + 1))\n      if (NegInsVal.getValueType() == VT.getVectorElementType()) // FIXME\n        return DAG.getNode(ISD::INSERT_VECTOR_ELT, SDLoc(Op), VT, InsVector,\n                           NegInsVal, Op.getOperand(2));\n    break;\n  }\n  case ISD::FSUB:\n  case ISD::XOR:\n  case X86ISD::FXOR: {\n    SDValue Op1 = Op.getOperand(1);\n    SDValue Op0 = Op.getOperand(0);\n\n    // For XOR and FXOR, we want to check if constant\n    // bits of Op1 are sign bit masks. For FSUB, we\n    // have to check if constant bits of Op0 are sign\n    // bit masks and hence we swap the operands.\n    if (Opc == ISD::FSUB)\n      std::swap(Op0, Op1);\n\n    APInt UndefElts;\n    SmallVector<APInt, 16> EltBits;\n    // Extract constant bits and see if they are all\n    // sign bit masks. Ignore the undef elements.\n    if (getTargetConstantBitsFromNode(Op1, ScalarSize, UndefElts, EltBits,\n                                      /* AllowWholeUndefs */ true,\n                                      /* AllowPartialUndefs */ false)) {\n      for (unsigned I = 0, E = EltBits.size(); I < E; I++)\n        if (!UndefElts[I] && !EltBits[I].isSignMask())\n          return SDValue();\n\n      return peekThroughBitcasts(Op0);\n    }\n  }\n  }\n\n  return SDValue();\n}\n\nstatic unsigned negateFMAOpcode(unsigned Opcode, bool NegMul, bool NegAcc,\n                                bool NegRes) {\n  if (NegMul) {\n    switch (Opcode) {\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case ISD::FMA:              Opcode = X86ISD::FNMADD;        break;\n    case ISD::STRICT_FMA:       Opcode = X86ISD::STRICT_FNMADD; break;\n    case X86ISD::FMADD_RND:     Opcode = X86ISD::FNMADD_RND;    break;\n    case X86ISD::FMSUB:         Opcode = X86ISD::FNMSUB;        break;\n    case X86ISD::STRICT_FMSUB:  Opcode = X86ISD::STRICT_FNMSUB; break;\n    case X86ISD::FMSUB_RND:     Opcode = X86ISD::FNMSUB_RND;    break;\n    case X86ISD::FNMADD:        Opcode = ISD::FMA;              break;\n    case X86ISD::STRICT_FNMADD: Opcode = ISD::STRICT_FMA;       break;\n    case X86ISD::FNMADD_RND:    Opcode = X86ISD::FMADD_RND;     break;\n    case X86ISD::FNMSUB:        Opcode = X86ISD::FMSUB;         break;\n    case X86ISD::STRICT_FNMSUB: Opcode = X86ISD::STRICT_FMSUB;  break;\n    case X86ISD::FNMSUB_RND:    Opcode = X86ISD::FMSUB_RND;     break;\n    }\n  }\n\n  if (NegAcc) {\n    switch (Opcode) {\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case ISD::FMA:              Opcode = X86ISD::FMSUB;         break;\n    case ISD::STRICT_FMA:       Opcode = X86ISD::STRICT_FMSUB;  break;\n    case X86ISD::FMADD_RND:     Opcode = X86ISD::FMSUB_RND;     break;\n    case X86ISD::FMSUB:         Opcode = ISD::FMA;              break;\n    case X86ISD::STRICT_FMSUB:  Opcode = ISD::STRICT_FMA;       break;\n    case X86ISD::FMSUB_RND:     Opcode = X86ISD::FMADD_RND;     break;\n    case X86ISD::FNMADD:        Opcode = X86ISD::FNMSUB;        break;\n    case X86ISD::STRICT_FNMADD: Opcode = X86ISD::STRICT_FNMSUB; break;\n    case X86ISD::FNMADD_RND:    Opcode = X86ISD::FNMSUB_RND;    break;\n    case X86ISD::FNMSUB:        Opcode = X86ISD::FNMADD;        break;\n    case X86ISD::STRICT_FNMSUB: Opcode = X86ISD::STRICT_FNMADD; break;\n    case X86ISD::FNMSUB_RND:    Opcode = X86ISD::FNMADD_RND;    break;\n    case X86ISD::FMADDSUB:      Opcode = X86ISD::FMSUBADD;      break;\n    case X86ISD::FMADDSUB_RND:  Opcode = X86ISD::FMSUBADD_RND;  break;\n    case X86ISD::FMSUBADD:      Opcode = X86ISD::FMADDSUB;      break;\n    case X86ISD::FMSUBADD_RND:  Opcode = X86ISD::FMADDSUB_RND;  break;\n    }\n  }\n\n  if (NegRes) {\n    switch (Opcode) {\n    // For accuracy reason, we never combine fneg and fma under strict FP.\n    default: llvm_unreachable(\"Unexpected opcode\");\n    case ISD::FMA:             Opcode = X86ISD::FNMSUB;       break;\n    case X86ISD::FMADD_RND:    Opcode = X86ISD::FNMSUB_RND;   break;\n    case X86ISD::FMSUB:        Opcode = X86ISD::FNMADD;       break;\n    case X86ISD::FMSUB_RND:    Opcode = X86ISD::FNMADD_RND;   break;\n    case X86ISD::FNMADD:       Opcode = X86ISD::FMSUB;        break;\n    case X86ISD::FNMADD_RND:   Opcode = X86ISD::FMSUB_RND;    break;\n    case X86ISD::FNMSUB:       Opcode = ISD::FMA;             break;\n    case X86ISD::FNMSUB_RND:   Opcode = X86ISD::FMADD_RND;    break;\n    }\n  }\n\n  return Opcode;\n}\n\n/// Do target-specific dag combines on floating point negations.\nstatic SDValue combineFneg(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI,\n                           const X86Subtarget &Subtarget) {\n  EVT OrigVT = N->getValueType(0);\n  SDValue Arg = isFNEG(DAG, N);\n  if (!Arg)\n    return SDValue();\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  EVT VT = Arg.getValueType();\n  EVT SVT = VT.getScalarType();\n  SDLoc DL(N);\n\n  // Let legalize expand this if it isn't a legal type yet.\n  if (!TLI.isTypeLegal(VT))\n    return SDValue();\n\n  // If we're negating a FMUL node on a target with FMA, then we can avoid the\n  // use of a constant by performing (-0 - A*B) instead.\n  // FIXME: Check rounding control flags as well once it becomes available.\n  if (Arg.getOpcode() == ISD::FMUL && (SVT == MVT::f32 || SVT == MVT::f64) &&\n      Arg->getFlags().hasNoSignedZeros() && Subtarget.hasAnyFMA()) {\n    SDValue Zero = DAG.getConstantFP(0.0, DL, VT);\n    SDValue NewNode = DAG.getNode(X86ISD::FNMSUB, DL, VT, Arg.getOperand(0),\n                                  Arg.getOperand(1), Zero);\n    return DAG.getBitcast(OrigVT, NewNode);\n  }\n\n  bool CodeSize = DAG.getMachineFunction().getFunction().hasOptSize();\n  bool LegalOperations = !DCI.isBeforeLegalizeOps();\n  if (SDValue NegArg =\n          TLI.getNegatedExpression(Arg, DAG, LegalOperations, CodeSize))\n    return DAG.getBitcast(OrigVT, NegArg);\n\n  return SDValue();\n}\n\nSDValue X86TargetLowering::getNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                                bool LegalOperations,\n                                                bool ForCodeSize,\n                                                NegatibleCost &Cost,\n                                                unsigned Depth) const {\n  // fneg patterns are removable even if they have multiple uses.\n  if (SDValue Arg = isFNEG(DAG, Op.getNode(), Depth)) {\n    Cost = NegatibleCost::Cheaper;\n    return DAG.getBitcast(Op.getValueType(), Arg);\n  }\n\n  EVT VT = Op.getValueType();\n  EVT SVT = VT.getScalarType();\n  unsigned Opc = Op.getOpcode();\n  switch (Opc) {\n  case ISD::FMA:\n  case X86ISD::FMSUB:\n  case X86ISD::FNMADD:\n  case X86ISD::FNMSUB:\n  case X86ISD::FMADD_RND:\n  case X86ISD::FMSUB_RND:\n  case X86ISD::FNMADD_RND:\n  case X86ISD::FNMSUB_RND: {\n    if (!Op.hasOneUse() || !Subtarget.hasAnyFMA() || !isTypeLegal(VT) ||\n        !(SVT == MVT::f32 || SVT == MVT::f64) ||\n        !isOperationLegal(ISD::FMA, VT))\n      break;\n\n    // This is always negatible for free but we might be able to remove some\n    // extra operand negations as well.\n    SmallVector<SDValue, 4> NewOps(Op.getNumOperands(), SDValue());\n    for (int i = 0; i != 3; ++i)\n      NewOps[i] = getCheaperNegatedExpression(\n          Op.getOperand(i), DAG, LegalOperations, ForCodeSize, Depth + 1);\n\n    bool NegA = !!NewOps[0];\n    bool NegB = !!NewOps[1];\n    bool NegC = !!NewOps[2];\n    unsigned NewOpc = negateFMAOpcode(Opc, NegA != NegB, NegC, true);\n\n    Cost = (NegA || NegB || NegC) ? NegatibleCost::Cheaper\n                                  : NegatibleCost::Neutral;\n\n    // Fill in the non-negated ops with the original values.\n    for (int i = 0, e = Op.getNumOperands(); i != e; ++i)\n      if (!NewOps[i])\n        NewOps[i] = Op.getOperand(i);\n    return DAG.getNode(NewOpc, SDLoc(Op), VT, NewOps);\n  }\n  case X86ISD::FRCP:\n    if (SDValue NegOp0 =\n            getNegatedExpression(Op.getOperand(0), DAG, LegalOperations,\n                                 ForCodeSize, Cost, Depth + 1))\n      return DAG.getNode(Opc, SDLoc(Op), VT, NegOp0);\n    break;\n  }\n\n  return TargetLowering::getNegatedExpression(Op, DAG, LegalOperations,\n                                              ForCodeSize, Cost, Depth);\n}\n\nstatic SDValue lowerX86FPLogicOp(SDNode *N, SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget) {\n  MVT VT = N->getSimpleValueType(0);\n  // If we have integer vector types available, use the integer opcodes.\n  if (!VT.isVector() || !Subtarget.hasSSE2())\n    return SDValue();\n\n  SDLoc dl(N);\n\n  unsigned IntBits = VT.getScalarSizeInBits();\n  MVT IntSVT = MVT::getIntegerVT(IntBits);\n  MVT IntVT = MVT::getVectorVT(IntSVT, VT.getSizeInBits() / IntBits);\n\n  SDValue Op0 = DAG.getBitcast(IntVT, N->getOperand(0));\n  SDValue Op1 = DAG.getBitcast(IntVT, N->getOperand(1));\n  unsigned IntOpcode;\n  switch (N->getOpcode()) {\n  default: llvm_unreachable(\"Unexpected FP logic op\");\n  case X86ISD::FOR:   IntOpcode = ISD::OR; break;\n  case X86ISD::FXOR:  IntOpcode = ISD::XOR; break;\n  case X86ISD::FAND:  IntOpcode = ISD::AND; break;\n  case X86ISD::FANDN: IntOpcode = X86ISD::ANDNP; break;\n  }\n  SDValue IntOp = DAG.getNode(IntOpcode, dl, IntVT, Op0, Op1);\n  return DAG.getBitcast(VT, IntOp);\n}\n\n\n/// Fold a xor(setcc cond, val), 1 --> setcc (inverted(cond), val)\nstatic SDValue foldXor1SetCC(SDNode *N, SelectionDAG &DAG) {\n  if (N->getOpcode() != ISD::XOR)\n    return SDValue();\n\n  SDValue LHS = N->getOperand(0);\n  if (!isOneConstant(N->getOperand(1)) || LHS->getOpcode() != X86ISD::SETCC)\n    return SDValue();\n\n  X86::CondCode NewCC = X86::GetOppositeBranchCondition(\n      X86::CondCode(LHS->getConstantOperandVal(0)));\n  SDLoc DL(N);\n  return getSETCC(NewCC, LHS->getOperand(1), DL, DAG);\n}\n\nstatic SDValue combineXor(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  // If this is SSE1 only convert to FXOR to avoid scalarization.\n  if (Subtarget.hasSSE1() && !Subtarget.hasSSE2() &&\n      N->getValueType(0) == MVT::v4i32) {\n    return DAG.getBitcast(\n        MVT::v4i32, DAG.getNode(X86ISD::FXOR, SDLoc(N), MVT::v4f32,\n                                DAG.getBitcast(MVT::v4f32, N->getOperand(0)),\n                                DAG.getBitcast(MVT::v4f32, N->getOperand(1))));\n  }\n\n  if (SDValue Cmp = foldVectorXorShiftIntoCmp(N, DAG, Subtarget))\n    return Cmp;\n\n  if (SDValue R = combineBitOpWithMOVMSK(N, DAG))\n    return R;\n\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  if (SDValue SetCC = foldXor1SetCC(N, DAG))\n    return SetCC;\n\n  if (SDValue RV = foldXorTruncShiftIntoCmp(N, DAG))\n    return RV;\n\n  if (SDValue FPLogic = convertIntLogicToFPLogic(N, DAG, Subtarget))\n    return FPLogic;\n\n  return combineFneg(N, DAG, DCI, Subtarget);\n}\n\nstatic SDValue combineBEXTR(SDNode *N, SelectionDAG &DAG,\n                            TargetLowering::DAGCombinerInfo &DCI,\n                            const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  unsigned NumBits = VT.getSizeInBits();\n\n  // TODO - Constant Folding.\n\n  // Simplify the inputs.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  APInt DemandedMask(APInt::getAllOnesValue(NumBits));\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0), DemandedMask, DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\nstatic bool isNullFPScalarOrVectorConst(SDValue V) {\n  return isNullFPConstant(V) || ISD::isBuildVectorAllZeros(V.getNode());\n}\n\n/// If a value is a scalar FP zero or a vector FP zero (potentially including\n/// undefined elements), return a zero constant that may be used to fold away\n/// that value. In the case of a vector, the returned constant will not contain\n/// undefined elements even if the input parameter does. This makes it suitable\n/// to be used as a replacement operand with operations (eg, bitwise-and) where\n/// an undef should not propagate.\nstatic SDValue getNullFPConstForNullVal(SDValue V, SelectionDAG &DAG,\n                                        const X86Subtarget &Subtarget) {\n  if (!isNullFPScalarOrVectorConst(V))\n    return SDValue();\n\n  if (V.getValueType().isVector())\n    return getZeroVector(V.getSimpleValueType(), Subtarget, DAG, SDLoc(V));\n\n  return V;\n}\n\nstatic SDValue combineFAndFNotToFAndn(SDNode *N, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n\n  // Vector types are handled in combineANDXORWithAllOnesIntoANDNP().\n  if (!((VT == MVT::f32 && Subtarget.hasSSE1()) ||\n        (VT == MVT::f64 && Subtarget.hasSSE2()) ||\n        (VT == MVT::v4f32 && Subtarget.hasSSE1() && !Subtarget.hasSSE2())))\n    return SDValue();\n\n  auto isAllOnesConstantFP = [](SDValue V) {\n    if (V.getSimpleValueType().isVector())\n      return ISD::isBuildVectorAllOnes(V.getNode());\n    auto *C = dyn_cast<ConstantFPSDNode>(V);\n    return C && C->getConstantFPValue()->isAllOnesValue();\n  };\n\n  // fand (fxor X, -1), Y --> fandn X, Y\n  if (N0.getOpcode() == X86ISD::FXOR && isAllOnesConstantFP(N0.getOperand(1)))\n    return DAG.getNode(X86ISD::FANDN, DL, VT, N0.getOperand(0), N1);\n\n  // fand X, (fxor Y, -1) --> fandn Y, X\n  if (N1.getOpcode() == X86ISD::FXOR && isAllOnesConstantFP(N1.getOperand(1)))\n    return DAG.getNode(X86ISD::FANDN, DL, VT, N1.getOperand(0), N0);\n\n  return SDValue();\n}\n\n/// Do target-specific dag combines on X86ISD::FAND nodes.\nstatic SDValue combineFAnd(SDNode *N, SelectionDAG &DAG,\n                           const X86Subtarget &Subtarget) {\n  // FAND(0.0, x) -> 0.0\n  if (SDValue V = getNullFPConstForNullVal(N->getOperand(0), DAG, Subtarget))\n    return V;\n\n  // FAND(x, 0.0) -> 0.0\n  if (SDValue V = getNullFPConstForNullVal(N->getOperand(1), DAG, Subtarget))\n    return V;\n\n  if (SDValue V = combineFAndFNotToFAndn(N, DAG, Subtarget))\n    return V;\n\n  return lowerX86FPLogicOp(N, DAG, Subtarget);\n}\n\n/// Do target-specific dag combines on X86ISD::FANDN nodes.\nstatic SDValue combineFAndn(SDNode *N, SelectionDAG &DAG,\n                            const X86Subtarget &Subtarget) {\n  // FANDN(0.0, x) -> x\n  if (isNullFPScalarOrVectorConst(N->getOperand(0)))\n    return N->getOperand(1);\n\n  // FANDN(x, 0.0) -> 0.0\n  if (SDValue V = getNullFPConstForNullVal(N->getOperand(1), DAG, Subtarget))\n    return V;\n\n  return lowerX86FPLogicOp(N, DAG, Subtarget);\n}\n\n/// Do target-specific dag combines on X86ISD::FOR and X86ISD::FXOR nodes.\nstatic SDValue combineFOr(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == X86ISD::FOR || N->getOpcode() == X86ISD::FXOR);\n\n  // F[X]OR(0.0, x) -> x\n  if (isNullFPScalarOrVectorConst(N->getOperand(0)))\n    return N->getOperand(1);\n\n  // F[X]OR(x, 0.0) -> x\n  if (isNullFPScalarOrVectorConst(N->getOperand(1)))\n    return N->getOperand(0);\n\n  if (SDValue NewVal = combineFneg(N, DAG, DCI, Subtarget))\n    return NewVal;\n\n  return lowerX86FPLogicOp(N, DAG, Subtarget);\n}\n\n/// Do target-specific dag combines on X86ISD::FMIN and X86ISD::FMAX nodes.\nstatic SDValue combineFMinFMax(SDNode *N, SelectionDAG &DAG) {\n  assert(N->getOpcode() == X86ISD::FMIN || N->getOpcode() == X86ISD::FMAX);\n\n  // FMIN/FMAX are commutative if no NaNs and no negative zeros are allowed.\n  if (!DAG.getTarget().Options.NoNaNsFPMath ||\n      !DAG.getTarget().Options.NoSignedZerosFPMath)\n    return SDValue();\n\n  // If we run in unsafe-math mode, then convert the FMAX and FMIN nodes\n  // into FMINC and FMAXC, which are Commutative operations.\n  unsigned NewOp = 0;\n  switch (N->getOpcode()) {\n    default: llvm_unreachable(\"unknown opcode\");\n    case X86ISD::FMIN:  NewOp = X86ISD::FMINC; break;\n    case X86ISD::FMAX:  NewOp = X86ISD::FMAXC; break;\n  }\n\n  return DAG.getNode(NewOp, SDLoc(N), N->getValueType(0),\n                     N->getOperand(0), N->getOperand(1));\n}\n\nstatic SDValue combineFMinNumFMaxNum(SDNode *N, SelectionDAG &DAG,\n                                     const X86Subtarget &Subtarget) {\n  if (Subtarget.useSoftFloat())\n    return SDValue();\n\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  EVT VT = N->getValueType(0);\n  if (!((Subtarget.hasSSE1() && VT == MVT::f32) ||\n        (Subtarget.hasSSE2() && VT == MVT::f64) ||\n        (VT.isVector() && TLI.isTypeLegal(VT))))\n    return SDValue();\n\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDLoc DL(N);\n  auto MinMaxOp = N->getOpcode() == ISD::FMAXNUM ? X86ISD::FMAX : X86ISD::FMIN;\n\n  // If we don't have to respect NaN inputs, this is a direct translation to x86\n  // min/max instructions.\n  if (DAG.getTarget().Options.NoNaNsFPMath || N->getFlags().hasNoNaNs())\n    return DAG.getNode(MinMaxOp, DL, VT, Op0, Op1, N->getFlags());\n\n  // If one of the operands is known non-NaN use the native min/max instructions\n  // with the non-NaN input as second operand.\n  if (DAG.isKnownNeverNaN(Op1))\n    return DAG.getNode(MinMaxOp, DL, VT, Op0, Op1, N->getFlags());\n  if (DAG.isKnownNeverNaN(Op0))\n    return DAG.getNode(MinMaxOp, DL, VT, Op1, Op0, N->getFlags());\n\n  // If we have to respect NaN inputs, this takes at least 3 instructions.\n  // Favor a library call when operating on a scalar and minimizing code size.\n  if (!VT.isVector() && DAG.getMachineFunction().getFunction().hasMinSize())\n    return SDValue();\n\n  EVT SetCCType = TLI.getSetCCResultType(DAG.getDataLayout(), *DAG.getContext(),\n                                         VT);\n\n  // There are 4 possibilities involving NaN inputs, and these are the required\n  // outputs:\n  //                   Op1\n  //               Num     NaN\n  //            ----------------\n  //       Num  |  Max  |  Op0 |\n  // Op0        ----------------\n  //       NaN  |  Op1  |  NaN |\n  //            ----------------\n  //\n  // The SSE FP max/min instructions were not designed for this case, but rather\n  // to implement:\n  //   Min = Op1 < Op0 ? Op1 : Op0\n  //   Max = Op1 > Op0 ? Op1 : Op0\n  //\n  // So they always return Op0 if either input is a NaN. However, we can still\n  // use those instructions for fmaxnum by selecting away a NaN input.\n\n  // If either operand is NaN, the 2nd source operand (Op0) is passed through.\n  SDValue MinOrMax = DAG.getNode(MinMaxOp, DL, VT, Op1, Op0);\n  SDValue IsOp0Nan = DAG.getSetCC(DL, SetCCType, Op0, Op0, ISD::SETUO);\n\n  // If Op0 is a NaN, select Op1. Otherwise, select the max. If both operands\n  // are NaN, the NaN value of Op1 is the result.\n  return DAG.getSelect(DL, VT, IsOp0Nan, Op1, MinOrMax);\n}\n\nstatic SDValue combineX86INT_TO_FP(SDNode *N, SelectionDAG &DAG,\n                                   TargetLowering::DAGCombinerInfo &DCI) {\n  EVT VT = N->getValueType(0);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  APInt KnownUndef, KnownZero;\n  APInt DemandedElts = APInt::getAllOnesValue(VT.getVectorNumElements());\n  if (TLI.SimplifyDemandedVectorElts(SDValue(N, 0), DemandedElts, KnownUndef,\n                                     KnownZero, DCI))\n    return SDValue(N, 0);\n\n  // Convert a full vector load into vzload when not all bits are needed.\n  SDValue In = N->getOperand(0);\n  MVT InVT = In.getSimpleValueType();\n  if (VT.getVectorNumElements() < InVT.getVectorNumElements() &&\n      ISD::isNormalLoad(In.getNode()) && In.hasOneUse()) {\n    assert(InVT.is128BitVector() && \"Expected 128-bit input vector\");\n    LoadSDNode *LN = cast<LoadSDNode>(N->getOperand(0));\n    unsigned NumBits = InVT.getScalarSizeInBits() * VT.getVectorNumElements();\n    MVT MemVT = MVT::getIntegerVT(NumBits);\n    MVT LoadVT = MVT::getVectorVT(MemVT, 128 / NumBits);\n    if (SDValue VZLoad = narrowLoadToVZLoad(LN, MemVT, LoadVT, DAG)) {\n      SDLoc dl(N);\n      SDValue Convert = DAG.getNode(N->getOpcode(), dl, VT,\n                                    DAG.getBitcast(InVT, VZLoad));\n      DCI.CombineTo(N, Convert);\n      DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n      DCI.recursivelyDeleteUnusedNodes(LN);\n      return SDValue(N, 0);\n    }\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineCVTP2I_CVTTP2I(SDNode *N, SelectionDAG &DAG,\n                                     TargetLowering::DAGCombinerInfo &DCI) {\n  bool IsStrict = N->isTargetStrictFPOpcode();\n  EVT VT = N->getValueType(0);\n\n  // Convert a full vector load into vzload when not all bits are needed.\n  SDValue In = N->getOperand(IsStrict ? 1 : 0);\n  MVT InVT = In.getSimpleValueType();\n  if (VT.getVectorNumElements() < InVT.getVectorNumElements() &&\n      ISD::isNormalLoad(In.getNode()) && In.hasOneUse()) {\n    assert(InVT.is128BitVector() && \"Expected 128-bit input vector\");\n    LoadSDNode *LN = cast<LoadSDNode>(In);\n    unsigned NumBits = InVT.getScalarSizeInBits() * VT.getVectorNumElements();\n    MVT MemVT = MVT::getFloatingPointVT(NumBits);\n    MVT LoadVT = MVT::getVectorVT(MemVT, 128 / NumBits);\n    if (SDValue VZLoad = narrowLoadToVZLoad(LN, MemVT, LoadVT, DAG)) {\n      SDLoc dl(N);\n      if (IsStrict) {\n        SDValue Convert =\n            DAG.getNode(N->getOpcode(), dl, {VT, MVT::Other},\n                        {N->getOperand(0), DAG.getBitcast(InVT, VZLoad)});\n        DCI.CombineTo(N, Convert, Convert.getValue(1));\n      } else {\n        SDValue Convert =\n            DAG.getNode(N->getOpcode(), dl, VT, DAG.getBitcast(InVT, VZLoad));\n        DCI.CombineTo(N, Convert);\n      }\n      DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n      DCI.recursivelyDeleteUnusedNodes(LN);\n      return SDValue(N, 0);\n    }\n  }\n\n  return SDValue();\n}\n\n/// Do target-specific dag combines on X86ISD::ANDNP nodes.\nstatic SDValue combineAndnp(SDNode *N, SelectionDAG &DAG,\n                            TargetLowering::DAGCombinerInfo &DCI,\n                            const X86Subtarget &Subtarget) {\n  MVT VT = N->getSimpleValueType(0);\n\n  // ANDNP(0, x) -> x\n  if (ISD::isBuildVectorAllZeros(N->getOperand(0).getNode()))\n    return N->getOperand(1);\n\n  // ANDNP(x, 0) -> 0\n  if (ISD::isBuildVectorAllZeros(N->getOperand(1).getNode()))\n    return DAG.getConstant(0, SDLoc(N), VT);\n\n  // Turn ANDNP back to AND if input is inverted.\n  if (SDValue Not = IsNOT(N->getOperand(0), DAG))\n    return DAG.getNode(ISD::AND, SDLoc(N), VT, DAG.getBitcast(VT, Not),\n                       N->getOperand(1));\n\n  // Attempt to recursively combine a bitmask ANDNP with shuffles.\n  if (VT.isVector() && (VT.getScalarSizeInBits() % 8) == 0) {\n    SDValue Op(N, 0);\n    if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n      return Res;\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineBT(SDNode *N, SelectionDAG &DAG,\n                         TargetLowering::DAGCombinerInfo &DCI) {\n  SDValue N1 = N->getOperand(1);\n\n  // BT ignores high bits in the bit index operand.\n  unsigned BitWidth = N1.getValueSizeInBits();\n  APInt DemandedMask = APInt::getLowBitsSet(BitWidth, Log2_32(BitWidth));\n  if (DAG.getTargetLoweringInfo().SimplifyDemandedBits(N1, DemandedMask, DCI)) {\n    if (N->getOpcode() != ISD::DELETED_NODE)\n      DCI.AddToWorklist(N);\n    return SDValue(N, 0);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineCVTPH2PS(SDNode *N, SelectionDAG &DAG,\n                               TargetLowering::DAGCombinerInfo &DCI) {\n  bool IsStrict = N->getOpcode() == X86ISD::STRICT_CVTPH2PS;\n  SDValue Src = N->getOperand(IsStrict ? 1 : 0);\n\n  if (N->getValueType(0) == MVT::v4f32 && Src.getValueType() == MVT::v8i16) {\n    APInt KnownUndef, KnownZero;\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    APInt DemandedElts = APInt::getLowBitsSet(8, 4);\n    if (TLI.SimplifyDemandedVectorElts(Src, DemandedElts, KnownUndef, KnownZero,\n                                       DCI)) {\n      if (N->getOpcode() != ISD::DELETED_NODE)\n        DCI.AddToWorklist(N);\n      return SDValue(N, 0);\n    }\n\n    // Convert a full vector load into vzload when not all bits are needed.\n    if (ISD::isNormalLoad(Src.getNode()) && Src.hasOneUse()) {\n      LoadSDNode *LN = cast<LoadSDNode>(N->getOperand(IsStrict ? 1 : 0));\n      if (SDValue VZLoad = narrowLoadToVZLoad(LN, MVT::i64, MVT::v2i64, DAG)) {\n        SDLoc dl(N);\n        if (IsStrict) {\n          SDValue Convert = DAG.getNode(\n              N->getOpcode(), dl, {MVT::v4f32, MVT::Other},\n              {N->getOperand(0), DAG.getBitcast(MVT::v8i16, VZLoad)});\n          DCI.CombineTo(N, Convert, Convert.getValue(1));\n        } else {\n          SDValue Convert = DAG.getNode(N->getOpcode(), dl, MVT::v4f32,\n                                        DAG.getBitcast(MVT::v8i16, VZLoad));\n          DCI.CombineTo(N, Convert);\n        }\n\n        DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), VZLoad.getValue(1));\n        DCI.recursivelyDeleteUnusedNodes(LN);\n        return SDValue(N, 0);\n      }\n    }\n  }\n\n  return SDValue();\n}\n\n// Try to combine sext_in_reg of a cmov of constants by extending the constants.\nstatic SDValue combineSextInRegCmov(SDNode *N, SelectionDAG &DAG) {\n  assert(N->getOpcode() == ISD::SIGN_EXTEND_INREG);\n\n  EVT DstVT = N->getValueType(0);\n\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT ExtraVT = cast<VTSDNode>(N1)->getVT();\n\n  if (ExtraVT != MVT::i8 && ExtraVT != MVT::i16)\n    return SDValue();\n\n  // Look through single use any_extends / truncs.\n  SDValue IntermediateBitwidthOp;\n  if ((N0.getOpcode() == ISD::ANY_EXTEND || N0.getOpcode() == ISD::TRUNCATE) &&\n      N0.hasOneUse()) {\n    IntermediateBitwidthOp = N0;\n    N0 = N0.getOperand(0);\n  }\n\n  // See if we have a single use cmov.\n  if (N0.getOpcode() != X86ISD::CMOV || !N0.hasOneUse())\n    return SDValue();\n\n  SDValue CMovOp0 = N0.getOperand(0);\n  SDValue CMovOp1 = N0.getOperand(1);\n\n  // Make sure both operands are constants.\n  if (!isa<ConstantSDNode>(CMovOp0.getNode()) ||\n      !isa<ConstantSDNode>(CMovOp1.getNode()))\n    return SDValue();\n\n  SDLoc DL(N);\n\n  // If we looked through an any_extend/trunc above, add one to the constants.\n  if (IntermediateBitwidthOp) {\n    unsigned IntermediateOpc = IntermediateBitwidthOp.getOpcode();\n    CMovOp0 = DAG.getNode(IntermediateOpc, DL, DstVT, CMovOp0);\n    CMovOp1 = DAG.getNode(IntermediateOpc, DL, DstVT, CMovOp1);\n  }\n\n  CMovOp0 = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, DstVT, CMovOp0, N1);\n  CMovOp1 = DAG.getNode(ISD::SIGN_EXTEND_INREG, DL, DstVT, CMovOp1, N1);\n\n  EVT CMovVT = DstVT;\n  // We do not want i16 CMOV's. Promote to i32 and truncate afterwards.\n  if (DstVT == MVT::i16) {\n    CMovVT = MVT::i32;\n    CMovOp0 = DAG.getNode(ISD::ZERO_EXTEND, DL, CMovVT, CMovOp0);\n    CMovOp1 = DAG.getNode(ISD::ZERO_EXTEND, DL, CMovVT, CMovOp1);\n  }\n\n  SDValue CMov = DAG.getNode(X86ISD::CMOV, DL, CMovVT, CMovOp0, CMovOp1,\n                             N0.getOperand(2), N0.getOperand(3));\n\n  if (CMovVT != DstVT)\n    CMov = DAG.getNode(ISD::TRUNCATE, DL, DstVT, CMov);\n\n  return CMov;\n}\n\nstatic SDValue combineSignExtendInReg(SDNode *N, SelectionDAG &DAG,\n                                      const X86Subtarget &Subtarget) {\n  assert(N->getOpcode() == ISD::SIGN_EXTEND_INREG);\n\n  if (SDValue V = combineSextInRegCmov(N, DAG))\n    return V;\n\n  EVT VT = N->getValueType(0);\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  EVT ExtraVT = cast<VTSDNode>(N1)->getVT();\n  SDLoc dl(N);\n\n  // The SIGN_EXTEND_INREG to v4i64 is expensive operation on the\n  // both SSE and AVX2 since there is no sign-extended shift right\n  // operation on a vector with 64-bit elements.\n  //(sext_in_reg (v4i64 anyext (v4i32 x )), ExtraVT) ->\n  // (v4i64 sext (v4i32 sext_in_reg (v4i32 x , ExtraVT)))\n  if (VT == MVT::v4i64 && (N0.getOpcode() == ISD::ANY_EXTEND ||\n                           N0.getOpcode() == ISD::SIGN_EXTEND)) {\n    SDValue N00 = N0.getOperand(0);\n\n    // EXTLOAD has a better solution on AVX2,\n    // it may be replaced with X86ISD::VSEXT node.\n    if (N00.getOpcode() == ISD::LOAD && Subtarget.hasInt256())\n      if (!ISD::isNormalLoad(N00.getNode()))\n        return SDValue();\n\n    // Attempt to promote any comparison mask ops before moving the\n    // SIGN_EXTEND_INREG in the way.\n    if (SDValue Promote = PromoteMaskArithmetic(N0.getNode(), DAG, Subtarget))\n      return DAG.getNode(ISD::SIGN_EXTEND_INREG, dl, VT, Promote, N1);\n\n    if (N00.getValueType() == MVT::v4i32 && ExtraVT.getSizeInBits() < 128) {\n      SDValue Tmp =\n          DAG.getNode(ISD::SIGN_EXTEND_INREG, dl, MVT::v4i32, N00, N1);\n      return DAG.getNode(ISD::SIGN_EXTEND, dl, MVT::v4i64, Tmp);\n    }\n  }\n  return SDValue();\n}\n\n/// sext(add_nsw(x, C)) --> add(sext(x), C_sext)\n/// zext(add_nuw(x, C)) --> add(zext(x), C_zext)\n/// Promoting a sign/zero extension ahead of a no overflow 'add' exposes\n/// opportunities to combine math ops, use an LEA, or use a complex addressing\n/// mode. This can eliminate extend, add, and shift instructions.\nstatic SDValue promoteExtBeforeAdd(SDNode *Ext, SelectionDAG &DAG,\n                                   const X86Subtarget &Subtarget) {\n  if (Ext->getOpcode() != ISD::SIGN_EXTEND &&\n      Ext->getOpcode() != ISD::ZERO_EXTEND)\n    return SDValue();\n\n  // TODO: This should be valid for other integer types.\n  EVT VT = Ext->getValueType(0);\n  if (VT != MVT::i64)\n    return SDValue();\n\n  SDValue Add = Ext->getOperand(0);\n  if (Add.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  bool Sext = Ext->getOpcode() == ISD::SIGN_EXTEND;\n  bool NSW = Add->getFlags().hasNoSignedWrap();\n  bool NUW = Add->getFlags().hasNoUnsignedWrap();\n\n  // We need an 'add nsw' feeding into the 'sext' or 'add nuw' feeding\n  // into the 'zext'\n  if ((Sext && !NSW) || (!Sext && !NUW))\n    return SDValue();\n\n  // Having a constant operand to the 'add' ensures that we are not increasing\n  // the instruction count because the constant is extended for free below.\n  // A constant operand can also become the displacement field of an LEA.\n  auto *AddOp1 = dyn_cast<ConstantSDNode>(Add.getOperand(1));\n  if (!AddOp1)\n    return SDValue();\n\n  // Don't make the 'add' bigger if there's no hope of combining it with some\n  // other 'add' or 'shl' instruction.\n  // TODO: It may be profitable to generate simpler LEA instructions in place\n  // of single 'add' instructions, but the cost model for selecting an LEA\n  // currently has a high threshold.\n  bool HasLEAPotential = false;\n  for (auto *User : Ext->uses()) {\n    if (User->getOpcode() == ISD::ADD || User->getOpcode() == ISD::SHL) {\n      HasLEAPotential = true;\n      break;\n    }\n  }\n  if (!HasLEAPotential)\n    return SDValue();\n\n  // Everything looks good, so pull the '{s|z}ext' ahead of the 'add'.\n  int64_t AddConstant = Sext ? AddOp1->getSExtValue() : AddOp1->getZExtValue();\n  SDValue AddOp0 = Add.getOperand(0);\n  SDValue NewExt = DAG.getNode(Ext->getOpcode(), SDLoc(Ext), VT, AddOp0);\n  SDValue NewConstant = DAG.getConstant(AddConstant, SDLoc(Add), VT);\n\n  // The wider add is guaranteed to not wrap because both operands are\n  // sign-extended.\n  SDNodeFlags Flags;\n  Flags.setNoSignedWrap(NSW);\n  Flags.setNoUnsignedWrap(NUW);\n  return DAG.getNode(ISD::ADD, SDLoc(Add), VT, NewExt, NewConstant, Flags);\n}\n\n// If we face {ANY,SIGN,ZERO}_EXTEND that is applied to a CMOV with constant\n// operands and the result of CMOV is not used anywhere else - promote CMOV\n// itself instead of promoting its result. This could be beneficial, because:\n//     1) X86TargetLowering::EmitLoweredSelect later can do merging of two\n//        (or more) pseudo-CMOVs only when they go one-after-another and\n//        getting rid of result extension code after CMOV will help that.\n//     2) Promotion of constant CMOV arguments is free, hence the\n//        {ANY,SIGN,ZERO}_EXTEND will just be deleted.\n//     3) 16-bit CMOV encoding is 4 bytes, 32-bit CMOV is 3-byte, so this\n//        promotion is also good in terms of code-size.\n//        (64-bit CMOV is 4-bytes, that's why we don't do 32-bit => 64-bit\n//         promotion).\nstatic SDValue combineToExtendCMOV(SDNode *Extend, SelectionDAG &DAG) {\n  SDValue CMovN = Extend->getOperand(0);\n  if (CMovN.getOpcode() != X86ISD::CMOV || !CMovN.hasOneUse())\n    return SDValue();\n\n  EVT TargetVT = Extend->getValueType(0);\n  unsigned ExtendOpcode = Extend->getOpcode();\n  SDLoc DL(Extend);\n\n  EVT VT = CMovN.getValueType();\n  SDValue CMovOp0 = CMovN.getOperand(0);\n  SDValue CMovOp1 = CMovN.getOperand(1);\n\n  if (!isa<ConstantSDNode>(CMovOp0.getNode()) ||\n      !isa<ConstantSDNode>(CMovOp1.getNode()))\n    return SDValue();\n\n  // Only extend to i32 or i64.\n  if (TargetVT != MVT::i32 && TargetVT != MVT::i64)\n    return SDValue();\n\n  // Only extend from i16 unless its a sign_extend from i32. Zext/aext from i32\n  // are free.\n  if (VT != MVT::i16 && !(ExtendOpcode == ISD::SIGN_EXTEND && VT == MVT::i32))\n    return SDValue();\n\n  // If this a zero extend to i64, we should only extend to i32 and use a free\n  // zero extend to finish.\n  EVT ExtendVT = TargetVT;\n  if (TargetVT == MVT::i64 && ExtendOpcode != ISD::SIGN_EXTEND)\n    ExtendVT = MVT::i32;\n\n  CMovOp0 = DAG.getNode(ExtendOpcode, DL, ExtendVT, CMovOp0);\n  CMovOp1 = DAG.getNode(ExtendOpcode, DL, ExtendVT, CMovOp1);\n\n  SDValue Res = DAG.getNode(X86ISD::CMOV, DL, ExtendVT, CMovOp0, CMovOp1,\n                            CMovN.getOperand(2), CMovN.getOperand(3));\n\n  // Finish extending if needed.\n  if (ExtendVT != TargetVT)\n    Res = DAG.getNode(ExtendOpcode, DL, TargetVT, Res);\n\n  return Res;\n}\n\n// Convert (vXiY *ext(vXi1 bitcast(iX))) to extend_in_reg(broadcast(iX)).\n// This is more or less the reverse of combineBitcastvxi1.\nstatic SDValue\ncombineToExtendBoolVectorInReg(SDNode *N, SelectionDAG &DAG,\n                               TargetLowering::DAGCombinerInfo &DCI,\n                               const X86Subtarget &Subtarget) {\n  unsigned Opcode = N->getOpcode();\n  if (Opcode != ISD::SIGN_EXTEND && Opcode != ISD::ZERO_EXTEND &&\n      Opcode != ISD::ANY_EXTEND)\n    return SDValue();\n  if (!DCI.isBeforeLegalizeOps())\n    return SDValue();\n  if (!Subtarget.hasSSE2() || Subtarget.hasAVX512())\n    return SDValue();\n\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n  EVT SVT = VT.getScalarType();\n  EVT InSVT = N0.getValueType().getScalarType();\n  unsigned EltSizeInBits = SVT.getSizeInBits();\n\n  // Input type must be extending a bool vector (bit-casted from a scalar\n  // integer) to legal integer types.\n  if (!VT.isVector())\n    return SDValue();\n  if (SVT != MVT::i64 && SVT != MVT::i32 && SVT != MVT::i16 && SVT != MVT::i8)\n    return SDValue();\n  if (InSVT != MVT::i1 || N0.getOpcode() != ISD::BITCAST)\n    return SDValue();\n\n  SDValue N00 = N0.getOperand(0);\n  EVT SclVT = N0.getOperand(0).getValueType();\n  if (!SclVT.isScalarInteger())\n    return SDValue();\n\n  SDLoc DL(N);\n  SDValue Vec;\n  SmallVector<int, 32> ShuffleMask;\n  unsigned NumElts = VT.getVectorNumElements();\n  assert(NumElts == SclVT.getSizeInBits() && \"Unexpected bool vector size\");\n\n  // Broadcast the scalar integer to the vector elements.\n  if (NumElts > EltSizeInBits) {\n    // If the scalar integer is greater than the vector element size, then we\n    // must split it down into sub-sections for broadcasting. For example:\n    //   i16 -> v16i8 (i16 -> v8i16 -> v16i8) with 2 sub-sections.\n    //   i32 -> v32i8 (i32 -> v8i32 -> v32i8) with 4 sub-sections.\n    assert((NumElts % EltSizeInBits) == 0 && \"Unexpected integer scale\");\n    unsigned Scale = NumElts / EltSizeInBits;\n    EVT BroadcastVT =\n        EVT::getVectorVT(*DAG.getContext(), SclVT, EltSizeInBits);\n    Vec = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, BroadcastVT, N00);\n    Vec = DAG.getBitcast(VT, Vec);\n\n    for (unsigned i = 0; i != Scale; ++i)\n      ShuffleMask.append(EltSizeInBits, i);\n    Vec = DAG.getVectorShuffle(VT, DL, Vec, Vec, ShuffleMask);\n  } else if (Subtarget.hasAVX2() && NumElts < EltSizeInBits &&\n             (SclVT == MVT::i8 || SclVT == MVT::i16 || SclVT == MVT::i32)) {\n    // If we have register broadcast instructions, use the scalar size as the\n    // element type for the shuffle. Then cast to the wider element type. The\n    // widened bits won't be used, and this might allow the use of a broadcast\n    // load.\n    assert((EltSizeInBits % NumElts) == 0 && \"Unexpected integer scale\");\n    unsigned Scale = EltSizeInBits / NumElts;\n    EVT BroadcastVT =\n        EVT::getVectorVT(*DAG.getContext(), SclVT, NumElts * Scale);\n    Vec = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, BroadcastVT, N00);\n    ShuffleMask.append(NumElts * Scale, 0);\n    Vec = DAG.getVectorShuffle(BroadcastVT, DL, Vec, Vec, ShuffleMask);\n    Vec = DAG.getBitcast(VT, Vec);\n  } else {\n    // For smaller scalar integers, we can simply any-extend it to the vector\n    // element size (we don't care about the upper bits) and broadcast it to all\n    // elements.\n    SDValue Scl = DAG.getAnyExtOrTrunc(N00, DL, SVT);\n    Vec = DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, VT, Scl);\n    ShuffleMask.append(NumElts, 0);\n    Vec = DAG.getVectorShuffle(VT, DL, Vec, Vec, ShuffleMask);\n  }\n\n  // Now, mask the relevant bit in each element.\n  SmallVector<SDValue, 32> Bits;\n  for (unsigned i = 0; i != NumElts; ++i) {\n    int BitIdx = (i % EltSizeInBits);\n    APInt Bit = APInt::getBitsSet(EltSizeInBits, BitIdx, BitIdx + 1);\n    Bits.push_back(DAG.getConstant(Bit, DL, SVT));\n  }\n  SDValue BitMask = DAG.getBuildVector(VT, DL, Bits);\n  Vec = DAG.getNode(ISD::AND, DL, VT, Vec, BitMask);\n\n  // Compare against the bitmask and extend the result.\n  EVT CCVT = EVT::getVectorVT(*DAG.getContext(), MVT::i1, NumElts);\n  Vec = DAG.getSetCC(DL, CCVT, Vec, BitMask, ISD::SETEQ);\n  Vec = DAG.getSExtOrTrunc(Vec, DL, VT);\n\n  // For SEXT, this is now done, otherwise shift the result down for\n  // zero-extension.\n  if (Opcode == ISD::SIGN_EXTEND)\n    return Vec;\n  return DAG.getNode(ISD::SRL, DL, VT, Vec,\n                     DAG.getConstant(EltSizeInBits - 1, DL, VT));\n}\n\n// Attempt to combine a (sext/zext (setcc)) to a setcc with a xmm/ymm/zmm\n// result type.\nstatic SDValue combineExtSetcc(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n  SDLoc dl(N);\n\n  // Only do this combine with AVX512 for vector extends.\n  if (!Subtarget.hasAVX512() || !VT.isVector() || N0.getOpcode() != ISD::SETCC)\n    return SDValue();\n\n  // Only combine legal element types.\n  EVT SVT = VT.getVectorElementType();\n  if (SVT != MVT::i8 && SVT != MVT::i16 && SVT != MVT::i32 &&\n      SVT != MVT::i64 && SVT != MVT::f32 && SVT != MVT::f64)\n    return SDValue();\n\n  // We can only do this if the vector size in 256 bits or less.\n  unsigned Size = VT.getSizeInBits();\n  if (Size > 256 && Subtarget.useAVX512Regs())\n    return SDValue();\n\n  // Don't fold if the condition code can't be handled by PCMPEQ/PCMPGT since\n  // that's the only integer compares with we have.\n  ISD::CondCode CC = cast<CondCodeSDNode>(N0.getOperand(2))->get();\n  if (ISD::isUnsignedIntSetCC(CC))\n    return SDValue();\n\n  // Only do this combine if the extension will be fully consumed by the setcc.\n  EVT N00VT = N0.getOperand(0).getValueType();\n  EVT MatchingVecType = N00VT.changeVectorElementTypeToInteger();\n  if (Size != MatchingVecType.getSizeInBits())\n    return SDValue();\n\n  SDValue Res = DAG.getSetCC(dl, VT, N0.getOperand(0), N0.getOperand(1), CC);\n\n  if (N->getOpcode() == ISD::ZERO_EXTEND)\n    Res = DAG.getZeroExtendInReg(Res, dl, N0.getValueType());\n\n  return Res;\n}\n\nstatic SDValue combineSext(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI,\n                           const X86Subtarget &Subtarget) {\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n  SDLoc DL(N);\n\n  // (i32 (sext (i8 (x86isd::setcc_carry)))) -> (i32 (x86isd::setcc_carry))\n  if (!DCI.isBeforeLegalizeOps() &&\n      N0.getOpcode() == X86ISD::SETCC_CARRY) {\n    SDValue Setcc = DAG.getNode(X86ISD::SETCC_CARRY, DL, VT, N0->getOperand(0),\n                                 N0->getOperand(1));\n    bool ReplaceOtherUses = !N0.hasOneUse();\n    DCI.CombineTo(N, Setcc);\n    // Replace other uses with a truncate of the widened setcc_carry.\n    if (ReplaceOtherUses) {\n      SDValue Trunc = DAG.getNode(ISD::TRUNCATE, SDLoc(N0),\n                                  N0.getValueType(), Setcc);\n      DCI.CombineTo(N0.getNode(), Trunc);\n    }\n\n    return SDValue(N, 0);\n  }\n\n  if (SDValue NewCMov = combineToExtendCMOV(N, DAG))\n    return NewCMov;\n\n  if (!DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  if (SDValue V = combineExtSetcc(N, DAG, Subtarget))\n    return V;\n\n  if (SDValue V = combineToExtendBoolVectorInReg(N, DAG, DCI, Subtarget))\n    return V;\n\n  if (VT.isVector()) {\n    if (SDValue R = PromoteMaskArithmetic(N, DAG, Subtarget))\n      return R;\n\n    if (N0.getOpcode() == ISD::SIGN_EXTEND_VECTOR_INREG)\n      return DAG.getNode(N0.getOpcode(), DL, VT, N0.getOperand(0));\n  }\n\n  if (SDValue NewAdd = promoteExtBeforeAdd(N, DAG, Subtarget))\n    return NewAdd;\n\n  return SDValue();\n}\n\nstatic SDValue combineFMA(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  SDLoc dl(N);\n  EVT VT = N->getValueType(0);\n  bool IsStrict = N->isStrictFPOpcode() || N->isTargetStrictFPOpcode();\n\n  // Let legalize expand this if it isn't a legal type yet.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (!TLI.isTypeLegal(VT))\n    return SDValue();\n\n  SDValue A = N->getOperand(IsStrict ? 1 : 0);\n  SDValue B = N->getOperand(IsStrict ? 2 : 1);\n  SDValue C = N->getOperand(IsStrict ? 3 : 2);\n\n  // If the operation allows fast-math and the target does not support FMA,\n  // split this into mul+add to avoid libcall(s).\n  SDNodeFlags Flags = N->getFlags();\n  if (!IsStrict && Flags.hasAllowReassociation() &&\n      TLI.isOperationExpand(ISD::FMA, VT)) {\n    SDValue Fmul = DAG.getNode(ISD::FMUL, dl, VT, A, B, Flags);\n    return DAG.getNode(ISD::FADD, dl, VT, Fmul, C, Flags);\n  }\n\n  EVT ScalarVT = VT.getScalarType();\n  if ((ScalarVT != MVT::f32 && ScalarVT != MVT::f64) || !Subtarget.hasAnyFMA())\n    return SDValue();\n\n  auto invertIfNegative = [&DAG, &TLI, &DCI](SDValue &V) {\n    bool CodeSize = DAG.getMachineFunction().getFunction().hasOptSize();\n    bool LegalOperations = !DCI.isBeforeLegalizeOps();\n    if (SDValue NegV = TLI.getCheaperNegatedExpression(V, DAG, LegalOperations,\n                                                       CodeSize)) {\n      V = NegV;\n      return true;\n    }\n    // Look through extract_vector_elts. If it comes from an FNEG, create a\n    // new extract from the FNEG input.\n    if (V.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n        isNullConstant(V.getOperand(1))) {\n      SDValue Vec = V.getOperand(0);\n      if (SDValue NegV = TLI.getCheaperNegatedExpression(\n              Vec, DAG, LegalOperations, CodeSize)) {\n        V = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, SDLoc(V), V.getValueType(),\n                        NegV, V.getOperand(1));\n        return true;\n      }\n    }\n\n    return false;\n  };\n\n  // Do not convert the passthru input of scalar intrinsics.\n  // FIXME: We could allow negations of the lower element only.\n  bool NegA = invertIfNegative(A);\n  bool NegB = invertIfNegative(B);\n  bool NegC = invertIfNegative(C);\n\n  if (!NegA && !NegB && !NegC)\n    return SDValue();\n\n  unsigned NewOpcode =\n      negateFMAOpcode(N->getOpcode(), NegA != NegB, NegC, false);\n\n  if (IsStrict) {\n    assert(N->getNumOperands() == 4 && \"Shouldn't be greater than 4\");\n    return DAG.getNode(NewOpcode, dl, {VT, MVT::Other},\n                       {N->getOperand(0), A, B, C});\n  } else {\n    if (N->getNumOperands() == 4)\n      return DAG.getNode(NewOpcode, dl, VT, A, B, C, N->getOperand(3));\n    return DAG.getNode(NewOpcode, dl, VT, A, B, C);\n  }\n}\n\n// Combine FMADDSUB(A, B, FNEG(C)) -> FMSUBADD(A, B, C)\n// Combine FMSUBADD(A, B, FNEG(C)) -> FMADDSUB(A, B, C)\nstatic SDValue combineFMADDSUB(SDNode *N, SelectionDAG &DAG,\n                               TargetLowering::DAGCombinerInfo &DCI) {\n  SDLoc dl(N);\n  EVT VT = N->getValueType(0);\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  bool CodeSize = DAG.getMachineFunction().getFunction().hasOptSize();\n  bool LegalOperations = !DCI.isBeforeLegalizeOps();\n\n  SDValue N2 = N->getOperand(2);\n\n  SDValue NegN2 =\n      TLI.getCheaperNegatedExpression(N2, DAG, LegalOperations, CodeSize);\n  if (!NegN2)\n    return SDValue();\n  unsigned NewOpcode = negateFMAOpcode(N->getOpcode(), false, true, false);\n\n  if (N->getNumOperands() == 4)\n    return DAG.getNode(NewOpcode, dl, VT, N->getOperand(0), N->getOperand(1),\n                       NegN2, N->getOperand(3));\n  return DAG.getNode(NewOpcode, dl, VT, N->getOperand(0), N->getOperand(1),\n                     NegN2);\n}\n\nstatic SDValue combineZext(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI,\n                           const X86Subtarget &Subtarget) {\n  SDLoc dl(N);\n  SDValue N0 = N->getOperand(0);\n  EVT VT = N->getValueType(0);\n\n  // (i32 (aext (i8 (x86isd::setcc_carry)))) -> (i32 (x86isd::setcc_carry))\n  // FIXME: Is this needed? We don't seem to have any tests for it.\n  if (!DCI.isBeforeLegalizeOps() && N->getOpcode() == ISD::ANY_EXTEND &&\n      N0.getOpcode() == X86ISD::SETCC_CARRY) {\n    SDValue Setcc = DAG.getNode(X86ISD::SETCC_CARRY, dl, VT, N0->getOperand(0),\n                                 N0->getOperand(1));\n    bool ReplaceOtherUses = !N0.hasOneUse();\n    DCI.CombineTo(N, Setcc);\n    // Replace other uses with a truncate of the widened setcc_carry.\n    if (ReplaceOtherUses) {\n      SDValue Trunc = DAG.getNode(ISD::TRUNCATE, SDLoc(N0),\n                                  N0.getValueType(), Setcc);\n      DCI.CombineTo(N0.getNode(), Trunc);\n    }\n\n    return SDValue(N, 0);\n  }\n\n  if (SDValue NewCMov = combineToExtendCMOV(N, DAG))\n    return NewCMov;\n\n  if (DCI.isBeforeLegalizeOps())\n    if (SDValue V = combineExtSetcc(N, DAG, Subtarget))\n      return V;\n\n  if (SDValue V = combineToExtendBoolVectorInReg(N, DAG, DCI, Subtarget))\n    return V;\n\n  if (VT.isVector())\n    if (SDValue R = PromoteMaskArithmetic(N, DAG, Subtarget))\n      return R;\n\n  if (SDValue NewAdd = promoteExtBeforeAdd(N, DAG, Subtarget))\n    return NewAdd;\n\n  if (SDValue R = combineOrCmpEqZeroToCtlzSrl(N, DAG, DCI, Subtarget))\n    return R;\n\n  // TODO: Combine with any target/faux shuffle.\n  if (N0.getOpcode() == X86ISD::PACKUS && N0.getValueSizeInBits() == 128 &&\n      VT.getScalarSizeInBits() == N0.getOperand(0).getScalarValueSizeInBits()) {\n    SDValue N00 = N0.getOperand(0);\n    SDValue N01 = N0.getOperand(1);\n    unsigned NumSrcEltBits = N00.getScalarValueSizeInBits();\n    APInt ZeroMask = APInt::getHighBitsSet(NumSrcEltBits, NumSrcEltBits / 2);\n    if ((N00.isUndef() || DAG.MaskedValueIsZero(N00, ZeroMask)) &&\n        (N01.isUndef() || DAG.MaskedValueIsZero(N01, ZeroMask))) {\n      return concatSubVectors(N00, N01, DAG, dl);\n    }\n  }\n\n  return SDValue();\n}\n\n/// Recursive helper for combineVectorSizedSetCCEquality() to see if we have a\n/// recognizable memcmp expansion.\nstatic bool isOrXorXorTree(SDValue X, bool Root = true) {\n  if (X.getOpcode() == ISD::OR)\n    return isOrXorXorTree(X.getOperand(0), false) &&\n           isOrXorXorTree(X.getOperand(1), false);\n  if (Root)\n    return false;\n  return X.getOpcode() == ISD::XOR;\n}\n\n/// Recursive helper for combineVectorSizedSetCCEquality() to emit the memcmp\n/// expansion.\ntemplate<typename F>\nstatic SDValue emitOrXorXorTree(SDValue X, SDLoc &DL, SelectionDAG &DAG,\n                                EVT VecVT, EVT CmpVT, bool HasPT, F SToV) {\n  SDValue Op0 = X.getOperand(0);\n  SDValue Op1 = X.getOperand(1);\n  if (X.getOpcode() == ISD::OR) {\n    SDValue A = emitOrXorXorTree(Op0, DL, DAG, VecVT, CmpVT, HasPT, SToV);\n    SDValue B = emitOrXorXorTree(Op1, DL, DAG, VecVT, CmpVT, HasPT, SToV);\n    if (VecVT != CmpVT)\n      return DAG.getNode(ISD::OR, DL, CmpVT, A, B);\n    if (HasPT)\n      return DAG.getNode(ISD::OR, DL, VecVT, A, B);\n    return DAG.getNode(ISD::AND, DL, CmpVT, A, B);\n  } else if (X.getOpcode() == ISD::XOR) {\n    SDValue A = SToV(Op0);\n    SDValue B = SToV(Op1);\n    if (VecVT != CmpVT)\n      return DAG.getSetCC(DL, CmpVT, A, B, ISD::SETNE);\n    if (HasPT)\n      return DAG.getNode(ISD::XOR, DL, VecVT, A, B);\n    return DAG.getSetCC(DL, CmpVT, A, B, ISD::SETEQ);\n  }\n  llvm_unreachable(\"Impossible\");\n}\n\n/// Try to map a 128-bit or larger integer comparison to vector instructions\n/// before type legalization splits it up into chunks.\nstatic SDValue combineVectorSizedSetCCEquality(SDNode *SetCC, SelectionDAG &DAG,\n                                               const X86Subtarget &Subtarget) {\n  ISD::CondCode CC = cast<CondCodeSDNode>(SetCC->getOperand(2))->get();\n  assert((CC == ISD::SETNE || CC == ISD::SETEQ) && \"Bad comparison predicate\");\n\n  // We're looking for an oversized integer equality comparison.\n  SDValue X = SetCC->getOperand(0);\n  SDValue Y = SetCC->getOperand(1);\n  EVT OpVT = X.getValueType();\n  unsigned OpSize = OpVT.getSizeInBits();\n  if (!OpVT.isScalarInteger() || OpSize < 128)\n    return SDValue();\n\n  // Ignore a comparison with zero because that gets special treatment in\n  // EmitTest(). But make an exception for the special case of a pair of\n  // logically-combined vector-sized operands compared to zero. This pattern may\n  // be generated by the memcmp expansion pass with oversized integer compares\n  // (see PR33325).\n  bool IsOrXorXorTreeCCZero = isNullConstant(Y) && isOrXorXorTree(X);\n  if (isNullConstant(Y) && !IsOrXorXorTreeCCZero)\n    return SDValue();\n\n  // Don't perform this combine if constructing the vector will be expensive.\n  auto IsVectorBitCastCheap = [](SDValue X) {\n    X = peekThroughBitcasts(X);\n    return isa<ConstantSDNode>(X) || X.getValueType().isVector() ||\n           X.getOpcode() == ISD::LOAD;\n  };\n  if ((!IsVectorBitCastCheap(X) || !IsVectorBitCastCheap(Y)) &&\n      !IsOrXorXorTreeCCZero)\n    return SDValue();\n\n  EVT VT = SetCC->getValueType(0);\n  SDLoc DL(SetCC);\n\n  // Use XOR (plus OR) and PTEST after SSE4.1 for 128/256-bit operands.\n  // Use PCMPNEQ (plus OR) and KORTEST for 512-bit operands.\n  // Otherwise use PCMPEQ (plus AND) and mask testing.\n  if ((OpSize == 128 && Subtarget.hasSSE2()) ||\n      (OpSize == 256 && Subtarget.hasAVX()) ||\n      (OpSize == 512 && Subtarget.useAVX512Regs())) {\n    bool HasPT = Subtarget.hasSSE41();\n\n    // PTEST and MOVMSK are slow on Knights Landing and Knights Mill and widened\n    // vector registers are essentially free. (Technically, widening registers\n    // prevents load folding, but the tradeoff is worth it.)\n    bool PreferKOT = Subtarget.preferMaskRegisters();\n    bool NeedZExt = PreferKOT && !Subtarget.hasVLX() && OpSize != 512;\n\n    EVT VecVT = MVT::v16i8;\n    EVT CmpVT = PreferKOT ? MVT::v16i1 : VecVT;\n    if (OpSize == 256) {\n      VecVT = MVT::v32i8;\n      CmpVT = PreferKOT ? MVT::v32i1 : VecVT;\n    }\n    EVT CastVT = VecVT;\n    bool NeedsAVX512FCast = false;\n    if (OpSize == 512 || NeedZExt) {\n      if (Subtarget.hasBWI()) {\n        VecVT = MVT::v64i8;\n        CmpVT = MVT::v64i1;\n        if (OpSize == 512)\n          CastVT = VecVT;\n      } else {\n        VecVT = MVT::v16i32;\n        CmpVT = MVT::v16i1;\n        CastVT = OpSize == 512 ? VecVT :\n                 OpSize == 256 ? MVT::v8i32 : MVT::v4i32;\n        NeedsAVX512FCast = true;\n      }\n    }\n\n    auto ScalarToVector = [&](SDValue X) -> SDValue {\n      bool TmpZext = false;\n      EVT TmpCastVT = CastVT;\n      if (X.getOpcode() == ISD::ZERO_EXTEND) {\n        SDValue OrigX = X.getOperand(0);\n        unsigned OrigSize = OrigX.getScalarValueSizeInBits();\n        if (OrigSize < OpSize) {\n          if (OrigSize == 128) {\n            TmpCastVT = NeedsAVX512FCast ? MVT::v4i32 : MVT::v16i8;\n            X = OrigX;\n            TmpZext = true;\n          } else if (OrigSize == 256) {\n            TmpCastVT = NeedsAVX512FCast ? MVT::v8i32 : MVT::v32i8;\n            X = OrigX;\n            TmpZext = true;\n          }\n        }\n      }\n      X = DAG.getBitcast(TmpCastVT, X);\n      if (!NeedZExt && !TmpZext)\n        return X;\n      return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VecVT,\n                         DAG.getConstant(0, DL, VecVT), X,\n                         DAG.getVectorIdxConstant(0, DL));\n    };\n\n    SDValue Cmp;\n    if (IsOrXorXorTreeCCZero) {\n      // This is a bitwise-combined equality comparison of 2 pairs of vectors:\n      // setcc i128 (or (xor A, B), (xor C, D)), 0, eq|ne\n      // Use 2 vector equality compares and 'and' the results before doing a\n      // MOVMSK.\n      Cmp = emitOrXorXorTree(X, DL, DAG, VecVT, CmpVT, HasPT, ScalarToVector);\n    } else {\n      SDValue VecX = ScalarToVector(X);\n      SDValue VecY = ScalarToVector(Y);\n      if (VecVT != CmpVT) {\n        Cmp = DAG.getSetCC(DL, CmpVT, VecX, VecY, ISD::SETNE);\n      } else if (HasPT) {\n        Cmp = DAG.getNode(ISD::XOR, DL, VecVT, VecX, VecY);\n      } else {\n        Cmp = DAG.getSetCC(DL, CmpVT, VecX, VecY, ISD::SETEQ);\n      }\n    }\n    // AVX512 should emit a setcc that will lower to kortest.\n    if (VecVT != CmpVT) {\n      EVT KRegVT = CmpVT == MVT::v64i1 ? MVT::i64 :\n                   CmpVT == MVT::v32i1 ? MVT::i32 : MVT::i16;\n      return DAG.getSetCC(DL, VT, DAG.getBitcast(KRegVT, Cmp),\n                          DAG.getConstant(0, DL, KRegVT), CC);\n    }\n    if (HasPT) {\n      SDValue BCCmp = DAG.getBitcast(OpSize == 256 ? MVT::v4i64 : MVT::v2i64,\n                                     Cmp);\n      SDValue PT = DAG.getNode(X86ISD::PTEST, DL, MVT::i32, BCCmp, BCCmp);\n      X86::CondCode X86CC = CC == ISD::SETEQ ? X86::COND_E : X86::COND_NE;\n      SDValue X86SetCC = getSETCC(X86CC, PT, DL, DAG);\n      return DAG.getNode(ISD::TRUNCATE, DL, VT, X86SetCC.getValue(0));\n    }\n    // If all bytes match (bitmask is 0x(FFFF)FFFF), that's equality.\n    // setcc i128 X, Y, eq --> setcc (pmovmskb (pcmpeqb X, Y)), 0xFFFF, eq\n    // setcc i128 X, Y, ne --> setcc (pmovmskb (pcmpeqb X, Y)), 0xFFFF, ne\n    assert(Cmp.getValueType() == MVT::v16i8 &&\n           \"Non 128-bit vector on pre-SSE41 target\");\n    SDValue MovMsk = DAG.getNode(X86ISD::MOVMSK, DL, MVT::i32, Cmp);\n    SDValue FFFFs = DAG.getConstant(0xFFFF, DL, MVT::i32);\n    return DAG.getSetCC(DL, VT, MovMsk, FFFFs, CC);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineSetCC(SDNode *N, SelectionDAG &DAG,\n                            const X86Subtarget &Subtarget) {\n  const ISD::CondCode CC = cast<CondCodeSDNode>(N->getOperand(2))->get();\n  const SDValue LHS = N->getOperand(0);\n  const SDValue RHS = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n  EVT OpVT = LHS.getValueType();\n  SDLoc DL(N);\n\n  if (CC == ISD::SETNE || CC == ISD::SETEQ) {\n    if (SDValue V = combineVectorSizedSetCCEquality(N, DAG, Subtarget))\n      return V;\n\n    if (VT == MVT::i1 && isNullConstant(RHS)) {\n      SDValue X86CC;\n      if (SDValue V =\n              MatchVectorAllZeroTest(LHS, CC, DL, Subtarget, DAG, X86CC))\n        return DAG.getNode(ISD::TRUNCATE, DL, VT,\n                           DAG.getNode(X86ISD::SETCC, DL, MVT::i8, X86CC, V));\n    }\n  }\n\n  if (VT.isVector() && VT.getVectorElementType() == MVT::i1 &&\n      (CC == ISD::SETNE || CC == ISD::SETEQ || ISD::isSignedIntSetCC(CC))) {\n    // Using temporaries to avoid messing up operand ordering for later\n    // transformations if this doesn't work.\n    SDValue Op0 = LHS;\n    SDValue Op1 = RHS;\n    ISD::CondCode TmpCC = CC;\n    // Put build_vector on the right.\n    if (Op0.getOpcode() == ISD::BUILD_VECTOR) {\n      std::swap(Op0, Op1);\n      TmpCC = ISD::getSetCCSwappedOperands(TmpCC);\n    }\n\n    bool IsSEXT0 =\n        (Op0.getOpcode() == ISD::SIGN_EXTEND) &&\n        (Op0.getOperand(0).getValueType().getVectorElementType() == MVT::i1);\n    bool IsVZero1 = ISD::isBuildVectorAllZeros(Op1.getNode());\n\n    if (IsSEXT0 && IsVZero1) {\n      assert(VT == Op0.getOperand(0).getValueType() &&\n             \"Unexpected operand type\");\n      if (TmpCC == ISD::SETGT)\n        return DAG.getConstant(0, DL, VT);\n      if (TmpCC == ISD::SETLE)\n        return DAG.getConstant(1, DL, VT);\n      if (TmpCC == ISD::SETEQ || TmpCC == ISD::SETGE)\n        return DAG.getNOT(DL, Op0.getOperand(0), VT);\n\n      assert((TmpCC == ISD::SETNE || TmpCC == ISD::SETLT) &&\n             \"Unexpected condition code!\");\n      return Op0.getOperand(0);\n    }\n  }\n\n  // If we have AVX512, but not BWI and this is a vXi16/vXi8 setcc, just\n  // pre-promote its result type since vXi1 vectors don't get promoted\n  // during type legalization.\n  // NOTE: The element count check is to ignore operand types that need to\n  // go through type promotion to a 128-bit vector.\n  if (Subtarget.hasAVX512() && !Subtarget.hasBWI() && VT.isVector() &&\n      VT.getVectorElementType() == MVT::i1 &&\n      (OpVT.getVectorElementType() == MVT::i8 ||\n       OpVT.getVectorElementType() == MVT::i16)) {\n    SDValue Setcc = DAG.getSetCC(DL, OpVT, LHS, RHS, CC);\n    return DAG.getNode(ISD::TRUNCATE, DL, VT, Setcc);\n  }\n\n  // For an SSE1-only target, lower a comparison of v4f32 to X86ISD::CMPP early\n  // to avoid scalarization via legalization because v4i32 is not a legal type.\n  if (Subtarget.hasSSE1() && !Subtarget.hasSSE2() && VT == MVT::v4i32 &&\n      LHS.getValueType() == MVT::v4f32)\n    return LowerVSETCC(SDValue(N, 0), Subtarget, DAG);\n\n  return SDValue();\n}\n\nstatic SDValue combineMOVMSK(SDNode *N, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI,\n                             const X86Subtarget &Subtarget) {\n  SDValue Src = N->getOperand(0);\n  MVT SrcVT = Src.getSimpleValueType();\n  MVT VT = N->getSimpleValueType(0);\n  unsigned NumBits = VT.getScalarSizeInBits();\n  unsigned NumElts = SrcVT.getVectorNumElements();\n\n  // Perform constant folding.\n  if (ISD::isBuildVectorOfConstantSDNodes(Src.getNode())) {\n    assert(VT == MVT::i32 && \"Unexpected result type\");\n    APInt Imm(32, 0);\n    for (unsigned Idx = 0, e = Src.getNumOperands(); Idx < e; ++Idx) {\n      if (!Src.getOperand(Idx).isUndef() &&\n          Src.getConstantOperandAPInt(Idx).isNegative())\n        Imm.setBit(Idx);\n    }\n    return DAG.getConstant(Imm, SDLoc(N), VT);\n  }\n\n  // Look through int->fp bitcasts that don't change the element width.\n  unsigned EltWidth = SrcVT.getScalarSizeInBits();\n  if (Subtarget.hasSSE2() && Src.getOpcode() == ISD::BITCAST &&\n      Src.getOperand(0).getScalarValueSizeInBits() == EltWidth)\n    return DAG.getNode(X86ISD::MOVMSK, SDLoc(N), VT, Src.getOperand(0));\n\n  // Fold movmsk(not(x)) -> not(movmsk(x)) to improve folding of movmsk results\n  // with scalar comparisons.\n  if (SDValue NotSrc = IsNOT(Src, DAG)) {\n    SDLoc DL(N);\n    APInt NotMask = APInt::getLowBitsSet(NumBits, NumElts);\n    NotSrc = DAG.getBitcast(SrcVT, NotSrc);\n    return DAG.getNode(ISD::XOR, DL, VT,\n                       DAG.getNode(X86ISD::MOVMSK, DL, VT, NotSrc),\n                       DAG.getConstant(NotMask, DL, VT));\n  }\n\n  // Fold movmsk(icmp_sgt(x,-1)) -> not(movmsk(x)) to improve folding of movmsk\n  // results with scalar comparisons.\n  if (Src.getOpcode() == X86ISD::PCMPGT &&\n      ISD::isBuildVectorAllOnes(Src.getOperand(1).getNode())) {\n    SDLoc DL(N);\n    APInt NotMask = APInt::getLowBitsSet(NumBits, NumElts);\n    return DAG.getNode(ISD::XOR, DL, VT,\n                       DAG.getNode(X86ISD::MOVMSK, DL, VT, Src.getOperand(0)),\n                       DAG.getConstant(NotMask, DL, VT));\n  }\n\n  // Simplify the inputs.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  APInt DemandedMask(APInt::getAllOnesValue(NumBits));\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0), DemandedMask, DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\nstatic SDValue combineX86GatherScatter(SDNode *N, SelectionDAG &DAG,\n                                       TargetLowering::DAGCombinerInfo &DCI) {\n  // With vector masks we only demand the upper bit of the mask.\n  SDValue Mask = cast<X86MaskedGatherScatterSDNode>(N)->getMask();\n  if (Mask.getScalarValueSizeInBits() != 1) {\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    APInt DemandedMask(APInt::getSignMask(Mask.getScalarValueSizeInBits()));\n    if (TLI.SimplifyDemandedBits(Mask, DemandedMask, DCI)) {\n      if (N->getOpcode() != ISD::DELETED_NODE)\n        DCI.AddToWorklist(N);\n      return SDValue(N, 0);\n    }\n  }\n\n  return SDValue();\n}\n\nstatic SDValue rebuildGatherScatter(MaskedGatherScatterSDNode *GorS,\n                                    SDValue Index, SDValue Base, SDValue Scale,\n                                    SelectionDAG &DAG) {\n  SDLoc DL(GorS);\n\n  if (auto *Gather = dyn_cast<MaskedGatherSDNode>(GorS)) {\n    SDValue Ops[] = { Gather->getChain(), Gather->getPassThru(),\n                      Gather->getMask(), Base, Index, Scale } ;\n    return DAG.getMaskedGather(Gather->getVTList(),\n                               Gather->getMemoryVT(), DL, Ops,\n                               Gather->getMemOperand(),\n                               Gather->getIndexType(),\n                               Gather->getExtensionType());\n  }\n  auto *Scatter = cast<MaskedScatterSDNode>(GorS);\n  SDValue Ops[] = { Scatter->getChain(), Scatter->getValue(),\n                    Scatter->getMask(), Base, Index, Scale };\n  return DAG.getMaskedScatter(Scatter->getVTList(),\n                              Scatter->getMemoryVT(), DL,\n                              Ops, Scatter->getMemOperand(),\n                              Scatter->getIndexType(),\n                              Scatter->isTruncatingStore());\n}\n\nstatic SDValue combineGatherScatter(SDNode *N, SelectionDAG &DAG,\n                                    TargetLowering::DAGCombinerInfo &DCI) {\n  SDLoc DL(N);\n  auto *GorS = cast<MaskedGatherScatterSDNode>(N);\n  SDValue Index = GorS->getIndex();\n  SDValue Base = GorS->getBasePtr();\n  SDValue Scale = GorS->getScale();\n\n  if (DCI.isBeforeLegalize()) {\n    unsigned IndexWidth = Index.getScalarValueSizeInBits();\n\n    // Shrink constant indices if they are larger than 32-bits.\n    // Only do this before legalize types since v2i64 could become v2i32.\n    // FIXME: We could check that the type is legal if we're after legalize\n    // types, but then we would need to construct test cases where that happens.\n    // FIXME: We could support more than just constant vectors, but we need to\n    // careful with costing. A truncate that can be optimized out would be fine.\n    // Otherwise we might only want to create a truncate if it avoids a split.\n    if (auto *BV = dyn_cast<BuildVectorSDNode>(Index)) {\n      if (BV->isConstant() && IndexWidth > 32 &&\n          DAG.ComputeNumSignBits(Index) > (IndexWidth - 32)) {\n        unsigned NumElts = Index.getValueType().getVectorNumElements();\n        EVT NewVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32, NumElts);\n        Index = DAG.getNode(ISD::TRUNCATE, DL, NewVT, Index);\n        return rebuildGatherScatter(GorS, Index, Base, Scale, DAG);\n      }\n    }\n\n    // Shrink any sign/zero extends from 32 or smaller to larger than 32 if\n    // there are sufficient sign bits. Only do this before legalize types to\n    // avoid creating illegal types in truncate.\n    if ((Index.getOpcode() == ISD::SIGN_EXTEND ||\n         Index.getOpcode() == ISD::ZERO_EXTEND) &&\n        IndexWidth > 32 &&\n        Index.getOperand(0).getScalarValueSizeInBits() <= 32 &&\n        DAG.ComputeNumSignBits(Index) > (IndexWidth - 32)) {\n      unsigned NumElts = Index.getValueType().getVectorNumElements();\n      EVT NewVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32, NumElts);\n      Index = DAG.getNode(ISD::TRUNCATE, DL, NewVT, Index);\n      return rebuildGatherScatter(GorS, Index, Base, Scale, DAG);\n    }\n  }\n\n  if (DCI.isBeforeLegalizeOps()) {\n    unsigned IndexWidth = Index.getScalarValueSizeInBits();\n\n    // Make sure the index is either i32 or i64\n    if (IndexWidth != 32 && IndexWidth != 64) {\n      MVT EltVT = IndexWidth > 32 ? MVT::i64 : MVT::i32;\n      EVT IndexVT = EVT::getVectorVT(*DAG.getContext(), EltVT,\n                                   Index.getValueType().getVectorNumElements());\n      Index = DAG.getSExtOrTrunc(Index, DL, IndexVT);\n      return rebuildGatherScatter(GorS, Index, Base, Scale, DAG);\n    }\n  }\n\n  // With vector masks we only demand the upper bit of the mask.\n  SDValue Mask = GorS->getMask();\n  if (Mask.getScalarValueSizeInBits() != 1) {\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    APInt DemandedMask(APInt::getSignMask(Mask.getScalarValueSizeInBits()));\n    if (TLI.SimplifyDemandedBits(Mask, DemandedMask, DCI)) {\n      if (N->getOpcode() != ISD::DELETED_NODE)\n        DCI.AddToWorklist(N);\n      return SDValue(N, 0);\n    }\n  }\n\n  return SDValue();\n}\n\n// Optimize  RES = X86ISD::SETCC CONDCODE, EFLAG_INPUT\nstatic SDValue combineX86SetCC(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  SDLoc DL(N);\n  X86::CondCode CC = X86::CondCode(N->getConstantOperandVal(0));\n  SDValue EFLAGS = N->getOperand(1);\n\n  // Try to simplify the EFLAGS and condition code operands.\n  if (SDValue Flags = combineSetCCEFLAGS(EFLAGS, CC, DAG, Subtarget))\n    return getSETCC(CC, Flags, DL, DAG);\n\n  return SDValue();\n}\n\n/// Optimize branch condition evaluation.\nstatic SDValue combineBrCond(SDNode *N, SelectionDAG &DAG,\n                             const X86Subtarget &Subtarget) {\n  SDLoc DL(N);\n  SDValue EFLAGS = N->getOperand(3);\n  X86::CondCode CC = X86::CondCode(N->getConstantOperandVal(2));\n\n  // Try to simplify the EFLAGS and condition code operands.\n  // Make sure to not keep references to operands, as combineSetCCEFLAGS can\n  // RAUW them under us.\n  if (SDValue Flags = combineSetCCEFLAGS(EFLAGS, CC, DAG, Subtarget)) {\n    SDValue Cond = DAG.getTargetConstant(CC, DL, MVT::i8);\n    return DAG.getNode(X86ISD::BRCOND, DL, N->getVTList(), N->getOperand(0),\n                       N->getOperand(1), Cond, Flags);\n  }\n\n  return SDValue();\n}\n\n// TODO: Could we move this to DAGCombine?\nstatic SDValue combineVectorCompareAndMaskUnaryOp(SDNode *N,\n                                                  SelectionDAG &DAG) {\n  // Take advantage of vector comparisons (etc.) producing 0 or -1 in each lane\n  // to optimize away operation when it's from a constant.\n  //\n  // The general transformation is:\n  //    UNARYOP(AND(VECTOR_CMP(x,y), constant)) -->\n  //       AND(VECTOR_CMP(x,y), constant2)\n  //    constant2 = UNARYOP(constant)\n\n  // Early exit if this isn't a vector operation, the operand of the\n  // unary operation isn't a bitwise AND, or if the sizes of the operations\n  // aren't the same.\n  EVT VT = N->getValueType(0);\n  bool IsStrict = N->isStrictFPOpcode();\n  unsigned NumEltBits = VT.getScalarSizeInBits();\n  SDValue Op0 = N->getOperand(IsStrict ? 1 : 0);\n  if (!VT.isVector() || Op0.getOpcode() != ISD::AND ||\n      DAG.ComputeNumSignBits(Op0.getOperand(0)) != NumEltBits ||\n      VT.getSizeInBits() != Op0.getValueSizeInBits())\n    return SDValue();\n\n  // Now check that the other operand of the AND is a constant. We could\n  // make the transformation for non-constant splats as well, but it's unclear\n  // that would be a benefit as it would not eliminate any operations, just\n  // perform one more step in scalar code before moving to the vector unit.\n  if (auto *BV = dyn_cast<BuildVectorSDNode>(Op0.getOperand(1))) {\n    // Bail out if the vector isn't a constant.\n    if (!BV->isConstant())\n      return SDValue();\n\n    // Everything checks out. Build up the new and improved node.\n    SDLoc DL(N);\n    EVT IntVT = BV->getValueType(0);\n    // Create a new constant of the appropriate type for the transformed\n    // DAG.\n    SDValue SourceConst;\n    if (IsStrict)\n      SourceConst = DAG.getNode(N->getOpcode(), DL, {VT, MVT::Other},\n                                {N->getOperand(0), SDValue(BV, 0)});\n    else\n      SourceConst = DAG.getNode(N->getOpcode(), DL, VT, SDValue(BV, 0));\n    // The AND node needs bitcasts to/from an integer vector type around it.\n    SDValue MaskConst = DAG.getBitcast(IntVT, SourceConst);\n    SDValue NewAnd = DAG.getNode(ISD::AND, DL, IntVT, Op0->getOperand(0),\n                                 MaskConst);\n    SDValue Res = DAG.getBitcast(VT, NewAnd);\n    if (IsStrict)\n      return DAG.getMergeValues({Res, SourceConst.getValue(1)}, DL);\n    return Res;\n  }\n\n  return SDValue();\n}\n\n/// If we are converting a value to floating-point, try to replace scalar\n/// truncate of an extracted vector element with a bitcast. This tries to keep\n/// the sequence on XMM registers rather than moving between vector and GPRs.\nstatic SDValue combineToFPTruncExtElt(SDNode *N, SelectionDAG &DAG) {\n  // TODO: This is currently only used by combineSIntToFP, but it is generalized\n  //       to allow being called by any similar cast opcode.\n  // TODO: Consider merging this into lowering: vectorizeExtractedCast().\n  SDValue Trunc = N->getOperand(0);\n  if (!Trunc.hasOneUse() || Trunc.getOpcode() != ISD::TRUNCATE)\n    return SDValue();\n\n  SDValue ExtElt = Trunc.getOperand(0);\n  if (!ExtElt.hasOneUse() || ExtElt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n      !isNullConstant(ExtElt.getOperand(1)))\n    return SDValue();\n\n  EVT TruncVT = Trunc.getValueType();\n  EVT SrcVT = ExtElt.getValueType();\n  unsigned DestWidth = TruncVT.getSizeInBits();\n  unsigned SrcWidth = SrcVT.getSizeInBits();\n  if (SrcWidth % DestWidth != 0)\n    return SDValue();\n\n  // inttofp (trunc (extelt X, 0)) --> inttofp (extelt (bitcast X), 0)\n  EVT SrcVecVT = ExtElt.getOperand(0).getValueType();\n  unsigned VecWidth = SrcVecVT.getSizeInBits();\n  unsigned NumElts = VecWidth / DestWidth;\n  EVT BitcastVT = EVT::getVectorVT(*DAG.getContext(), TruncVT, NumElts);\n  SDValue BitcastVec = DAG.getBitcast(BitcastVT, ExtElt.getOperand(0));\n  SDLoc DL(N);\n  SDValue NewExtElt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, TruncVT,\n                                  BitcastVec, ExtElt.getOperand(1));\n  return DAG.getNode(N->getOpcode(), DL, N->getValueType(0), NewExtElt);\n}\n\nstatic SDValue combineUIntToFP(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  bool IsStrict = N->isStrictFPOpcode();\n  SDValue Op0 = N->getOperand(IsStrict ? 1 : 0);\n  EVT VT = N->getValueType(0);\n  EVT InVT = Op0.getValueType();\n\n  // UINT_TO_FP(vXi1) -> SINT_TO_FP(ZEXT(vXi1 to vXi32))\n  // UINT_TO_FP(vXi8) -> SINT_TO_FP(ZEXT(vXi8 to vXi32))\n  // UINT_TO_FP(vXi16) -> SINT_TO_FP(ZEXT(vXi16 to vXi32))\n  if (InVT.isVector() && InVT.getScalarSizeInBits() < 32) {\n    SDLoc dl(N);\n    EVT DstVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32,\n                                 InVT.getVectorNumElements());\n    SDValue P = DAG.getNode(ISD::ZERO_EXTEND, dl, DstVT, Op0);\n\n    // UINT_TO_FP isn't legal without AVX512 so use SINT_TO_FP.\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {VT, MVT::Other},\n                         {N->getOperand(0), P});\n    return DAG.getNode(ISD::SINT_TO_FP, dl, VT, P);\n  }\n\n  // Since UINT_TO_FP is legal (it's marked custom), dag combiner won't\n  // optimize it to a SINT_TO_FP when the sign bit is known zero. Perform\n  // the optimization here.\n  if (DAG.SignBitIsZero(Op0)) {\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_SINT_TO_FP, SDLoc(N), {VT, MVT::Other},\n                         {N->getOperand(0), Op0});\n    return DAG.getNode(ISD::SINT_TO_FP, SDLoc(N), VT, Op0);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineSIntToFP(SDNode *N, SelectionDAG &DAG,\n                               TargetLowering::DAGCombinerInfo &DCI,\n                               const X86Subtarget &Subtarget) {\n  // First try to optimize away the conversion entirely when it's\n  // conditionally from a constant. Vectors only.\n  bool IsStrict = N->isStrictFPOpcode();\n  if (SDValue Res = combineVectorCompareAndMaskUnaryOp(N, DAG))\n    return Res;\n\n  // Now move on to more general possibilities.\n  SDValue Op0 = N->getOperand(IsStrict ? 1 : 0);\n  EVT VT = N->getValueType(0);\n  EVT InVT = Op0.getValueType();\n\n  // SINT_TO_FP(vXi1) -> SINT_TO_FP(SEXT(vXi1 to vXi32))\n  // SINT_TO_FP(vXi8) -> SINT_TO_FP(SEXT(vXi8 to vXi32))\n  // SINT_TO_FP(vXi16) -> SINT_TO_FP(SEXT(vXi16 to vXi32))\n  if (InVT.isVector() && InVT.getScalarSizeInBits() < 32) {\n    SDLoc dl(N);\n    EVT DstVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32,\n                                 InVT.getVectorNumElements());\n    SDValue P = DAG.getNode(ISD::SIGN_EXTEND, dl, DstVT, Op0);\n    if (IsStrict)\n      return DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {VT, MVT::Other},\n                         {N->getOperand(0), P});\n    return DAG.getNode(ISD::SINT_TO_FP, dl, VT, P);\n  }\n\n  // Without AVX512DQ we only support i64 to float scalar conversion. For both\n  // vectors and scalars, see if we know that the upper bits are all the sign\n  // bit, in which case we can truncate the input to i32 and convert from that.\n  if (InVT.getScalarSizeInBits() > 32 && !Subtarget.hasDQI()) {\n    unsigned BitWidth = InVT.getScalarSizeInBits();\n    unsigned NumSignBits = DAG.ComputeNumSignBits(Op0);\n    if (NumSignBits >= (BitWidth - 31)) {\n      EVT TruncVT = MVT::i32;\n      if (InVT.isVector())\n        TruncVT = EVT::getVectorVT(*DAG.getContext(), TruncVT,\n                                   InVT.getVectorNumElements());\n      SDLoc dl(N);\n      if (DCI.isBeforeLegalize() || TruncVT != MVT::v2i32) {\n        SDValue Trunc = DAG.getNode(ISD::TRUNCATE, dl, TruncVT, Op0);\n        if (IsStrict)\n          return DAG.getNode(ISD::STRICT_SINT_TO_FP, dl, {VT, MVT::Other},\n                             {N->getOperand(0), Trunc});\n        return DAG.getNode(ISD::SINT_TO_FP, dl, VT, Trunc);\n      }\n      // If we're after legalize and the type is v2i32 we need to shuffle and\n      // use CVTSI2P.\n      assert(InVT == MVT::v2i64 && \"Unexpected VT!\");\n      SDValue Cast = DAG.getBitcast(MVT::v4i32, Op0);\n      SDValue Shuf = DAG.getVectorShuffle(MVT::v4i32, dl, Cast, Cast,\n                                          { 0, 2, -1, -1 });\n      if (IsStrict)\n        return DAG.getNode(X86ISD::STRICT_CVTSI2P, dl, {VT, MVT::Other},\n                           {N->getOperand(0), Shuf});\n      return DAG.getNode(X86ISD::CVTSI2P, dl, VT, Shuf);\n    }\n  }\n\n  // Transform (SINT_TO_FP (i64 ...)) into an x87 operation if we have\n  // a 32-bit target where SSE doesn't support i64->FP operations.\n  if (!Subtarget.useSoftFloat() && Subtarget.hasX87() &&\n      Op0.getOpcode() == ISD::LOAD) {\n    LoadSDNode *Ld = cast<LoadSDNode>(Op0.getNode());\n\n    // This transformation is not supported if the result type is f16 or f128.\n    if (VT == MVT::f16 || VT == MVT::f128)\n      return SDValue();\n\n    // If we have AVX512DQ we can use packed conversion instructions unless\n    // the VT is f80.\n    if (Subtarget.hasDQI() && VT != MVT::f80)\n      return SDValue();\n\n    if (Ld->isSimple() && !VT.isVector() && ISD::isNormalLoad(Op0.getNode()) &&\n        Op0.hasOneUse() && !Subtarget.is64Bit() && InVT == MVT::i64) {\n      std::pair<SDValue, SDValue> Tmp =\n          Subtarget.getTargetLowering()->BuildFILD(\n              VT, InVT, SDLoc(N), Ld->getChain(), Ld->getBasePtr(),\n              Ld->getPointerInfo(), Ld->getOriginalAlign(), DAG);\n      DAG.ReplaceAllUsesOfValueWith(Op0.getValue(1), Tmp.second);\n      return Tmp.first;\n    }\n  }\n\n  if (IsStrict)\n    return SDValue();\n\n  if (SDValue V = combineToFPTruncExtElt(N, DAG))\n    return V;\n\n  return SDValue();\n}\n\nstatic bool needCarryOrOverflowFlag(SDValue Flags) {\n  assert(Flags.getValueType() == MVT::i32 && \"Unexpected VT!\");\n\n  for (SDNode::use_iterator UI = Flags->use_begin(), UE = Flags->use_end();\n         UI != UE; ++UI) {\n    SDNode *User = *UI;\n\n    X86::CondCode CC;\n    switch (User->getOpcode()) {\n    default:\n      // Be conservative.\n      return true;\n    case X86ISD::SETCC:\n    case X86ISD::SETCC_CARRY:\n      CC = (X86::CondCode)User->getConstantOperandVal(0);\n      break;\n    case X86ISD::BRCOND:\n      CC = (X86::CondCode)User->getConstantOperandVal(2);\n      break;\n    case X86ISD::CMOV:\n      CC = (X86::CondCode)User->getConstantOperandVal(2);\n      break;\n    }\n\n    switch (CC) {\n    default: break;\n    case X86::COND_A: case X86::COND_AE:\n    case X86::COND_B: case X86::COND_BE:\n    case X86::COND_O: case X86::COND_NO:\n    case X86::COND_G: case X86::COND_GE:\n    case X86::COND_L: case X86::COND_LE:\n      return true;\n    }\n  }\n\n  return false;\n}\n\nstatic bool onlyZeroFlagUsed(SDValue Flags) {\n  assert(Flags.getValueType() == MVT::i32 && \"Unexpected VT!\");\n\n  for (SDNode::use_iterator UI = Flags->use_begin(), UE = Flags->use_end();\n         UI != UE; ++UI) {\n    SDNode *User = *UI;\n\n    unsigned CCOpNo;\n    switch (User->getOpcode()) {\n    default:\n      // Be conservative.\n      return false;\n    case X86ISD::SETCC:       CCOpNo = 0; break;\n    case X86ISD::SETCC_CARRY: CCOpNo = 0; break;\n    case X86ISD::BRCOND:      CCOpNo = 2; break;\n    case X86ISD::CMOV:        CCOpNo = 2; break;\n    }\n\n    X86::CondCode CC = (X86::CondCode)User->getConstantOperandVal(CCOpNo);\n    if (CC != X86::COND_E && CC != X86::COND_NE)\n      return false;\n  }\n\n  return true;\n}\n\nstatic SDValue combineCMP(SDNode *N, SelectionDAG &DAG) {\n  // Only handle test patterns.\n  if (!isNullConstant(N->getOperand(1)))\n    return SDValue();\n\n  // If we have a CMP of a truncated binop, see if we can make a smaller binop\n  // and use its flags directly.\n  // TODO: Maybe we should try promoting compares that only use the zero flag\n  // first if we can prove the upper bits with computeKnownBits?\n  SDLoc dl(N);\n  SDValue Op = N->getOperand(0);\n  EVT VT = Op.getValueType();\n\n  // If we have a constant logical shift that's only used in a comparison\n  // against zero turn it into an equivalent AND. This allows turning it into\n  // a TEST instruction later.\n  if ((Op.getOpcode() == ISD::SRL || Op.getOpcode() == ISD::SHL) &&\n      Op.hasOneUse() && isa<ConstantSDNode>(Op.getOperand(1)) &&\n      onlyZeroFlagUsed(SDValue(N, 0))) {\n    unsigned BitWidth = VT.getSizeInBits();\n    const APInt &ShAmt = Op.getConstantOperandAPInt(1);\n    if (ShAmt.ult(BitWidth)) { // Avoid undefined shifts.\n      unsigned MaskBits = BitWidth - ShAmt.getZExtValue();\n      APInt Mask = Op.getOpcode() == ISD::SRL\n                       ? APInt::getHighBitsSet(BitWidth, MaskBits)\n                       : APInt::getLowBitsSet(BitWidth, MaskBits);\n      if (Mask.isSignedIntN(32)) {\n        Op = DAG.getNode(ISD::AND, dl, VT, Op.getOperand(0),\n                         DAG.getConstant(Mask, dl, VT));\n        return DAG.getNode(X86ISD::CMP, dl, MVT::i32, Op,\n                           DAG.getConstant(0, dl, VT));\n      }\n    }\n  }\n\n  // Look for a truncate with a single use.\n  if (Op.getOpcode() != ISD::TRUNCATE || !Op.hasOneUse())\n    return SDValue();\n\n  Op = Op.getOperand(0);\n\n  // Arithmetic op can only have one use.\n  if (!Op.hasOneUse())\n    return SDValue();\n\n  unsigned NewOpc;\n  switch (Op.getOpcode()) {\n  default: return SDValue();\n  case ISD::AND:\n    // Skip and with constant. We have special handling for and with immediate\n    // during isel to generate test instructions.\n    if (isa<ConstantSDNode>(Op.getOperand(1)))\n      return SDValue();\n    NewOpc = X86ISD::AND;\n    break;\n  case ISD::OR:  NewOpc = X86ISD::OR;  break;\n  case ISD::XOR: NewOpc = X86ISD::XOR; break;\n  case ISD::ADD:\n    // If the carry or overflow flag is used, we can't truncate.\n    if (needCarryOrOverflowFlag(SDValue(N, 0)))\n      return SDValue();\n    NewOpc = X86ISD::ADD;\n    break;\n  case ISD::SUB:\n    // If the carry or overflow flag is used, we can't truncate.\n    if (needCarryOrOverflowFlag(SDValue(N, 0)))\n      return SDValue();\n    NewOpc = X86ISD::SUB;\n    break;\n  }\n\n  // We found an op we can narrow. Truncate its inputs.\n  SDValue Op0 = DAG.getNode(ISD::TRUNCATE, dl, VT, Op.getOperand(0));\n  SDValue Op1 = DAG.getNode(ISD::TRUNCATE, dl, VT, Op.getOperand(1));\n\n  // Use a X86 specific opcode to avoid DAG combine messing with it.\n  SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n  Op = DAG.getNode(NewOpc, dl, VTs, Op0, Op1);\n\n  // For AND, keep a CMP so that we can match the test pattern.\n  if (NewOpc == X86ISD::AND)\n    return DAG.getNode(X86ISD::CMP, dl, MVT::i32, Op,\n                       DAG.getConstant(0, dl, VT));\n\n  // Return the flags.\n  return Op.getValue(1);\n}\n\nstatic SDValue combineX86AddSub(SDNode *N, SelectionDAG &DAG,\n                                TargetLowering::DAGCombinerInfo &DCI) {\n  assert((X86ISD::ADD == N->getOpcode() || X86ISD::SUB == N->getOpcode()) &&\n         \"Expected X86ISD::ADD or X86ISD::SUB\");\n\n  SDLoc DL(N);\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n  MVT VT = LHS.getSimpleValueType();\n  unsigned GenericOpc = X86ISD::ADD == N->getOpcode() ? ISD::ADD : ISD::SUB;\n\n  // If we don't use the flag result, simplify back to a generic ADD/SUB.\n  if (!N->hasAnyUseOfValue(1)) {\n    SDValue Res = DAG.getNode(GenericOpc, DL, VT, LHS, RHS);\n    return DAG.getMergeValues({Res, DAG.getConstant(0, DL, MVT::i32)}, DL);\n  }\n\n  // Fold any similar generic ADD/SUB opcodes to reuse this node.\n  auto MatchGeneric = [&](SDValue N0, SDValue N1, bool Negate) {\n    SDValue Ops[] = {N0, N1};\n    SDVTList VTs = DAG.getVTList(N->getValueType(0));\n    if (SDNode *GenericAddSub = DAG.getNodeIfExists(GenericOpc, VTs, Ops)) {\n      SDValue Op(N, 0);\n      if (Negate)\n        Op = DAG.getNode(ISD::SUB, DL, VT, DAG.getConstant(0, DL, VT), Op);\n      DCI.CombineTo(GenericAddSub, Op);\n    }\n  };\n  MatchGeneric(LHS, RHS, false);\n  MatchGeneric(RHS, LHS, X86ISD::SUB == N->getOpcode());\n\n  return SDValue();\n}\n\nstatic SDValue combineSBB(SDNode *N, SelectionDAG &DAG) {\n  if (SDValue Flags = combineCarryThroughADD(N->getOperand(2), DAG)) {\n    MVT VT = N->getSimpleValueType(0);\n    SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n    return DAG.getNode(X86ISD::SBB, SDLoc(N), VTs,\n                       N->getOperand(0), N->getOperand(1),\n                       Flags);\n  }\n\n  // Fold SBB(SUB(X,Y),0,Carry) -> SBB(X,Y,Carry)\n  // iff the flag result is dead.\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  if (Op0.getOpcode() == ISD::SUB && isNullConstant(Op1) &&\n      !N->hasAnyUseOfValue(1))\n    return DAG.getNode(X86ISD::SBB, SDLoc(N), N->getVTList(), Op0.getOperand(0),\n                       Op0.getOperand(1), N->getOperand(2));\n\n  return SDValue();\n}\n\n// Optimize RES, EFLAGS = X86ISD::ADC LHS, RHS, EFLAGS\nstatic SDValue combineADC(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI) {\n  // If the LHS and RHS of the ADC node are zero, then it can't overflow and\n  // the result is either zero or one (depending on the input carry bit).\n  // Strength reduce this down to a \"set on carry\" aka SETCC_CARRY&1.\n  if (X86::isZeroNode(N->getOperand(0)) &&\n      X86::isZeroNode(N->getOperand(1)) &&\n      // We don't have a good way to replace an EFLAGS use, so only do this when\n      // dead right now.\n      SDValue(N, 1).use_empty()) {\n    SDLoc DL(N);\n    EVT VT = N->getValueType(0);\n    SDValue CarryOut = DAG.getConstant(0, DL, N->getValueType(1));\n    SDValue Res1 =\n        DAG.getNode(ISD::AND, DL, VT,\n                    DAG.getNode(X86ISD::SETCC_CARRY, DL, VT,\n                                DAG.getTargetConstant(X86::COND_B, DL, MVT::i8),\n                                N->getOperand(2)),\n                    DAG.getConstant(1, DL, VT));\n    return DCI.CombineTo(N, Res1, CarryOut);\n  }\n\n  if (SDValue Flags = combineCarryThroughADD(N->getOperand(2), DAG)) {\n    MVT VT = N->getSimpleValueType(0);\n    SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n    return DAG.getNode(X86ISD::ADC, SDLoc(N), VTs,\n                       N->getOperand(0), N->getOperand(1),\n                       Flags);\n  }\n\n  return SDValue();\n}\n\n/// If this is an add or subtract where one operand is produced by a cmp+setcc,\n/// then try to convert it to an ADC or SBB. This replaces TEST+SET+{ADD/SUB}\n/// with CMP+{ADC, SBB}.\nstatic SDValue combineAddOrSubToADCOrSBB(SDNode *N, SelectionDAG &DAG) {\n  bool IsSub = N->getOpcode() == ISD::SUB;\n  SDValue X = N->getOperand(0);\n  SDValue Y = N->getOperand(1);\n\n  // If this is an add, canonicalize a zext operand to the RHS.\n  // TODO: Incomplete? What if both sides are zexts?\n  if (!IsSub && X.getOpcode() == ISD::ZERO_EXTEND &&\n      Y.getOpcode() != ISD::ZERO_EXTEND)\n    std::swap(X, Y);\n\n  // Look through a one-use zext.\n  bool PeekedThroughZext = false;\n  if (Y.getOpcode() == ISD::ZERO_EXTEND && Y.hasOneUse()) {\n    Y = Y.getOperand(0);\n    PeekedThroughZext = true;\n  }\n\n  // If this is an add, canonicalize a setcc operand to the RHS.\n  // TODO: Incomplete? What if both sides are setcc?\n  // TODO: Should we allow peeking through a zext of the other operand?\n  if (!IsSub && !PeekedThroughZext && X.getOpcode() == X86ISD::SETCC &&\n      Y.getOpcode() != X86ISD::SETCC)\n    std::swap(X, Y);\n\n  if (Y.getOpcode() != X86ISD::SETCC || !Y.hasOneUse())\n    return SDValue();\n\n  SDLoc DL(N);\n  EVT VT = N->getValueType(0);\n  X86::CondCode CC = (X86::CondCode)Y.getConstantOperandVal(0);\n\n  // If X is -1 or 0, then we have an opportunity to avoid constants required in\n  // the general case below.\n  auto *ConstantX = dyn_cast<ConstantSDNode>(X);\n  if (ConstantX) {\n    if ((!IsSub && CC == X86::COND_AE && ConstantX->isAllOnesValue()) ||\n        (IsSub && CC == X86::COND_B && ConstantX->isNullValue())) {\n      // This is a complicated way to get -1 or 0 from the carry flag:\n      // -1 + SETAE --> -1 + (!CF) --> CF ? -1 : 0 --> SBB %eax, %eax\n      //  0 - SETB  -->  0 -  (CF) --> CF ? -1 : 0 --> SBB %eax, %eax\n      return DAG.getNode(X86ISD::SETCC_CARRY, DL, VT,\n                         DAG.getTargetConstant(X86::COND_B, DL, MVT::i8),\n                         Y.getOperand(1));\n    }\n\n    if ((!IsSub && CC == X86::COND_BE && ConstantX->isAllOnesValue()) ||\n        (IsSub && CC == X86::COND_A && ConstantX->isNullValue())) {\n      SDValue EFLAGS = Y->getOperand(1);\n      if (EFLAGS.getOpcode() == X86ISD::SUB && EFLAGS.hasOneUse() &&\n          EFLAGS.getValueType().isInteger() &&\n          !isa<ConstantSDNode>(EFLAGS.getOperand(1))) {\n        // Swap the operands of a SUB, and we have the same pattern as above.\n        // -1 + SETBE (SUB A, B) --> -1 + SETAE (SUB B, A) --> SUB + SBB\n        //  0 - SETA  (SUB A, B) -->  0 - SETB  (SUB B, A) --> SUB + SBB\n        SDValue NewSub = DAG.getNode(\n            X86ISD::SUB, SDLoc(EFLAGS), EFLAGS.getNode()->getVTList(),\n            EFLAGS.getOperand(1), EFLAGS.getOperand(0));\n        SDValue NewEFLAGS = SDValue(NewSub.getNode(), EFLAGS.getResNo());\n        return DAG.getNode(X86ISD::SETCC_CARRY, DL, VT,\n                           DAG.getTargetConstant(X86::COND_B, DL, MVT::i8),\n                           NewEFLAGS);\n      }\n    }\n  }\n\n  if (CC == X86::COND_B) {\n    // X + SETB Z --> adc X, 0\n    // X - SETB Z --> sbb X, 0\n    return DAG.getNode(IsSub ? X86ISD::SBB : X86ISD::ADC, DL,\n                       DAG.getVTList(VT, MVT::i32), X,\n                       DAG.getConstant(0, DL, VT), Y.getOperand(1));\n  }\n\n  if (CC == X86::COND_A) {\n    SDValue EFLAGS = Y.getOperand(1);\n    // Try to convert COND_A into COND_B in an attempt to facilitate\n    // materializing \"setb reg\".\n    //\n    // Do not flip \"e > c\", where \"c\" is a constant, because Cmp instruction\n    // cannot take an immediate as its first operand.\n    //\n    if (EFLAGS.getOpcode() == X86ISD::SUB && EFLAGS.getNode()->hasOneUse() &&\n        EFLAGS.getValueType().isInteger() &&\n        !isa<ConstantSDNode>(EFLAGS.getOperand(1))) {\n      SDValue NewSub = DAG.getNode(X86ISD::SUB, SDLoc(EFLAGS),\n                                   EFLAGS.getNode()->getVTList(),\n                                   EFLAGS.getOperand(1), EFLAGS.getOperand(0));\n      SDValue NewEFLAGS = NewSub.getValue(EFLAGS.getResNo());\n      return DAG.getNode(IsSub ? X86ISD::SBB : X86ISD::ADC, DL,\n                         DAG.getVTList(VT, MVT::i32), X,\n                         DAG.getConstant(0, DL, VT), NewEFLAGS);\n    }\n  }\n\n  if (CC == X86::COND_AE) {\n    // X + SETAE --> sbb X, -1\n    // X - SETAE --> adc X, -1\n    return DAG.getNode(IsSub ? X86ISD::ADC : X86ISD::SBB, DL,\n                       DAG.getVTList(VT, MVT::i32), X,\n                       DAG.getConstant(-1, DL, VT), Y.getOperand(1));\n  }\n\n  if (CC == X86::COND_BE) {\n    // X + SETBE --> sbb X, -1\n    // X - SETBE --> adc X, -1\n    SDValue EFLAGS = Y.getOperand(1);\n    // Try to convert COND_BE into COND_AE in an attempt to facilitate\n    // materializing \"setae reg\".\n    //\n    // Do not flip \"e <= c\", where \"c\" is a constant, because Cmp instruction\n    // cannot take an immediate as its first operand.\n    //\n    if (EFLAGS.getOpcode() == X86ISD::SUB && EFLAGS.getNode()->hasOneUse() &&\n        EFLAGS.getValueType().isInteger() &&\n        !isa<ConstantSDNode>(EFLAGS.getOperand(1))) {\n      SDValue NewSub = DAG.getNode(\n          X86ISD::SUB, SDLoc(EFLAGS), EFLAGS.getNode()->getVTList(),\n          EFLAGS.getOperand(1), EFLAGS.getOperand(0));\n      SDValue NewEFLAGS = NewSub.getValue(EFLAGS.getResNo());\n      return DAG.getNode(IsSub ? X86ISD::ADC : X86ISD::SBB, DL,\n                         DAG.getVTList(VT, MVT::i32), X,\n                         DAG.getConstant(-1, DL, VT), NewEFLAGS);\n    }\n  }\n\n  if (CC != X86::COND_E && CC != X86::COND_NE)\n    return SDValue();\n\n  SDValue Cmp = Y.getOperand(1);\n  if (Cmp.getOpcode() != X86ISD::CMP || !Cmp.hasOneUse() ||\n      !X86::isZeroNode(Cmp.getOperand(1)) ||\n      !Cmp.getOperand(0).getValueType().isInteger())\n    return SDValue();\n\n  SDValue Z = Cmp.getOperand(0);\n  EVT ZVT = Z.getValueType();\n\n  // If X is -1 or 0, then we have an opportunity to avoid constants required in\n  // the general case below.\n  if (ConstantX) {\n    // 'neg' sets the carry flag when Z != 0, so create 0 or -1 using 'sbb' with\n    // fake operands:\n    //  0 - (Z != 0) --> sbb %eax, %eax, (neg Z)\n    // -1 + (Z == 0) --> sbb %eax, %eax, (neg Z)\n    if ((IsSub && CC == X86::COND_NE && ConstantX->isNullValue()) ||\n        (!IsSub && CC == X86::COND_E && ConstantX->isAllOnesValue())) {\n      SDValue Zero = DAG.getConstant(0, DL, ZVT);\n      SDVTList X86SubVTs = DAG.getVTList(ZVT, MVT::i32);\n      SDValue Neg = DAG.getNode(X86ISD::SUB, DL, X86SubVTs, Zero, Z);\n      return DAG.getNode(X86ISD::SETCC_CARRY, DL, VT,\n                         DAG.getTargetConstant(X86::COND_B, DL, MVT::i8),\n                         SDValue(Neg.getNode(), 1));\n    }\n\n    // cmp with 1 sets the carry flag when Z == 0, so create 0 or -1 using 'sbb'\n    // with fake operands:\n    //  0 - (Z == 0) --> sbb %eax, %eax, (cmp Z, 1)\n    // -1 + (Z != 0) --> sbb %eax, %eax, (cmp Z, 1)\n    if ((IsSub && CC == X86::COND_E && ConstantX->isNullValue()) ||\n        (!IsSub && CC == X86::COND_NE && ConstantX->isAllOnesValue())) {\n      SDValue One = DAG.getConstant(1, DL, ZVT);\n      SDVTList X86SubVTs = DAG.getVTList(ZVT, MVT::i32);\n      SDValue Cmp1 = DAG.getNode(X86ISD::SUB, DL, X86SubVTs, Z, One);\n      return DAG.getNode(X86ISD::SETCC_CARRY, DL, VT,\n                         DAG.getTargetConstant(X86::COND_B, DL, MVT::i8),\n                         Cmp1.getValue(1));\n    }\n  }\n\n  // (cmp Z, 1) sets the carry flag if Z is 0.\n  SDValue One = DAG.getConstant(1, DL, ZVT);\n  SDVTList X86SubVTs = DAG.getVTList(ZVT, MVT::i32);\n  SDValue Cmp1 = DAG.getNode(X86ISD::SUB, DL, X86SubVTs, Z, One);\n\n  // Add the flags type for ADC/SBB nodes.\n  SDVTList VTs = DAG.getVTList(VT, MVT::i32);\n\n  // X - (Z != 0) --> sub X, (zext(setne Z, 0)) --> adc X, -1, (cmp Z, 1)\n  // X + (Z != 0) --> add X, (zext(setne Z, 0)) --> sbb X, -1, (cmp Z, 1)\n  if (CC == X86::COND_NE)\n    return DAG.getNode(IsSub ? X86ISD::ADC : X86ISD::SBB, DL, VTs, X,\n                       DAG.getConstant(-1ULL, DL, VT), Cmp1.getValue(1));\n\n  // X - (Z == 0) --> sub X, (zext(sete  Z, 0)) --> sbb X, 0, (cmp Z, 1)\n  // X + (Z == 0) --> add X, (zext(sete  Z, 0)) --> adc X, 0, (cmp Z, 1)\n  return DAG.getNode(IsSub ? X86ISD::SBB : X86ISD::ADC, DL, VTs, X,\n                     DAG.getConstant(0, DL, VT), Cmp1.getValue(1));\n}\n\nstatic SDValue matchPMADDWD(SelectionDAG &DAG, SDValue Op0, SDValue Op1,\n                            const SDLoc &DL, EVT VT,\n                            const X86Subtarget &Subtarget) {\n  // Example of pattern we try to detect:\n  // t := (v8i32 mul (sext (v8i16 x0), (sext (v8i16 x1))))\n  //(add (build_vector (extract_elt t, 0),\n  //                   (extract_elt t, 2),\n  //                   (extract_elt t, 4),\n  //                   (extract_elt t, 6)),\n  //     (build_vector (extract_elt t, 1),\n  //                   (extract_elt t, 3),\n  //                   (extract_elt t, 5),\n  //                   (extract_elt t, 7)))\n\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  if (Op0.getOpcode() != ISD::BUILD_VECTOR ||\n      Op1.getOpcode() != ISD::BUILD_VECTOR)\n    return SDValue();\n\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::i32 ||\n      VT.getVectorNumElements() < 4 ||\n      !isPowerOf2_32(VT.getVectorNumElements()))\n    return SDValue();\n\n  // Check if one of Op0,Op1 is of the form:\n  // (build_vector (extract_elt Mul, 0),\n  //               (extract_elt Mul, 2),\n  //               (extract_elt Mul, 4),\n  //                   ...\n  // the other is of the form:\n  // (build_vector (extract_elt Mul, 1),\n  //               (extract_elt Mul, 3),\n  //               (extract_elt Mul, 5),\n  //                   ...\n  // and identify Mul.\n  SDValue Mul;\n  for (unsigned i = 0, e = VT.getVectorNumElements(); i != e; i += 2) {\n    SDValue Op0L = Op0->getOperand(i), Op1L = Op1->getOperand(i),\n            Op0H = Op0->getOperand(i + 1), Op1H = Op1->getOperand(i + 1);\n    // TODO: Be more tolerant to undefs.\n    if (Op0L.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        Op1L.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        Op0H.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        Op1H.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return SDValue();\n    auto *Const0L = dyn_cast<ConstantSDNode>(Op0L->getOperand(1));\n    auto *Const1L = dyn_cast<ConstantSDNode>(Op1L->getOperand(1));\n    auto *Const0H = dyn_cast<ConstantSDNode>(Op0H->getOperand(1));\n    auto *Const1H = dyn_cast<ConstantSDNode>(Op1H->getOperand(1));\n    if (!Const0L || !Const1L || !Const0H || !Const1H)\n      return SDValue();\n    unsigned Idx0L = Const0L->getZExtValue(), Idx1L = Const1L->getZExtValue(),\n             Idx0H = Const0H->getZExtValue(), Idx1H = Const1H->getZExtValue();\n    // Commutativity of mul allows factors of a product to reorder.\n    if (Idx0L > Idx1L)\n      std::swap(Idx0L, Idx1L);\n    if (Idx0H > Idx1H)\n      std::swap(Idx0H, Idx1H);\n    // Commutativity of add allows pairs of factors to reorder.\n    if (Idx0L > Idx0H) {\n      std::swap(Idx0L, Idx0H);\n      std::swap(Idx1L, Idx1H);\n    }\n    if (Idx0L != 2 * i || Idx1L != 2 * i + 1 || Idx0H != 2 * i + 2 ||\n        Idx1H != 2 * i + 3)\n      return SDValue();\n    if (!Mul) {\n      // First time an extract_elt's source vector is visited. Must be a MUL\n      // with 2X number of vector elements than the BUILD_VECTOR.\n      // Both extracts must be from same MUL.\n      Mul = Op0L->getOperand(0);\n      if (Mul->getOpcode() != ISD::MUL ||\n          Mul.getValueType().getVectorNumElements() != 2 * e)\n        return SDValue();\n    }\n    // Check that the extract is from the same MUL previously seen.\n    if (Mul != Op0L->getOperand(0) || Mul != Op1L->getOperand(0) ||\n        Mul != Op0H->getOperand(0) || Mul != Op1H->getOperand(0))\n      return SDValue();\n  }\n\n  // Check if the Mul source can be safely shrunk.\n  ShrinkMode Mode;\n  if (!canReduceVMulWidth(Mul.getNode(), DAG, Mode) ||\n      Mode == ShrinkMode::MULU16)\n    return SDValue();\n\n  EVT TruncVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16,\n                                 VT.getVectorNumElements() * 2);\n  SDValue N0 = DAG.getNode(ISD::TRUNCATE, DL, TruncVT, Mul.getOperand(0));\n  SDValue N1 = DAG.getNode(ISD::TRUNCATE, DL, TruncVT, Mul.getOperand(1));\n\n  auto PMADDBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                         ArrayRef<SDValue> Ops) {\n    EVT InVT = Ops[0].getValueType();\n    assert(InVT == Ops[1].getValueType() && \"Operands' types mismatch\");\n    EVT ResVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32,\n                                 InVT.getVectorNumElements() / 2);\n    return DAG.getNode(X86ISD::VPMADDWD, DL, ResVT, Ops[0], Ops[1]);\n  };\n  return SplitOpsAndApply(DAG, Subtarget, DL, VT, { N0, N1 }, PMADDBuilder);\n}\n\n// Attempt to turn this pattern into PMADDWD.\n// (add (mul (sext (build_vector)), (sext (build_vector))),\n//      (mul (sext (build_vector)), (sext (build_vector)))\nstatic SDValue matchPMADDWD_2(SelectionDAG &DAG, SDValue N0, SDValue N1,\n                              const SDLoc &DL, EVT VT,\n                              const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasSSE2())\n    return SDValue();\n\n  if (N0.getOpcode() != ISD::MUL || N1.getOpcode() != ISD::MUL)\n    return SDValue();\n\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::i32 ||\n      VT.getVectorNumElements() < 4 ||\n      !isPowerOf2_32(VT.getVectorNumElements()))\n    return SDValue();\n\n  SDValue N00 = N0.getOperand(0);\n  SDValue N01 = N0.getOperand(1);\n  SDValue N10 = N1.getOperand(0);\n  SDValue N11 = N1.getOperand(1);\n\n  // All inputs need to be sign extends.\n  // TODO: Support ZERO_EXTEND from known positive?\n  if (N00.getOpcode() != ISD::SIGN_EXTEND ||\n      N01.getOpcode() != ISD::SIGN_EXTEND ||\n      N10.getOpcode() != ISD::SIGN_EXTEND ||\n      N11.getOpcode() != ISD::SIGN_EXTEND)\n    return SDValue();\n\n  // Peek through the extends.\n  N00 = N00.getOperand(0);\n  N01 = N01.getOperand(0);\n  N10 = N10.getOperand(0);\n  N11 = N11.getOperand(0);\n\n  // Must be extending from vXi16.\n  EVT InVT = N00.getValueType();\n  if (InVT.getVectorElementType() != MVT::i16 || N01.getValueType() != InVT ||\n      N10.getValueType() != InVT || N11.getValueType() != InVT)\n    return SDValue();\n\n  // All inputs should be build_vectors.\n  if (N00.getOpcode() != ISD::BUILD_VECTOR ||\n      N01.getOpcode() != ISD::BUILD_VECTOR ||\n      N10.getOpcode() != ISD::BUILD_VECTOR ||\n      N11.getOpcode() != ISD::BUILD_VECTOR)\n    return SDValue();\n\n  // For each element, we need to ensure we have an odd element from one vector\n  // multiplied by the odd element of another vector and the even element from\n  // one of the same vectors being multiplied by the even element from the\n  // other vector. So we need to make sure for each element i, this operator\n  // is being performed:\n  //  A[2 * i] * B[2 * i] + A[2 * i + 1] * B[2 * i + 1]\n  SDValue In0, In1;\n  for (unsigned i = 0; i != N00.getNumOperands(); ++i) {\n    SDValue N00Elt = N00.getOperand(i);\n    SDValue N01Elt = N01.getOperand(i);\n    SDValue N10Elt = N10.getOperand(i);\n    SDValue N11Elt = N11.getOperand(i);\n    // TODO: Be more tolerant to undefs.\n    if (N00Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N01Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N10Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT ||\n        N11Elt.getOpcode() != ISD::EXTRACT_VECTOR_ELT)\n      return SDValue();\n    auto *ConstN00Elt = dyn_cast<ConstantSDNode>(N00Elt.getOperand(1));\n    auto *ConstN01Elt = dyn_cast<ConstantSDNode>(N01Elt.getOperand(1));\n    auto *ConstN10Elt = dyn_cast<ConstantSDNode>(N10Elt.getOperand(1));\n    auto *ConstN11Elt = dyn_cast<ConstantSDNode>(N11Elt.getOperand(1));\n    if (!ConstN00Elt || !ConstN01Elt || !ConstN10Elt || !ConstN11Elt)\n      return SDValue();\n    unsigned IdxN00 = ConstN00Elt->getZExtValue();\n    unsigned IdxN01 = ConstN01Elt->getZExtValue();\n    unsigned IdxN10 = ConstN10Elt->getZExtValue();\n    unsigned IdxN11 = ConstN11Elt->getZExtValue();\n    // Add is commutative so indices can be reordered.\n    if (IdxN00 > IdxN10) {\n      std::swap(IdxN00, IdxN10);\n      std::swap(IdxN01, IdxN11);\n    }\n    // N0 indices be the even element. N1 indices must be the next odd element.\n    if (IdxN00 != 2 * i || IdxN10 != 2 * i + 1 ||\n        IdxN01 != 2 * i || IdxN11 != 2 * i + 1)\n      return SDValue();\n    SDValue N00In = N00Elt.getOperand(0);\n    SDValue N01In = N01Elt.getOperand(0);\n    SDValue N10In = N10Elt.getOperand(0);\n    SDValue N11In = N11Elt.getOperand(0);\n    // First time we find an input capture it.\n    if (!In0) {\n      In0 = N00In;\n      In1 = N01In;\n    }\n    // Mul is commutative so the input vectors can be in any order.\n    // Canonicalize to make the compares easier.\n    if (In0 != N00In)\n      std::swap(N00In, N01In);\n    if (In0 != N10In)\n      std::swap(N10In, N11In);\n    if (In0 != N00In || In1 != N01In || In0 != N10In || In1 != N11In)\n      return SDValue();\n  }\n\n  auto PMADDBuilder = [](SelectionDAG &DAG, const SDLoc &DL,\n                         ArrayRef<SDValue> Ops) {\n    // Shrink by adding truncate nodes and let DAGCombine fold with the\n    // sources.\n    EVT OpVT = Ops[0].getValueType();\n    assert(OpVT.getScalarType() == MVT::i16 &&\n           \"Unexpected scalar element type\");\n    assert(OpVT == Ops[1].getValueType() && \"Operands' types mismatch\");\n    EVT ResVT = EVT::getVectorVT(*DAG.getContext(), MVT::i32,\n                                 OpVT.getVectorNumElements() / 2);\n    return DAG.getNode(X86ISD::VPMADDWD, DL, ResVT, Ops[0], Ops[1]);\n  };\n  return SplitOpsAndApply(DAG, Subtarget, DL, VT, { In0, In1 },\n                          PMADDBuilder);\n}\n\nstatic SDValue combineAddOrSubToHADDorHSUB(SDNode *N, SelectionDAG &DAG,\n                                           const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  bool IsAdd = N->getOpcode() == ISD::ADD;\n  auto HorizOpcode = IsAdd ? X86ISD::HADD : X86ISD::HSUB;\n  assert((IsAdd || N->getOpcode() == ISD::SUB) && \"Wrong opcode\");\n\n  SmallVector<int, 8> PostShuffleMask;\n  if ((VT == MVT::v8i16 || VT == MVT::v4i32 || VT == MVT::v16i16 ||\n       VT == MVT::v8i32) &&\n      Subtarget.hasSSSE3() &&\n      isHorizontalBinOp(HorizOpcode, Op0, Op1, DAG, Subtarget, IsAdd,\n                        PostShuffleMask)) {\n    auto HOpBuilder = [HorizOpcode](SelectionDAG &DAG, const SDLoc &DL,\n                                    ArrayRef<SDValue> Ops) {\n      return DAG.getNode(HorizOpcode, DL, Ops[0].getValueType(), Ops);\n    };\n    SDValue HorizBinOp =\n        SplitOpsAndApply(DAG, Subtarget, SDLoc(N), VT, {Op0, Op1}, HOpBuilder);\n    if (!PostShuffleMask.empty())\n      HorizBinOp = DAG.getVectorShuffle(VT, SDLoc(HorizBinOp), HorizBinOp,\n                                        DAG.getUNDEF(VT), PostShuffleMask);\n    return HorizBinOp;\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineAdd(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n\n  if (SDValue MAdd = matchPMADDWD(DAG, Op0, Op1, SDLoc(N), VT, Subtarget))\n    return MAdd;\n  if (SDValue MAdd = matchPMADDWD_2(DAG, Op0, Op1, SDLoc(N), VT, Subtarget))\n    return MAdd;\n\n  // Try to synthesize horizontal adds from adds of shuffles.\n  if (SDValue V = combineAddOrSubToHADDorHSUB(N, DAG, Subtarget))\n    return V;\n\n  // If vectors of i1 are legal, turn (add (zext (vXi1 X)), Y) into\n  // (sub Y, (sext (vXi1 X))).\n  // FIXME: We have the (sub Y, (zext (vXi1 X))) -> (add (sext (vXi1 X)), Y) in\n  // generic DAG combine without a legal type check, but adding this there\n  // caused regressions.\n  if (VT.isVector()) {\n    const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n    if (Op0.getOpcode() == ISD::ZERO_EXTEND &&\n        Op0.getOperand(0).getValueType().getVectorElementType() == MVT::i1 &&\n        TLI.isTypeLegal(Op0.getOperand(0).getValueType())) {\n      SDLoc DL(N);\n      SDValue SExt = DAG.getNode(ISD::SIGN_EXTEND, DL, VT, Op0.getOperand(0));\n      return DAG.getNode(ISD::SUB, DL, VT, Op1, SExt);\n    }\n\n    if (Op1.getOpcode() == ISD::ZERO_EXTEND &&\n        Op1.getOperand(0).getValueType().getVectorElementType() == MVT::i1 &&\n        TLI.isTypeLegal(Op1.getOperand(0).getValueType())) {\n      SDLoc DL(N);\n      SDValue SExt = DAG.getNode(ISD::SIGN_EXTEND, DL, VT, Op1.getOperand(0));\n      return DAG.getNode(ISD::SUB, DL, VT, Op0, SExt);\n    }\n  }\n\n  return combineAddOrSubToADCOrSBB(N, DAG);\n}\n\nstatic SDValue combineSubToSubus(SDNode *N, SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget) {\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  EVT VT = N->getValueType(0);\n\n  if (!VT.isVector())\n    return SDValue();\n\n  // PSUBUS is supported, starting from SSE2.\n  EVT EltVT = VT.getVectorElementType();\n  if (!(Subtarget.hasSSE2() &&\n        (EltVT == MVT::i8 || EltVT == MVT::i16 || VT == MVT::v8i32 ||\n         VT == MVT::v8i64 || VT == MVT::v16i32)))\n    return SDValue();\n\n  SDValue SubusLHS, SubusRHS;\n  // Try to find umax(a,b) - b or a - umin(a,b) patterns\n  // they may be converted to subus(a,b).\n  // TODO: Need to add IR canonicalization for this code.\n  if (Op0.getOpcode() == ISD::UMAX) {\n    SubusRHS = Op1;\n    SDValue MaxLHS = Op0.getOperand(0);\n    SDValue MaxRHS = Op0.getOperand(1);\n    if (MaxLHS == Op1)\n      SubusLHS = MaxRHS;\n    else if (MaxRHS == Op1)\n      SubusLHS = MaxLHS;\n    else\n      return SDValue();\n  } else if (Op1.getOpcode() == ISD::UMIN) {\n    SubusLHS = Op0;\n    SDValue MinLHS = Op1.getOperand(0);\n    SDValue MinRHS = Op1.getOperand(1);\n    if (MinLHS == Op0)\n      SubusRHS = MinRHS;\n    else if (MinRHS == Op0)\n      SubusRHS = MinLHS;\n    else\n      return SDValue();\n  } else if (Op1.getOpcode() == ISD::TRUNCATE &&\n             Op1.getOperand(0).getOpcode() == ISD::UMIN &&\n             (EltVT == MVT::i8 || EltVT == MVT::i16)) {\n    // Special case where the UMIN has been truncated. Try to push the truncate\n    // further up. This is similar to the i32/i64 special processing.\n    SubusLHS = Op0;\n    SDValue MinLHS = Op1.getOperand(0).getOperand(0);\n    SDValue MinRHS = Op1.getOperand(0).getOperand(1);\n    EVT TruncVT = Op1.getOperand(0).getValueType();\n    if (!(Subtarget.hasSSE2() &&\n          (TruncVT == MVT::v8i32 || TruncVT == MVT::v8i64 ||\n           TruncVT == MVT::v16i32)))\n      return SDValue();\n    SDValue OpToSaturate;\n    if (MinLHS.getOpcode() == ISD::ZERO_EXTEND &&\n        MinLHS.getOperand(0) == Op0)\n      OpToSaturate = MinRHS;\n    else if (MinRHS.getOpcode() == ISD::ZERO_EXTEND &&\n             MinRHS.getOperand(0) == Op0)\n      OpToSaturate = MinLHS;\n    else\n      return SDValue();\n\n    // Saturate the non-extended input and then truncate it.\n    SDLoc DL(N);\n    SDValue SaturationConst =\n        DAG.getConstant(APInt::getLowBitsSet(TruncVT.getScalarSizeInBits(),\n                                             VT.getScalarSizeInBits()),\n                        DL, TruncVT);\n    SDValue UMin = DAG.getNode(ISD::UMIN, DL, TruncVT, OpToSaturate,\n                               SaturationConst);\n    SubusRHS = DAG.getNode(ISD::TRUNCATE, DL, VT, UMin);\n  } else\n    return SDValue();\n\n  // PSUBUS doesn't support v8i32/v8i64/v16i32, but it can be enabled with\n  // special preprocessing in some cases.\n  if (EltVT == MVT::i8 || EltVT == MVT::i16)\n    return DAG.getNode(ISD::USUBSAT, SDLoc(N), VT, SubusLHS, SubusRHS);\n\n  assert((VT == MVT::v8i32 || VT == MVT::v16i32 || VT == MVT::v8i64) &&\n         \"Unexpected VT!\");\n\n  // Special preprocessing case can be only applied\n  // if the value was zero extended from 16 bit,\n  // so we require first 16 bits to be zeros for 32 bit\n  // values, or first 48 bits for 64 bit values.\n  KnownBits Known = DAG.computeKnownBits(SubusLHS);\n  unsigned NumZeros = Known.countMinLeadingZeros();\n  if (NumZeros < (VT.getScalarSizeInBits() - 16))\n    return SDValue();\n\n  EVT ExtType = SubusLHS.getValueType();\n  EVT ShrinkedType;\n  if (VT == MVT::v8i32 || VT == MVT::v8i64)\n    ShrinkedType = MVT::v8i16;\n  else\n    ShrinkedType = NumZeros >= 24 ? MVT::v16i8 : MVT::v16i16;\n\n  // If SubusLHS is zeroextended - truncate SubusRHS to it's\n  // size SubusRHS = umin(0xFFF.., SubusRHS).\n  SDValue SaturationConst =\n      DAG.getConstant(APInt::getLowBitsSet(ExtType.getScalarSizeInBits(),\n                                           ShrinkedType.getScalarSizeInBits()),\n                      SDLoc(SubusLHS), ExtType);\n  SDValue UMin = DAG.getNode(ISD::UMIN, SDLoc(SubusLHS), ExtType, SubusRHS,\n                             SaturationConst);\n  SDValue NewSubusLHS =\n      DAG.getZExtOrTrunc(SubusLHS, SDLoc(SubusLHS), ShrinkedType);\n  SDValue NewSubusRHS = DAG.getZExtOrTrunc(UMin, SDLoc(SubusRHS), ShrinkedType);\n  SDValue Psubus = DAG.getNode(ISD::USUBSAT, SDLoc(N), ShrinkedType,\n                               NewSubusLHS, NewSubusRHS);\n\n  // Zero extend the result, it may be used somewhere as 32 bit,\n  // if not zext and following trunc will shrink.\n  return DAG.getZExtOrTrunc(Psubus, SDLoc(N), ExtType);\n}\n\nstatic SDValue combineSub(SDNode *N, SelectionDAG &DAG,\n                          TargetLowering::DAGCombinerInfo &DCI,\n                          const X86Subtarget &Subtarget) {\n  SDValue Op0 = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n\n  // X86 can't encode an immediate LHS of a sub. See if we can push the\n  // negation into a preceding instruction.\n  if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op0)) {\n    // If the RHS of the sub is a XOR with one use and a constant, invert the\n    // immediate. Then add one to the LHS of the sub so we can turn\n    // X-Y -> X+~Y+1, saving one register.\n    if (Op1->hasOneUse() && Op1.getOpcode() == ISD::XOR &&\n        isa<ConstantSDNode>(Op1.getOperand(1))) {\n      const APInt &XorC = Op1.getConstantOperandAPInt(1);\n      EVT VT = Op0.getValueType();\n      SDValue NewXor = DAG.getNode(ISD::XOR, SDLoc(Op1), VT,\n                                   Op1.getOperand(0),\n                                   DAG.getConstant(~XorC, SDLoc(Op1), VT));\n      return DAG.getNode(ISD::ADD, SDLoc(N), VT, NewXor,\n                         DAG.getConstant(C->getAPIntValue() + 1, SDLoc(N), VT));\n    }\n  }\n\n  // Try to synthesize horizontal subs from subs of shuffles.\n  if (SDValue V = combineAddOrSubToHADDorHSUB(N, DAG, Subtarget))\n    return V;\n\n  // Try to create PSUBUS if SUB's argument is max/min\n  if (SDValue V = combineSubToSubus(N, DAG, Subtarget))\n    return V;\n\n  return combineAddOrSubToADCOrSBB(N, DAG);\n}\n\nstatic SDValue combineVectorCompare(SDNode *N, SelectionDAG &DAG,\n                                    const X86Subtarget &Subtarget) {\n  MVT VT = N->getSimpleValueType(0);\n  SDLoc DL(N);\n\n  if (N->getOperand(0) == N->getOperand(1)) {\n    if (N->getOpcode() == X86ISD::PCMPEQ)\n      return DAG.getConstant(-1, DL, VT);\n    if (N->getOpcode() == X86ISD::PCMPGT)\n      return DAG.getConstant(0, DL, VT);\n  }\n\n  return SDValue();\n}\n\n/// Helper that combines an array of subvector ops as if they were the operands\n/// of a ISD::CONCAT_VECTORS node, but may have come from another source (e.g.\n/// ISD::INSERT_SUBVECTOR). The ops are assumed to be of the same type.\nstatic SDValue combineConcatVectorOps(const SDLoc &DL, MVT VT,\n                                      ArrayRef<SDValue> Ops, SelectionDAG &DAG,\n                                      TargetLowering::DAGCombinerInfo &DCI,\n                                      const X86Subtarget &Subtarget) {\n  assert(Subtarget.hasAVX() && \"AVX assumed for concat_vectors\");\n  unsigned EltSizeInBits = VT.getScalarSizeInBits();\n\n  if (llvm::all_of(Ops, [](SDValue Op) { return Op.isUndef(); }))\n    return DAG.getUNDEF(VT);\n\n  if (llvm::all_of(Ops, [](SDValue Op) {\n        return ISD::isBuildVectorAllZeros(Op.getNode());\n      }))\n    return getZeroVector(VT, Subtarget, DAG, DL);\n\n  SDValue Op0 = Ops[0];\n  bool IsSplat = llvm::all_of(Ops, [&Op0](SDValue Op) { return Op == Op0; });\n\n  // Repeated subvectors.\n  if (IsSplat &&\n      (VT.is256BitVector() || (VT.is512BitVector() && Subtarget.hasAVX512()))) {\n    // If this broadcast is inserted into both halves, use a larger broadcast.\n    if (Op0.getOpcode() == X86ISD::VBROADCAST)\n      return DAG.getNode(Op0.getOpcode(), DL, VT, Op0.getOperand(0));\n\n    // If this scalar/subvector broadcast_load is inserted into both halves, use\n    // a larger broadcast_load. Update other uses to use an extracted subvector.\n    if (Op0.getOpcode() == X86ISD::VBROADCAST_LOAD ||\n        Op0.getOpcode() == X86ISD::SUBV_BROADCAST_LOAD) {\n      auto *MemIntr = cast<MemIntrinsicSDNode>(Op0);\n      SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n      SDValue Ops[] = {MemIntr->getChain(), MemIntr->getBasePtr()};\n      SDValue BcastLd = DAG.getMemIntrinsicNode(Op0.getOpcode(), DL, Tys, Ops,\n                                                MemIntr->getMemoryVT(),\n                                                MemIntr->getMemOperand());\n      DAG.ReplaceAllUsesOfValueWith(\n          Op0, extractSubVector(BcastLd, 0, DAG, DL, Op0.getValueSizeInBits()));\n      DAG.ReplaceAllUsesOfValueWith(SDValue(MemIntr, 1), BcastLd.getValue(1));\n      return BcastLd;\n    }\n\n    // If this is a simple subvector load repeated across multiple lanes, then\n    // broadcast the load. Update other uses to use an extracted subvector.\n    if (auto *Ld = dyn_cast<LoadSDNode>(Op0)) {\n      if (Ld->isSimple() && !Ld->isNonTemporal() &&\n          Ld->getExtensionType() == ISD::NON_EXTLOAD) {\n        SDVTList Tys = DAG.getVTList(VT, MVT::Other);\n        SDValue Ops[] = {Ld->getChain(), Ld->getBasePtr()};\n        SDValue BcastLd =\n            DAG.getMemIntrinsicNode(X86ISD::SUBV_BROADCAST_LOAD, DL, Tys, Ops,\n                                    Ld->getMemoryVT(), Ld->getMemOperand());\n        DAG.ReplaceAllUsesOfValueWith(\n            Op0,\n            extractSubVector(BcastLd, 0, DAG, DL, Op0.getValueSizeInBits()));\n        DAG.ReplaceAllUsesOfValueWith(SDValue(Ld, 1), BcastLd.getValue(1));\n        return BcastLd;\n      }\n    }\n\n    // concat_vectors(movddup(x),movddup(x)) -> broadcast(x)\n    if (Op0.getOpcode() == X86ISD::MOVDDUP && VT == MVT::v4f64 &&\n        (Subtarget.hasAVX2() || MayFoldLoad(Op0.getOperand(0))))\n      return DAG.getNode(X86ISD::VBROADCAST, DL, VT,\n                         DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, MVT::f64,\n                                     Op0.getOperand(0),\n                                     DAG.getIntPtrConstant(0, DL)));\n\n    // concat_vectors(scalar_to_vector(x),scalar_to_vector(x)) -> broadcast(x)\n    if (Op0.getOpcode() == ISD::SCALAR_TO_VECTOR &&\n        (Subtarget.hasAVX2() ||\n         (EltSizeInBits >= 32 && MayFoldLoad(Op0.getOperand(0)))) &&\n        Op0.getOperand(0).getValueType() == VT.getScalarType())\n      return DAG.getNode(X86ISD::VBROADCAST, DL, VT, Op0.getOperand(0));\n\n    // concat_vectors(extract_subvector(broadcast(x)),\n    //                extract_subvector(broadcast(x))) -> broadcast(x)\n    if (Op0.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n        Op0.getOperand(0).getValueType() == VT) {\n      if (Op0.getOperand(0).getOpcode() == X86ISD::VBROADCAST ||\n          Op0.getOperand(0).getOpcode() == X86ISD::VBROADCAST_LOAD)\n        return Op0.getOperand(0);\n    }\n  }\n\n  // Repeated opcode.\n  // TODO - combineX86ShufflesRecursively should handle shuffle concatenation\n  // but it currently struggles with different vector widths.\n  if (llvm::all_of(Ops, [Op0](SDValue Op) {\n        return Op.getOpcode() == Op0.getOpcode();\n      })) {\n    unsigned NumOps = Ops.size();\n    switch (Op0.getOpcode()) {\n    case X86ISD::SHUFP: {\n      // Add SHUFPD support if/when necessary.\n      if (!IsSplat && VT.getScalarType() == MVT::f32 &&\n          llvm::all_of(Ops, [Op0](SDValue Op) {\n            return Op.getOperand(2) == Op0.getOperand(2);\n          })) {\n        SmallVector<SDValue, 2> LHS, RHS;\n        for (unsigned i = 0; i != NumOps; ++i) {\n          LHS.push_back(Ops[i].getOperand(0));\n          RHS.push_back(Ops[i].getOperand(1));\n        }\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, LHS),\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, RHS),\n                           Op0.getOperand(2));\n      }\n      break;\n    }\n    case X86ISD::PSHUFHW:\n    case X86ISD::PSHUFLW:\n    case X86ISD::PSHUFD:\n      if (!IsSplat && NumOps == 2 && VT.is256BitVector() &&\n          Subtarget.hasInt256() && Op0.getOperand(1) == Ops[1].getOperand(1)) {\n        SmallVector<SDValue, 2> Src;\n        for (unsigned i = 0; i != NumOps; ++i)\n          Src.push_back(Ops[i].getOperand(0));\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Src),\n                           Op0.getOperand(1));\n      }\n      LLVM_FALLTHROUGH;\n    case X86ISD::VPERMILPI:\n      // TODO - add support for vXf64/vXi64 shuffles.\n      if (!IsSplat && NumOps == 2 && (VT == MVT::v8f32 || VT == MVT::v8i32) &&\n          Subtarget.hasAVX() && Op0.getOperand(1) == Ops[1].getOperand(1)) {\n        SmallVector<SDValue, 2> Src;\n        for (unsigned i = 0; i != NumOps; ++i)\n          Src.push_back(DAG.getBitcast(MVT::v4f32, Ops[i].getOperand(0)));\n        SDValue Res = DAG.getNode(ISD::CONCAT_VECTORS, DL, MVT::v8f32, Src);\n        Res = DAG.getNode(X86ISD::VPERMILPI, DL, MVT::v8f32, Res,\n                          Op0.getOperand(1));\n        return DAG.getBitcast(VT, Res);\n      }\n      break;\n    case X86ISD::VPERMV3:\n      if (!IsSplat && NumOps == 2 && VT.is512BitVector()) {\n        MVT OpVT = Op0.getSimpleValueType();\n        int NumSrcElts = OpVT.getVectorNumElements();\n        SmallVector<int, 64> ConcatMask;\n        for (unsigned i = 0; i != NumOps; ++i) {\n          bool IsUnary;\n          SmallVector<int, 64> SubMask;\n          SmallVector<SDValue, 2> SubOps;\n          if (!getTargetShuffleMask(Ops[i].getNode(), OpVT, false, SubOps,\n                                    SubMask, IsUnary))\n            break;\n          for (int M : SubMask) {\n            if (0 <= M) {\n              M += M < NumSrcElts ? 0 : NumSrcElts;\n              M += i * NumSrcElts;\n            }\n            ConcatMask.push_back(M);\n          }\n        }\n        if (ConcatMask.size() == (NumOps * NumSrcElts)) {\n          SDValue Src0 = concatSubVectors(Ops[0].getOperand(0),\n                                          Ops[1].getOperand(0), DAG, DL);\n          SDValue Src1 = concatSubVectors(Ops[0].getOperand(2),\n                                          Ops[1].getOperand(2), DAG, DL);\n          MVT IntMaskSVT = MVT::getIntegerVT(VT.getScalarSizeInBits());\n          MVT IntMaskVT = MVT::getVectorVT(IntMaskSVT, NumOps * NumSrcElts);\n          SDValue Mask = getConstVector(ConcatMask, IntMaskVT, DAG, DL, true);\n          return DAG.getNode(X86ISD::VPERMV3, DL, VT, Src0, Mask, Src1);\n        }\n      }\n      break;\n    case X86ISD::VSHLI:\n    case X86ISD::VSRAI:\n    case X86ISD::VSRLI:\n      if (((VT.is256BitVector() && Subtarget.hasInt256()) ||\n           (VT.is512BitVector() && Subtarget.useAVX512Regs() &&\n            (EltSizeInBits >= 32 || Subtarget.useBWIRegs()))) &&\n          llvm::all_of(Ops, [Op0](SDValue Op) {\n            return Op0.getOperand(1) == Op.getOperand(1);\n          })) {\n        SmallVector<SDValue, 2> Src;\n        for (unsigned i = 0; i != NumOps; ++i)\n          Src.push_back(Ops[i].getOperand(0));\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Src),\n                           Op0.getOperand(1));\n      }\n      break;\n    case X86ISD::VPERMI:\n    case X86ISD::VROTLI:\n    case X86ISD::VROTRI:\n      if (VT.is512BitVector() && Subtarget.useAVX512Regs() &&\n          llvm::all_of(Ops, [Op0](SDValue Op) {\n            return Op0.getOperand(1) == Op.getOperand(1);\n          })) {\n        SmallVector<SDValue, 2> Src;\n        for (unsigned i = 0; i != NumOps; ++i)\n          Src.push_back(Ops[i].getOperand(0));\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, Src),\n                           Op0.getOperand(1));\n      }\n      break;\n    case ISD::AND:\n    case ISD::OR:\n    case ISD::XOR:\n    case X86ISD::ANDNP:\n      // TODO: Add 256-bit support.\n      if (!IsSplat && VT.is512BitVector()) {\n        SmallVector<SDValue, 2> LHS, RHS;\n        for (unsigned i = 0; i != NumOps; ++i) {\n          LHS.push_back(Ops[i].getOperand(0));\n          RHS.push_back(Ops[i].getOperand(1));\n        }\n        MVT SrcVT = Op0.getOperand(0).getSimpleValueType();\n        SrcVT = MVT::getVectorVT(SrcVT.getScalarType(),\n                                 NumOps * SrcVT.getVectorNumElements());\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, SrcVT, LHS),\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, SrcVT, RHS));\n      }\n      break;\n    case X86ISD::HADD:\n    case X86ISD::HSUB:\n    case X86ISD::FHADD:\n    case X86ISD::FHSUB:\n    case X86ISD::PACKSS:\n    case X86ISD::PACKUS:\n      if (!IsSplat && VT.is256BitVector() &&\n          (VT.isFloatingPoint() || Subtarget.hasInt256())) {\n        SmallVector<SDValue, 2> LHS, RHS;\n        for (unsigned i = 0; i != NumOps; ++i) {\n          LHS.push_back(Ops[i].getOperand(0));\n          RHS.push_back(Ops[i].getOperand(1));\n        }\n        MVT SrcVT = Op0.getOperand(0).getSimpleValueType();\n        SrcVT = MVT::getVectorVT(SrcVT.getScalarType(),\n                                 NumOps * SrcVT.getVectorNumElements());\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, SrcVT, LHS),\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, SrcVT, RHS));\n      }\n      break;\n    case X86ISD::PALIGNR:\n      if (!IsSplat &&\n          ((VT.is256BitVector() && Subtarget.hasInt256()) ||\n           (VT.is512BitVector() && Subtarget.useBWIRegs())) &&\n          llvm::all_of(Ops, [Op0](SDValue Op) {\n            return Op0.getOperand(2) == Op.getOperand(2);\n          })) {\n        SmallVector<SDValue, 2> LHS, RHS;\n        for (unsigned i = 0; i != NumOps; ++i) {\n          LHS.push_back(Ops[i].getOperand(0));\n          RHS.push_back(Ops[i].getOperand(1));\n        }\n        return DAG.getNode(Op0.getOpcode(), DL, VT,\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, LHS),\n                           DAG.getNode(ISD::CONCAT_VECTORS, DL, VT, RHS),\n                           Op0.getOperand(2));\n      }\n      break;\n    }\n  }\n\n  // Fold subvector loads into one.\n  // If needed, look through bitcasts to get to the load.\n  if (auto *FirstLd = dyn_cast<LoadSDNode>(peekThroughBitcasts(Op0))) {\n    bool Fast;\n    const X86TargetLowering *TLI = Subtarget.getTargetLowering();\n    if (TLI->allowsMemoryAccess(*DAG.getContext(), DAG.getDataLayout(), VT,\n                                *FirstLd->getMemOperand(), &Fast) &&\n        Fast) {\n      if (SDValue Ld =\n              EltsFromConsecutiveLoads(VT, Ops, DL, DAG, Subtarget, false))\n        return Ld;\n    }\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineConcatVectors(SDNode *N, SelectionDAG &DAG,\n                                    TargetLowering::DAGCombinerInfo &DCI,\n                                    const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  EVT SrcVT = N->getOperand(0).getValueType();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // Don't do anything for i1 vectors.\n  if (VT.getVectorElementType() == MVT::i1)\n    return SDValue();\n\n  if (Subtarget.hasAVX() && TLI.isTypeLegal(VT) && TLI.isTypeLegal(SrcVT)) {\n    SmallVector<SDValue, 4> Ops(N->op_begin(), N->op_end());\n    if (SDValue R = combineConcatVectorOps(SDLoc(N), VT.getSimpleVT(), Ops, DAG,\n                                           DCI, Subtarget))\n      return R;\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineInsertSubvector(SDNode *N, SelectionDAG &DAG,\n                                      TargetLowering::DAGCombinerInfo &DCI,\n                                      const X86Subtarget &Subtarget) {\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  MVT OpVT = N->getSimpleValueType(0);\n\n  bool IsI1Vector = OpVT.getVectorElementType() == MVT::i1;\n\n  SDLoc dl(N);\n  SDValue Vec = N->getOperand(0);\n  SDValue SubVec = N->getOperand(1);\n\n  uint64_t IdxVal = N->getConstantOperandVal(2);\n  MVT SubVecVT = SubVec.getSimpleValueType();\n\n  if (Vec.isUndef() && SubVec.isUndef())\n    return DAG.getUNDEF(OpVT);\n\n  // Inserting undefs/zeros into zeros/undefs is a zero vector.\n  if ((Vec.isUndef() || ISD::isBuildVectorAllZeros(Vec.getNode())) &&\n      (SubVec.isUndef() || ISD::isBuildVectorAllZeros(SubVec.getNode())))\n    return getZeroVector(OpVT, Subtarget, DAG, dl);\n\n  if (ISD::isBuildVectorAllZeros(Vec.getNode())) {\n    // If we're inserting into a zero vector and then into a larger zero vector,\n    // just insert into the larger zero vector directly.\n    if (SubVec.getOpcode() == ISD::INSERT_SUBVECTOR &&\n        ISD::isBuildVectorAllZeros(SubVec.getOperand(0).getNode())) {\n      uint64_t Idx2Val = SubVec.getConstantOperandVal(2);\n      return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, OpVT,\n                         getZeroVector(OpVT, Subtarget, DAG, dl),\n                         SubVec.getOperand(1),\n                         DAG.getIntPtrConstant(IdxVal + Idx2Val, dl));\n    }\n\n    // If we're inserting into a zero vector and our input was extracted from an\n    // insert into a zero vector of the same type and the extraction was at\n    // least as large as the original insertion. Just insert the original\n    // subvector into a zero vector.\n    if (SubVec.getOpcode() == ISD::EXTRACT_SUBVECTOR && IdxVal == 0 &&\n        isNullConstant(SubVec.getOperand(1)) &&\n        SubVec.getOperand(0).getOpcode() == ISD::INSERT_SUBVECTOR) {\n      SDValue Ins = SubVec.getOperand(0);\n      if (isNullConstant(Ins.getOperand(2)) &&\n          ISD::isBuildVectorAllZeros(Ins.getOperand(0).getNode()) &&\n          Ins.getOperand(1).getValueSizeInBits().getFixedSize() <=\n              SubVecVT.getFixedSizeInBits())\n        return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, OpVT,\n                           getZeroVector(OpVT, Subtarget, DAG, dl),\n                           Ins.getOperand(1), N->getOperand(2));\n    }\n  }\n\n  // Stop here if this is an i1 vector.\n  if (IsI1Vector)\n    return SDValue();\n\n  // If this is an insert of an extract, combine to a shuffle. Don't do this\n  // if the insert or extract can be represented with a subregister operation.\n  if (SubVec.getOpcode() == ISD::EXTRACT_SUBVECTOR &&\n      SubVec.getOperand(0).getSimpleValueType() == OpVT &&\n      (IdxVal != 0 ||\n       !(Vec.isUndef() || ISD::isBuildVectorAllZeros(Vec.getNode())))) {\n    int ExtIdxVal = SubVec.getConstantOperandVal(1);\n    if (ExtIdxVal != 0) {\n      int VecNumElts = OpVT.getVectorNumElements();\n      int SubVecNumElts = SubVecVT.getVectorNumElements();\n      SmallVector<int, 64> Mask(VecNumElts);\n      // First create an identity shuffle mask.\n      for (int i = 0; i != VecNumElts; ++i)\n        Mask[i] = i;\n      // Now insert the extracted portion.\n      for (int i = 0; i != SubVecNumElts; ++i)\n        Mask[i + IdxVal] = i + ExtIdxVal + VecNumElts;\n\n      return DAG.getVectorShuffle(OpVT, dl, Vec, SubVec.getOperand(0), Mask);\n    }\n  }\n\n  // Match concat_vector style patterns.\n  SmallVector<SDValue, 2> SubVectorOps;\n  if (collectConcatOps(N, SubVectorOps)) {\n    if (SDValue Fold =\n            combineConcatVectorOps(dl, OpVT, SubVectorOps, DAG, DCI, Subtarget))\n      return Fold;\n\n    // If we're inserting all zeros into the upper half, change this to\n    // a concat with zero. We will match this to a move\n    // with implicit upper bit zeroing during isel.\n    // We do this here because we don't want combineConcatVectorOps to\n    // create INSERT_SUBVECTOR from CONCAT_VECTORS.\n    if (SubVectorOps.size() == 2 &&\n        ISD::isBuildVectorAllZeros(SubVectorOps[1].getNode()))\n      return DAG.getNode(ISD::INSERT_SUBVECTOR, dl, OpVT,\n                         getZeroVector(OpVT, Subtarget, DAG, dl),\n                         SubVectorOps[0], DAG.getIntPtrConstant(0, dl));\n  }\n\n  // If this is a broadcast insert into an upper undef, use a larger broadcast.\n  if (Vec.isUndef() && IdxVal != 0 && SubVec.getOpcode() == X86ISD::VBROADCAST)\n    return DAG.getNode(X86ISD::VBROADCAST, dl, OpVT, SubVec.getOperand(0));\n\n  // If this is a broadcast load inserted into an upper undef, use a larger\n  // broadcast load.\n  if (Vec.isUndef() && IdxVal != 0 && SubVec.hasOneUse() &&\n      SubVec.getOpcode() == X86ISD::VBROADCAST_LOAD) {\n    auto *MemIntr = cast<MemIntrinsicSDNode>(SubVec);\n    SDVTList Tys = DAG.getVTList(OpVT, MVT::Other);\n    SDValue Ops[] = { MemIntr->getChain(), MemIntr->getBasePtr() };\n    SDValue BcastLd =\n        DAG.getMemIntrinsicNode(X86ISD::VBROADCAST_LOAD, dl, Tys, Ops,\n                                MemIntr->getMemoryVT(),\n                                MemIntr->getMemOperand());\n    DAG.ReplaceAllUsesOfValueWith(SDValue(MemIntr, 1), BcastLd.getValue(1));\n    return BcastLd;\n  }\n\n  return SDValue();\n}\n\n/// If we are extracting a subvector of a vector select and the select condition\n/// is composed of concatenated vectors, try to narrow the select width. This\n/// is a common pattern for AVX1 integer code because 256-bit selects may be\n/// legal, but there is almost no integer math/logic available for 256-bit.\n/// This function should only be called with legal types (otherwise, the calls\n/// to get simple value types will assert).\nstatic SDValue narrowExtractedVectorSelect(SDNode *Ext, SelectionDAG &DAG) {\n  SDValue Sel = peekThroughBitcasts(Ext->getOperand(0));\n  SmallVector<SDValue, 4> CatOps;\n  if (Sel.getOpcode() != ISD::VSELECT ||\n      !collectConcatOps(Sel.getOperand(0).getNode(), CatOps))\n    return SDValue();\n\n  // Note: We assume simple value types because this should only be called with\n  //       legal operations/types.\n  // TODO: This can be extended to handle extraction to 256-bits.\n  MVT VT = Ext->getSimpleValueType(0);\n  if (!VT.is128BitVector())\n    return SDValue();\n\n  MVT SelCondVT = Sel.getOperand(0).getSimpleValueType();\n  if (!SelCondVT.is256BitVector() && !SelCondVT.is512BitVector())\n    return SDValue();\n\n  MVT WideVT = Ext->getOperand(0).getSimpleValueType();\n  MVT SelVT = Sel.getSimpleValueType();\n  assert((SelVT.is256BitVector() || SelVT.is512BitVector()) &&\n         \"Unexpected vector type with legal operations\");\n\n  unsigned SelElts = SelVT.getVectorNumElements();\n  unsigned CastedElts = WideVT.getVectorNumElements();\n  unsigned ExtIdx = Ext->getConstantOperandVal(1);\n  if (SelElts % CastedElts == 0) {\n    // The select has the same or more (narrower) elements than the extract\n    // operand. The extraction index gets scaled by that factor.\n    ExtIdx *= (SelElts / CastedElts);\n  } else if (CastedElts % SelElts == 0) {\n    // The select has less (wider) elements than the extract operand. Make sure\n    // that the extraction index can be divided evenly.\n    unsigned IndexDivisor = CastedElts / SelElts;\n    if (ExtIdx % IndexDivisor != 0)\n      return SDValue();\n    ExtIdx /= IndexDivisor;\n  } else {\n    llvm_unreachable(\"Element count of simple vector types are not divisible?\");\n  }\n\n  unsigned NarrowingFactor = WideVT.getSizeInBits() / VT.getSizeInBits();\n  unsigned NarrowElts = SelElts / NarrowingFactor;\n  MVT NarrowSelVT = MVT::getVectorVT(SelVT.getVectorElementType(), NarrowElts);\n  SDLoc DL(Ext);\n  SDValue ExtCond = extract128BitVector(Sel.getOperand(0), ExtIdx, DAG, DL);\n  SDValue ExtT = extract128BitVector(Sel.getOperand(1), ExtIdx, DAG, DL);\n  SDValue ExtF = extract128BitVector(Sel.getOperand(2), ExtIdx, DAG, DL);\n  SDValue NarrowSel = DAG.getSelect(DL, NarrowSelVT, ExtCond, ExtT, ExtF);\n  return DAG.getBitcast(VT, NarrowSel);\n}\n\nstatic SDValue combineExtractSubvector(SDNode *N, SelectionDAG &DAG,\n                                       TargetLowering::DAGCombinerInfo &DCI,\n                                       const X86Subtarget &Subtarget) {\n  // For AVX1 only, if we are extracting from a 256-bit and+not (which will\n  // eventually get combined/lowered into ANDNP) with a concatenated operand,\n  // split the 'and' into 128-bit ops to avoid the concatenate and extract.\n  // We let generic combining take over from there to simplify the\n  // insert/extract and 'not'.\n  // This pattern emerges during AVX1 legalization. We handle it before lowering\n  // to avoid complications like splitting constant vector loads.\n\n  // Capture the original wide type in the likely case that we need to bitcast\n  // back to this type.\n  if (!N->getValueType(0).isSimple())\n    return SDValue();\n\n  MVT VT = N->getSimpleValueType(0);\n  SDValue InVec = N->getOperand(0);\n  unsigned IdxVal = N->getConstantOperandVal(1);\n  SDValue InVecBC = peekThroughBitcasts(InVec);\n  EVT InVecVT = InVec.getValueType();\n  unsigned SizeInBits = VT.getSizeInBits();\n  unsigned InSizeInBits = InVecVT.getSizeInBits();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  if (Subtarget.hasAVX() && !Subtarget.hasAVX2() &&\n      TLI.isTypeLegal(InVecVT) &&\n      InSizeInBits == 256 && InVecBC.getOpcode() == ISD::AND) {\n    auto isConcatenatedNot = [](SDValue V) {\n      V = peekThroughBitcasts(V);\n      if (!isBitwiseNot(V))\n        return false;\n      SDValue NotOp = V->getOperand(0);\n      return peekThroughBitcasts(NotOp).getOpcode() == ISD::CONCAT_VECTORS;\n    };\n    if (isConcatenatedNot(InVecBC.getOperand(0)) ||\n        isConcatenatedNot(InVecBC.getOperand(1))) {\n      // extract (and v4i64 X, (not (concat Y1, Y2))), n -> andnp v2i64 X(n), Y1\n      SDValue Concat = splitVectorIntBinary(InVecBC, DAG);\n      return DAG.getNode(ISD::EXTRACT_SUBVECTOR, SDLoc(N), VT,\n                         DAG.getBitcast(InVecVT, Concat), N->getOperand(1));\n    }\n  }\n\n  if (DCI.isBeforeLegalizeOps())\n    return SDValue();\n\n  if (SDValue V = narrowExtractedVectorSelect(N, DAG))\n    return V;\n\n  if (ISD::isBuildVectorAllZeros(InVec.getNode()))\n    return getZeroVector(VT, Subtarget, DAG, SDLoc(N));\n\n  if (ISD::isBuildVectorAllOnes(InVec.getNode())) {\n    if (VT.getScalarType() == MVT::i1)\n      return DAG.getConstant(1, SDLoc(N), VT);\n    return getOnesVector(VT, DAG, SDLoc(N));\n  }\n\n  if (InVec.getOpcode() == ISD::BUILD_VECTOR)\n    return DAG.getBuildVector(\n        VT, SDLoc(N),\n        InVec.getNode()->ops().slice(IdxVal, VT.getVectorNumElements()));\n\n  // If we are extracting from an insert into a zero vector, replace with a\n  // smaller insert into zero if we don't access less than the original\n  // subvector. Don't do this for i1 vectors.\n  if (VT.getVectorElementType() != MVT::i1 &&\n      InVec.getOpcode() == ISD::INSERT_SUBVECTOR && IdxVal == 0 &&\n      InVec.hasOneUse() && isNullConstant(InVec.getOperand(2)) &&\n      ISD::isBuildVectorAllZeros(InVec.getOperand(0).getNode()) &&\n      InVec.getOperand(1).getValueSizeInBits() <= SizeInBits) {\n    SDLoc DL(N);\n    return DAG.getNode(ISD::INSERT_SUBVECTOR, DL, VT,\n                       getZeroVector(VT, Subtarget, DAG, DL),\n                       InVec.getOperand(1), InVec.getOperand(2));\n  }\n\n  // If we're extracting an upper subvector from a broadcast we should just\n  // extract the lowest subvector instead which should allow\n  // SimplifyDemandedVectorElts do more simplifications.\n  if (IdxVal != 0 && (InVec.getOpcode() == X86ISD::VBROADCAST ||\n                      InVec.getOpcode() == X86ISD::VBROADCAST_LOAD))\n    return extractSubVector(InVec, 0, DAG, SDLoc(N), SizeInBits);\n\n  // If we're extracting a broadcasted subvector, just use the lowest subvector.\n  if (IdxVal != 0 && InVec.getOpcode() == X86ISD::SUBV_BROADCAST_LOAD &&\n      cast<MemIntrinsicSDNode>(InVec)->getMemoryVT() == VT)\n    return extractSubVector(InVec, 0, DAG, SDLoc(N), SizeInBits);\n\n  // Attempt to extract from the source of a shuffle vector.\n  if ((InSizeInBits % SizeInBits) == 0 &&\n      (IdxVal % VT.getVectorNumElements()) == 0) {\n    SmallVector<int, 32> ShuffleMask;\n    SmallVector<int, 32> ScaledMask;\n    SmallVector<SDValue, 2> ShuffleInputs;\n    unsigned NumSubVecs = InSizeInBits / SizeInBits;\n    // Decode the shuffle mask and scale it so its shuffling subvectors.\n    if (getTargetShuffleInputs(InVecBC, ShuffleInputs, ShuffleMask, DAG) &&\n        scaleShuffleElements(ShuffleMask, NumSubVecs, ScaledMask)) {\n      unsigned SubVecIdx = IdxVal / VT.getVectorNumElements();\n      if (ScaledMask[SubVecIdx] == SM_SentinelUndef)\n        return DAG.getUNDEF(VT);\n      if (ScaledMask[SubVecIdx] == SM_SentinelZero)\n        return getZeroVector(VT, Subtarget, DAG, SDLoc(N));\n      SDValue Src = ShuffleInputs[ScaledMask[SubVecIdx] / NumSubVecs];\n      if (Src.getValueSizeInBits() == InSizeInBits) {\n        unsigned SrcSubVecIdx = ScaledMask[SubVecIdx] % NumSubVecs;\n        unsigned SrcEltIdx = SrcSubVecIdx * VT.getVectorNumElements();\n        return extractSubVector(DAG.getBitcast(InVecVT, Src), SrcEltIdx, DAG,\n                                SDLoc(N), SizeInBits);\n      }\n    }\n  }\n\n  // If we're extracting the lowest subvector and we're the only user,\n  // we may be able to perform this with a smaller vector width.\n  unsigned InOpcode = InVec.getOpcode();\n  if (IdxVal == 0 && InVec.hasOneUse()) {\n    if (VT == MVT::v2f64 && InVecVT == MVT::v4f64) {\n      // v2f64 CVTDQ2PD(v4i32).\n      if (InOpcode == ISD::SINT_TO_FP &&\n          InVec.getOperand(0).getValueType() == MVT::v4i32) {\n        return DAG.getNode(X86ISD::CVTSI2P, SDLoc(N), VT, InVec.getOperand(0));\n      }\n      // v2f64 CVTUDQ2PD(v4i32).\n      if (InOpcode == ISD::UINT_TO_FP && Subtarget.hasVLX() &&\n          InVec.getOperand(0).getValueType() == MVT::v4i32) {\n        return DAG.getNode(X86ISD::CVTUI2P, SDLoc(N), VT, InVec.getOperand(0));\n      }\n      // v2f64 CVTPS2PD(v4f32).\n      if (InOpcode == ISD::FP_EXTEND &&\n          InVec.getOperand(0).getValueType() == MVT::v4f32) {\n        return DAG.getNode(X86ISD::VFPEXT, SDLoc(N), VT, InVec.getOperand(0));\n      }\n    }\n    if ((InOpcode == ISD::ANY_EXTEND ||\n         InOpcode == ISD::ANY_EXTEND_VECTOR_INREG ||\n         InOpcode == ISD::ZERO_EXTEND ||\n         InOpcode == ISD::ZERO_EXTEND_VECTOR_INREG ||\n         InOpcode == ISD::SIGN_EXTEND ||\n         InOpcode == ISD::SIGN_EXTEND_VECTOR_INREG) &&\n        (SizeInBits == 128 || SizeInBits == 256) &&\n        InVec.getOperand(0).getValueSizeInBits() >= SizeInBits) {\n      SDLoc DL(N);\n      SDValue Ext = InVec.getOperand(0);\n      if (Ext.getValueSizeInBits() > SizeInBits)\n        Ext = extractSubVector(Ext, 0, DAG, DL, SizeInBits);\n      unsigned ExtOp = getOpcode_EXTEND_VECTOR_INREG(InOpcode);\n      return DAG.getNode(ExtOp, DL, VT, Ext);\n    }\n    if (InOpcode == ISD::VSELECT &&\n        InVec.getOperand(0).getValueType().is256BitVector() &&\n        InVec.getOperand(1).getValueType().is256BitVector() &&\n        InVec.getOperand(2).getValueType().is256BitVector()) {\n      SDLoc DL(N);\n      SDValue Ext0 = extractSubVector(InVec.getOperand(0), 0, DAG, DL, 128);\n      SDValue Ext1 = extractSubVector(InVec.getOperand(1), 0, DAG, DL, 128);\n      SDValue Ext2 = extractSubVector(InVec.getOperand(2), 0, DAG, DL, 128);\n      return DAG.getNode(InOpcode, DL, VT, Ext0, Ext1, Ext2);\n    }\n    if (InOpcode == ISD::TRUNCATE && Subtarget.hasVLX() &&\n        (VT.is128BitVector() || VT.is256BitVector())) {\n      SDLoc DL(N);\n      SDValue InVecSrc = InVec.getOperand(0);\n      unsigned Scale = InVecSrc.getValueSizeInBits() / InSizeInBits;\n      SDValue Ext = extractSubVector(InVecSrc, 0, DAG, DL, Scale * SizeInBits);\n      return DAG.getNode(InOpcode, DL, VT, Ext);\n    }\n  }\n\n  // Always split vXi64 logical shifts where we're extracting the upper 32-bits\n  // as this is very likely to fold into a shuffle/truncation.\n  if ((InOpcode == X86ISD::VSHLI || InOpcode == X86ISD::VSRLI) &&\n      InVecVT.getScalarSizeInBits() == 64 &&\n      InVec.getConstantOperandAPInt(1) == 32) {\n    SDLoc DL(N);\n    SDValue Ext =\n        extractSubVector(InVec.getOperand(0), IdxVal, DAG, DL, SizeInBits);\n    return DAG.getNode(InOpcode, DL, VT, Ext, InVec.getOperand(1));\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineScalarToVector(SDNode *N, SelectionDAG &DAG) {\n  EVT VT = N->getValueType(0);\n  SDValue Src = N->getOperand(0);\n  SDLoc DL(N);\n\n  // If this is a scalar to vector to v1i1 from an AND with 1, bypass the and.\n  // This occurs frequently in our masked scalar intrinsic code and our\n  // floating point select lowering with AVX512.\n  // TODO: SimplifyDemandedBits instead?\n  if (VT == MVT::v1i1 && Src.getOpcode() == ISD::AND && Src.hasOneUse())\n    if (auto *C = dyn_cast<ConstantSDNode>(Src.getOperand(1)))\n      if (C->getAPIntValue().isOneValue())\n        return DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v1i1,\n                           Src.getOperand(0));\n\n  // Combine scalar_to_vector of an extract_vector_elt into an extract_subvec.\n  if (VT == MVT::v1i1 && Src.getOpcode() == ISD::EXTRACT_VECTOR_ELT &&\n      Src.hasOneUse() && Src.getOperand(0).getValueType().isVector() &&\n      Src.getOperand(0).getValueType().getVectorElementType() == MVT::i1)\n    if (auto *C = dyn_cast<ConstantSDNode>(Src.getOperand(1)))\n      if (C->isNullValue())\n        return DAG.getNode(ISD::EXTRACT_SUBVECTOR, DL, VT, Src.getOperand(0),\n                           Src.getOperand(1));\n\n  // Reduce v2i64 to v4i32 if we don't need the upper bits.\n  // TODO: Move to DAGCombine/SimplifyDemandedBits?\n  if (VT == MVT::v2i64 || VT == MVT::v2f64) {\n    auto IsAnyExt64 = [](SDValue Op) {\n      if (Op.getValueType() != MVT::i64 || !Op.hasOneUse())\n        return SDValue();\n      if (Op.getOpcode() == ISD::ANY_EXTEND &&\n          Op.getOperand(0).getScalarValueSizeInBits() <= 32)\n        return Op.getOperand(0);\n      if (auto *Ld = dyn_cast<LoadSDNode>(Op))\n        if (Ld->getExtensionType() == ISD::EXTLOAD &&\n            Ld->getMemoryVT().getScalarSizeInBits() <= 32)\n          return Op;\n      return SDValue();\n    };\n    if (SDValue ExtSrc = IsAnyExt64(peekThroughOneUseBitcasts(Src)))\n      return DAG.getBitcast(\n          VT, DAG.getNode(ISD::SCALAR_TO_VECTOR, DL, MVT::v4i32,\n                          DAG.getAnyExtOrTrunc(ExtSrc, DL, MVT::i32)));\n  }\n\n  // Combine (v2i64 (scalar_to_vector (i64 (bitconvert (mmx))))) to MOVQ2DQ.\n  if (VT == MVT::v2i64 && Src.getOpcode() == ISD::BITCAST &&\n      Src.getOperand(0).getValueType() == MVT::x86mmx)\n    return DAG.getNode(X86ISD::MOVQ2DQ, DL, VT, Src.getOperand(0));\n\n  return SDValue();\n}\n\n// Simplify PMULDQ and PMULUDQ operations.\nstatic SDValue combinePMULDQ(SDNode *N, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI,\n                             const X86Subtarget &Subtarget) {\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n\n  // Canonicalize constant to RHS.\n  if (DAG.isConstantIntBuildVectorOrConstantInt(LHS) &&\n      !DAG.isConstantIntBuildVectorOrConstantInt(RHS))\n    return DAG.getNode(N->getOpcode(), SDLoc(N), N->getValueType(0), RHS, LHS);\n\n  // Multiply by zero.\n  // Don't return RHS as it may contain UNDEFs.\n  if (ISD::isBuildVectorAllZeros(RHS.getNode()))\n    return DAG.getConstant(0, SDLoc(N), N->getValueType(0));\n\n  // PMULDQ/PMULUDQ only uses lower 32 bits from each vector element.\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0), APInt::getAllOnesValue(64), DCI))\n    return SDValue(N, 0);\n\n  // If the input is an extend_invec and the SimplifyDemandedBits call didn't\n  // convert it to any_extend_invec, due to the LegalOperations check, do the\n  // conversion directly to a vector shuffle manually. This exposes combine\n  // opportunities missed by combineEXTEND_VECTOR_INREG not calling\n  // combineX86ShufflesRecursively on SSE4.1 targets.\n  // FIXME: This is basically a hack around several other issues related to\n  // ANY_EXTEND_VECTOR_INREG.\n  if (N->getValueType(0) == MVT::v2i64 && LHS.hasOneUse() &&\n      (LHS.getOpcode() == ISD::ZERO_EXTEND_VECTOR_INREG ||\n       LHS.getOpcode() == ISD::SIGN_EXTEND_VECTOR_INREG) &&\n      LHS.getOperand(0).getValueType() == MVT::v4i32) {\n    SDLoc dl(N);\n    LHS = DAG.getVectorShuffle(MVT::v4i32, dl, LHS.getOperand(0),\n                               LHS.getOperand(0), { 0, -1, 1, -1 });\n    LHS = DAG.getBitcast(MVT::v2i64, LHS);\n    return DAG.getNode(N->getOpcode(), dl, MVT::v2i64, LHS, RHS);\n  }\n  if (N->getValueType(0) == MVT::v2i64 && RHS.hasOneUse() &&\n      (RHS.getOpcode() == ISD::ZERO_EXTEND_VECTOR_INREG ||\n       RHS.getOpcode() == ISD::SIGN_EXTEND_VECTOR_INREG) &&\n      RHS.getOperand(0).getValueType() == MVT::v4i32) {\n    SDLoc dl(N);\n    RHS = DAG.getVectorShuffle(MVT::v4i32, dl, RHS.getOperand(0),\n                               RHS.getOperand(0), { 0, -1, 1, -1 });\n    RHS = DAG.getBitcast(MVT::v2i64, RHS);\n    return DAG.getNode(N->getOpcode(), dl, MVT::v2i64, LHS, RHS);\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineEXTEND_VECTOR_INREG(SDNode *N, SelectionDAG &DAG,\n                                          TargetLowering::DAGCombinerInfo &DCI,\n                                          const X86Subtarget &Subtarget) {\n  EVT VT = N->getValueType(0);\n  SDValue In = N->getOperand(0);\n  unsigned Opcode = N->getOpcode();\n  unsigned InOpcode = In.getOpcode();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n\n  // Try to merge vector loads and extend_inreg to an extload.\n  if (!DCI.isBeforeLegalizeOps() && ISD::isNormalLoad(In.getNode()) &&\n      In.hasOneUse()) {\n    auto *Ld = cast<LoadSDNode>(In);\n    if (Ld->isSimple()) {\n      MVT SVT = In.getSimpleValueType().getVectorElementType();\n      ISD::LoadExtType Ext = Opcode == ISD::SIGN_EXTEND_VECTOR_INREG\n                                 ? ISD::SEXTLOAD\n                                 : ISD::ZEXTLOAD;\n      EVT MemVT =\n          EVT::getVectorVT(*DAG.getContext(), SVT, VT.getVectorNumElements());\n      if (TLI.isLoadExtLegal(Ext, VT, MemVT)) {\n        SDValue Load =\n            DAG.getExtLoad(Ext, SDLoc(N), VT, Ld->getChain(), Ld->getBasePtr(),\n                           Ld->getPointerInfo(), MemVT, Ld->getOriginalAlign(),\n                           Ld->getMemOperand()->getFlags());\n        DAG.ReplaceAllUsesOfValueWith(SDValue(Ld, 1), Load.getValue(1));\n        return Load;\n      }\n    }\n  }\n\n  // Fold EXTEND_VECTOR_INREG(EXTEND_VECTOR_INREG(X)) -> EXTEND_VECTOR_INREG(X).\n  if (Opcode == InOpcode)\n    return DAG.getNode(Opcode, SDLoc(N), VT, In.getOperand(0));\n\n  // Fold EXTEND_VECTOR_INREG(EXTRACT_SUBVECTOR(EXTEND(X),0))\n  // -> EXTEND_VECTOR_INREG(X).\n  // TODO: Handle non-zero subvector indices.\n  if (InOpcode == ISD::EXTRACT_SUBVECTOR && In.getConstantOperandVal(1) == 0 &&\n      In.getOperand(0).getOpcode() == getOpcode_EXTEND(Opcode) &&\n      In.getOperand(0).getOperand(0).getValueSizeInBits() ==\n          In.getValueSizeInBits())\n    return DAG.getNode(Opcode, SDLoc(N), VT, In.getOperand(0).getOperand(0));\n\n  // Attempt to combine as a shuffle.\n  // TODO: General ZERO_EXTEND_VECTOR_INREG support.\n  if (Opcode == ISD::ANY_EXTEND_VECTOR_INREG ||\n      (Opcode == ISD::ZERO_EXTEND_VECTOR_INREG && Subtarget.hasSSE41())) {\n    SDValue Op(N, 0);\n    if (TLI.isTypeLegal(VT) && TLI.isTypeLegal(In.getValueType()))\n      if (SDValue Res = combineX86ShufflesRecursively(Op, DAG, Subtarget))\n        return Res;\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combineKSHIFT(SDNode *N, SelectionDAG &DAG,\n                             TargetLowering::DAGCombinerInfo &DCI) {\n  EVT VT = N->getValueType(0);\n\n  if (ISD::isBuildVectorAllZeros(N->getOperand(0).getNode()))\n    return DAG.getConstant(0, SDLoc(N), VT);\n\n  APInt KnownUndef, KnownZero;\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  APInt DemandedElts = APInt::getAllOnesValue(VT.getVectorNumElements());\n  if (TLI.SimplifyDemandedVectorElts(SDValue(N, 0), DemandedElts, KnownUndef,\n                                     KnownZero, DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\n// Optimize (fp16_to_fp (fp_to_fp16 X)) to VCVTPS2PH followed by VCVTPH2PS.\n// Done as a combine because the lowering for fp16_to_fp and fp_to_fp16 produce\n// extra instructions between the conversion due to going to scalar and back.\nstatic SDValue combineFP16_TO_FP(SDNode *N, SelectionDAG &DAG,\n                                 const X86Subtarget &Subtarget) {\n  if (Subtarget.useSoftFloat() || !Subtarget.hasF16C())\n    return SDValue();\n\n  if (N->getOperand(0).getOpcode() != ISD::FP_TO_FP16)\n    return SDValue();\n\n  if (N->getValueType(0) != MVT::f32 ||\n      N->getOperand(0).getOperand(0).getValueType() != MVT::f32)\n    return SDValue();\n\n  SDLoc dl(N);\n  SDValue Res = DAG.getNode(ISD::SCALAR_TO_VECTOR, dl, MVT::v4f32,\n                            N->getOperand(0).getOperand(0));\n  Res = DAG.getNode(X86ISD::CVTPS2PH, dl, MVT::v8i16, Res,\n                    DAG.getTargetConstant(4, dl, MVT::i32));\n  Res = DAG.getNode(X86ISD::CVTPH2PS, dl, MVT::v4f32, Res);\n  return DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, MVT::f32, Res,\n                     DAG.getIntPtrConstant(0, dl));\n}\n\nstatic SDValue combineFP_EXTEND(SDNode *N, SelectionDAG &DAG,\n                                const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasF16C() || Subtarget.useSoftFloat())\n    return SDValue();\n\n  bool IsStrict = N->isStrictFPOpcode();\n  EVT VT = N->getValueType(0);\n  SDValue Src = N->getOperand(IsStrict ? 1 : 0);\n  EVT SrcVT = Src.getValueType();\n\n  if (!SrcVT.isVector() || SrcVT.getVectorElementType() != MVT::f16)\n    return SDValue();\n\n  if (VT.getVectorElementType() != MVT::f32 &&\n      VT.getVectorElementType() != MVT::f64)\n    return SDValue();\n\n  unsigned NumElts = VT.getVectorNumElements();\n  if (NumElts == 1 || !isPowerOf2_32(NumElts))\n    return SDValue();\n\n  SDLoc dl(N);\n\n  // Convert the input to vXi16.\n  EVT IntVT = SrcVT.changeVectorElementTypeToInteger();\n  Src = DAG.getBitcast(IntVT, Src);\n\n  // Widen to at least 8 input elements.\n  if (NumElts < 8) {\n    unsigned NumConcats = 8 / NumElts;\n    SDValue Fill = NumElts == 4 ? DAG.getUNDEF(IntVT)\n                                : DAG.getConstant(0, dl, IntVT);\n    SmallVector<SDValue, 4> Ops(NumConcats, Fill);\n    Ops[0] = Src;\n    Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v8i16, Ops);\n  }\n\n  // Destination is vXf32 with at least 4 elements.\n  EVT CvtVT = EVT::getVectorVT(*DAG.getContext(), MVT::f32,\n                               std::max(4U, NumElts));\n  SDValue Cvt, Chain;\n  if (IsStrict) {\n    Cvt = DAG.getNode(X86ISD::STRICT_CVTPH2PS, dl, {CvtVT, MVT::Other},\n                      {N->getOperand(0), Src});\n    Chain = Cvt.getValue(1);\n  } else {\n    Cvt = DAG.getNode(X86ISD::CVTPH2PS, dl, CvtVT, Src);\n  }\n\n  if (NumElts < 4) {\n    assert(NumElts == 2 && \"Unexpected size\");\n    Cvt = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, MVT::v2f32, Cvt,\n                      DAG.getIntPtrConstant(0, dl));\n  }\n\n  if (IsStrict) {\n    // Extend to the original VT if necessary.\n    if (Cvt.getValueType() != VT) {\n      Cvt = DAG.getNode(ISD::STRICT_FP_EXTEND, dl, {VT, MVT::Other},\n                        {Chain, Cvt});\n      Chain = Cvt.getValue(1);\n    }\n    return DAG.getMergeValues({Cvt, Chain}, dl);\n  }\n\n  // Extend to the original VT if necessary.\n  return DAG.getNode(ISD::FP_EXTEND, dl, VT, Cvt);\n}\n\n// Try to find a larger VBROADCAST_LOAD/SUBV_BROADCAST_LOAD that we can extract\n// from. Limit this to cases where the loads have the same input chain and the\n// output chains are unused. This avoids any memory ordering issues.\nstatic SDValue combineBROADCAST_LOAD(SDNode *N, SelectionDAG &DAG,\n                                     TargetLowering::DAGCombinerInfo &DCI) {\n  assert((N->getOpcode() == X86ISD::VBROADCAST_LOAD ||\n          N->getOpcode() == X86ISD::SUBV_BROADCAST_LOAD) &&\n         \"Unknown broadcast load type\");\n\n  // Only do this if the chain result is unused.\n  if (N->hasAnyUseOfValue(1))\n    return SDValue();\n\n  auto *MemIntrin = cast<MemIntrinsicSDNode>(N);\n\n  SDValue Ptr = MemIntrin->getBasePtr();\n  SDValue Chain = MemIntrin->getChain();\n  EVT VT = N->getSimpleValueType(0);\n  EVT MemVT = MemIntrin->getMemoryVT();\n\n  // Look at other users of our base pointer and try to find a wider broadcast.\n  // The input chain and the size of the memory VT must match.\n  for (SDNode *User : Ptr->uses())\n    if (User != N && User->getOpcode() == N->getOpcode() &&\n        cast<MemIntrinsicSDNode>(User)->getBasePtr() == Ptr &&\n        cast<MemIntrinsicSDNode>(User)->getChain() == Chain &&\n        cast<MemIntrinsicSDNode>(User)->getMemoryVT().getSizeInBits() ==\n            MemVT.getSizeInBits() &&\n        !User->hasAnyUseOfValue(1) &&\n        User->getValueSizeInBits(0).getFixedSize() > VT.getFixedSizeInBits()) {\n      SDValue Extract = extractSubVector(SDValue(User, 0), 0, DAG, SDLoc(N),\n                                         VT.getSizeInBits());\n      Extract = DAG.getBitcast(VT, Extract);\n      return DCI.CombineTo(N, Extract, SDValue(User, 1));\n    }\n\n  return SDValue();\n}\n\nstatic SDValue combineFP_ROUND(SDNode *N, SelectionDAG &DAG,\n                               const X86Subtarget &Subtarget) {\n  if (!Subtarget.hasF16C() || Subtarget.useSoftFloat())\n    return SDValue();\n\n  EVT VT = N->getValueType(0);\n  SDValue Src = N->getOperand(0);\n  EVT SrcVT = Src.getValueType();\n\n  if (!VT.isVector() || VT.getVectorElementType() != MVT::f16 ||\n      SrcVT.getVectorElementType() != MVT::f32)\n    return SDValue();\n\n  unsigned NumElts = VT.getVectorNumElements();\n  if (NumElts == 1 || !isPowerOf2_32(NumElts))\n    return SDValue();\n\n  SDLoc dl(N);\n\n  // Widen to at least 4 input elements.\n  if (NumElts < 4)\n    Src = DAG.getNode(ISD::CONCAT_VECTORS, dl, MVT::v4f32, Src,\n                      DAG.getConstantFP(0.0, dl, SrcVT));\n\n  // Destination is v8i16 with at least 8 elements.\n  EVT CvtVT = EVT::getVectorVT(*DAG.getContext(), MVT::i16,\n                               std::max(8U, NumElts));\n  SDValue Cvt = DAG.getNode(X86ISD::CVTPS2PH, dl, CvtVT, Src,\n                            DAG.getTargetConstant(4, dl, MVT::i32));\n\n  // Extract down to real number of elements.\n  if (NumElts < 8) {\n    EVT IntVT = VT.changeVectorElementTypeToInteger();\n    Cvt = DAG.getNode(ISD::EXTRACT_SUBVECTOR, dl, IntVT, Cvt,\n                      DAG.getIntPtrConstant(0, dl));\n  }\n\n  return DAG.getBitcast(VT, Cvt);\n}\n\nstatic SDValue combineMOVDQ2Q(SDNode *N, SelectionDAG &DAG) {\n  SDValue Src = N->getOperand(0);\n\n  // Turn MOVDQ2Q+simple_load into an mmx load.\n  if (ISD::isNormalLoad(Src.getNode()) && Src.hasOneUse()) {\n    LoadSDNode *LN = cast<LoadSDNode>(Src.getNode());\n\n    if (LN->isSimple()) {\n      SDValue NewLd = DAG.getLoad(MVT::x86mmx, SDLoc(N), LN->getChain(),\n                                  LN->getBasePtr(),\n                                  LN->getPointerInfo(),\n                                  LN->getOriginalAlign(),\n                                  LN->getMemOperand()->getFlags());\n      DAG.ReplaceAllUsesOfValueWith(SDValue(LN, 1), NewLd.getValue(1));\n      return NewLd;\n    }\n  }\n\n  return SDValue();\n}\n\nstatic SDValue combinePDEP(SDNode *N, SelectionDAG &DAG,\n                           TargetLowering::DAGCombinerInfo &DCI) {\n  unsigned NumBits = N->getSimpleValueType(0).getSizeInBits();\n  const TargetLowering &TLI = DAG.getTargetLoweringInfo();\n  if (TLI.SimplifyDemandedBits(SDValue(N, 0),\n                               APInt::getAllOnesValue(NumBits), DCI))\n    return SDValue(N, 0);\n\n  return SDValue();\n}\n\nSDValue X86TargetLowering::PerformDAGCombine(SDNode *N,\n                                             DAGCombinerInfo &DCI) const {\n  SelectionDAG &DAG = DCI.DAG;\n  switch (N->getOpcode()) {\n  default: break;\n  case ISD::SCALAR_TO_VECTOR:\n    return combineScalarToVector(N, DAG);\n  case ISD::EXTRACT_VECTOR_ELT:\n  case X86ISD::PEXTRW:\n  case X86ISD::PEXTRB:\n    return combineExtractVectorElt(N, DAG, DCI, Subtarget);\n  case ISD::CONCAT_VECTORS:\n    return combineConcatVectors(N, DAG, DCI, Subtarget);\n  case ISD::INSERT_SUBVECTOR:\n    return combineInsertSubvector(N, DAG, DCI, Subtarget);\n  case ISD::EXTRACT_SUBVECTOR:\n    return combineExtractSubvector(N, DAG, DCI, Subtarget);\n  case ISD::VSELECT:\n  case ISD::SELECT:\n  case X86ISD::BLENDV:      return combineSelect(N, DAG, DCI, Subtarget);\n  case ISD::BITCAST:        return combineBitcast(N, DAG, DCI, Subtarget);\n  case X86ISD::CMOV:        return combineCMov(N, DAG, DCI, Subtarget);\n  case X86ISD::CMP:         return combineCMP(N, DAG);\n  case ISD::ADD:            return combineAdd(N, DAG, DCI, Subtarget);\n  case ISD::SUB:            return combineSub(N, DAG, DCI, Subtarget);\n  case X86ISD::ADD:\n  case X86ISD::SUB:         return combineX86AddSub(N, DAG, DCI);\n  case X86ISD::SBB:         return combineSBB(N, DAG);\n  case X86ISD::ADC:         return combineADC(N, DAG, DCI);\n  case ISD::MUL:            return combineMul(N, DAG, DCI, Subtarget);\n  case ISD::SHL:            return combineShiftLeft(N, DAG);\n  case ISD::SRA:            return combineShiftRightArithmetic(N, DAG, Subtarget);\n  case ISD::SRL:            return combineShiftRightLogical(N, DAG, DCI, Subtarget);\n  case ISD::AND:            return combineAnd(N, DAG, DCI, Subtarget);\n  case ISD::OR:             return combineOr(N, DAG, DCI, Subtarget);\n  case ISD::XOR:            return combineXor(N, DAG, DCI, Subtarget);\n  case X86ISD::BEXTR:\n  case X86ISD::BEXTRI:      return combineBEXTR(N, DAG, DCI, Subtarget);\n  case ISD::LOAD:           return combineLoad(N, DAG, DCI, Subtarget);\n  case ISD::MLOAD:          return combineMaskedLoad(N, DAG, DCI, Subtarget);\n  case ISD::STORE:          return combineStore(N, DAG, DCI, Subtarget);\n  case ISD::MSTORE:         return combineMaskedStore(N, DAG, DCI, Subtarget);\n  case X86ISD::VEXTRACT_STORE:\n    return combineVEXTRACT_STORE(N, DAG, DCI, Subtarget);\n  case ISD::SINT_TO_FP:\n  case ISD::STRICT_SINT_TO_FP:\n    return combineSIntToFP(N, DAG, DCI, Subtarget);\n  case ISD::UINT_TO_FP:\n  case ISD::STRICT_UINT_TO_FP:\n    return combineUIntToFP(N, DAG, Subtarget);\n  case ISD::FADD:\n  case ISD::FSUB:           return combineFaddFsub(N, DAG, Subtarget);\n  case ISD::FNEG:           return combineFneg(N, DAG, DCI, Subtarget);\n  case ISD::TRUNCATE:       return combineTruncate(N, DAG, Subtarget);\n  case X86ISD::VTRUNC:      return combineVTRUNC(N, DAG, DCI);\n  case X86ISD::ANDNP:       return combineAndnp(N, DAG, DCI, Subtarget);\n  case X86ISD::FAND:        return combineFAnd(N, DAG, Subtarget);\n  case X86ISD::FANDN:       return combineFAndn(N, DAG, Subtarget);\n  case X86ISD::FXOR:\n  case X86ISD::FOR:         return combineFOr(N, DAG, DCI, Subtarget);\n  case X86ISD::FMIN:\n  case X86ISD::FMAX:        return combineFMinFMax(N, DAG);\n  case ISD::FMINNUM:\n  case ISD::FMAXNUM:        return combineFMinNumFMaxNum(N, DAG, Subtarget);\n  case X86ISD::CVTSI2P:\n  case X86ISD::CVTUI2P:     return combineX86INT_TO_FP(N, DAG, DCI);\n  case X86ISD::CVTP2SI:\n  case X86ISD::CVTP2UI:\n  case X86ISD::STRICT_CVTTP2SI:\n  case X86ISD::CVTTP2SI:\n  case X86ISD::STRICT_CVTTP2UI:\n  case X86ISD::CVTTP2UI:\n                            return combineCVTP2I_CVTTP2I(N, DAG, DCI);\n  case X86ISD::STRICT_CVTPH2PS:\n  case X86ISD::CVTPH2PS:    return combineCVTPH2PS(N, DAG, DCI);\n  case X86ISD::BT:          return combineBT(N, DAG, DCI);\n  case ISD::ANY_EXTEND:\n  case ISD::ZERO_EXTEND:    return combineZext(N, DAG, DCI, Subtarget);\n  case ISD::SIGN_EXTEND:    return combineSext(N, DAG, DCI, Subtarget);\n  case ISD::SIGN_EXTEND_INREG: return combineSignExtendInReg(N, DAG, Subtarget);\n  case ISD::ANY_EXTEND_VECTOR_INREG:\n  case ISD::SIGN_EXTEND_VECTOR_INREG:\n  case ISD::ZERO_EXTEND_VECTOR_INREG:\n    return combineEXTEND_VECTOR_INREG(N, DAG, DCI, Subtarget);\n  case ISD::SETCC:          return combineSetCC(N, DAG, Subtarget);\n  case X86ISD::SETCC:       return combineX86SetCC(N, DAG, Subtarget);\n  case X86ISD::BRCOND:      return combineBrCond(N, DAG, Subtarget);\n  case X86ISD::PACKSS:\n  case X86ISD::PACKUS:      return combineVectorPack(N, DAG, DCI, Subtarget);\n  case X86ISD::HADD:\n  case X86ISD::HSUB:\n  case X86ISD::FHADD:\n  case X86ISD::FHSUB:       return combineVectorHADDSUB(N, DAG, DCI, Subtarget);\n  case X86ISD::VSHL:\n  case X86ISD::VSRA:\n  case X86ISD::VSRL:\n    return combineVectorShiftVar(N, DAG, DCI, Subtarget);\n  case X86ISD::VSHLI:\n  case X86ISD::VSRAI:\n  case X86ISD::VSRLI:\n    return combineVectorShiftImm(N, DAG, DCI, Subtarget);\n  case ISD::INSERT_VECTOR_ELT:\n  case X86ISD::PINSRB:\n  case X86ISD::PINSRW:      return combineVectorInsert(N, DAG, DCI, Subtarget);\n  case X86ISD::SHUFP:       // Handle all target specific shuffles\n  case X86ISD::INSERTPS:\n  case X86ISD::EXTRQI:\n  case X86ISD::INSERTQI:\n  case X86ISD::VALIGN:\n  case X86ISD::PALIGNR:\n  case X86ISD::VSHLDQ:\n  case X86ISD::VSRLDQ:\n  case X86ISD::BLENDI:\n  case X86ISD::UNPCKH:\n  case X86ISD::UNPCKL:\n  case X86ISD::MOVHLPS:\n  case X86ISD::MOVLHPS:\n  case X86ISD::PSHUFB:\n  case X86ISD::PSHUFD:\n  case X86ISD::PSHUFHW:\n  case X86ISD::PSHUFLW:\n  case X86ISD::MOVSHDUP:\n  case X86ISD::MOVSLDUP:\n  case X86ISD::MOVDDUP:\n  case X86ISD::MOVSS:\n  case X86ISD::MOVSD:\n  case X86ISD::VBROADCAST:\n  case X86ISD::VPPERM:\n  case X86ISD::VPERMI:\n  case X86ISD::VPERMV:\n  case X86ISD::VPERMV3:\n  case X86ISD::VPERMIL2:\n  case X86ISD::VPERMILPI:\n  case X86ISD::VPERMILPV:\n  case X86ISD::VPERM2X128:\n  case X86ISD::SHUF128:\n  case X86ISD::VZEXT_MOVL:\n  case ISD::VECTOR_SHUFFLE: return combineShuffle(N, DAG, DCI,Subtarget);\n  case X86ISD::FMADD_RND:\n  case X86ISD::FMSUB:\n  case X86ISD::STRICT_FMSUB:\n  case X86ISD::FMSUB_RND:\n  case X86ISD::FNMADD:\n  case X86ISD::STRICT_FNMADD:\n  case X86ISD::FNMADD_RND:\n  case X86ISD::FNMSUB:\n  case X86ISD::STRICT_FNMSUB:\n  case X86ISD::FNMSUB_RND:\n  case ISD::FMA:\n  case ISD::STRICT_FMA:     return combineFMA(N, DAG, DCI, Subtarget);\n  case X86ISD::FMADDSUB_RND:\n  case X86ISD::FMSUBADD_RND:\n  case X86ISD::FMADDSUB:\n  case X86ISD::FMSUBADD:    return combineFMADDSUB(N, DAG, DCI);\n  case X86ISD::MOVMSK:      return combineMOVMSK(N, DAG, DCI, Subtarget);\n  case X86ISD::MGATHER:\n  case X86ISD::MSCATTER:    return combineX86GatherScatter(N, DAG, DCI);\n  case ISD::MGATHER:\n  case ISD::MSCATTER:       return combineGatherScatter(N, DAG, DCI);\n  case X86ISD::PCMPEQ:\n  case X86ISD::PCMPGT:      return combineVectorCompare(N, DAG, Subtarget);\n  case X86ISD::PMULDQ:\n  case X86ISD::PMULUDQ:     return combinePMULDQ(N, DAG, DCI, Subtarget);\n  case X86ISD::KSHIFTL:\n  case X86ISD::KSHIFTR:     return combineKSHIFT(N, DAG, DCI);\n  case ISD::FP16_TO_FP:     return combineFP16_TO_FP(N, DAG, Subtarget);\n  case ISD::STRICT_FP_EXTEND:\n  case ISD::FP_EXTEND:      return combineFP_EXTEND(N, DAG, Subtarget);\n  case ISD::FP_ROUND:       return combineFP_ROUND(N, DAG, Subtarget);\n  case X86ISD::VBROADCAST_LOAD:\n  case X86ISD::SUBV_BROADCAST_LOAD: return combineBROADCAST_LOAD(N, DAG, DCI);\n  case X86ISD::MOVDQ2Q:     return combineMOVDQ2Q(N, DAG);\n  case X86ISD::PDEP:        return combinePDEP(N, DAG, DCI);\n  }\n\n  return SDValue();\n}\n\nbool X86TargetLowering::isTypeDesirableForOp(unsigned Opc, EVT VT) const {\n  if (!isTypeLegal(VT))\n    return false;\n\n  // There are no vXi8 shifts.\n  if (Opc == ISD::SHL && VT.isVector() && VT.getVectorElementType() == MVT::i8)\n    return false;\n\n  // TODO: Almost no 8-bit ops are desirable because they have no actual\n  //       size/speed advantages vs. 32-bit ops, but they do have a major\n  //       potential disadvantage by causing partial register stalls.\n  //\n  // 8-bit multiply/shl is probably not cheaper than 32-bit multiply/shl, and\n  // we have specializations to turn 32-bit multiply/shl into LEA or other ops.\n  // Also, see the comment in \"IsDesirableToPromoteOp\" - where we additionally\n  // check for a constant operand to the multiply.\n  if ((Opc == ISD::MUL || Opc == ISD::SHL) && VT == MVT::i8)\n    return false;\n\n  // i16 instruction encodings are longer and some i16 instructions are slow,\n  // so those are not desirable.\n  if (VT == MVT::i16) {\n    switch (Opc) {\n    default:\n      break;\n    case ISD::LOAD:\n    case ISD::SIGN_EXTEND:\n    case ISD::ZERO_EXTEND:\n    case ISD::ANY_EXTEND:\n    case ISD::SHL:\n    case ISD::SRA:\n    case ISD::SRL:\n    case ISD::SUB:\n    case ISD::ADD:\n    case ISD::MUL:\n    case ISD::AND:\n    case ISD::OR:\n    case ISD::XOR:\n      return false;\n    }\n  }\n\n  // Any legal type not explicitly accounted for above here is desirable.\n  return true;\n}\n\nSDValue X86TargetLowering::expandIndirectJTBranch(const SDLoc& dl,\n                                                  SDValue Value, SDValue Addr,\n                                                  SelectionDAG &DAG) const {\n  const Module *M = DAG.getMachineFunction().getMMI().getModule();\n  Metadata *IsCFProtectionSupported = M->getModuleFlag(\"cf-protection-branch\");\n  if (IsCFProtectionSupported) {\n    // In case control-flow branch protection is enabled, we need to add\n    // notrack prefix to the indirect branch.\n    // In order to do that we create NT_BRIND SDNode.\n    // Upon ISEL, the pattern will convert it to jmp with NoTrack prefix.\n    return DAG.getNode(X86ISD::NT_BRIND, dl, MVT::Other, Value, Addr);\n  }\n\n  return TargetLowering::expandIndirectJTBranch(dl, Value, Addr, DAG);\n}\n\nbool X86TargetLowering::IsDesirableToPromoteOp(SDValue Op, EVT &PVT) const {\n  EVT VT = Op.getValueType();\n  bool Is8BitMulByConstant = VT == MVT::i8 && Op.getOpcode() == ISD::MUL &&\n                             isa<ConstantSDNode>(Op.getOperand(1));\n\n  // i16 is legal, but undesirable since i16 instruction encodings are longer\n  // and some i16 instructions are slow.\n  // 8-bit multiply-by-constant can usually be expanded to something cheaper\n  // using LEA and/or other ALU ops.\n  if (VT != MVT::i16 && !Is8BitMulByConstant)\n    return false;\n\n  auto IsFoldableRMW = [](SDValue Load, SDValue Op) {\n    if (!Op.hasOneUse())\n      return false;\n    SDNode *User = *Op->use_begin();\n    if (!ISD::isNormalStore(User))\n      return false;\n    auto *Ld = cast<LoadSDNode>(Load);\n    auto *St = cast<StoreSDNode>(User);\n    return Ld->getBasePtr() == St->getBasePtr();\n  };\n\n  auto IsFoldableAtomicRMW = [](SDValue Load, SDValue Op) {\n    if (!Load.hasOneUse() || Load.getOpcode() != ISD::ATOMIC_LOAD)\n      return false;\n    if (!Op.hasOneUse())\n      return false;\n    SDNode *User = *Op->use_begin();\n    if (User->getOpcode() != ISD::ATOMIC_STORE)\n      return false;\n    auto *Ld = cast<AtomicSDNode>(Load);\n    auto *St = cast<AtomicSDNode>(User);\n    return Ld->getBasePtr() == St->getBasePtr();\n  };\n\n  bool Commute = false;\n  switch (Op.getOpcode()) {\n  default: return false;\n  case ISD::SIGN_EXTEND:\n  case ISD::ZERO_EXTEND:\n  case ISD::ANY_EXTEND:\n    break;\n  case ISD::SHL:\n  case ISD::SRA:\n  case ISD::SRL: {\n    SDValue N0 = Op.getOperand(0);\n    // Look out for (store (shl (load), x)).\n    if (MayFoldLoad(N0) && IsFoldableRMW(N0, Op))\n      return false;\n    break;\n  }\n  case ISD::ADD:\n  case ISD::MUL:\n  case ISD::AND:\n  case ISD::OR:\n  case ISD::XOR:\n    Commute = true;\n    LLVM_FALLTHROUGH;\n  case ISD::SUB: {\n    SDValue N0 = Op.getOperand(0);\n    SDValue N1 = Op.getOperand(1);\n    // Avoid disabling potential load folding opportunities.\n    if (MayFoldLoad(N1) &&\n        (!Commute || !isa<ConstantSDNode>(N0) ||\n         (Op.getOpcode() != ISD::MUL && IsFoldableRMW(N1, Op))))\n      return false;\n    if (MayFoldLoad(N0) &&\n        ((Commute && !isa<ConstantSDNode>(N1)) ||\n         (Op.getOpcode() != ISD::MUL && IsFoldableRMW(N0, Op))))\n      return false;\n    if (IsFoldableAtomicRMW(N0, Op) ||\n        (Commute && IsFoldableAtomicRMW(N1, Op)))\n      return false;\n  }\n  }\n\n  PVT = MVT::i32;\n  return true;\n}\n\n//===----------------------------------------------------------------------===//\n//                           X86 Inline Assembly Support\n//===----------------------------------------------------------------------===//\n\n// Helper to match a string separated by whitespace.\nstatic bool matchAsm(StringRef S, ArrayRef<const char *> Pieces) {\n  S = S.substr(S.find_first_not_of(\" \\t\")); // Skip leading whitespace.\n\n  for (StringRef Piece : Pieces) {\n    if (!S.startswith(Piece)) // Check if the piece matches.\n      return false;\n\n    S = S.substr(Piece.size());\n    StringRef::size_type Pos = S.find_first_not_of(\" \\t\");\n    if (Pos == 0) // We matched a prefix.\n      return false;\n\n    S = S.substr(Pos);\n  }\n\n  return S.empty();\n}\n\nstatic bool clobbersFlagRegisters(const SmallVector<StringRef, 4> &AsmPieces) {\n\n  if (AsmPieces.size() == 3 || AsmPieces.size() == 4) {\n    if (std::count(AsmPieces.begin(), AsmPieces.end(), \"~{cc}\") &&\n        std::count(AsmPieces.begin(), AsmPieces.end(), \"~{flags}\") &&\n        std::count(AsmPieces.begin(), AsmPieces.end(), \"~{fpsr}\")) {\n\n      if (AsmPieces.size() == 3)\n        return true;\n      else if (std::count(AsmPieces.begin(), AsmPieces.end(), \"~{dirflag}\"))\n        return true;\n    }\n  }\n  return false;\n}\n\nbool X86TargetLowering::ExpandInlineAsm(CallInst *CI) const {\n  InlineAsm *IA = cast<InlineAsm>(CI->getCalledOperand());\n\n  const std::string &AsmStr = IA->getAsmString();\n\n  IntegerType *Ty = dyn_cast<IntegerType>(CI->getType());\n  if (!Ty || Ty->getBitWidth() % 16 != 0)\n    return false;\n\n  // TODO: should remove alternatives from the asmstring: \"foo {a|b}\" -> \"foo a\"\n  SmallVector<StringRef, 4> AsmPieces;\n  SplitString(AsmStr, AsmPieces, \";\\n\");\n\n  switch (AsmPieces.size()) {\n  default: return false;\n  case 1:\n    // FIXME: this should verify that we are targeting a 486 or better.  If not,\n    // we will turn this bswap into something that will be lowered to logical\n    // ops instead of emitting the bswap asm.  For now, we don't support 486 or\n    // lower so don't worry about this.\n    // bswap $0\n    if (matchAsm(AsmPieces[0], {\"bswap\", \"$0\"}) ||\n        matchAsm(AsmPieces[0], {\"bswapl\", \"$0\"}) ||\n        matchAsm(AsmPieces[0], {\"bswapq\", \"$0\"}) ||\n        matchAsm(AsmPieces[0], {\"bswap\", \"${0:q}\"}) ||\n        matchAsm(AsmPieces[0], {\"bswapl\", \"${0:q}\"}) ||\n        matchAsm(AsmPieces[0], {\"bswapq\", \"${0:q}\"})) {\n      // No need to check constraints, nothing other than the equivalent of\n      // \"=r,0\" would be valid here.\n      return IntrinsicLowering::LowerToByteSwap(CI);\n    }\n\n    // rorw $$8, ${0:w}  -->  llvm.bswap.i16\n    if (CI->getType()->isIntegerTy(16) &&\n        IA->getConstraintString().compare(0, 5, \"=r,0,\") == 0 &&\n        (matchAsm(AsmPieces[0], {\"rorw\", \"$$8,\", \"${0:w}\"}) ||\n         matchAsm(AsmPieces[0], {\"rolw\", \"$$8,\", \"${0:w}\"}))) {\n      AsmPieces.clear();\n      StringRef ConstraintsStr = IA->getConstraintString();\n      SplitString(StringRef(ConstraintsStr).substr(5), AsmPieces, \",\");\n      array_pod_sort(AsmPieces.begin(), AsmPieces.end());\n      if (clobbersFlagRegisters(AsmPieces))\n        return IntrinsicLowering::LowerToByteSwap(CI);\n    }\n    break;\n  case 3:\n    if (CI->getType()->isIntegerTy(32) &&\n        IA->getConstraintString().compare(0, 5, \"=r,0,\") == 0 &&\n        matchAsm(AsmPieces[0], {\"rorw\", \"$$8,\", \"${0:w}\"}) &&\n        matchAsm(AsmPieces[1], {\"rorl\", \"$$16,\", \"$0\"}) &&\n        matchAsm(AsmPieces[2], {\"rorw\", \"$$8,\", \"${0:w}\"})) {\n      AsmPieces.clear();\n      StringRef ConstraintsStr = IA->getConstraintString();\n      SplitString(StringRef(ConstraintsStr).substr(5), AsmPieces, \",\");\n      array_pod_sort(AsmPieces.begin(), AsmPieces.end());\n      if (clobbersFlagRegisters(AsmPieces))\n        return IntrinsicLowering::LowerToByteSwap(CI);\n    }\n\n    if (CI->getType()->isIntegerTy(64)) {\n      InlineAsm::ConstraintInfoVector Constraints = IA->ParseConstraints();\n      if (Constraints.size() >= 2 &&\n          Constraints[0].Codes.size() == 1 && Constraints[0].Codes[0] == \"A\" &&\n          Constraints[1].Codes.size() == 1 && Constraints[1].Codes[0] == \"0\") {\n        // bswap %eax / bswap %edx / xchgl %eax, %edx  -> llvm.bswap.i64\n        if (matchAsm(AsmPieces[0], {\"bswap\", \"%eax\"}) &&\n            matchAsm(AsmPieces[1], {\"bswap\", \"%edx\"}) &&\n            matchAsm(AsmPieces[2], {\"xchgl\", \"%eax,\", \"%edx\"}))\n          return IntrinsicLowering::LowerToByteSwap(CI);\n      }\n    }\n    break;\n  }\n  return false;\n}\n\nstatic X86::CondCode parseConstraintCode(llvm::StringRef Constraint) {\n  X86::CondCode Cond = StringSwitch<X86::CondCode>(Constraint)\n                           .Case(\"{@cca}\", X86::COND_A)\n                           .Case(\"{@ccae}\", X86::COND_AE)\n                           .Case(\"{@ccb}\", X86::COND_B)\n                           .Case(\"{@ccbe}\", X86::COND_BE)\n                           .Case(\"{@ccc}\", X86::COND_B)\n                           .Case(\"{@cce}\", X86::COND_E)\n                           .Case(\"{@ccz}\", X86::COND_E)\n                           .Case(\"{@ccg}\", X86::COND_G)\n                           .Case(\"{@ccge}\", X86::COND_GE)\n                           .Case(\"{@ccl}\", X86::COND_L)\n                           .Case(\"{@ccle}\", X86::COND_LE)\n                           .Case(\"{@ccna}\", X86::COND_BE)\n                           .Case(\"{@ccnae}\", X86::COND_B)\n                           .Case(\"{@ccnb}\", X86::COND_AE)\n                           .Case(\"{@ccnbe}\", X86::COND_A)\n                           .Case(\"{@ccnc}\", X86::COND_AE)\n                           .Case(\"{@ccne}\", X86::COND_NE)\n                           .Case(\"{@ccnz}\", X86::COND_NE)\n                           .Case(\"{@ccng}\", X86::COND_LE)\n                           .Case(\"{@ccnge}\", X86::COND_L)\n                           .Case(\"{@ccnl}\", X86::COND_GE)\n                           .Case(\"{@ccnle}\", X86::COND_G)\n                           .Case(\"{@ccno}\", X86::COND_NO)\n                           .Case(\"{@ccnp}\", X86::COND_NP)\n                           .Case(\"{@ccns}\", X86::COND_NS)\n                           .Case(\"{@cco}\", X86::COND_O)\n                           .Case(\"{@ccp}\", X86::COND_P)\n                           .Case(\"{@ccs}\", X86::COND_S)\n                           .Default(X86::COND_INVALID);\n  return Cond;\n}\n\n/// Given a constraint letter, return the type of constraint for this target.\nX86TargetLowering::ConstraintType\nX86TargetLowering::getConstraintType(StringRef Constraint) const {\n  if (Constraint.size() == 1) {\n    switch (Constraint[0]) {\n    case 'R':\n    case 'q':\n    case 'Q':\n    case 'f':\n    case 't':\n    case 'u':\n    case 'y':\n    case 'x':\n    case 'v':\n    case 'l':\n    case 'k': // AVX512 masking registers.\n      return C_RegisterClass;\n    case 'a':\n    case 'b':\n    case 'c':\n    case 'd':\n    case 'S':\n    case 'D':\n    case 'A':\n      return C_Register;\n    case 'I':\n    case 'J':\n    case 'K':\n    case 'N':\n    case 'G':\n    case 'L':\n    case 'M':\n      return C_Immediate;\n    case 'C':\n    case 'e':\n    case 'Z':\n      return C_Other;\n    default:\n      break;\n    }\n  }\n  else if (Constraint.size() == 2) {\n    switch (Constraint[0]) {\n    default:\n      break;\n    case 'Y':\n      switch (Constraint[1]) {\n      default:\n        break;\n      case 'z':\n        return C_Register;\n      case 'i':\n      case 'm':\n      case 'k':\n      case 't':\n      case '2':\n        return C_RegisterClass;\n      }\n    }\n  } else if (parseConstraintCode(Constraint) != X86::COND_INVALID)\n    return C_Other;\n  return TargetLowering::getConstraintType(Constraint);\n}\n\n/// Examine constraint type and operand type and determine a weight value.\n/// This object must already have been set up with the operand type\n/// and the current alternative constraint selected.\nTargetLowering::ConstraintWeight\n  X86TargetLowering::getSingleConstraintMatchWeight(\n    AsmOperandInfo &info, const char *constraint) const {\n  ConstraintWeight weight = CW_Invalid;\n  Value *CallOperandVal = info.CallOperandVal;\n    // If we don't have a value, we can't do a match,\n    // but allow it at the lowest weight.\n  if (!CallOperandVal)\n    return CW_Default;\n  Type *type = CallOperandVal->getType();\n  // Look at the constraint type.\n  switch (*constraint) {\n  default:\n    weight = TargetLowering::getSingleConstraintMatchWeight(info, constraint);\n    LLVM_FALLTHROUGH;\n  case 'R':\n  case 'q':\n  case 'Q':\n  case 'a':\n  case 'b':\n  case 'c':\n  case 'd':\n  case 'S':\n  case 'D':\n  case 'A':\n    if (CallOperandVal->getType()->isIntegerTy())\n      weight = CW_SpecificReg;\n    break;\n  case 'f':\n  case 't':\n  case 'u':\n    if (type->isFloatingPointTy())\n      weight = CW_SpecificReg;\n    break;\n  case 'y':\n    if (type->isX86_MMXTy() && Subtarget.hasMMX())\n      weight = CW_SpecificReg;\n    break;\n  case 'Y':\n    if (StringRef(constraint).size() != 2)\n      break;\n    switch (constraint[1]) {\n      default:\n        return CW_Invalid;\n      // XMM0\n      case 'z':\n        if (((type->getPrimitiveSizeInBits() == 128) && Subtarget.hasSSE1()) ||\n            ((type->getPrimitiveSizeInBits() == 256) && Subtarget.hasAVX()) ||\n            ((type->getPrimitiveSizeInBits() == 512) && Subtarget.hasAVX512()))\n          return CW_SpecificReg;\n        return CW_Invalid;\n      // Conditional OpMask regs (AVX512)\n      case 'k':\n        if ((type->getPrimitiveSizeInBits() == 64) && Subtarget.hasAVX512())\n          return CW_Register;\n        return CW_Invalid;\n      // Any MMX reg\n      case 'm':\n        if (type->isX86_MMXTy() && Subtarget.hasMMX())\n          return weight;\n        return CW_Invalid;\n      // Any SSE reg when ISA >= SSE2, same as 'x'\n      case 'i':\n      case 't':\n      case '2':\n        if (!Subtarget.hasSSE2())\n          return CW_Invalid;\n        break;\n    }\n    break;\n  case 'v':\n    if ((type->getPrimitiveSizeInBits() == 512) && Subtarget.hasAVX512())\n      weight = CW_Register;\n    LLVM_FALLTHROUGH;\n  case 'x':\n    if (((type->getPrimitiveSizeInBits() == 128) && Subtarget.hasSSE1()) ||\n        ((type->getPrimitiveSizeInBits() == 256) && Subtarget.hasAVX()))\n      weight = CW_Register;\n    break;\n  case 'k':\n    // Enable conditional vector operations using %k<#> registers.\n    if ((type->getPrimitiveSizeInBits() == 64) && Subtarget.hasAVX512())\n      weight = CW_Register;\n    break;\n  case 'I':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(info.CallOperandVal)) {\n      if (C->getZExtValue() <= 31)\n        weight = CW_Constant;\n    }\n    break;\n  case 'J':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if (C->getZExtValue() <= 63)\n        weight = CW_Constant;\n    }\n    break;\n  case 'K':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if ((C->getSExtValue() >= -0x80) && (C->getSExtValue() <= 0x7f))\n        weight = CW_Constant;\n    }\n    break;\n  case 'L':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if ((C->getZExtValue() == 0xff) || (C->getZExtValue() == 0xffff))\n        weight = CW_Constant;\n    }\n    break;\n  case 'M':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if (C->getZExtValue() <= 3)\n        weight = CW_Constant;\n    }\n    break;\n  case 'N':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if (C->getZExtValue() <= 0xff)\n        weight = CW_Constant;\n    }\n    break;\n  case 'G':\n  case 'C':\n    if (isa<ConstantFP>(CallOperandVal)) {\n      weight = CW_Constant;\n    }\n    break;\n  case 'e':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if ((C->getSExtValue() >= -0x80000000LL) &&\n          (C->getSExtValue() <= 0x7fffffffLL))\n        weight = CW_Constant;\n    }\n    break;\n  case 'Z':\n    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal)) {\n      if (C->getZExtValue() <= 0xffffffff)\n        weight = CW_Constant;\n    }\n    break;\n  }\n  return weight;\n}\n\n/// Try to replace an X constraint, which matches anything, with another that\n/// has more specific requirements based on the type of the corresponding\n/// operand.\nconst char *X86TargetLowering::\nLowerXConstraint(EVT ConstraintVT) const {\n  // FP X constraints get lowered to SSE1/2 registers if available, otherwise\n  // 'f' like normal targets.\n  if (ConstraintVT.isFloatingPoint()) {\n    if (Subtarget.hasSSE1())\n      return \"x\";\n  }\n\n  return TargetLowering::LowerXConstraint(ConstraintVT);\n}\n\n// Lower @cc targets via setcc.\nSDValue X86TargetLowering::LowerAsmOutputForConstraint(\n    SDValue &Chain, SDValue &Flag, const SDLoc &DL,\n    const AsmOperandInfo &OpInfo, SelectionDAG &DAG) const {\n  X86::CondCode Cond = parseConstraintCode(OpInfo.ConstraintCode);\n  if (Cond == X86::COND_INVALID)\n    return SDValue();\n  // Check that return type is valid.\n  if (OpInfo.ConstraintVT.isVector() || !OpInfo.ConstraintVT.isInteger() ||\n      OpInfo.ConstraintVT.getSizeInBits() < 8)\n    report_fatal_error(\"Flag output operand is of invalid type\");\n\n  // Get EFLAGS register. Only update chain when copyfrom is glued.\n  if (Flag.getNode()) {\n    Flag = DAG.getCopyFromReg(Chain, DL, X86::EFLAGS, MVT::i32, Flag);\n    Chain = Flag.getValue(1);\n  } else\n    Flag = DAG.getCopyFromReg(Chain, DL, X86::EFLAGS, MVT::i32);\n  // Extract CC code.\n  SDValue CC = getSETCC(Cond, Flag, DL, DAG);\n  // Extend to 32-bits\n  SDValue Result = DAG.getNode(ISD::ZERO_EXTEND, DL, OpInfo.ConstraintVT, CC);\n\n  return Result;\n}\n\n/// Lower the specified operand into the Ops vector.\n/// If it is invalid, don't add anything to Ops.\nvoid X86TargetLowering::LowerAsmOperandForConstraint(SDValue Op,\n                                                     std::string &Constraint,\n                                                     std::vector<SDValue>&Ops,\n                                                     SelectionDAG &DAG) const {\n  SDValue Result;\n\n  // Only support length 1 constraints for now.\n  if (Constraint.length() > 1) return;\n\n  char ConstraintLetter = Constraint[0];\n  switch (ConstraintLetter) {\n  default: break;\n  case 'I':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() <= 31) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'J':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() <= 63) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'K':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (isInt<8>(C->getSExtValue())) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'L':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() == 0xff || C->getZExtValue() == 0xffff ||\n          (Subtarget.is64Bit() && C->getZExtValue() == 0xffffffff)) {\n        Result = DAG.getTargetConstant(C->getSExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'M':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() <= 3) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'N':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() <= 255) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'O':\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (C->getZExtValue() <= 127) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    return;\n  case 'e': {\n    // 32-bit signed value\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (ConstantInt::isValueValidForType(Type::getInt32Ty(*DAG.getContext()),\n                                           C->getSExtValue())) {\n        // Widen to 64 bits here to get it sign extended.\n        Result = DAG.getTargetConstant(C->getSExtValue(), SDLoc(Op), MVT::i64);\n        break;\n      }\n    // FIXME gcc accepts some relocatable values here too, but only in certain\n    // memory models; it's complicated.\n    }\n    return;\n  }\n  case 'Z': {\n    // 32-bit unsigned value\n    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {\n      if (ConstantInt::isValueValidForType(Type::getInt32Ty(*DAG.getContext()),\n                                           C->getZExtValue())) {\n        Result = DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),\n                                       Op.getValueType());\n        break;\n      }\n    }\n    // FIXME gcc accepts some relocatable values here too, but only in certain\n    // memory models; it's complicated.\n    return;\n  }\n  case 'i': {\n    // Literal immediates are always ok.\n    if (ConstantSDNode *CST = dyn_cast<ConstantSDNode>(Op)) {\n      bool IsBool = CST->getConstantIntValue()->getBitWidth() == 1;\n      BooleanContent BCont = getBooleanContents(MVT::i64);\n      ISD::NodeType ExtOpc = IsBool ? getExtendForContent(BCont)\n                                    : ISD::SIGN_EXTEND;\n      int64_t ExtVal = ExtOpc == ISD::ZERO_EXTEND ? CST->getZExtValue()\n                                                  : CST->getSExtValue();\n      Result = DAG.getTargetConstant(ExtVal, SDLoc(Op), MVT::i64);\n      break;\n    }\n\n    // In any sort of PIC mode addresses need to be computed at runtime by\n    // adding in a register or some sort of table lookup.  These can't\n    // be used as immediates.\n    if (Subtarget.isPICStyleGOT() || Subtarget.isPICStyleStubPIC())\n      return;\n\n    // If we are in non-pic codegen mode, we allow the address of a global (with\n    // an optional displacement) to be used with 'i'.\n    if (auto *GA = dyn_cast<GlobalAddressSDNode>(Op))\n      // If we require an extra load to get this address, as in PIC mode, we\n      // can't accept it.\n      if (isGlobalStubReference(\n              Subtarget.classifyGlobalReference(GA->getGlobal())))\n        return;\n    break;\n  }\n  }\n\n  if (Result.getNode()) {\n    Ops.push_back(Result);\n    return;\n  }\n  return TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);\n}\n\n/// Check if \\p RC is a general purpose register class.\n/// I.e., GR* or one of their variant.\nstatic bool isGRClass(const TargetRegisterClass &RC) {\n  return RC.hasSuperClassEq(&X86::GR8RegClass) ||\n         RC.hasSuperClassEq(&X86::GR16RegClass) ||\n         RC.hasSuperClassEq(&X86::GR32RegClass) ||\n         RC.hasSuperClassEq(&X86::GR64RegClass) ||\n         RC.hasSuperClassEq(&X86::LOW32_ADDR_ACCESS_RBPRegClass);\n}\n\n/// Check if \\p RC is a vector register class.\n/// I.e., FR* / VR* or one of their variant.\nstatic bool isFRClass(const TargetRegisterClass &RC) {\n  return RC.hasSuperClassEq(&X86::FR32XRegClass) ||\n         RC.hasSuperClassEq(&X86::FR64XRegClass) ||\n         RC.hasSuperClassEq(&X86::VR128XRegClass) ||\n         RC.hasSuperClassEq(&X86::VR256XRegClass) ||\n         RC.hasSuperClassEq(&X86::VR512RegClass);\n}\n\n/// Check if \\p RC is a mask register class.\n/// I.e., VK* or one of their variant.\nstatic bool isVKClass(const TargetRegisterClass &RC) {\n  return RC.hasSuperClassEq(&X86::VK1RegClass) ||\n         RC.hasSuperClassEq(&X86::VK2RegClass) ||\n         RC.hasSuperClassEq(&X86::VK4RegClass) ||\n         RC.hasSuperClassEq(&X86::VK8RegClass) ||\n         RC.hasSuperClassEq(&X86::VK16RegClass) ||\n         RC.hasSuperClassEq(&X86::VK32RegClass) ||\n         RC.hasSuperClassEq(&X86::VK64RegClass);\n}\n\nstd::pair<unsigned, const TargetRegisterClass *>\nX86TargetLowering::getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,\n                                                StringRef Constraint,\n                                                MVT VT) const {\n  // First, see if this is a constraint that directly corresponds to an LLVM\n  // register class.\n  if (Constraint.size() == 1) {\n    // GCC Constraint Letters\n    switch (Constraint[0]) {\n    default: break;\n    // 'A' means [ER]AX + [ER]DX.\n    case 'A':\n      if (Subtarget.is64Bit())\n        return std::make_pair(X86::RAX, &X86::GR64_ADRegClass);\n      assert((Subtarget.is32Bit() || Subtarget.is16Bit()) &&\n             \"Expecting 64, 32 or 16 bit subtarget\");\n      return std::make_pair(X86::EAX, &X86::GR32_ADRegClass);\n\n      // TODO: Slight differences here in allocation order and leaving\n      // RIP in the class. Do they matter any more here than they do\n      // in the normal allocation?\n    case 'k':\n      if (Subtarget.hasAVX512()) {\n        if (VT == MVT::i1)\n          return std::make_pair(0U, &X86::VK1RegClass);\n        if (VT == MVT::i8)\n          return std::make_pair(0U, &X86::VK8RegClass);\n        if (VT == MVT::i16)\n          return std::make_pair(0U, &X86::VK16RegClass);\n      }\n      if (Subtarget.hasBWI()) {\n        if (VT == MVT::i32)\n          return std::make_pair(0U, &X86::VK32RegClass);\n        if (VT == MVT::i64)\n          return std::make_pair(0U, &X86::VK64RegClass);\n      }\n      break;\n    case 'q':   // GENERAL_REGS in 64-bit mode, Q_REGS in 32-bit mode.\n      if (Subtarget.is64Bit()) {\n        if (VT == MVT::i8 || VT == MVT::i1)\n          return std::make_pair(0U, &X86::GR8RegClass);\n        if (VT == MVT::i16)\n          return std::make_pair(0U, &X86::GR16RegClass);\n        if (VT == MVT::i32 || VT == MVT::f32)\n          return std::make_pair(0U, &X86::GR32RegClass);\n        if (VT != MVT::f80)\n          return std::make_pair(0U, &X86::GR64RegClass);\n        break;\n      }\n      LLVM_FALLTHROUGH;\n      // 32-bit fallthrough\n    case 'Q':   // Q_REGS\n      if (VT == MVT::i8 || VT == MVT::i1)\n        return std::make_pair(0U, &X86::GR8_ABCD_LRegClass);\n      if (VT == MVT::i16)\n        return std::make_pair(0U, &X86::GR16_ABCDRegClass);\n      if (VT == MVT::i32 || VT == MVT::f32 || !Subtarget.is64Bit())\n        return std::make_pair(0U, &X86::GR32_ABCDRegClass);\n      if (VT != MVT::f80)\n        return std::make_pair(0U, &X86::GR64_ABCDRegClass);\n      break;\n    case 'r':   // GENERAL_REGS\n    case 'l':   // INDEX_REGS\n      if (VT == MVT::i8 || VT == MVT::i1)\n        return std::make_pair(0U, &X86::GR8RegClass);\n      if (VT == MVT::i16)\n        return std::make_pair(0U, &X86::GR16RegClass);\n      if (VT == MVT::i32 || VT == MVT::f32 || !Subtarget.is64Bit())\n        return std::make_pair(0U, &X86::GR32RegClass);\n      if (VT != MVT::f80)\n        return std::make_pair(0U, &X86::GR64RegClass);\n      break;\n    case 'R':   // LEGACY_REGS\n      if (VT == MVT::i8 || VT == MVT::i1)\n        return std::make_pair(0U, &X86::GR8_NOREXRegClass);\n      if (VT == MVT::i16)\n        return std::make_pair(0U, &X86::GR16_NOREXRegClass);\n      if (VT == MVT::i32 || VT == MVT::f32 || !Subtarget.is64Bit())\n        return std::make_pair(0U, &X86::GR32_NOREXRegClass);\n      if (VT != MVT::f80)\n        return std::make_pair(0U, &X86::GR64_NOREXRegClass);\n      break;\n    case 'f':  // FP Stack registers.\n      // If SSE is enabled for this VT, use f80 to ensure the isel moves the\n      // value to the correct fpstack register class.\n      if (VT == MVT::f32 && !isScalarFPTypeInSSEReg(VT))\n        return std::make_pair(0U, &X86::RFP32RegClass);\n      if (VT == MVT::f64 && !isScalarFPTypeInSSEReg(VT))\n        return std::make_pair(0U, &X86::RFP64RegClass);\n      if (VT == MVT::f32 || VT == MVT::f64 || VT == MVT::f80)\n        return std::make_pair(0U, &X86::RFP80RegClass);\n      break;\n    case 'y':   // MMX_REGS if MMX allowed.\n      if (!Subtarget.hasMMX()) break;\n      return std::make_pair(0U, &X86::VR64RegClass);\n    case 'v':\n    case 'x':   // SSE_REGS if SSE1 allowed or AVX_REGS if AVX allowed\n      if (!Subtarget.hasSSE1()) break;\n      bool VConstraint = (Constraint[0] == 'v');\n\n      switch (VT.SimpleTy) {\n      default: break;\n      // Scalar SSE types.\n      case MVT::f32:\n      case MVT::i32:\n        if (VConstraint && Subtarget.hasVLX())\n          return std::make_pair(0U, &X86::FR32XRegClass);\n        return std::make_pair(0U, &X86::FR32RegClass);\n      case MVT::f64:\n      case MVT::i64:\n        if (VConstraint && Subtarget.hasVLX())\n          return std::make_pair(0U, &X86::FR64XRegClass);\n        return std::make_pair(0U, &X86::FR64RegClass);\n      case MVT::i128:\n        if (Subtarget.is64Bit()) {\n          if (VConstraint && Subtarget.hasVLX())\n            return std::make_pair(0U, &X86::VR128XRegClass);\n          return std::make_pair(0U, &X86::VR128RegClass);\n        }\n        break;\n      // Vector types and fp128.\n      case MVT::f128:\n      case MVT::v16i8:\n      case MVT::v8i16:\n      case MVT::v4i32:\n      case MVT::v2i64:\n      case MVT::v4f32:\n      case MVT::v2f64:\n        if (VConstraint && Subtarget.hasVLX())\n          return std::make_pair(0U, &X86::VR128XRegClass);\n        return std::make_pair(0U, &X86::VR128RegClass);\n      // AVX types.\n      case MVT::v32i8:\n      case MVT::v16i16:\n      case MVT::v8i32:\n      case MVT::v4i64:\n      case MVT::v8f32:\n      case MVT::v4f64:\n        if (VConstraint && Subtarget.hasVLX())\n          return std::make_pair(0U, &X86::VR256XRegClass);\n        if (Subtarget.hasAVX())\n          return std::make_pair(0U, &X86::VR256RegClass);\n        break;\n      case MVT::v64i8:\n      case MVT::v32i16:\n      case MVT::v8f64:\n      case MVT::v16f32:\n      case MVT::v16i32:\n      case MVT::v8i64:\n        if (!Subtarget.hasAVX512()) break;\n        if (VConstraint)\n          return std::make_pair(0U, &X86::VR512RegClass);\n        return std::make_pair(0U, &X86::VR512_0_15RegClass);\n      }\n      break;\n    }\n  } else if (Constraint.size() == 2 && Constraint[0] == 'Y') {\n    switch (Constraint[1]) {\n    default:\n      break;\n    case 'i':\n    case 't':\n    case '2':\n      return getRegForInlineAsmConstraint(TRI, \"x\", VT);\n    case 'm':\n      if (!Subtarget.hasMMX()) break;\n      return std::make_pair(0U, &X86::VR64RegClass);\n    case 'z':\n      if (!Subtarget.hasSSE1()) break;\n      switch (VT.SimpleTy) {\n      default: break;\n      // Scalar SSE types.\n      case MVT::f32:\n      case MVT::i32:\n        return std::make_pair(X86::XMM0, &X86::FR32RegClass);\n      case MVT::f64:\n      case MVT::i64:\n        return std::make_pair(X86::XMM0, &X86::FR64RegClass);\n      case MVT::f128:\n      case MVT::v16i8:\n      case MVT::v8i16:\n      case MVT::v4i32:\n      case MVT::v2i64:\n      case MVT::v4f32:\n      case MVT::v2f64:\n        return std::make_pair(X86::XMM0, &X86::VR128RegClass);\n      // AVX types.\n      case MVT::v32i8:\n      case MVT::v16i16:\n      case MVT::v8i32:\n      case MVT::v4i64:\n      case MVT::v8f32:\n      case MVT::v4f64:\n        if (Subtarget.hasAVX())\n          return std::make_pair(X86::YMM0, &X86::VR256RegClass);\n        break;\n      case MVT::v64i8:\n      case MVT::v32i16:\n      case MVT::v8f64:\n      case MVT::v16f32:\n      case MVT::v16i32:\n      case MVT::v8i64:\n        if (Subtarget.hasAVX512())\n          return std::make_pair(X86::ZMM0, &X86::VR512_0_15RegClass);\n        break;\n      }\n      break;\n    case 'k':\n      // This register class doesn't allocate k0 for masked vector operation.\n      if (Subtarget.hasAVX512()) {\n        if (VT == MVT::i1)\n          return std::make_pair(0U, &X86::VK1WMRegClass);\n        if (VT == MVT::i8)\n          return std::make_pair(0U, &X86::VK8WMRegClass);\n        if (VT == MVT::i16)\n          return std::make_pair(0U, &X86::VK16WMRegClass);\n      }\n      if (Subtarget.hasBWI()) {\n        if (VT == MVT::i32)\n          return std::make_pair(0U, &X86::VK32WMRegClass);\n        if (VT == MVT::i64)\n          return std::make_pair(0U, &X86::VK64WMRegClass);\n      }\n      break;\n    }\n  }\n\n  if (parseConstraintCode(Constraint) != X86::COND_INVALID)\n    return std::make_pair(0U, &X86::GR32RegClass);\n\n  // Use the default implementation in TargetLowering to convert the register\n  // constraint into a member of a register class.\n  std::pair<Register, const TargetRegisterClass*> Res;\n  Res = TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);\n\n  // Not found as a standard register?\n  if (!Res.second) {\n    // Only match x87 registers if the VT is one SelectionDAGBuilder can convert\n    // to/from f80.\n    if (VT == MVT::Other || VT == MVT::f32 || VT == MVT::f64 || VT == MVT::f80) {\n      // Map st(0) -> st(7) -> ST0\n      if (Constraint.size() == 7 && Constraint[0] == '{' &&\n          tolower(Constraint[1]) == 's' && tolower(Constraint[2]) == 't' &&\n          Constraint[3] == '(' &&\n          (Constraint[4] >= '0' && Constraint[4] <= '7') &&\n          Constraint[5] == ')' && Constraint[6] == '}') {\n        // st(7) is not allocatable and thus not a member of RFP80. Return\n        // singleton class in cases where we have a reference to it.\n        if (Constraint[4] == '7')\n          return std::make_pair(X86::FP7, &X86::RFP80_7RegClass);\n        return std::make_pair(X86::FP0 + Constraint[4] - '0',\n                              &X86::RFP80RegClass);\n      }\n\n      // GCC allows \"st(0)\" to be called just plain \"st\".\n      if (StringRef(\"{st}\").equals_lower(Constraint))\n        return std::make_pair(X86::FP0, &X86::RFP80RegClass);\n    }\n\n    // flags -> EFLAGS\n    if (StringRef(\"{flags}\").equals_lower(Constraint))\n      return std::make_pair(X86::EFLAGS, &X86::CCRRegClass);\n\n    // dirflag -> DF\n    // Only allow for clobber.\n    if (StringRef(\"{dirflag}\").equals_lower(Constraint) && VT == MVT::Other)\n      return std::make_pair(X86::DF, &X86::DFCCRRegClass);\n\n    // fpsr -> FPSW\n    if (StringRef(\"{fpsr}\").equals_lower(Constraint))\n      return std::make_pair(X86::FPSW, &X86::FPCCRRegClass);\n\n    return Res;\n  }\n\n  // Make sure it isn't a register that requires 64-bit mode.\n  if (!Subtarget.is64Bit() &&\n      (isFRClass(*Res.second) || isGRClass(*Res.second)) &&\n      TRI->getEncodingValue(Res.first) >= 8) {\n    // Register requires REX prefix, but we're in 32-bit mode.\n    return std::make_pair(0, nullptr);\n  }\n\n  // Make sure it isn't a register that requires AVX512.\n  if (!Subtarget.hasAVX512() && isFRClass(*Res.second) &&\n      TRI->getEncodingValue(Res.first) & 0x10) {\n    // Register requires EVEX prefix.\n    return std::make_pair(0, nullptr);\n  }\n\n  // Otherwise, check to see if this is a register class of the wrong value\n  // type.  For example, we want to map \"{ax},i32\" -> {eax}, we don't want it to\n  // turn into {ax},{dx}.\n  // MVT::Other is used to specify clobber names.\n  if (TRI->isTypeLegalForClass(*Res.second, VT) || VT == MVT::Other)\n    return Res;   // Correct type already, nothing to do.\n\n  // Get a matching integer of the correct size. i.e. \"ax\" with MVT::32 should\n  // return \"eax\". This should even work for things like getting 64bit integer\n  // registers when given an f64 type.\n  const TargetRegisterClass *Class = Res.second;\n  // The generic code will match the first register class that contains the\n  // given register. Thus, based on the ordering of the tablegened file,\n  // the \"plain\" GR classes might not come first.\n  // Therefore, use a helper method.\n  if (isGRClass(*Class)) {\n    unsigned Size = VT.getSizeInBits();\n    if (Size == 1) Size = 8;\n    Register DestReg = getX86SubSuperRegisterOrZero(Res.first, Size);\n    if (DestReg > 0) {\n      bool is64Bit = Subtarget.is64Bit();\n      const TargetRegisterClass *RC =\n          Size == 8 ? (is64Bit ? &X86::GR8RegClass : &X86::GR8_NOREXRegClass)\n        : Size == 16 ? (is64Bit ? &X86::GR16RegClass : &X86::GR16_NOREXRegClass)\n        : Size == 32 ? (is64Bit ? &X86::GR32RegClass : &X86::GR32_NOREXRegClass)\n        : Size == 64 ? (is64Bit ? &X86::GR64RegClass : nullptr)\n        : nullptr;\n      if (Size == 64 && !is64Bit) {\n        // Model GCC's behavior here and select a fixed pair of 32-bit\n        // registers.\n        switch (DestReg) {\n        case X86::RAX:\n          return std::make_pair(X86::EAX, &X86::GR32_ADRegClass);\n        case X86::RDX:\n          return std::make_pair(X86::EDX, &X86::GR32_DCRegClass);\n        case X86::RCX:\n          return std::make_pair(X86::ECX, &X86::GR32_CBRegClass);\n        case X86::RBX:\n          return std::make_pair(X86::EBX, &X86::GR32_BSIRegClass);\n        case X86::RSI:\n          return std::make_pair(X86::ESI, &X86::GR32_SIDIRegClass);\n        case X86::RDI:\n          return std::make_pair(X86::EDI, &X86::GR32_DIBPRegClass);\n        case X86::RBP:\n          return std::make_pair(X86::EBP, &X86::GR32_BPSPRegClass);\n        default:\n          return std::make_pair(0, nullptr);\n        }\n      }\n      if (RC && RC->contains(DestReg))\n        return std::make_pair(DestReg, RC);\n      return Res;\n    }\n    // No register found/type mismatch.\n    return std::make_pair(0, nullptr);\n  } else if (isFRClass(*Class)) {\n    // Handle references to XMM physical registers that got mapped into the\n    // wrong class.  This can happen with constraints like {xmm0} where the\n    // target independent register mapper will just pick the first match it can\n    // find, ignoring the required type.\n\n    // TODO: Handle f128 and i128 in FR128RegClass after it is tested well.\n    if (VT == MVT::f32 || VT == MVT::i32)\n      Res.second = &X86::FR32XRegClass;\n    else if (VT == MVT::f64 || VT == MVT::i64)\n      Res.second = &X86::FR64XRegClass;\n    else if (TRI->isTypeLegalForClass(X86::VR128XRegClass, VT))\n      Res.second = &X86::VR128XRegClass;\n    else if (TRI->isTypeLegalForClass(X86::VR256XRegClass, VT))\n      Res.second = &X86::VR256XRegClass;\n    else if (TRI->isTypeLegalForClass(X86::VR512RegClass, VT))\n      Res.second = &X86::VR512RegClass;\n    else {\n      // Type mismatch and not a clobber: Return an error;\n      Res.first = 0;\n      Res.second = nullptr;\n    }\n  } else if (isVKClass(*Class)) {\n    if (VT == MVT::i1)\n      Res.second = &X86::VK1RegClass;\n    else if (VT == MVT::i8)\n      Res.second = &X86::VK8RegClass;\n    else if (VT == MVT::i16)\n      Res.second = &X86::VK16RegClass;\n    else if (VT == MVT::i32)\n      Res.second = &X86::VK32RegClass;\n    else if (VT == MVT::i64)\n      Res.second = &X86::VK64RegClass;\n    else {\n      // Type mismatch and not a clobber: Return an error;\n      Res.first = 0;\n      Res.second = nullptr;\n    }\n  }\n\n  return Res;\n}\n\nint X86TargetLowering::getScalingFactorCost(const DataLayout &DL,\n                                            const AddrMode &AM, Type *Ty,\n                                            unsigned AS) const {\n  // Scaling factors are not free at all.\n  // An indexed folded instruction, i.e., inst (reg1, reg2, scale),\n  // will take 2 allocations in the out of order engine instead of 1\n  // for plain addressing mode, i.e. inst (reg1).\n  // E.g.,\n  // vaddps (%rsi,%rdx), %ymm0, %ymm1\n  // Requires two allocations (one for the load, one for the computation)\n  // whereas:\n  // vaddps (%rsi), %ymm0, %ymm1\n  // Requires just 1 allocation, i.e., freeing allocations for other operations\n  // and having less micro operations to execute.\n  //\n  // For some X86 architectures, this is even worse because for instance for\n  // stores, the complex addressing mode forces the instruction to use the\n  // \"load\" ports instead of the dedicated \"store\" port.\n  // E.g., on Haswell:\n  // vmovaps %ymm1, (%r8, %rdi) can use port 2 or 3.\n  // vmovaps %ymm1, (%r8) can use port 2, 3, or 7.\n  if (isLegalAddressingMode(DL, AM, Ty, AS))\n    // Scale represents reg2 * scale, thus account for 1\n    // as soon as we use a second register.\n    return AM.Scale != 0;\n  return -1;\n}\n\nbool X86TargetLowering::isIntDivCheap(EVT VT, AttributeList Attr) const {\n  // Integer division on x86 is expensive. However, when aggressively optimizing\n  // for code size, we prefer to use a div instruction, as it is usually smaller\n  // than the alternative sequence.\n  // The exception to this is vector division. Since x86 doesn't have vector\n  // integer division, leaving the division as-is is a loss even in terms of\n  // size, because it will have to be scalarized, while the alternative code\n  // sequence can be performed in vector form.\n  bool OptSize = Attr.hasFnAttribute(Attribute::MinSize);\n  return OptSize && !VT.isVector();\n}\n\nvoid X86TargetLowering::initializeSplitCSR(MachineBasicBlock *Entry) const {\n  if (!Subtarget.is64Bit())\n    return;\n\n  // Update IsSplitCSR in X86MachineFunctionInfo.\n  X86MachineFunctionInfo *AFI =\n      Entry->getParent()->getInfo<X86MachineFunctionInfo>();\n  AFI->setIsSplitCSR(true);\n}\n\nvoid X86TargetLowering::insertCopiesSplitCSR(\n    MachineBasicBlock *Entry,\n    const SmallVectorImpl<MachineBasicBlock *> &Exits) const {\n  const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n  const MCPhysReg *IStart = TRI->getCalleeSavedRegsViaCopy(Entry->getParent());\n  if (!IStart)\n    return;\n\n  const TargetInstrInfo *TII = Subtarget.getInstrInfo();\n  MachineRegisterInfo *MRI = &Entry->getParent()->getRegInfo();\n  MachineBasicBlock::iterator MBBI = Entry->begin();\n  for (const MCPhysReg *I = IStart; *I; ++I) {\n    const TargetRegisterClass *RC = nullptr;\n    if (X86::GR64RegClass.contains(*I))\n      RC = &X86::GR64RegClass;\n    else\n      llvm_unreachable(\"Unexpected register class in CSRsViaCopy!\");\n\n    Register NewVR = MRI->createVirtualRegister(RC);\n    // Create copy from CSR to a virtual register.\n    // FIXME: this currently does not emit CFI pseudo-instructions, it works\n    // fine for CXX_FAST_TLS since the C++-style TLS access functions should be\n    // nounwind. If we want to generalize this later, we may need to emit\n    // CFI pseudo-instructions.\n    assert(\n        Entry->getParent()->getFunction().hasFnAttribute(Attribute::NoUnwind) &&\n        \"Function should be nounwind in insertCopiesSplitCSR!\");\n    Entry->addLiveIn(*I);\n    BuildMI(*Entry, MBBI, DebugLoc(), TII->get(TargetOpcode::COPY), NewVR)\n        .addReg(*I);\n\n    // Insert the copy-back instructions right before the terminator.\n    for (auto *Exit : Exits)\n      BuildMI(*Exit, Exit->getFirstTerminator(), DebugLoc(),\n              TII->get(TargetOpcode::COPY), *I)\n          .addReg(NewVR);\n  }\n}\n\nbool X86TargetLowering::supportSwiftError() const {\n  return Subtarget.is64Bit();\n}\n\n/// Returns true if stack probing through a function call is requested.\nbool X86TargetLowering::hasStackProbeSymbol(MachineFunction &MF) const {\n  return !getStackProbeSymbolName(MF).empty();\n}\n\n/// Returns true if stack probing through inline assembly is requested.\nbool X86TargetLowering::hasInlineStackProbe(MachineFunction &MF) const {\n\n  // No inline stack probe for Windows, they have their own mechanism.\n  if (Subtarget.isOSWindows() ||\n      MF.getFunction().hasFnAttribute(\"no-stack-arg-probe\"))\n    return false;\n\n  // If the function specifically requests inline stack probes, emit them.\n  if (MF.getFunction().hasFnAttribute(\"probe-stack\"))\n    return MF.getFunction().getFnAttribute(\"probe-stack\").getValueAsString() ==\n           \"inline-asm\";\n\n  return false;\n}\n\n/// Returns the name of the symbol used to emit stack probes or the empty\n/// string if not applicable.\nStringRef\nX86TargetLowering::getStackProbeSymbolName(MachineFunction &MF) const {\n  // Inline Stack probes disable stack probe call\n  if (hasInlineStackProbe(MF))\n    return \"\";\n\n  // If the function specifically requests stack probes, emit them.\n  if (MF.getFunction().hasFnAttribute(\"probe-stack\"))\n    return MF.getFunction().getFnAttribute(\"probe-stack\").getValueAsString();\n\n  // Generally, if we aren't on Windows, the platform ABI does not include\n  // support for stack probes, so don't emit them.\n  if (!Subtarget.isOSWindows() || Subtarget.isTargetMachO() ||\n      MF.getFunction().hasFnAttribute(\"no-stack-arg-probe\"))\n    return \"\";\n\n  // We need a stack probe to conform to the Windows ABI. Choose the right\n  // symbol.\n  if (Subtarget.is64Bit())\n    return Subtarget.isTargetCygMing() ? \"___chkstk_ms\" : \"__chkstk\";\n  return Subtarget.isTargetCygMing() ? \"_alloca\" : \"_chkstk\";\n}\n\nunsigned\nX86TargetLowering::getStackProbeSize(MachineFunction &MF) const {\n  // The default stack probe size is 4096 if the function has no stackprobesize\n  // attribute.\n  unsigned StackProbeSize = 4096;\n  const Function &Fn = MF.getFunction();\n  if (Fn.hasFnAttribute(\"stack-probe-size\"))\n    Fn.getFnAttribute(\"stack-probe-size\")\n        .getValueAsString()\n        .getAsInteger(0, StackProbeSize);\n  return StackProbeSize;\n}\n\nAlign X86TargetLowering::getPrefLoopAlignment(MachineLoop *ML) const {\n  if (ML->isInnermost() &&\n      ExperimentalPrefInnermostLoopAlignment.getNumOccurrences())\n    return Align(1ULL << ExperimentalPrefInnermostLoopAlignment);\n  return TargetLowering::getPrefLoopAlignment();\n}\n"}, "3": {"id": 3, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h", "content": "//===-- X86ISelLowering.h - X86 DAG Lowering Interface ----------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the interfaces that X86 uses to lower LLVM code into a\n// selection DAG.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_LIB_TARGET_X86_X86ISELLOWERING_H\n#define LLVM_LIB_TARGET_X86_X86ISELLOWERING_H\n\n#include \"llvm/CodeGen/TargetLowering.h\"\n\nnamespace llvm {\n  class X86Subtarget;\n  class X86TargetMachine;\n\n  namespace X86ISD {\n    // X86 Specific DAG Nodes\n  enum NodeType : unsigned {\n    // Start the numbering where the builtin ops leave off.\n    FIRST_NUMBER = ISD::BUILTIN_OP_END,\n\n    /// Bit scan forward.\n    BSF,\n    /// Bit scan reverse.\n    BSR,\n\n    /// X86 funnel/double shift i16 instructions. These correspond to\n    /// X86::SHLDW and X86::SHRDW instructions which have different amt\n    /// modulo rules to generic funnel shifts.\n    /// NOTE: The operand order matches ISD::FSHL/FSHR not SHLD/SHRD.\n    FSHL,\n    FSHR,\n\n    /// Bitwise logical AND of floating point values. This corresponds\n    /// to X86::ANDPS or X86::ANDPD.\n    FAND,\n\n    /// Bitwise logical OR of floating point values. This corresponds\n    /// to X86::ORPS or X86::ORPD.\n    FOR,\n\n    /// Bitwise logical XOR of floating point values. This corresponds\n    /// to X86::XORPS or X86::XORPD.\n    FXOR,\n\n    ///  Bitwise logical ANDNOT of floating point values. This\n    /// corresponds to X86::ANDNPS or X86::ANDNPD.\n    FANDN,\n\n    /// These operations represent an abstract X86 call\n    /// instruction, which includes a bunch of information.  In particular the\n    /// operands of these node are:\n    ///\n    ///     #0 - The incoming token chain\n    ///     #1 - The callee\n    ///     #2 - The number of arg bytes the caller pushes on the stack.\n    ///     #3 - The number of arg bytes the callee pops off the stack.\n    ///     #4 - The value to pass in AL/AX/EAX (optional)\n    ///     #5 - The value to pass in DL/DX/EDX (optional)\n    ///\n    /// The result values of these nodes are:\n    ///\n    ///     #0 - The outgoing token chain\n    ///     #1 - The first register result value (optional)\n    ///     #2 - The second register result value (optional)\n    ///\n    CALL,\n\n    /// Same as call except it adds the NoTrack prefix.\n    NT_CALL,\n\n    /// X86 compare and logical compare instructions.\n    CMP,\n    FCMP,\n    COMI,\n    UCOMI,\n\n    /// X86 bit-test instructions.\n    BT,\n\n    /// X86 SetCC. Operand 0 is condition code, and operand 1 is the EFLAGS\n    /// operand, usually produced by a CMP instruction.\n    SETCC,\n\n    /// X86 Select\n    SELECTS,\n\n    // Same as SETCC except it's materialized with a sbb and the value is all\n    // one's or all zero's.\n    SETCC_CARRY, // R = carry_bit ? ~0 : 0\n\n    /// X86 FP SETCC, implemented with CMP{cc}SS/CMP{cc}SD.\n    /// Operands are two FP values to compare; result is a mask of\n    /// 0s or 1s.  Generally DTRT for C/C++ with NaNs.\n    FSETCC,\n\n    /// X86 FP SETCC, similar to above, but with output as an i1 mask and\n    /// and a version with SAE.\n    FSETCCM,\n    FSETCCM_SAE,\n\n    /// X86 conditional moves. Operand 0 and operand 1 are the two values\n    /// to select from. Operand 2 is the condition code, and operand 3 is the\n    /// flag operand produced by a CMP or TEST instruction.\n    CMOV,\n\n    /// X86 conditional branches. Operand 0 is the chain operand, operand 1\n    /// is the block to branch if condition is true, operand 2 is the\n    /// condition code, and operand 3 is the flag operand produced by a CMP\n    /// or TEST instruction.\n    BRCOND,\n\n    /// BRIND node with NoTrack prefix. Operand 0 is the chain operand and\n    /// operand 1 is the target address.\n    NT_BRIND,\n\n    /// Return with a flag operand. Operand 0 is the chain operand, operand\n    /// 1 is the number of bytes of stack to pop.\n    RET_FLAG,\n\n    /// Return from interrupt. Operand 0 is the number of bytes to pop.\n    IRET,\n\n    /// Repeat fill, corresponds to X86::REP_STOSx.\n    REP_STOS,\n\n    /// Repeat move, corresponds to X86::REP_MOVSx.\n    REP_MOVS,\n\n    /// On Darwin, this node represents the result of the popl\n    /// at function entry, used for PIC code.\n    GlobalBaseReg,\n\n    /// A wrapper node for TargetConstantPool, TargetJumpTable,\n    /// TargetExternalSymbol, TargetGlobalAddress, TargetGlobalTLSAddress,\n    /// MCSymbol and TargetBlockAddress.\n    Wrapper,\n\n    /// Special wrapper used under X86-64 PIC mode for RIP\n    /// relative displacements.\n    WrapperRIP,\n\n    /// Copies a 64-bit value from an MMX vector to the low word\n    /// of an XMM vector, with the high word zero filled.\n    MOVQ2DQ,\n\n    /// Copies a 64-bit value from the low word of an XMM vector\n    /// to an MMX vector.\n    MOVDQ2Q,\n\n    /// Copies a 32-bit value from the low word of a MMX\n    /// vector to a GPR.\n    MMX_MOVD2W,\n\n    /// Copies a GPR into the low 32-bit word of a MMX vector\n    /// and zero out the high word.\n    MMX_MOVW2D,\n\n    /// Extract an 8-bit value from a vector and zero extend it to\n    /// i32, corresponds to X86::PEXTRB.\n    PEXTRB,\n\n    /// Extract a 16-bit value from a vector and zero extend it to\n    /// i32, corresponds to X86::PEXTRW.\n    PEXTRW,\n\n    /// Insert any element of a 4 x float vector into any element\n    /// of a destination 4 x floatvector.\n    INSERTPS,\n\n    /// Insert the lower 8-bits of a 32-bit value to a vector,\n    /// corresponds to X86::PINSRB.\n    PINSRB,\n\n    /// Insert the lower 16-bits of a 32-bit value to a vector,\n    /// corresponds to X86::PINSRW.\n    PINSRW,\n\n    /// Shuffle 16 8-bit values within a vector.\n    PSHUFB,\n\n    /// Compute Sum of Absolute Differences.\n    PSADBW,\n    /// Compute Double Block Packed Sum-Absolute-Differences\n    DBPSADBW,\n\n    /// Bitwise Logical AND NOT of Packed FP values.\n    ANDNP,\n\n    /// Blend where the selector is an immediate.\n    BLENDI,\n\n    /// Dynamic (non-constant condition) vector blend where only the sign bits\n    /// of the condition elements are used. This is used to enforce that the\n    /// condition mask is not valid for generic VSELECT optimizations. This\n    /// is also used to implement the intrinsics.\n    /// Operands are in VSELECT order: MASK, TRUE, FALSE\n    BLENDV,\n\n    /// Combined add and sub on an FP vector.\n    ADDSUB,\n\n    //  FP vector ops with rounding mode.\n    FADD_RND,\n    FADDS,\n    FADDS_RND,\n    FSUB_RND,\n    FSUBS,\n    FSUBS_RND,\n    FMUL_RND,\n    FMULS,\n    FMULS_RND,\n    FDIV_RND,\n    FDIVS,\n    FDIVS_RND,\n    FMAX_SAE,\n    FMAXS_SAE,\n    FMIN_SAE,\n    FMINS_SAE,\n    FSQRT_RND,\n    FSQRTS,\n    FSQRTS_RND,\n\n    // FP vector get exponent.\n    FGETEXP,\n    FGETEXP_SAE,\n    FGETEXPS,\n    FGETEXPS_SAE,\n    // Extract Normalized Mantissas.\n    VGETMANT,\n    VGETMANT_SAE,\n    VGETMANTS,\n    VGETMANTS_SAE,\n    // FP Scale.\n    SCALEF,\n    SCALEF_RND,\n    SCALEFS,\n    SCALEFS_RND,\n\n    // Unsigned Integer average.\n    AVG,\n\n    /// Integer horizontal add/sub.\n    HADD,\n    HSUB,\n\n    /// Floating point horizontal add/sub.\n    FHADD,\n    FHSUB,\n\n    // Detect Conflicts Within a Vector\n    CONFLICT,\n\n    /// Floating point max and min.\n    FMAX,\n    FMIN,\n\n    /// Commutative FMIN and FMAX.\n    FMAXC,\n    FMINC,\n\n    /// Scalar intrinsic floating point max and min.\n    FMAXS,\n    FMINS,\n\n    /// Floating point reciprocal-sqrt and reciprocal approximation.\n    /// Note that these typically require refinement\n    /// in order to obtain suitable precision.\n    FRSQRT,\n    FRCP,\n\n    // AVX-512 reciprocal approximations with a little more precision.\n    RSQRT14,\n    RSQRT14S,\n    RCP14,\n    RCP14S,\n\n    // Thread Local Storage.\n    TLSADDR,\n\n    // Thread Local Storage. A call to get the start address\n    // of the TLS block for the current module.\n    TLSBASEADDR,\n\n    // Thread Local Storage.  When calling to an OS provided\n    // thunk at the address from an earlier relocation.\n    TLSCALL,\n\n    // Exception Handling helpers.\n    EH_RETURN,\n\n    // SjLj exception handling setjmp.\n    EH_SJLJ_SETJMP,\n\n    // SjLj exception handling longjmp.\n    EH_SJLJ_LONGJMP,\n\n    // SjLj exception handling dispatch.\n    EH_SJLJ_SETUP_DISPATCH,\n\n    /// Tail call return. See X86TargetLowering::LowerCall for\n    /// the list of operands.\n    TC_RETURN,\n\n    // Vector move to low scalar and zero higher vector elements.\n    VZEXT_MOVL,\n\n    // Vector integer truncate.\n    VTRUNC,\n    // Vector integer truncate with unsigned/signed saturation.\n    VTRUNCUS,\n    VTRUNCS,\n\n    // Masked version of the above. Used when less than a 128-bit result is\n    // produced since the mask only applies to the lower elements and can't\n    // be represented by a select.\n    // SRC, PASSTHRU, MASK\n    VMTRUNC,\n    VMTRUNCUS,\n    VMTRUNCS,\n\n    // Vector FP extend.\n    VFPEXT,\n    VFPEXT_SAE,\n    VFPEXTS,\n    VFPEXTS_SAE,\n\n    // Vector FP round.\n    VFPROUND,\n    VFPROUND_RND,\n    VFPROUNDS,\n    VFPROUNDS_RND,\n\n    // Masked version of above. Used for v2f64->v4f32.\n    // SRC, PASSTHRU, MASK\n    VMFPROUND,\n\n    // 128-bit vector logical left / right shift\n    VSHLDQ,\n    VSRLDQ,\n\n    // Vector shift elements\n    VSHL,\n    VSRL,\n    VSRA,\n\n    // Vector variable shift\n    VSHLV,\n    VSRLV,\n    VSRAV,\n\n    // Vector shift elements by immediate\n    VSHLI,\n    VSRLI,\n    VSRAI,\n\n    // Shifts of mask registers.\n    KSHIFTL,\n    KSHIFTR,\n\n    // Bit rotate by immediate\n    VROTLI,\n    VROTRI,\n\n    // Vector packed double/float comparison.\n    CMPP,\n\n    // Vector integer comparisons.\n    PCMPEQ,\n    PCMPGT,\n\n    // v8i16 Horizontal minimum and position.\n    PHMINPOS,\n\n    MULTISHIFT,\n\n    /// Vector comparison generating mask bits for fp and\n    /// integer signed and unsigned data types.\n    CMPM,\n    // Vector mask comparison generating mask bits for FP values.\n    CMPMM,\n    // Vector mask comparison with SAE for FP values.\n    CMPMM_SAE,\n\n    // Arithmetic operations with FLAGS results.\n    ADD,\n    SUB,\n    ADC,\n    SBB,\n    SMUL,\n    UMUL,\n    OR,\n    XOR,\n    AND,\n\n    // Bit field extract.\n    BEXTR,\n    BEXTRI,\n\n    // Zero High Bits Starting with Specified Bit Position.\n    BZHI,\n\n    // Parallel extract and deposit.\n    PDEP,\n    PEXT,\n\n    // X86-specific multiply by immediate.\n    MUL_IMM,\n\n    // Vector sign bit extraction.\n    MOVMSK,\n\n    // Vector bitwise comparisons.\n    PTEST,\n\n    // Vector packed fp sign bitwise comparisons.\n    TESTP,\n\n    // OR/AND test for masks.\n    KORTEST,\n    KTEST,\n\n    // ADD for masks.\n    KADD,\n\n    // Several flavors of instructions with vector shuffle behaviors.\n    // Saturated signed/unnsigned packing.\n    PACKSS,\n    PACKUS,\n    // Intra-lane alignr.\n    PALIGNR,\n    // AVX512 inter-lane alignr.\n    VALIGN,\n    PSHUFD,\n    PSHUFHW,\n    PSHUFLW,\n    SHUFP,\n    // VBMI2 Concat & Shift.\n    VSHLD,\n    VSHRD,\n    VSHLDV,\n    VSHRDV,\n    // Shuffle Packed Values at 128-bit granularity.\n    SHUF128,\n    MOVDDUP,\n    MOVSHDUP,\n    MOVSLDUP,\n    MOVLHPS,\n    MOVHLPS,\n    MOVSD,\n    MOVSS,\n    UNPCKL,\n    UNPCKH,\n    VPERMILPV,\n    VPERMILPI,\n    VPERMI,\n    VPERM2X128,\n\n    // Variable Permute (VPERM).\n    // Res = VPERMV MaskV, V0\n    VPERMV,\n\n    // 3-op Variable Permute (VPERMT2).\n    // Res = VPERMV3 V0, MaskV, V1\n    VPERMV3,\n\n    // Bitwise ternary logic.\n    VPTERNLOG,\n    // Fix Up Special Packed Float32/64 values.\n    VFIXUPIMM,\n    VFIXUPIMM_SAE,\n    VFIXUPIMMS,\n    VFIXUPIMMS_SAE,\n    // Range Restriction Calculation For Packed Pairs of Float32/64 values.\n    VRANGE,\n    VRANGE_SAE,\n    VRANGES,\n    VRANGES_SAE,\n    // Reduce - Perform Reduction Transformation on scalar\\packed FP.\n    VREDUCE,\n    VREDUCE_SAE,\n    VREDUCES,\n    VREDUCES_SAE,\n    // RndScale - Round FP Values To Include A Given Number Of Fraction Bits.\n    // Also used by the legacy (V)ROUND intrinsics where we mask out the\n    // scaling part of the immediate.\n    VRNDSCALE,\n    VRNDSCALE_SAE,\n    VRNDSCALES,\n    VRNDSCALES_SAE,\n    // Tests Types Of a FP Values for packed types.\n    VFPCLASS,\n    // Tests Types Of a FP Values for scalar types.\n    VFPCLASSS,\n\n    // Broadcast (splat) scalar or element 0 of a vector. If the operand is\n    // a vector, this node may change the vector length as part of the splat.\n    VBROADCAST,\n    // Broadcast mask to vector.\n    VBROADCASTM,\n\n    /// SSE4A Extraction and Insertion.\n    EXTRQI,\n    INSERTQI,\n\n    // XOP arithmetic/logical shifts.\n    VPSHA,\n    VPSHL,\n    // XOP signed/unsigned integer comparisons.\n    VPCOM,\n    VPCOMU,\n    // XOP packed permute bytes.\n    VPPERM,\n    // XOP two source permutation.\n    VPERMIL2,\n\n    // Vector multiply packed unsigned doubleword integers.\n    PMULUDQ,\n    // Vector multiply packed signed doubleword integers.\n    PMULDQ,\n    // Vector Multiply Packed UnsignedIntegers with Round and Scale.\n    MULHRS,\n\n    // Multiply and Add Packed Integers.\n    VPMADDUBSW,\n    VPMADDWD,\n\n    // AVX512IFMA multiply and add.\n    // NOTE: These are different than the instruction and perform\n    // op0 x op1 + op2.\n    VPMADD52L,\n    VPMADD52H,\n\n    // VNNI\n    VPDPBUSD,\n    VPDPBUSDS,\n    VPDPWSSD,\n    VPDPWSSDS,\n\n    // FMA nodes.\n    // We use the target independent ISD::FMA for the non-inverted case.\n    FNMADD,\n    FMSUB,\n    FNMSUB,\n    FMADDSUB,\n    FMSUBADD,\n\n    // FMA with rounding mode.\n    FMADD_RND,\n    FNMADD_RND,\n    FMSUB_RND,\n    FNMSUB_RND,\n    FMADDSUB_RND,\n    FMSUBADD_RND,\n\n    // Compress and expand.\n    COMPRESS,\n    EXPAND,\n\n    // Bits shuffle\n    VPSHUFBITQMB,\n\n    // Convert Unsigned/Integer to Floating-Point Value with rounding mode.\n    SINT_TO_FP_RND,\n    UINT_TO_FP_RND,\n    SCALAR_SINT_TO_FP,\n    SCALAR_UINT_TO_FP,\n    SCALAR_SINT_TO_FP_RND,\n    SCALAR_UINT_TO_FP_RND,\n\n    // Vector float/double to signed/unsigned integer.\n    CVTP2SI,\n    CVTP2UI,\n    CVTP2SI_RND,\n    CVTP2UI_RND,\n    // Scalar float/double to signed/unsigned integer.\n    CVTS2SI,\n    CVTS2UI,\n    CVTS2SI_RND,\n    CVTS2UI_RND,\n\n    // Vector float/double to signed/unsigned integer with truncation.\n    CVTTP2SI,\n    CVTTP2UI,\n    CVTTP2SI_SAE,\n    CVTTP2UI_SAE,\n    // Scalar float/double to signed/unsigned integer with truncation.\n    CVTTS2SI,\n    CVTTS2UI,\n    CVTTS2SI_SAE,\n    CVTTS2UI_SAE,\n\n    // Vector signed/unsigned integer to float/double.\n    CVTSI2P,\n    CVTUI2P,\n\n    // Masked versions of above. Used for v2f64->v4f32.\n    // SRC, PASSTHRU, MASK\n    MCVTP2SI,\n    MCVTP2UI,\n    MCVTTP2SI,\n    MCVTTP2UI,\n    MCVTSI2P,\n    MCVTUI2P,\n\n    // Vector float to bfloat16.\n    // Convert TWO packed single data to one packed BF16 data\n    CVTNE2PS2BF16,\n    // Convert packed single data to packed BF16 data\n    CVTNEPS2BF16,\n    // Masked version of above.\n    // SRC, PASSTHRU, MASK\n    MCVTNEPS2BF16,\n\n    // Dot product of BF16 pairs to accumulated into\n    // packed single precision.\n    DPBF16PS,\n\n    // Save xmm argument registers to the stack, according to %al. An operator\n    // is needed so that this can be expanded with control flow.\n    VASTART_SAVE_XMM_REGS,\n\n    // Windows's _chkstk call to do stack probing.\n    WIN_ALLOCA,\n\n    // For allocating variable amounts of stack space when using\n    // segmented stacks. Check if the current stacklet has enough space, and\n    // falls back to heap allocation if not.\n    SEG_ALLOCA,\n\n    // For allocating stack space when using stack clash protector.\n    // Allocation is performed by block, and each block is probed.\n    PROBED_ALLOCA,\n\n    // Memory barriers.\n    MEMBARRIER,\n    MFENCE,\n\n    // Get a random integer and indicate whether it is valid in CF.\n    RDRAND,\n\n    // Get a NIST SP800-90B & C compliant random integer and\n    // indicate whether it is valid in CF.\n    RDSEED,\n\n    // Protection keys\n    // RDPKRU - Operand 0 is chain. Operand 1 is value for ECX.\n    // WRPKRU - Operand 0 is chain. Operand 1 is value for EDX. Operand 2 is\n    // value for ECX.\n    RDPKRU,\n    WRPKRU,\n\n    // SSE42 string comparisons.\n    // These nodes produce 3 results, index, mask, and flags. X86ISelDAGToDAG\n    // will emit one or two instructions based on which results are used. If\n    // flags and index/mask this allows us to use a single instruction since\n    // we won't have to pick and opcode for flags. Instead we can rely on the\n    // DAG to CSE everything and decide at isel.\n    PCMPISTR,\n    PCMPESTR,\n\n    // Test if in transactional execution.\n    XTEST,\n\n    // ERI instructions.\n    RSQRT28,\n    RSQRT28_SAE,\n    RSQRT28S,\n    RSQRT28S_SAE,\n    RCP28,\n    RCP28_SAE,\n    RCP28S,\n    RCP28S_SAE,\n    EXP2,\n    EXP2_SAE,\n\n    // Conversions between float and half-float.\n    CVTPS2PH,\n    CVTPH2PS,\n    CVTPH2PS_SAE,\n\n    // Masked version of above.\n    // SRC, RND, PASSTHRU, MASK\n    MCVTPS2PH,\n\n    // Galois Field Arithmetic Instructions\n    GF2P8AFFINEINVQB,\n    GF2P8AFFINEQB,\n    GF2P8MULB,\n\n    // LWP insert record.\n    LWPINS,\n\n    // User level wait\n    UMWAIT,\n    TPAUSE,\n\n    // Enqueue Stores Instructions\n    ENQCMD,\n    ENQCMDS,\n\n    // For avx512-vp2intersect\n    VP2INTERSECT,\n\n    // User level interrupts - testui\n    TESTUI,\n\n    /// X86 strict FP compare instructions.\n    STRICT_FCMP = ISD::FIRST_TARGET_STRICTFP_OPCODE,\n    STRICT_FCMPS,\n\n    // Vector packed double/float comparison.\n    STRICT_CMPP,\n\n    /// Vector comparison generating mask bits for fp and\n    /// integer signed and unsigned data types.\n    STRICT_CMPM,\n\n    // Vector float/double to signed/unsigned integer with truncation.\n    STRICT_CVTTP2SI,\n    STRICT_CVTTP2UI,\n\n    // Vector FP extend.\n    STRICT_VFPEXT,\n\n    // Vector FP round.\n    STRICT_VFPROUND,\n\n    // RndScale - Round FP Values To Include A Given Number Of Fraction Bits.\n    // Also used by the legacy (V)ROUND intrinsics where we mask out the\n    // scaling part of the immediate.\n    STRICT_VRNDSCALE,\n\n    // Vector signed/unsigned integer to float/double.\n    STRICT_CVTSI2P,\n    STRICT_CVTUI2P,\n\n    // Strict FMA nodes.\n    STRICT_FNMADD,\n    STRICT_FMSUB,\n    STRICT_FNMSUB,\n\n    // Conversions between float and half-float.\n    STRICT_CVTPS2PH,\n    STRICT_CVTPH2PS,\n\n    // WARNING: Only add nodes here if they are stric FP nodes. Non-memory and\n    // non-strict FP nodes should be above FIRST_TARGET_STRICTFP_OPCODE.\n\n    // Compare and swap.\n    LCMPXCHG_DAG = ISD::FIRST_TARGET_MEMORY_OPCODE,\n    LCMPXCHG8_DAG,\n    LCMPXCHG16_DAG,\n    LCMPXCHG16_SAVE_RBX_DAG,\n\n    /// LOCK-prefixed arithmetic read-modify-write instructions.\n    /// EFLAGS, OUTCHAIN = LADD(INCHAIN, PTR, RHS)\n    LADD,\n    LSUB,\n    LOR,\n    LXOR,\n    LAND,\n\n    // Load, scalar_to_vector, and zero extend.\n    VZEXT_LOAD,\n\n    // extract_vector_elt, store.\n    VEXTRACT_STORE,\n\n    // scalar broadcast from memory.\n    VBROADCAST_LOAD,\n\n    // subvector broadcast from memory.\n    SUBV_BROADCAST_LOAD,\n\n    // Store FP control world into i16 memory.\n    FNSTCW16m,\n\n    /// This instruction implements FP_TO_SINT with the\n    /// integer destination in memory and a FP reg source.  This corresponds\n    /// to the X86::FIST*m instructions and the rounding mode change stuff. It\n    /// has two inputs (token chain and address) and two outputs (int value\n    /// and token chain). Memory VT specifies the type to store to.\n    FP_TO_INT_IN_MEM,\n\n    /// This instruction implements SINT_TO_FP with the\n    /// integer source in memory and FP reg result.  This corresponds to the\n    /// X86::FILD*m instructions. It has two inputs (token chain and address)\n    /// and two outputs (FP value and token chain). The integer source type is\n    /// specified by the memory VT.\n    FILD,\n\n    /// This instruction implements a fp->int store from FP stack\n    /// slots. This corresponds to the fist instruction. It takes a\n    /// chain operand, value to store, address, and glue. The memory VT\n    /// specifies the type to store as.\n    FIST,\n\n    /// This instruction implements an extending load to FP stack slots.\n    /// This corresponds to the X86::FLD32m / X86::FLD64m. It takes a chain\n    /// operand, and ptr to load from. The memory VT specifies the type to\n    /// load from.\n    FLD,\n\n    /// This instruction implements a truncating store from FP stack\n    /// slots. This corresponds to the X86::FST32m / X86::FST64m. It takes a\n    /// chain operand, value to store, address, and glue. The memory VT\n    /// specifies the type to store as.\n    FST,\n\n    /// These instructions grab the address of the next argument\n    /// from a va_list. (reads and modifies the va_list in memory)\n    VAARG_64,\n    VAARG_X32,\n\n    // Vector truncating store with unsigned/signed saturation\n    VTRUNCSTOREUS,\n    VTRUNCSTORES,\n    // Vector truncating masked store with unsigned/signed saturation\n    VMTRUNCSTOREUS,\n    VMTRUNCSTORES,\n\n    // X86 specific gather and scatter\n    MGATHER,\n    MSCATTER,\n\n    // Key locker nodes that produce flags.\n    AESENC128KL,\n    AESDEC128KL,\n    AESENC256KL,\n    AESDEC256KL,\n    AESENCWIDE128KL,\n    AESDECWIDE128KL,\n    AESENCWIDE256KL,\n    AESDECWIDE256KL,\n\n    // WARNING: Do not add anything in the end unless you want the node to\n    // have memop! In fact, starting from FIRST_TARGET_MEMORY_OPCODE all\n    // opcodes will be thought as target memory ops!\n  };\n  } // end namespace X86ISD\n\n  /// Define some predicates that are used for node matching.\n  namespace X86 {\n    /// Returns true if Elt is a constant zero or floating point constant +0.0.\n    bool isZeroNode(SDValue Elt);\n\n    /// Returns true of the given offset can be\n    /// fit into displacement field of the instruction.\n    bool isOffsetSuitableForCodeModel(int64_t Offset, CodeModel::Model M,\n                                      bool hasSymbolicDisplacement);\n\n    /// Determines whether the callee is required to pop its\n    /// own arguments. Callee pop is necessary to support tail calls.\n    bool isCalleePop(CallingConv::ID CallingConv,\n                     bool is64Bit, bool IsVarArg, bool GuaranteeTCO);\n\n    /// If Op is a constant whose elements are all the same constant or\n    /// undefined, return true and return the constant value in \\p SplatVal.\n    /// If we have undef bits that don't cover an entire element, we treat these\n    /// as zero if AllowPartialUndefs is set, else we fail and return false.\n    bool isConstantSplat(SDValue Op, APInt &SplatVal,\n                         bool AllowPartialUndefs = true);\n  } // end namespace X86\n\n  //===--------------------------------------------------------------------===//\n  //  X86 Implementation of the TargetLowering interface\n  class X86TargetLowering final : public TargetLowering {\n  public:\n    explicit X86TargetLowering(const X86TargetMachine &TM,\n                               const X86Subtarget &STI);\n\n    unsigned getJumpTableEncoding() const override;\n    bool useSoftFloat() const override;\n\n    void markLibCallAttributes(MachineFunction *MF, unsigned CC,\n                               ArgListTy &Args) const override;\n\n    MVT getScalarShiftAmountTy(const DataLayout &, EVT VT) const override {\n      return MVT::i8;\n    }\n\n    const MCExpr *\n    LowerCustomJumpTableEntry(const MachineJumpTableInfo *MJTI,\n                              const MachineBasicBlock *MBB, unsigned uid,\n                              MCContext &Ctx) const override;\n\n    /// Returns relocation base for the given PIC jumptable.\n    SDValue getPICJumpTableRelocBase(SDValue Table,\n                                     SelectionDAG &DAG) const override;\n    const MCExpr *\n    getPICJumpTableRelocBaseExpr(const MachineFunction *MF,\n                                 unsigned JTI, MCContext &Ctx) const override;\n\n    /// Return the desired alignment for ByVal aggregate\n    /// function arguments in the caller parameter area. For X86, aggregates\n    /// that contains are placed at 16-byte boundaries while the rest are at\n    /// 4-byte boundaries.\n    unsigned getByValTypeAlignment(Type *Ty,\n                                   const DataLayout &DL) const override;\n\n    EVT getOptimalMemOpType(const MemOp &Op,\n                            const AttributeList &FuncAttributes) const override;\n\n    /// Returns true if it's safe to use load / store of the\n    /// specified type to expand memcpy / memset inline. This is mostly true\n    /// for all types except for some special cases. For example, on X86\n    /// targets without SSE2 f64 load / store are done with fldl / fstpl which\n    /// also does type conversion. Note the specified type doesn't have to be\n    /// legal as the hook is used before type legalization.\n    bool isSafeMemOpType(MVT VT) const override;\n\n    /// Returns true if the target allows unaligned memory accesses of the\n    /// specified type. Returns whether it is \"fast\" in the last argument.\n    bool allowsMisalignedMemoryAccesses(EVT VT, unsigned AS, unsigned Align,\n                                        MachineMemOperand::Flags Flags,\n                                        bool *Fast) const override;\n\n    /// Provide custom lowering hooks for some operations.\n    ///\n    SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const override;\n\n    /// Replace the results of node with an illegal result\n    /// type with new values built out of custom code.\n    ///\n    void ReplaceNodeResults(SDNode *N, SmallVectorImpl<SDValue>&Results,\n                            SelectionDAG &DAG) const override;\n\n    SDValue PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const override;\n\n    /// Return true if the target has native support for\n    /// the specified value type and it is 'desirable' to use the type for the\n    /// given node type. e.g. On x86 i16 is legal, but undesirable since i16\n    /// instruction encodings are longer and some i16 instructions are slow.\n    bool isTypeDesirableForOp(unsigned Opc, EVT VT) const override;\n\n    /// Return true if the target has native support for the\n    /// specified value type and it is 'desirable' to use the type. e.g. On x86\n    /// i16 is legal, but undesirable since i16 instruction encodings are longer\n    /// and some i16 instructions are slow.\n    bool IsDesirableToPromoteOp(SDValue Op, EVT &PVT) const override;\n\n    /// Return the newly negated expression if the cost is not expensive and\n    /// set the cost in \\p Cost to indicate that if it is cheaper or neutral to\n    /// do the negation.\n    SDValue getNegatedExpression(SDValue Op, SelectionDAG &DAG,\n                                 bool LegalOperations, bool ForCodeSize,\n                                 NegatibleCost &Cost,\n                                 unsigned Depth) const override;\n\n    MachineBasicBlock *\n    EmitInstrWithCustomInserter(MachineInstr &MI,\n                                MachineBasicBlock *MBB) const override;\n\n    /// This method returns the name of a target specific DAG node.\n    const char *getTargetNodeName(unsigned Opcode) const override;\n\n    /// Do not merge vector stores after legalization because that may conflict\n    /// with x86-specific store splitting optimizations.\n    bool mergeStoresAfterLegalization(EVT MemVT) const override {\n      return !MemVT.isVector();\n    }\n\n    bool canMergeStoresTo(unsigned AddressSpace, EVT MemVT,\n                          const SelectionDAG &DAG) const override;\n\n    bool isCheapToSpeculateCttz() const override;\n\n    bool isCheapToSpeculateCtlz() const override;\n\n    bool isCtlzFast() const override;\n\n    bool hasBitPreservingFPLogic(EVT VT) const override {\n      return VT == MVT::f32 || VT == MVT::f64 || VT.isVector();\n    }\n\n    bool isMultiStoresCheaperThanBitsMerge(EVT LTy, EVT HTy) const override {\n      // If the pair to store is a mixture of float and int values, we will\n      // save two bitwise instructions and one float-to-int instruction and\n      // increase one store instruction. There is potentially a more\n      // significant benefit because it avoids the float->int domain switch\n      // for input value. So It is more likely a win.\n      if ((LTy.isFloatingPoint() && HTy.isInteger()) ||\n          (LTy.isInteger() && HTy.isFloatingPoint()))\n        return true;\n      // If the pair only contains int values, we will save two bitwise\n      // instructions and increase one store instruction (costing one more\n      // store buffer). Since the benefit is more blurred so we leave\n      // such pair out until we get testcase to prove it is a win.\n      return false;\n    }\n\n    bool isMaskAndCmp0FoldingBeneficial(const Instruction &AndI) const override;\n\n    bool hasAndNotCompare(SDValue Y) const override;\n\n    bool hasAndNot(SDValue Y) const override;\n\n    bool hasBitTest(SDValue X, SDValue Y) const override;\n\n    bool shouldProduceAndByConstByHoistingConstFromShiftsLHSOfAnd(\n        SDValue X, ConstantSDNode *XC, ConstantSDNode *CC, SDValue Y,\n        unsigned OldShiftOpcode, unsigned NewShiftOpcode,\n        SelectionDAG &DAG) const override;\n\n    bool shouldFoldConstantShiftPairToMask(const SDNode *N,\n                                           CombineLevel Level) const override;\n\n    bool shouldFoldMaskToVariableShiftPair(SDValue Y) const override;\n\n    bool\n    shouldTransformSignedTruncationCheck(EVT XVT,\n                                         unsigned KeptBits) const override {\n      // For vectors, we don't have a preference..\n      if (XVT.isVector())\n        return false;\n\n      auto VTIsOk = [](EVT VT) -> bool {\n        return VT == MVT::i8 || VT == MVT::i16 || VT == MVT::i32 ||\n               VT == MVT::i64;\n      };\n\n      // We are ok with KeptBitsVT being byte/word/dword, what MOVS supports.\n      // XVT will be larger than KeptBitsVT.\n      MVT KeptBitsVT = MVT::getIntegerVT(KeptBits);\n      return VTIsOk(XVT) && VTIsOk(KeptBitsVT);\n    }\n\n    bool shouldExpandShift(SelectionDAG &DAG, SDNode *N) const override;\n\n    bool shouldSplatInsEltVarIndex(EVT VT) const override;\n\n    bool convertSetCCLogicToBitwiseLogic(EVT VT) const override {\n      return VT.isScalarInteger();\n    }\n\n    /// Vector-sized comparisons are fast using PCMPEQ + PMOVMSK or PTEST.\n    MVT hasFastEqualityCompare(unsigned NumBits) const override;\n\n    /// Return the value type to use for ISD::SETCC.\n    EVT getSetCCResultType(const DataLayout &DL, LLVMContext &Context,\n                           EVT VT) const override;\n\n    bool targetShrinkDemandedConstant(SDValue Op, const APInt &DemandedBits,\n                                      const APInt &DemandedElts,\n                                      TargetLoweringOpt &TLO) const override;\n\n    /// Determine which of the bits specified in Mask are known to be either\n    /// zero or one and return them in the KnownZero/KnownOne bitsets.\n    void computeKnownBitsForTargetNode(const SDValue Op,\n                                       KnownBits &Known,\n                                       const APInt &DemandedElts,\n                                       const SelectionDAG &DAG,\n                                       unsigned Depth = 0) const override;\n\n    /// Determine the number of bits in the operation that are sign bits.\n    unsigned ComputeNumSignBitsForTargetNode(SDValue Op,\n                                             const APInt &DemandedElts,\n                                             const SelectionDAG &DAG,\n                                             unsigned Depth) const override;\n\n    bool SimplifyDemandedVectorEltsForTargetNode(SDValue Op,\n                                                 const APInt &DemandedElts,\n                                                 APInt &KnownUndef,\n                                                 APInt &KnownZero,\n                                                 TargetLoweringOpt &TLO,\n                                                 unsigned Depth) const override;\n\n    bool SimplifyDemandedVectorEltsForTargetShuffle(SDValue Op,\n                                                    const APInt &DemandedElts,\n                                                    unsigned MaskIndex,\n                                                    TargetLoweringOpt &TLO,\n                                                    unsigned Depth) const;\n\n    bool SimplifyDemandedBitsForTargetNode(SDValue Op,\n                                           const APInt &DemandedBits,\n                                           const APInt &DemandedElts,\n                                           KnownBits &Known,\n                                           TargetLoweringOpt &TLO,\n                                           unsigned Depth) const override;\n\n    SDValue SimplifyMultipleUseDemandedBitsForTargetNode(\n        SDValue Op, const APInt &DemandedBits, const APInt &DemandedElts,\n        SelectionDAG &DAG, unsigned Depth) const override;\n\n    const Constant *getTargetConstantFromLoad(LoadSDNode *LD) const override;\n\n    SDValue unwrapAddress(SDValue N) const override;\n\n    SDValue getReturnAddressFrameIndex(SelectionDAG &DAG) const;\n\n    bool ExpandInlineAsm(CallInst *CI) const override;\n\n    ConstraintType getConstraintType(StringRef Constraint) const override;\n\n    /// Examine constraint string and operand type and determine a weight value.\n    /// The operand object must already have been set up with the operand type.\n    ConstraintWeight\n      getSingleConstraintMatchWeight(AsmOperandInfo &info,\n                                     const char *constraint) const override;\n\n    const char *LowerXConstraint(EVT ConstraintVT) const override;\n\n    /// Lower the specified operand into the Ops vector. If it is invalid, don't\n    /// add anything to Ops. If hasMemory is true it means one of the asm\n    /// constraint of the inline asm instruction being processed is 'm'.\n    void LowerAsmOperandForConstraint(SDValue Op,\n                                      std::string &Constraint,\n                                      std::vector<SDValue> &Ops,\n                                      SelectionDAG &DAG) const override;\n\n    unsigned\n    getInlineAsmMemConstraint(StringRef ConstraintCode) const override {\n      if (ConstraintCode == \"o\")\n        return InlineAsm::Constraint_o;\n      else if (ConstraintCode == \"v\")\n        return InlineAsm::Constraint_v;\n      else if (ConstraintCode == \"X\")\n        return InlineAsm::Constraint_X;\n      return TargetLowering::getInlineAsmMemConstraint(ConstraintCode);\n    }\n\n    /// Handle Lowering flag assembly outputs.\n    SDValue LowerAsmOutputForConstraint(SDValue &Chain, SDValue &Flag,\n                                        const SDLoc &DL,\n                                        const AsmOperandInfo &Constraint,\n                                        SelectionDAG &DAG) const override;\n\n    /// Given a physical register constraint\n    /// (e.g. {edx}), return the register number and the register class for the\n    /// register.  This should only be used for C_Register constraints.  On\n    /// error, this returns a register number of 0.\n    std::pair<unsigned, const TargetRegisterClass *>\n    getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,\n                                 StringRef Constraint, MVT VT) const override;\n\n    /// Return true if the addressing mode represented\n    /// by AM is legal for this target, for a load/store of the specified type.\n    bool isLegalAddressingMode(const DataLayout &DL, const AddrMode &AM,\n                               Type *Ty, unsigned AS,\n                               Instruction *I = nullptr) const override;\n\n    /// Return true if the specified immediate is legal\n    /// icmp immediate, that is the target has icmp instructions which can\n    /// compare a register against the immediate without having to materialize\n    /// the immediate into a register.\n    bool isLegalICmpImmediate(int64_t Imm) const override;\n\n    /// Return true if the specified immediate is legal\n    /// add immediate, that is the target has add instructions which can\n    /// add a register and the immediate without having to materialize\n    /// the immediate into a register.\n    bool isLegalAddImmediate(int64_t Imm) const override;\n\n    bool isLegalStoreImmediate(int64_t Imm) const override;\n\n    /// Return the cost of the scaling factor used in the addressing\n    /// mode represented by AM for this target, for a load/store\n    /// of the specified type.\n    /// If the AM is supported, the return value must be >= 0.\n    /// If the AM is not supported, it returns a negative value.\n    int getScalingFactorCost(const DataLayout &DL, const AddrMode &AM, Type *Ty,\n                             unsigned AS) const override;\n\n    /// This is used to enable splatted operand transforms for vector shifts\n    /// and vector funnel shifts.\n    bool isVectorShiftByScalarCheap(Type *Ty) const override;\n\n    /// Add x86-specific opcodes to the default list.\n    bool isBinOp(unsigned Opcode) const override;\n\n    /// Returns true if the opcode is a commutative binary operation.\n    bool isCommutativeBinOp(unsigned Opcode) const override;\n\n    /// Return true if it's free to truncate a value of\n    /// type Ty1 to type Ty2. e.g. On x86 it's free to truncate a i32 value in\n    /// register EAX to i16 by referencing its sub-register AX.\n    bool isTruncateFree(Type *Ty1, Type *Ty2) const override;\n    bool isTruncateFree(EVT VT1, EVT VT2) const override;\n\n    bool allowTruncateForTailCall(Type *Ty1, Type *Ty2) const override;\n\n    /// Return true if any actual instruction that defines a\n    /// value of type Ty1 implicit zero-extends the value to Ty2 in the result\n    /// register. This does not necessarily include registers defined in\n    /// unknown ways, such as incoming arguments, or copies from unknown\n    /// virtual registers. Also, if isTruncateFree(Ty2, Ty1) is true, this\n    /// does not necessarily apply to truncate instructions. e.g. on x86-64,\n    /// all instructions that define 32-bit values implicit zero-extend the\n    /// result out to 64 bits.\n    bool isZExtFree(Type *Ty1, Type *Ty2) const override;\n    bool isZExtFree(EVT VT1, EVT VT2) const override;\n    bool isZExtFree(SDValue Val, EVT VT2) const override;\n\n    bool shouldSinkOperands(Instruction *I,\n                            SmallVectorImpl<Use *> &Ops) const override;\n    bool shouldConvertPhiType(Type *From, Type *To) const override;\n\n    /// Return true if folding a vector load into ExtVal (a sign, zero, or any\n    /// extend node) is profitable.\n    bool isVectorLoadExtDesirable(SDValue) const override;\n\n    /// Return true if an FMA operation is faster than a pair of fmul and fadd\n    /// instructions. fmuladd intrinsics will be expanded to FMAs when this\n    /// method returns true, otherwise fmuladd is expanded to fmul + fadd.\n    bool isFMAFasterThanFMulAndFAdd(const MachineFunction &MF,\n                                    EVT VT) const override;\n\n    /// Return true if it's profitable to narrow\n    /// operations of type VT1 to VT2. e.g. on x86, it's profitable to narrow\n    /// from i32 to i8 but not from i32 to i16.\n    bool isNarrowingProfitable(EVT VT1, EVT VT2) const override;\n\n    /// Given an intrinsic, checks if on the target the intrinsic will need to map\n    /// to a MemIntrinsicNode (touches memory). If this is the case, it returns\n    /// true and stores the intrinsic information into the IntrinsicInfo that was\n    /// passed to the function.\n    bool getTgtMemIntrinsic(IntrinsicInfo &Info, const CallInst &I,\n                            MachineFunction &MF,\n                            unsigned Intrinsic) const override;\n\n    /// Returns true if the target can instruction select the\n    /// specified FP immediate natively. If false, the legalizer will\n    /// materialize the FP immediate as a load from a constant pool.\n    bool isFPImmLegal(const APFloat &Imm, EVT VT,\n                      bool ForCodeSize) const override;\n\n    /// Targets can use this to indicate that they only support *some*\n    /// VECTOR_SHUFFLE operations, those with specific masks. By default, if a\n    /// target supports the VECTOR_SHUFFLE node, all mask values are assumed to\n    /// be legal.\n    bool isShuffleMaskLegal(ArrayRef<int> Mask, EVT VT) const override;\n\n    /// Similar to isShuffleMaskLegal. Targets can use this to indicate if there\n    /// is a suitable VECTOR_SHUFFLE that can be used to replace a VAND with a\n    /// constant pool entry.\n    bool isVectorClearMaskLegal(ArrayRef<int> Mask, EVT VT) const override;\n\n    /// Returns true if lowering to a jump table is allowed.\n    bool areJTsAllowed(const Function *Fn) const override;\n\n    /// If true, then instruction selection should\n    /// seek to shrink the FP constant of the specified type to a smaller type\n    /// in order to save space and / or reduce runtime.\n    bool ShouldShrinkFPConstant(EVT VT) const override {\n      // Don't shrink FP constpool if SSE2 is available since cvtss2sd is more\n      // expensive than a straight movsd. On the other hand, it's important to\n      // shrink long double fp constant since fldt is very slow.\n      return !X86ScalarSSEf64 || VT == MVT::f80;\n    }\n\n    /// Return true if we believe it is correct and profitable to reduce the\n    /// load node to a smaller type.\n    bool shouldReduceLoadWidth(SDNode *Load, ISD::LoadExtType ExtTy,\n                               EVT NewVT) const override;\n\n    /// Return true if the specified scalar FP type is computed in an SSE\n    /// register, not on the X87 floating point stack.\n    bool isScalarFPTypeInSSEReg(EVT VT) const {\n      return (VT == MVT::f64 && X86ScalarSSEf64) || // f64 is when SSE2\n             (VT == MVT::f32 && X86ScalarSSEf32);   // f32 is when SSE1\n    }\n\n    /// Returns true if it is beneficial to convert a load of a constant\n    /// to just the constant itself.\n    bool shouldConvertConstantLoadToIntImm(const APInt &Imm,\n                                           Type *Ty) const override;\n\n    bool reduceSelectOfFPConstantLoads(EVT CmpOpVT) const override;\n\n    bool convertSelectOfConstantsToMath(EVT VT) const override;\n\n    bool decomposeMulByConstant(LLVMContext &Context, EVT VT,\n                                SDValue C) const override;\n\n    /// Return true if EXTRACT_SUBVECTOR is cheap for this result type\n    /// with this index.\n    bool isExtractSubvectorCheap(EVT ResVT, EVT SrcVT,\n                                 unsigned Index) const override;\n\n    /// Scalar ops always have equal or better analysis/performance/power than\n    /// the vector equivalent, so this always makes sense if the scalar op is\n    /// supported.\n    bool shouldScalarizeBinop(SDValue) const override;\n\n    /// Extract of a scalar FP value from index 0 of a vector is free.\n    bool isExtractVecEltCheap(EVT VT, unsigned Index) const override {\n      EVT EltVT = VT.getScalarType();\n      return (EltVT == MVT::f32 || EltVT == MVT::f64) && Index == 0;\n    }\n\n    /// Overflow nodes should get combined/lowered to optimal instructions\n    /// (they should allow eliminating explicit compares by getting flags from\n    /// math ops).\n    bool shouldFormOverflowOp(unsigned Opcode, EVT VT,\n                              bool MathUsed) const override;\n\n    bool storeOfVectorConstantIsCheap(EVT MemVT, unsigned NumElem,\n                                      unsigned AddrSpace) const override {\n      // If we can replace more than 2 scalar stores, there will be a reduction\n      // in instructions even after we add a vector constant load.\n      return NumElem > 2;\n    }\n\n    bool isLoadBitCastBeneficial(EVT LoadVT, EVT BitcastVT,\n                                 const SelectionDAG &DAG,\n                                 const MachineMemOperand &MMO) const override;\n\n    /// Intel processors have a unified instruction and data cache\n    const char * getClearCacheBuiltinName() const override {\n      return nullptr; // nothing to do, move along.\n    }\n\n    Register getRegisterByName(const char* RegName, LLT VT,\n                               const MachineFunction &MF) const override;\n\n    /// If a physical register, this returns the register that receives the\n    /// exception address on entry to an EH pad.\n    Register\n    getExceptionPointerRegister(const Constant *PersonalityFn) const override;\n\n    /// If a physical register, this returns the register that receives the\n    /// exception typeid on entry to a landing pad.\n    Register\n    getExceptionSelectorRegister(const Constant *PersonalityFn) const override;\n\n    virtual bool needsFixedCatchObjects() const override;\n\n    /// This method returns a target specific FastISel object,\n    /// or null if the target does not support \"fast\" ISel.\n    FastISel *createFastISel(FunctionLoweringInfo &funcInfo,\n                             const TargetLibraryInfo *libInfo) const override;\n\n    /// If the target has a standard location for the stack protector cookie,\n    /// returns the address of that location. Otherwise, returns nullptr.\n    Value *getIRStackGuard(IRBuilder<> &IRB) const override;\n\n    bool useLoadStackGuardNode() const override;\n    bool useStackGuardXorFP() const override;\n    void insertSSPDeclarations(Module &M) const override;\n    Value *getSDagStackGuard(const Module &M) const override;\n    Function *getSSPStackGuardCheck(const Module &M) const override;\n    SDValue emitStackGuardXorFP(SelectionDAG &DAG, SDValue Val,\n                                const SDLoc &DL) const override;\n\n\n    /// Return true if the target stores SafeStack pointer at a fixed offset in\n    /// some non-standard address space, and populates the address space and\n    /// offset as appropriate.\n    Value *getSafeStackPointerLocation(IRBuilder<> &IRB) const override;\n\n    std::pair<SDValue, SDValue> BuildFILD(EVT DstVT, EVT SrcVT, const SDLoc &DL,\n                                          SDValue Chain, SDValue Pointer,\n                                          MachinePointerInfo PtrInfo,\n                                          Align Alignment,\n                                          SelectionDAG &DAG) const;\n\n    /// Customize the preferred legalization strategy for certain types.\n    LegalizeTypeAction getPreferredVectorAction(MVT VT) const override;\n\n    bool softPromoteHalfType() const override { return true; }\n\n    MVT getRegisterTypeForCallingConv(LLVMContext &Context, CallingConv::ID CC,\n                                      EVT VT) const override;\n\n    unsigned getNumRegistersForCallingConv(LLVMContext &Context,\n                                           CallingConv::ID CC,\n                                           EVT VT) const override;\n\n    unsigned getVectorTypeBreakdownForCallingConv(\n        LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,\n        unsigned &NumIntermediates, MVT &RegisterVT) const override;\n\n    bool isIntDivCheap(EVT VT, AttributeList Attr) const override;\n\n    bool supportSwiftError() const override;\n\n    bool hasStackProbeSymbol(MachineFunction &MF) const override;\n    bool hasInlineStackProbe(MachineFunction &MF) const override;\n    StringRef getStackProbeSymbolName(MachineFunction &MF) const override;\n\n    unsigned getStackProbeSize(MachineFunction &MF) const;\n\n    bool hasVectorBlend() const override { return true; }\n\n    unsigned getMaxSupportedInterleaveFactor() const override { return 4; }\n\n    /// Lower interleaved load(s) into target specific\n    /// instructions/intrinsics.\n    bool lowerInterleavedLoad(LoadInst *LI,\n                              ArrayRef<ShuffleVectorInst *> Shuffles,\n                              ArrayRef<unsigned> Indices,\n                              unsigned Factor) const override;\n\n    /// Lower interleaved store(s) into target specific\n    /// instructions/intrinsics.\n    bool lowerInterleavedStore(StoreInst *SI, ShuffleVectorInst *SVI,\n                               unsigned Factor) const override;\n\n    SDValue expandIndirectJTBranch(const SDLoc& dl, SDValue Value,\n                                   SDValue Addr, SelectionDAG &DAG)\n                                   const override;\n\n    Align getPrefLoopAlignment(MachineLoop *ML) const override;\n\n  protected:\n    std::pair<const TargetRegisterClass *, uint8_t>\n    findRepresentativeClass(const TargetRegisterInfo *TRI,\n                            MVT VT) const override;\n\n  private:\n    /// Keep a reference to the X86Subtarget around so that we can\n    /// make the right decision when generating code for different targets.\n    const X86Subtarget &Subtarget;\n\n    /// Select between SSE or x87 floating point ops.\n    /// When SSE is available, use it for f32 operations.\n    /// When SSE2 is available, use it for f64 operations.\n    bool X86ScalarSSEf32;\n    bool X86ScalarSSEf64;\n\n    /// A list of legal FP immediates.\n    std::vector<APFloat> LegalFPImmediates;\n\n    /// Indicate that this x86 target can instruction\n    /// select the specified FP immediate natively.\n    void addLegalFPImmediate(const APFloat& Imm) {\n      LegalFPImmediates.push_back(Imm);\n    }\n\n    SDValue LowerCallResult(SDValue Chain, SDValue InFlag,\n                            CallingConv::ID CallConv, bool isVarArg,\n                            const SmallVectorImpl<ISD::InputArg> &Ins,\n                            const SDLoc &dl, SelectionDAG &DAG,\n                            SmallVectorImpl<SDValue> &InVals,\n                            uint32_t *RegMask) const;\n    SDValue LowerMemArgument(SDValue Chain, CallingConv::ID CallConv,\n                             const SmallVectorImpl<ISD::InputArg> &ArgInfo,\n                             const SDLoc &dl, SelectionDAG &DAG,\n                             const CCValAssign &VA, MachineFrameInfo &MFI,\n                             unsigned i) const;\n    SDValue LowerMemOpCallTo(SDValue Chain, SDValue StackPtr, SDValue Arg,\n                             const SDLoc &dl, SelectionDAG &DAG,\n                             const CCValAssign &VA,\n                             ISD::ArgFlagsTy Flags, bool isByval) const;\n\n    // Call lowering helpers.\n\n    /// Check whether the call is eligible for tail call optimization. Targets\n    /// that want to do tail call optimization should implement this function.\n    bool IsEligibleForTailCallOptimization(SDValue Callee,\n                                           CallingConv::ID CalleeCC,\n                                           bool isVarArg,\n                                           bool isCalleeStructRet,\n                                           bool isCallerStructRet,\n                                           Type *RetTy,\n                                    const SmallVectorImpl<ISD::OutputArg> &Outs,\n                                    const SmallVectorImpl<SDValue> &OutVals,\n                                    const SmallVectorImpl<ISD::InputArg> &Ins,\n                                           SelectionDAG& DAG) const;\n    SDValue EmitTailCallLoadRetAddr(SelectionDAG &DAG, SDValue &OutRetAddr,\n                                    SDValue Chain, bool IsTailCall,\n                                    bool Is64Bit, int FPDiff,\n                                    const SDLoc &dl) const;\n\n    unsigned GetAlignedArgumentStackSize(unsigned StackSize,\n                                         SelectionDAG &DAG) const;\n\n    unsigned getAddressSpace(void) const;\n\n    SDValue FP_TO_INTHelper(SDValue Op, SelectionDAG &DAG, bool IsSigned,\n                            SDValue &Chain) const;\n    SDValue LRINT_LLRINTHelper(SDNode *N, SelectionDAG &DAG) const;\n\n    SDValue LowerBUILD_VECTOR(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerVSELECT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerEXTRACT_VECTOR_ELT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerINSERT_VECTOR_ELT(SDValue Op, SelectionDAG &DAG) const;\n\n    unsigned getGlobalWrapperKind(const GlobalValue *GV = nullptr,\n                                  const unsigned char OpFlags = 0) const;\n    SDValue LowerConstantPool(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerBlockAddress(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerGlobalAddress(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerExternalSymbol(SDValue Op, SelectionDAG &DAG) const;\n\n    /// Creates target global address or external symbol nodes for calls or\n    /// other uses.\n    SDValue LowerGlobalOrExternal(SDValue Op, SelectionDAG &DAG,\n                                  bool ForCall) const;\n\n    SDValue LowerSINT_TO_FP(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerUINT_TO_FP(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerTRUNCATE(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFP_TO_INT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFP_TO_INT_SAT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerLRINT_LLRINT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerSETCC(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerSETCCCARRY(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerSELECT(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerBRCOND(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerJumpTable(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerDYNAMIC_STACKALLOC(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerVASTART(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerVAARG(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerADDROFRETURNADDR(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFRAME_TO_ARGS_OFFSET(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const;\n    SDValue lowerEH_SJLJ_SETJMP(SDValue Op, SelectionDAG &DAG) const;\n    SDValue lowerEH_SJLJ_LONGJMP(SDValue Op, SelectionDAG &DAG) const;\n    SDValue lowerEH_SJLJ_SETUP_DISPATCH(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerINIT_TRAMPOLINE(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFLT_ROUNDS_(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerWin64_i128OP(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerGC_TRANSITION(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerINTRINSIC_WO_CHAIN(SDValue Op, SelectionDAG &DAG) const;\n    SDValue lowerFaddFsub(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFP_EXTEND(SDValue Op, SelectionDAG &DAG) const;\n    SDValue LowerFP_ROUND(SDValue Op, SelectionDAG &DAG) const;\n\n    SDValue\n    LowerFormalArguments(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,\n                         const SmallVectorImpl<ISD::InputArg> &Ins,\n                         const SDLoc &dl, SelectionDAG &DAG,\n                         SmallVectorImpl<SDValue> &InVals) const override;\n    SDValue LowerCall(CallLoweringInfo &CLI,\n                      SmallVectorImpl<SDValue> &InVals) const override;\n\n    SDValue LowerReturn(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,\n                        const SmallVectorImpl<ISD::OutputArg> &Outs,\n                        const SmallVectorImpl<SDValue> &OutVals,\n                        const SDLoc &dl, SelectionDAG &DAG) const override;\n\n    bool supportSplitCSR(MachineFunction *MF) const override {\n      return MF->getFunction().getCallingConv() == CallingConv::CXX_FAST_TLS &&\n          MF->getFunction().hasFnAttribute(Attribute::NoUnwind);\n    }\n    void initializeSplitCSR(MachineBasicBlock *Entry) const override;\n    void insertCopiesSplitCSR(\n      MachineBasicBlock *Entry,\n      const SmallVectorImpl<MachineBasicBlock *> &Exits) const override;\n\n    bool isUsedByReturnOnly(SDNode *N, SDValue &Chain) const override;\n\n    bool mayBeEmittedAsTailCall(const CallInst *CI) const override;\n\n    EVT getTypeForExtReturn(LLVMContext &Context, EVT VT,\n                            ISD::NodeType ExtendKind) const override;\n\n    bool CanLowerReturn(CallingConv::ID CallConv, MachineFunction &MF,\n                        bool isVarArg,\n                        const SmallVectorImpl<ISD::OutputArg> &Outs,\n                        LLVMContext &Context) const override;\n\n    const MCPhysReg *getScratchRegisters(CallingConv::ID CC) const override;\n\n    TargetLoweringBase::AtomicExpansionKind\n    shouldExpandAtomicLoadInIR(LoadInst *LI) const override;\n    bool shouldExpandAtomicStoreInIR(StoreInst *SI) const override;\n    TargetLoweringBase::AtomicExpansionKind\n    shouldExpandAtomicRMWInIR(AtomicRMWInst *AI) const override;\n\n    LoadInst *\n    lowerIdempotentRMWIntoFencedLoad(AtomicRMWInst *AI) const override;\n\n    bool lowerAtomicStoreAsStoreSDNode(const StoreInst &SI) const override;\n    bool lowerAtomicLoadAsLoadSDNode(const LoadInst &LI) const override;\n\n    bool needsCmpXchgNb(Type *MemType) const;\n\n    void SetupEntryBlockForSjLj(MachineInstr &MI, MachineBasicBlock *MBB,\n                                MachineBasicBlock *DispatchBB, int FI) const;\n\n    // Utility function to emit the low-level va_arg code for X86-64.\n    MachineBasicBlock *\n    EmitVAARGWithCustomInserter(MachineInstr &MI, MachineBasicBlock *MBB) const;\n\n    /// Utility function to emit the xmm reg save portion of va_start.\n    MachineBasicBlock *\n    EmitVAStartSaveXMMRegsWithCustomInserter(MachineInstr &BInstr,\n                                             MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredCascadedSelect(MachineInstr &MI1,\n                                                 MachineInstr &MI2,\n                                                 MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredSelect(MachineInstr &I,\n                                         MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredCatchRet(MachineInstr &MI,\n                                           MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredSegAlloca(MachineInstr &MI,\n                                            MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredProbedAlloca(MachineInstr &MI,\n                                               MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredTLSAddr(MachineInstr &MI,\n                                          MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredTLSCall(MachineInstr &MI,\n                                          MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *EmitLoweredIndirectThunk(MachineInstr &MI,\n                                                MachineBasicBlock *BB) const;\n\n    MachineBasicBlock *emitEHSjLjSetJmp(MachineInstr &MI,\n                                        MachineBasicBlock *MBB) const;\n\n    void emitSetJmpShadowStackFix(MachineInstr &MI,\n                                  MachineBasicBlock *MBB) const;\n\n    MachineBasicBlock *emitEHSjLjLongJmp(MachineInstr &MI,\n                                         MachineBasicBlock *MBB) const;\n\n    MachineBasicBlock *emitLongJmpShadowStackFix(MachineInstr &MI,\n                                                 MachineBasicBlock *MBB) const;\n\n    MachineBasicBlock *EmitSjLjDispatchBlock(MachineInstr &MI,\n                                             MachineBasicBlock *MBB) const;\n\n    /// Emit flags for the given setcc condition and operands. Also returns the\n    /// corresponding X86 condition code constant in X86CC.\n    SDValue emitFlagsForSetcc(SDValue Op0, SDValue Op1, ISD::CondCode CC,\n                              const SDLoc &dl, SelectionDAG &DAG,\n                              SDValue &X86CC) const;\n\n    /// Check if replacement of SQRT with RSQRT should be disabled.\n    bool isFsqrtCheap(SDValue Op, SelectionDAG &DAG) const override;\n\n    /// Use rsqrt* to speed up sqrt calculations.\n    SDValue getSqrtEstimate(SDValue Op, SelectionDAG &DAG, int Enabled,\n                            int &RefinementSteps, bool &UseOneConstNR,\n                            bool Reciprocal) const override;\n\n    /// Use rcp* to speed up fdiv calculations.\n    SDValue getRecipEstimate(SDValue Op, SelectionDAG &DAG, int Enabled,\n                             int &RefinementSteps) const override;\n\n    /// Reassociate floating point divisions into multiply by reciprocal.\n    unsigned combineRepeatedFPDivisors() const override;\n\n    SDValue BuildSDIVPow2(SDNode *N, const APInt &Divisor, SelectionDAG &DAG,\n                          SmallVectorImpl<SDNode *> &Created) const override;\n  };\n\n  namespace X86 {\n    FastISel *createFastISel(FunctionLoweringInfo &funcInfo,\n                             const TargetLibraryInfo *libInfo);\n  } // end namespace X86\n\n  // X86 specific Gather/Scatter nodes.\n  // The class has the same order of operands as MaskedGatherScatterSDNode for\n  // convenience.\n  class X86MaskedGatherScatterSDNode : public MemIntrinsicSDNode {\n  public:\n    // This is a intended as a utility and should never be directly created.\n    X86MaskedGatherScatterSDNode() = delete;\n    ~X86MaskedGatherScatterSDNode() = delete;\n\n    const SDValue &getBasePtr() const { return getOperand(3); }\n    const SDValue &getIndex()   const { return getOperand(4); }\n    const SDValue &getMask()    const { return getOperand(2); }\n    const SDValue &getScale()   const { return getOperand(5); }\n\n    static bool classof(const SDNode *N) {\n      return N->getOpcode() == X86ISD::MGATHER ||\n             N->getOpcode() == X86ISD::MSCATTER;\n    }\n  };\n\n  class X86MaskedGatherSDNode : public X86MaskedGatherScatterSDNode {\n  public:\n    const SDValue &getPassThru() const { return getOperand(1); }\n\n    static bool classof(const SDNode *N) {\n      return N->getOpcode() == X86ISD::MGATHER;\n    }\n  };\n\n  class X86MaskedScatterSDNode : public X86MaskedGatherScatterSDNode {\n  public:\n    const SDValue &getValue() const { return getOperand(1); }\n\n    static bool classof(const SDNode *N) {\n      return N->getOpcode() == X86ISD::MSCATTER;\n    }\n  };\n\n  /// Generate unpacklo/unpackhi shuffle mask.\n  void createUnpackShuffleMask(EVT VT, SmallVectorImpl<int> &Mask, bool Lo,\n                               bool Unary);\n\n  /// Similar to unpacklo/unpackhi, but without the 128-bit lane limitation\n  /// imposed by AVX and specific to the unary pattern. Example:\n  /// v8iX Lo --> <0, 0, 1, 1, 2, 2, 3, 3>\n  /// v8iX Hi --> <4, 4, 5, 5, 6, 6, 7, 7>\n  void createSplat2ShuffleMask(MVT VT, SmallVectorImpl<int> &Mask, bool Lo);\n\n} // end namespace llvm\n\n#endif // LLVM_LIB_TARGET_X86_X86ISELLOWERING_H\n"}}, "reports": [{"events": [{"location": {"col": 28, "file": 4, "line": 51005}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 3, "line": 1131}, "message": "differing parameters are named here: ('Constraint'), in definition: ('OpInfo')"}, {"location": {"col": 13, "file": 3, "line": 1131}, "message": "function 'llvm::X86TargetLowering::LowerAsmOutputForConstraint' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h", "reportHash": "f8b8cd75739fe3cb8905f3a5d1985731", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 20, "file": 4, "line": 3227}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 3, "line": 1444}, "message": "differing parameters are named here: ('ArgInfo'), in definition: ('Ins')"}, {"location": {"col": 13, "file": 3, "line": 1444}, "message": "function 'llvm::X86TargetLowering::LowerMemArgument' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h", "reportHash": "5770f1370c7c7250c54b29286c858b3c", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 39, "file": 4, "line": 32043}, "message": "the definition seen here"}, {"location": {"col": 5, "file": 3, "line": 1590}, "message": "differing parameters are named here: ('BInstr'), in definition: ('MI')"}, {"location": {"col": 5, "file": 3, "line": 1590}, "message": "function 'llvm::X86TargetLowering::EmitVAStartSaveXMMRegsWithCustomInserter' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h", "reportHash": "ab3fce706454d00765edd42025ca863b", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 20, "file": 4, "line": 32229}, "message": "the definition seen here"}, {"location": {"col": 24, "file": 3, "line": 1593}, "message": "differing parameters are named here: ('MI1', 'MI2'), in definition: ('FirstCMOV', 'SecondCascadedCMOV')"}, {"location": {"col": 24, "file": 3, "line": 1593}, "message": "function 'llvm::X86TargetLowering::EmitLoweredCascadedSelect' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h", "reportHash": "66d58d3ddff95e6efd66d8d3a2cf2236", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
