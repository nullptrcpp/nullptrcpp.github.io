<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"4": {"id": 4, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp", "content": "//===-- X86InstrInfo.cpp - X86 Instruction Information --------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the X86 implementation of the TargetInstrInfo class.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"X86InstrInfo.h\"\n#include \"X86.h\"\n#include \"X86InstrBuilder.h\"\n#include \"X86InstrFoldTables.h\"\n#include \"X86MachineFunctionInfo.h\"\n#include \"X86Subtarget.h\"\n#include \"X86TargetMachine.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/Sequence.h\"\n#include \"llvm/CodeGen/LivePhysRegs.h\"\n#include \"llvm/CodeGen/LiveVariables.h\"\n#include \"llvm/CodeGen/MachineConstantPool.h\"\n#include \"llvm/CodeGen/MachineDominators.h\"\n#include \"llvm/CodeGen/MachineFrameInfo.h\"\n#include \"llvm/CodeGen/MachineInstrBuilder.h\"\n#include \"llvm/CodeGen/MachineModuleInfo.h\"\n#include \"llvm/CodeGen/MachineRegisterInfo.h\"\n#include \"llvm/CodeGen/StackMaps.h\"\n#include \"llvm/IR/DebugInfoMetadata.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/MC/MCAsmInfo.h\"\n#include \"llvm/MC/MCExpr.h\"\n#include \"llvm/MC/MCInst.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"llvm/Target/TargetOptions.h\"\n\nusing namespace llvm;\n\n#define DEBUG_TYPE \"x86-instr-info\"\n\n#define GET_INSTRINFO_CTOR_DTOR\n#include \"X86GenInstrInfo.inc\"\n\nstatic cl::opt<bool>\n    NoFusing(\"disable-spill-fusing\",\n             cl::desc(\"Disable fusing of spill code into instructions\"),\n             cl::Hidden);\nstatic cl::opt<bool>\nPrintFailedFusing(\"print-failed-fuse-candidates\",\n                  cl::desc(\"Print instructions that the allocator wants to\"\n                           \" fuse, but the X86 backend currently can't\"),\n                  cl::Hidden);\nstatic cl::opt<bool>\nReMatPICStubLoad(\"remat-pic-stub-load\",\n                 cl::desc(\"Re-materialize load from stub in PIC mode\"),\n                 cl::init(false), cl::Hidden);\nstatic cl::opt<unsigned>\nPartialRegUpdateClearance(\"partial-reg-update-clearance\",\n                          cl::desc(\"Clearance between two register writes \"\n                                   \"for inserting XOR to avoid partial \"\n                                   \"register update\"),\n                          cl::init(64), cl::Hidden);\nstatic cl::opt<unsigned>\nUndefRegClearance(\"undef-reg-clearance\",\n                  cl::desc(\"How many idle instructions we would like before \"\n                           \"certain undef register reads\"),\n                  cl::init(128), cl::Hidden);\n\n\n// Pin the vtable to this file.\nvoid X86InstrInfo::anchor() {}\n\nX86InstrInfo::X86InstrInfo(X86Subtarget &STI)\n    : X86GenInstrInfo((STI.isTarget64BitLP64() ? X86::ADJCALLSTACKDOWN64\n                                               : X86::ADJCALLSTACKDOWN32),\n                      (STI.isTarget64BitLP64() ? X86::ADJCALLSTACKUP64\n                                               : X86::ADJCALLSTACKUP32),\n                      X86::CATCHRET,\n                      (STI.is64Bit() ? X86::RETQ : X86::RETL)),\n      Subtarget(STI), RI(STI.getTargetTriple()) {\n}\n\nbool\nX86InstrInfo::isCoalescableExtInstr(const MachineInstr &MI,\n                                    Register &SrcReg, Register &DstReg,\n                                    unsigned &SubIdx) const {\n  switch (MI.getOpcode()) {\n  default: break;\n  case X86::MOVSX16rr8:\n  case X86::MOVZX16rr8:\n  case X86::MOVSX32rr8:\n  case X86::MOVZX32rr8:\n  case X86::MOVSX64rr8:\n    if (!Subtarget.is64Bit())\n      // It's not always legal to reference the low 8-bit of the larger\n      // register in 32-bit mode.\n      return false;\n    LLVM_FALLTHROUGH;\n  case X86::MOVSX32rr16:\n  case X86::MOVZX32rr16:\n  case X86::MOVSX64rr16:\n  case X86::MOVSX64rr32: {\n    if (MI.getOperand(0).getSubReg() || MI.getOperand(1).getSubReg())\n      // Be conservative.\n      return false;\n    SrcReg = MI.getOperand(1).getReg();\n    DstReg = MI.getOperand(0).getReg();\n    switch (MI.getOpcode()) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::MOVSX16rr8:\n    case X86::MOVZX16rr8:\n    case X86::MOVSX32rr8:\n    case X86::MOVZX32rr8:\n    case X86::MOVSX64rr8:\n      SubIdx = X86::sub_8bit;\n      break;\n    case X86::MOVSX32rr16:\n    case X86::MOVZX32rr16:\n    case X86::MOVSX64rr16:\n      SubIdx = X86::sub_16bit;\n      break;\n    case X86::MOVSX64rr32:\n      SubIdx = X86::sub_32bit;\n      break;\n    }\n    return true;\n  }\n  }\n  return false;\n}\n\nbool X86InstrInfo::isDataInvariant(MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default:\n    // By default, assume that the instruction is not data invariant.\n    return false;\n\n    // Some target-independent operations that trivially lower to data-invariant\n    // instructions.\n  case TargetOpcode::COPY:\n  case TargetOpcode::INSERT_SUBREG:\n  case TargetOpcode::SUBREG_TO_REG:\n    return true;\n\n  // On x86 it is believed that imul is constant time w.r.t. the loaded data.\n  // However, they set flags and are perhaps the most surprisingly constant\n  // time operations so we call them out here separately.\n  case X86::IMUL16rr:\n  case X86::IMUL16rri8:\n  case X86::IMUL16rri:\n  case X86::IMUL32rr:\n  case X86::IMUL32rri8:\n  case X86::IMUL32rri:\n  case X86::IMUL64rr:\n  case X86::IMUL64rri32:\n  case X86::IMUL64rri8:\n\n  // Bit scanning and counting instructions that are somewhat surprisingly\n  // constant time as they scan across bits and do other fairly complex\n  // operations like popcnt, but are believed to be constant time on x86.\n  // However, these set flags.\n  case X86::BSF16rr:\n  case X86::BSF32rr:\n  case X86::BSF64rr:\n  case X86::BSR16rr:\n  case X86::BSR32rr:\n  case X86::BSR64rr:\n  case X86::LZCNT16rr:\n  case X86::LZCNT32rr:\n  case X86::LZCNT64rr:\n  case X86::POPCNT16rr:\n  case X86::POPCNT32rr:\n  case X86::POPCNT64rr:\n  case X86::TZCNT16rr:\n  case X86::TZCNT32rr:\n  case X86::TZCNT64rr:\n\n  // Bit manipulation instructions are effectively combinations of basic\n  // arithmetic ops, and should still execute in constant time. These also\n  // set flags.\n  case X86::BLCFILL32rr:\n  case X86::BLCFILL64rr:\n  case X86::BLCI32rr:\n  case X86::BLCI64rr:\n  case X86::BLCIC32rr:\n  case X86::BLCIC64rr:\n  case X86::BLCMSK32rr:\n  case X86::BLCMSK64rr:\n  case X86::BLCS32rr:\n  case X86::BLCS64rr:\n  case X86::BLSFILL32rr:\n  case X86::BLSFILL64rr:\n  case X86::BLSI32rr:\n  case X86::BLSI64rr:\n  case X86::BLSIC32rr:\n  case X86::BLSIC64rr:\n  case X86::BLSMSK32rr:\n  case X86::BLSMSK64rr:\n  case X86::BLSR32rr:\n  case X86::BLSR64rr:\n  case X86::TZMSK32rr:\n  case X86::TZMSK64rr:\n\n  // Bit extracting and clearing instructions should execute in constant time,\n  // and set flags.\n  case X86::BEXTR32rr:\n  case X86::BEXTR64rr:\n  case X86::BEXTRI32ri:\n  case X86::BEXTRI64ri:\n  case X86::BZHI32rr:\n  case X86::BZHI64rr:\n\n  // Shift and rotate.\n  case X86::ROL8r1:\n  case X86::ROL16r1:\n  case X86::ROL32r1:\n  case X86::ROL64r1:\n  case X86::ROL8rCL:\n  case X86::ROL16rCL:\n  case X86::ROL32rCL:\n  case X86::ROL64rCL:\n  case X86::ROL8ri:\n  case X86::ROL16ri:\n  case X86::ROL32ri:\n  case X86::ROL64ri:\n  case X86::ROR8r1:\n  case X86::ROR16r1:\n  case X86::ROR32r1:\n  case X86::ROR64r1:\n  case X86::ROR8rCL:\n  case X86::ROR16rCL:\n  case X86::ROR32rCL:\n  case X86::ROR64rCL:\n  case X86::ROR8ri:\n  case X86::ROR16ri:\n  case X86::ROR32ri:\n  case X86::ROR64ri:\n  case X86::SAR8r1:\n  case X86::SAR16r1:\n  case X86::SAR32r1:\n  case X86::SAR64r1:\n  case X86::SAR8rCL:\n  case X86::SAR16rCL:\n  case X86::SAR32rCL:\n  case X86::SAR64rCL:\n  case X86::SAR8ri:\n  case X86::SAR16ri:\n  case X86::SAR32ri:\n  case X86::SAR64ri:\n  case X86::SHL8r1:\n  case X86::SHL16r1:\n  case X86::SHL32r1:\n  case X86::SHL64r1:\n  case X86::SHL8rCL:\n  case X86::SHL16rCL:\n  case X86::SHL32rCL:\n  case X86::SHL64rCL:\n  case X86::SHL8ri:\n  case X86::SHL16ri:\n  case X86::SHL32ri:\n  case X86::SHL64ri:\n  case X86::SHR8r1:\n  case X86::SHR16r1:\n  case X86::SHR32r1:\n  case X86::SHR64r1:\n  case X86::SHR8rCL:\n  case X86::SHR16rCL:\n  case X86::SHR32rCL:\n  case X86::SHR64rCL:\n  case X86::SHR8ri:\n  case X86::SHR16ri:\n  case X86::SHR32ri:\n  case X86::SHR64ri:\n  case X86::SHLD16rrCL:\n  case X86::SHLD32rrCL:\n  case X86::SHLD64rrCL:\n  case X86::SHLD16rri8:\n  case X86::SHLD32rri8:\n  case X86::SHLD64rri8:\n  case X86::SHRD16rrCL:\n  case X86::SHRD32rrCL:\n  case X86::SHRD64rrCL:\n  case X86::SHRD16rri8:\n  case X86::SHRD32rri8:\n  case X86::SHRD64rri8:\n\n  // Basic arithmetic is constant time on the input but does set flags.\n  case X86::ADC8rr:\n  case X86::ADC8ri:\n  case X86::ADC16rr:\n  case X86::ADC16ri:\n  case X86::ADC16ri8:\n  case X86::ADC32rr:\n  case X86::ADC32ri:\n  case X86::ADC32ri8:\n  case X86::ADC64rr:\n  case X86::ADC64ri8:\n  case X86::ADC64ri32:\n  case X86::ADD8rr:\n  case X86::ADD8ri:\n  case X86::ADD16rr:\n  case X86::ADD16ri:\n  case X86::ADD16ri8:\n  case X86::ADD32rr:\n  case X86::ADD32ri:\n  case X86::ADD32ri8:\n  case X86::ADD64rr:\n  case X86::ADD64ri8:\n  case X86::ADD64ri32:\n  case X86::AND8rr:\n  case X86::AND8ri:\n  case X86::AND16rr:\n  case X86::AND16ri:\n  case X86::AND16ri8:\n  case X86::AND32rr:\n  case X86::AND32ri:\n  case X86::AND32ri8:\n  case X86::AND64rr:\n  case X86::AND64ri8:\n  case X86::AND64ri32:\n  case X86::OR8rr:\n  case X86::OR8ri:\n  case X86::OR16rr:\n  case X86::OR16ri:\n  case X86::OR16ri8:\n  case X86::OR32rr:\n  case X86::OR32ri:\n  case X86::OR32ri8:\n  case X86::OR64rr:\n  case X86::OR64ri8:\n  case X86::OR64ri32:\n  case X86::SBB8rr:\n  case X86::SBB8ri:\n  case X86::SBB16rr:\n  case X86::SBB16ri:\n  case X86::SBB16ri8:\n  case X86::SBB32rr:\n  case X86::SBB32ri:\n  case X86::SBB32ri8:\n  case X86::SBB64rr:\n  case X86::SBB64ri8:\n  case X86::SBB64ri32:\n  case X86::SUB8rr:\n  case X86::SUB8ri:\n  case X86::SUB16rr:\n  case X86::SUB16ri:\n  case X86::SUB16ri8:\n  case X86::SUB32rr:\n  case X86::SUB32ri:\n  case X86::SUB32ri8:\n  case X86::SUB64rr:\n  case X86::SUB64ri8:\n  case X86::SUB64ri32:\n  case X86::XOR8rr:\n  case X86::XOR8ri:\n  case X86::XOR16rr:\n  case X86::XOR16ri:\n  case X86::XOR16ri8:\n  case X86::XOR32rr:\n  case X86::XOR32ri:\n  case X86::XOR32ri8:\n  case X86::XOR64rr:\n  case X86::XOR64ri8:\n  case X86::XOR64ri32:\n  // Arithmetic with just 32-bit and 64-bit variants and no immediates.\n  case X86::ADCX32rr:\n  case X86::ADCX64rr:\n  case X86::ADOX32rr:\n  case X86::ADOX64rr:\n  case X86::ANDN32rr:\n  case X86::ANDN64rr:\n  // Unary arithmetic operations.\n  case X86::DEC8r:\n  case X86::DEC16r:\n  case X86::DEC32r:\n  case X86::DEC64r:\n  case X86::INC8r:\n  case X86::INC16r:\n  case X86::INC32r:\n  case X86::INC64r:\n  case X86::NEG8r:\n  case X86::NEG16r:\n  case X86::NEG32r:\n  case X86::NEG64r:\n\n  // Unlike other arithmetic, NOT doesn't set EFLAGS.\n  case X86::NOT8r:\n  case X86::NOT16r:\n  case X86::NOT32r:\n  case X86::NOT64r:\n\n  // Various move instructions used to zero or sign extend things. Note that we\n  // intentionally don't support the _NOREX variants as we can't handle that\n  // register constraint anyways.\n  case X86::MOVSX16rr8:\n  case X86::MOVSX32rr8:\n  case X86::MOVSX32rr16:\n  case X86::MOVSX64rr8:\n  case X86::MOVSX64rr16:\n  case X86::MOVSX64rr32:\n  case X86::MOVZX16rr8:\n  case X86::MOVZX32rr8:\n  case X86::MOVZX32rr16:\n  case X86::MOVZX64rr8:\n  case X86::MOVZX64rr16:\n  case X86::MOV32rr:\n\n  // Arithmetic instructions that are both constant time and don't set flags.\n  case X86::RORX32ri:\n  case X86::RORX64ri:\n  case X86::SARX32rr:\n  case X86::SARX64rr:\n  case X86::SHLX32rr:\n  case X86::SHLX64rr:\n  case X86::SHRX32rr:\n  case X86::SHRX64rr:\n\n  // LEA doesn't actually access memory, and its arithmetic is constant time.\n  case X86::LEA16r:\n  case X86::LEA32r:\n  case X86::LEA64_32r:\n  case X86::LEA64r:\n    return true;\n  }\n}\n\nbool X86InstrInfo::isDataInvariantLoad(MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default:\n    // By default, assume that the load will immediately leak.\n    return false;\n\n  // On x86 it is believed that imul is constant time w.r.t. the loaded data.\n  // However, they set flags and are perhaps the most surprisingly constant\n  // time operations so we call them out here separately.\n  case X86::IMUL16rm:\n  case X86::IMUL16rmi8:\n  case X86::IMUL16rmi:\n  case X86::IMUL32rm:\n  case X86::IMUL32rmi8:\n  case X86::IMUL32rmi:\n  case X86::IMUL64rm:\n  case X86::IMUL64rmi32:\n  case X86::IMUL64rmi8:\n\n  // Bit scanning and counting instructions that are somewhat surprisingly\n  // constant time as they scan across bits and do other fairly complex\n  // operations like popcnt, but are believed to be constant time on x86.\n  // However, these set flags.\n  case X86::BSF16rm:\n  case X86::BSF32rm:\n  case X86::BSF64rm:\n  case X86::BSR16rm:\n  case X86::BSR32rm:\n  case X86::BSR64rm:\n  case X86::LZCNT16rm:\n  case X86::LZCNT32rm:\n  case X86::LZCNT64rm:\n  case X86::POPCNT16rm:\n  case X86::POPCNT32rm:\n  case X86::POPCNT64rm:\n  case X86::TZCNT16rm:\n  case X86::TZCNT32rm:\n  case X86::TZCNT64rm:\n\n  // Bit manipulation instructions are effectively combinations of basic\n  // arithmetic ops, and should still execute in constant time. These also\n  // set flags.\n  case X86::BLCFILL32rm:\n  case X86::BLCFILL64rm:\n  case X86::BLCI32rm:\n  case X86::BLCI64rm:\n  case X86::BLCIC32rm:\n  case X86::BLCIC64rm:\n  case X86::BLCMSK32rm:\n  case X86::BLCMSK64rm:\n  case X86::BLCS32rm:\n  case X86::BLCS64rm:\n  case X86::BLSFILL32rm:\n  case X86::BLSFILL64rm:\n  case X86::BLSI32rm:\n  case X86::BLSI64rm:\n  case X86::BLSIC32rm:\n  case X86::BLSIC64rm:\n  case X86::BLSMSK32rm:\n  case X86::BLSMSK64rm:\n  case X86::BLSR32rm:\n  case X86::BLSR64rm:\n  case X86::TZMSK32rm:\n  case X86::TZMSK64rm:\n\n  // Bit extracting and clearing instructions should execute in constant time,\n  // and set flags.\n  case X86::BEXTR32rm:\n  case X86::BEXTR64rm:\n  case X86::BEXTRI32mi:\n  case X86::BEXTRI64mi:\n  case X86::BZHI32rm:\n  case X86::BZHI64rm:\n\n  // Basic arithmetic is constant time on the input but does set flags.\n  case X86::ADC8rm:\n  case X86::ADC16rm:\n  case X86::ADC32rm:\n  case X86::ADC64rm:\n  case X86::ADCX32rm:\n  case X86::ADCX64rm:\n  case X86::ADD8rm:\n  case X86::ADD16rm:\n  case X86::ADD32rm:\n  case X86::ADD64rm:\n  case X86::ADOX32rm:\n  case X86::ADOX64rm:\n  case X86::AND8rm:\n  case X86::AND16rm:\n  case X86::AND32rm:\n  case X86::AND64rm:\n  case X86::ANDN32rm:\n  case X86::ANDN64rm:\n  case X86::OR8rm:\n  case X86::OR16rm:\n  case X86::OR32rm:\n  case X86::OR64rm:\n  case X86::SBB8rm:\n  case X86::SBB16rm:\n  case X86::SBB32rm:\n  case X86::SBB64rm:\n  case X86::SUB8rm:\n  case X86::SUB16rm:\n  case X86::SUB32rm:\n  case X86::SUB64rm:\n  case X86::XOR8rm:\n  case X86::XOR16rm:\n  case X86::XOR32rm:\n  case X86::XOR64rm:\n\n  // Integer multiply w/o affecting flags is still believed to be constant\n  // time on x86. Called out separately as this is among the most surprising\n  // instructions to exhibit that behavior.\n  case X86::MULX32rm:\n  case X86::MULX64rm:\n\n  // Arithmetic instructions that are both constant time and don't set flags.\n  case X86::RORX32mi:\n  case X86::RORX64mi:\n  case X86::SARX32rm:\n  case X86::SARX64rm:\n  case X86::SHLX32rm:\n  case X86::SHLX64rm:\n  case X86::SHRX32rm:\n  case X86::SHRX64rm:\n\n  // Conversions are believed to be constant time and don't set flags.\n  case X86::CVTTSD2SI64rm:\n  case X86::VCVTTSD2SI64rm:\n  case X86::VCVTTSD2SI64Zrm:\n  case X86::CVTTSD2SIrm:\n  case X86::VCVTTSD2SIrm:\n  case X86::VCVTTSD2SIZrm:\n  case X86::CVTTSS2SI64rm:\n  case X86::VCVTTSS2SI64rm:\n  case X86::VCVTTSS2SI64Zrm:\n  case X86::CVTTSS2SIrm:\n  case X86::VCVTTSS2SIrm:\n  case X86::VCVTTSS2SIZrm:\n  case X86::CVTSI2SDrm:\n  case X86::VCVTSI2SDrm:\n  case X86::VCVTSI2SDZrm:\n  case X86::CVTSI2SSrm:\n  case X86::VCVTSI2SSrm:\n  case X86::VCVTSI2SSZrm:\n  case X86::CVTSI642SDrm:\n  case X86::VCVTSI642SDrm:\n  case X86::VCVTSI642SDZrm:\n  case X86::CVTSI642SSrm:\n  case X86::VCVTSI642SSrm:\n  case X86::VCVTSI642SSZrm:\n  case X86::CVTSS2SDrm:\n  case X86::VCVTSS2SDrm:\n  case X86::VCVTSS2SDZrm:\n  case X86::CVTSD2SSrm:\n  case X86::VCVTSD2SSrm:\n  case X86::VCVTSD2SSZrm:\n  // AVX512 added unsigned integer conversions.\n  case X86::VCVTTSD2USI64Zrm:\n  case X86::VCVTTSD2USIZrm:\n  case X86::VCVTTSS2USI64Zrm:\n  case X86::VCVTTSS2USIZrm:\n  case X86::VCVTUSI2SDZrm:\n  case X86::VCVTUSI642SDZrm:\n  case X86::VCVTUSI2SSZrm:\n  case X86::VCVTUSI642SSZrm:\n\n  // Loads to register don't set flags.\n  case X86::MOV8rm:\n  case X86::MOV8rm_NOREX:\n  case X86::MOV16rm:\n  case X86::MOV32rm:\n  case X86::MOV64rm:\n  case X86::MOVSX16rm8:\n  case X86::MOVSX32rm16:\n  case X86::MOVSX32rm8:\n  case X86::MOVSX32rm8_NOREX:\n  case X86::MOVSX64rm16:\n  case X86::MOVSX64rm32:\n  case X86::MOVSX64rm8:\n  case X86::MOVZX16rm8:\n  case X86::MOVZX32rm16:\n  case X86::MOVZX32rm8:\n  case X86::MOVZX32rm8_NOREX:\n  case X86::MOVZX64rm16:\n  case X86::MOVZX64rm8:\n    return true;\n  }\n}\n\nint X86InstrInfo::getSPAdjust(const MachineInstr &MI) const {\n  const MachineFunction *MF = MI.getParent()->getParent();\n  const TargetFrameLowering *TFI = MF->getSubtarget().getFrameLowering();\n\n  if (isFrameInstr(MI)) {\n    int SPAdj = alignTo(getFrameSize(MI), TFI->getStackAlign());\n    SPAdj -= getFrameAdjustment(MI);\n    if (!isFrameSetup(MI))\n      SPAdj = -SPAdj;\n    return SPAdj;\n  }\n\n  // To know whether a call adjusts the stack, we need information\n  // that is bound to the following ADJCALLSTACKUP pseudo.\n  // Look for the next ADJCALLSTACKUP that follows the call.\n  if (MI.isCall()) {\n    const MachineBasicBlock *MBB = MI.getParent();\n    auto I = ++MachineBasicBlock::const_iterator(MI);\n    for (auto E = MBB->end(); I != E; ++I) {\n      if (I->getOpcode() == getCallFrameDestroyOpcode() ||\n          I->isCall())\n        break;\n    }\n\n    // If we could not find a frame destroy opcode, then it has already\n    // been simplified, so we don't care.\n    if (I->getOpcode() != getCallFrameDestroyOpcode())\n      return 0;\n\n    return -(I->getOperand(1).getImm());\n  }\n\n  // Currently handle only PUSHes we can reasonably expect to see\n  // in call sequences\n  switch (MI.getOpcode()) {\n  default:\n    return 0;\n  case X86::PUSH32i8:\n  case X86::PUSH32r:\n  case X86::PUSH32rmm:\n  case X86::PUSH32rmr:\n  case X86::PUSHi32:\n    return 4;\n  case X86::PUSH64i8:\n  case X86::PUSH64r:\n  case X86::PUSH64rmm:\n  case X86::PUSH64rmr:\n  case X86::PUSH64i32:\n    return 8;\n  }\n}\n\n/// Return true and the FrameIndex if the specified\n/// operand and follow operands form a reference to the stack frame.\nbool X86InstrInfo::isFrameOperand(const MachineInstr &MI, unsigned int Op,\n                                  int &FrameIndex) const {\n  if (MI.getOperand(Op + X86::AddrBaseReg).isFI() &&\n      MI.getOperand(Op + X86::AddrScaleAmt).isImm() &&\n      MI.getOperand(Op + X86::AddrIndexReg).isReg() &&\n      MI.getOperand(Op + X86::AddrDisp).isImm() &&\n      MI.getOperand(Op + X86::AddrScaleAmt).getImm() == 1 &&\n      MI.getOperand(Op + X86::AddrIndexReg).getReg() == 0 &&\n      MI.getOperand(Op + X86::AddrDisp).getImm() == 0) {\n    FrameIndex = MI.getOperand(Op + X86::AddrBaseReg).getIndex();\n    return true;\n  }\n  return false;\n}\n\nstatic bool isFrameLoadOpcode(int Opcode, unsigned &MemBytes) {\n  switch (Opcode) {\n  default:\n    return false;\n  case X86::MOV8rm:\n  case X86::KMOVBkm:\n    MemBytes = 1;\n    return true;\n  case X86::MOV16rm:\n  case X86::KMOVWkm:\n    MemBytes = 2;\n    return true;\n  case X86::MOV32rm:\n  case X86::MOVSSrm:\n  case X86::MOVSSrm_alt:\n  case X86::VMOVSSrm:\n  case X86::VMOVSSrm_alt:\n  case X86::VMOVSSZrm:\n  case X86::VMOVSSZrm_alt:\n  case X86::KMOVDkm:\n    MemBytes = 4;\n    return true;\n  case X86::MOV64rm:\n  case X86::LD_Fp64m:\n  case X86::MOVSDrm:\n  case X86::MOVSDrm_alt:\n  case X86::VMOVSDrm:\n  case X86::VMOVSDrm_alt:\n  case X86::VMOVSDZrm:\n  case X86::VMOVSDZrm_alt:\n  case X86::MMX_MOVD64rm:\n  case X86::MMX_MOVQ64rm:\n  case X86::KMOVQkm:\n    MemBytes = 8;\n    return true;\n  case X86::MOVAPSrm:\n  case X86::MOVUPSrm:\n  case X86::MOVAPDrm:\n  case X86::MOVUPDrm:\n  case X86::MOVDQArm:\n  case X86::MOVDQUrm:\n  case X86::VMOVAPSrm:\n  case X86::VMOVUPSrm:\n  case X86::VMOVAPDrm:\n  case X86::VMOVUPDrm:\n  case X86::VMOVDQArm:\n  case X86::VMOVDQUrm:\n  case X86::VMOVAPSZ128rm:\n  case X86::VMOVUPSZ128rm:\n  case X86::VMOVAPSZ128rm_NOVLX:\n  case X86::VMOVUPSZ128rm_NOVLX:\n  case X86::VMOVAPDZ128rm:\n  case X86::VMOVUPDZ128rm:\n  case X86::VMOVDQU8Z128rm:\n  case X86::VMOVDQU16Z128rm:\n  case X86::VMOVDQA32Z128rm:\n  case X86::VMOVDQU32Z128rm:\n  case X86::VMOVDQA64Z128rm:\n  case X86::VMOVDQU64Z128rm:\n    MemBytes = 16;\n    return true;\n  case X86::VMOVAPSYrm:\n  case X86::VMOVUPSYrm:\n  case X86::VMOVAPDYrm:\n  case X86::VMOVUPDYrm:\n  case X86::VMOVDQAYrm:\n  case X86::VMOVDQUYrm:\n  case X86::VMOVAPSZ256rm:\n  case X86::VMOVUPSZ256rm:\n  case X86::VMOVAPSZ256rm_NOVLX:\n  case X86::VMOVUPSZ256rm_NOVLX:\n  case X86::VMOVAPDZ256rm:\n  case X86::VMOVUPDZ256rm:\n  case X86::VMOVDQU8Z256rm:\n  case X86::VMOVDQU16Z256rm:\n  case X86::VMOVDQA32Z256rm:\n  case X86::VMOVDQU32Z256rm:\n  case X86::VMOVDQA64Z256rm:\n  case X86::VMOVDQU64Z256rm:\n    MemBytes = 32;\n    return true;\n  case X86::VMOVAPSZrm:\n  case X86::VMOVUPSZrm:\n  case X86::VMOVAPDZrm:\n  case X86::VMOVUPDZrm:\n  case X86::VMOVDQU8Zrm:\n  case X86::VMOVDQU16Zrm:\n  case X86::VMOVDQA32Zrm:\n  case X86::VMOVDQU32Zrm:\n  case X86::VMOVDQA64Zrm:\n  case X86::VMOVDQU64Zrm:\n    MemBytes = 64;\n    return true;\n  }\n}\n\nstatic bool isFrameStoreOpcode(int Opcode, unsigned &MemBytes) {\n  switch (Opcode) {\n  default:\n    return false;\n  case X86::MOV8mr:\n  case X86::KMOVBmk:\n    MemBytes = 1;\n    return true;\n  case X86::MOV16mr:\n  case X86::KMOVWmk:\n    MemBytes = 2;\n    return true;\n  case X86::MOV32mr:\n  case X86::MOVSSmr:\n  case X86::VMOVSSmr:\n  case X86::VMOVSSZmr:\n  case X86::KMOVDmk:\n    MemBytes = 4;\n    return true;\n  case X86::MOV64mr:\n  case X86::ST_FpP64m:\n  case X86::MOVSDmr:\n  case X86::VMOVSDmr:\n  case X86::VMOVSDZmr:\n  case X86::MMX_MOVD64mr:\n  case X86::MMX_MOVQ64mr:\n  case X86::MMX_MOVNTQmr:\n  case X86::KMOVQmk:\n    MemBytes = 8;\n    return true;\n  case X86::MOVAPSmr:\n  case X86::MOVUPSmr:\n  case X86::MOVAPDmr:\n  case X86::MOVUPDmr:\n  case X86::MOVDQAmr:\n  case X86::MOVDQUmr:\n  case X86::VMOVAPSmr:\n  case X86::VMOVUPSmr:\n  case X86::VMOVAPDmr:\n  case X86::VMOVUPDmr:\n  case X86::VMOVDQAmr:\n  case X86::VMOVDQUmr:\n  case X86::VMOVUPSZ128mr:\n  case X86::VMOVAPSZ128mr:\n  case X86::VMOVUPSZ128mr_NOVLX:\n  case X86::VMOVAPSZ128mr_NOVLX:\n  case X86::VMOVUPDZ128mr:\n  case X86::VMOVAPDZ128mr:\n  case X86::VMOVDQA32Z128mr:\n  case X86::VMOVDQU32Z128mr:\n  case X86::VMOVDQA64Z128mr:\n  case X86::VMOVDQU64Z128mr:\n  case X86::VMOVDQU8Z128mr:\n  case X86::VMOVDQU16Z128mr:\n    MemBytes = 16;\n    return true;\n  case X86::VMOVUPSYmr:\n  case X86::VMOVAPSYmr:\n  case X86::VMOVUPDYmr:\n  case X86::VMOVAPDYmr:\n  case X86::VMOVDQUYmr:\n  case X86::VMOVDQAYmr:\n  case X86::VMOVUPSZ256mr:\n  case X86::VMOVAPSZ256mr:\n  case X86::VMOVUPSZ256mr_NOVLX:\n  case X86::VMOVAPSZ256mr_NOVLX:\n  case X86::VMOVUPDZ256mr:\n  case X86::VMOVAPDZ256mr:\n  case X86::VMOVDQU8Z256mr:\n  case X86::VMOVDQU16Z256mr:\n  case X86::VMOVDQA32Z256mr:\n  case X86::VMOVDQU32Z256mr:\n  case X86::VMOVDQA64Z256mr:\n  case X86::VMOVDQU64Z256mr:\n    MemBytes = 32;\n    return true;\n  case X86::VMOVUPSZmr:\n  case X86::VMOVAPSZmr:\n  case X86::VMOVUPDZmr:\n  case X86::VMOVAPDZmr:\n  case X86::VMOVDQU8Zmr:\n  case X86::VMOVDQU16Zmr:\n  case X86::VMOVDQA32Zmr:\n  case X86::VMOVDQU32Zmr:\n  case X86::VMOVDQA64Zmr:\n  case X86::VMOVDQU64Zmr:\n    MemBytes = 64;\n    return true;\n  }\n  return false;\n}\n\nunsigned X86InstrInfo::isLoadFromStackSlot(const MachineInstr &MI,\n                                           int &FrameIndex) const {\n  unsigned Dummy;\n  return X86InstrInfo::isLoadFromStackSlot(MI, FrameIndex, Dummy);\n}\n\nunsigned X86InstrInfo::isLoadFromStackSlot(const MachineInstr &MI,\n                                           int &FrameIndex,\n                                           unsigned &MemBytes) const {\n  if (isFrameLoadOpcode(MI.getOpcode(), MemBytes))\n    if (MI.getOperand(0).getSubReg() == 0 && isFrameOperand(MI, 1, FrameIndex))\n      return MI.getOperand(0).getReg();\n  return 0;\n}\n\nunsigned X86InstrInfo::isLoadFromStackSlotPostFE(const MachineInstr &MI,\n                                                 int &FrameIndex) const {\n  unsigned Dummy;\n  if (isFrameLoadOpcode(MI.getOpcode(), Dummy)) {\n    unsigned Reg;\n    if ((Reg = isLoadFromStackSlot(MI, FrameIndex)))\n      return Reg;\n    // Check for post-frame index elimination operations\n    SmallVector<const MachineMemOperand *, 1> Accesses;\n    if (hasLoadFromStackSlot(MI, Accesses)) {\n      FrameIndex =\n          cast<FixedStackPseudoSourceValue>(Accesses.front()->getPseudoValue())\n              ->getFrameIndex();\n      return 1;\n    }\n  }\n  return 0;\n}\n\nunsigned X86InstrInfo::isStoreToStackSlot(const MachineInstr &MI,\n                                          int &FrameIndex) const {\n  unsigned Dummy;\n  return X86InstrInfo::isStoreToStackSlot(MI, FrameIndex, Dummy);\n}\n\nunsigned X86InstrInfo::isStoreToStackSlot(const MachineInstr &MI,\n                                          int &FrameIndex,\n                                          unsigned &MemBytes) const {\n  if (isFrameStoreOpcode(MI.getOpcode(), MemBytes))\n    if (MI.getOperand(X86::AddrNumOperands).getSubReg() == 0 &&\n        isFrameOperand(MI, 0, FrameIndex))\n      return MI.getOperand(X86::AddrNumOperands).getReg();\n  return 0;\n}\n\nunsigned X86InstrInfo::isStoreToStackSlotPostFE(const MachineInstr &MI,\n                                                int &FrameIndex) const {\n  unsigned Dummy;\n  if (isFrameStoreOpcode(MI.getOpcode(), Dummy)) {\n    unsigned Reg;\n    if ((Reg = isStoreToStackSlot(MI, FrameIndex)))\n      return Reg;\n    // Check for post-frame index elimination operations\n    SmallVector<const MachineMemOperand *, 1> Accesses;\n    if (hasStoreToStackSlot(MI, Accesses)) {\n      FrameIndex =\n          cast<FixedStackPseudoSourceValue>(Accesses.front()->getPseudoValue())\n              ->getFrameIndex();\n      return 1;\n    }\n  }\n  return 0;\n}\n\n/// Return true if register is PIC base; i.e.g defined by X86::MOVPC32r.\nstatic bool regIsPICBase(Register BaseReg, const MachineRegisterInfo &MRI) {\n  // Don't waste compile time scanning use-def chains of physregs.\n  if (!BaseReg.isVirtual())\n    return false;\n  bool isPICBase = false;\n  for (MachineRegisterInfo::def_instr_iterator I = MRI.def_instr_begin(BaseReg),\n         E = MRI.def_instr_end(); I != E; ++I) {\n    MachineInstr *DefMI = &*I;\n    if (DefMI->getOpcode() != X86::MOVPC32r)\n      return false;\n    assert(!isPICBase && \"More than one PIC base?\");\n    isPICBase = true;\n  }\n  return isPICBase;\n}\n\nbool X86InstrInfo::isReallyTriviallyReMaterializable(const MachineInstr &MI,\n                                                     AAResults *AA) const {\n  switch (MI.getOpcode()) {\n  default:\n    // This function should only be called for opcodes with the ReMaterializable\n    // flag set.\n    llvm_unreachable(\"Unknown rematerializable operation!\");\n    break;\n\n  case X86::LOAD_STACK_GUARD:\n  case X86::AVX1_SETALLONES:\n  case X86::AVX2_SETALLONES:\n  case X86::AVX512_128_SET0:\n  case X86::AVX512_256_SET0:\n  case X86::AVX512_512_SET0:\n  case X86::AVX512_512_SETALLONES:\n  case X86::AVX512_FsFLD0SD:\n  case X86::AVX512_FsFLD0SS:\n  case X86::AVX512_FsFLD0F128:\n  case X86::AVX_SET0:\n  case X86::FsFLD0SD:\n  case X86::FsFLD0SS:\n  case X86::FsFLD0F128:\n  case X86::KSET0D:\n  case X86::KSET0Q:\n  case X86::KSET0W:\n  case X86::KSET1D:\n  case X86::KSET1Q:\n  case X86::KSET1W:\n  case X86::MMX_SET0:\n  case X86::MOV32ImmSExti8:\n  case X86::MOV32r0:\n  case X86::MOV32r1:\n  case X86::MOV32r_1:\n  case X86::MOV32ri64:\n  case X86::MOV64ImmSExti8:\n  case X86::V_SET0:\n  case X86::V_SETALLONES:\n  case X86::MOV16ri:\n  case X86::MOV32ri:\n  case X86::MOV64ri:\n  case X86::MOV64ri32:\n  case X86::MOV8ri:\n    return true;\n\n  case X86::MOV8rm:\n  case X86::MOV8rm_NOREX:\n  case X86::MOV16rm:\n  case X86::MOV32rm:\n  case X86::MOV64rm:\n  case X86::MOVSSrm:\n  case X86::MOVSSrm_alt:\n  case X86::MOVSDrm:\n  case X86::MOVSDrm_alt:\n  case X86::MOVAPSrm:\n  case X86::MOVUPSrm:\n  case X86::MOVAPDrm:\n  case X86::MOVUPDrm:\n  case X86::MOVDQArm:\n  case X86::MOVDQUrm:\n  case X86::VMOVSSrm:\n  case X86::VMOVSSrm_alt:\n  case X86::VMOVSDrm:\n  case X86::VMOVSDrm_alt:\n  case X86::VMOVAPSrm:\n  case X86::VMOVUPSrm:\n  case X86::VMOVAPDrm:\n  case X86::VMOVUPDrm:\n  case X86::VMOVDQArm:\n  case X86::VMOVDQUrm:\n  case X86::VMOVAPSYrm:\n  case X86::VMOVUPSYrm:\n  case X86::VMOVAPDYrm:\n  case X86::VMOVUPDYrm:\n  case X86::VMOVDQAYrm:\n  case X86::VMOVDQUYrm:\n  case X86::MMX_MOVD64rm:\n  case X86::MMX_MOVQ64rm:\n  // AVX-512\n  case X86::VMOVSSZrm:\n  case X86::VMOVSSZrm_alt:\n  case X86::VMOVSDZrm:\n  case X86::VMOVSDZrm_alt:\n  case X86::VMOVAPDZ128rm:\n  case X86::VMOVAPDZ256rm:\n  case X86::VMOVAPDZrm:\n  case X86::VMOVAPSZ128rm:\n  case X86::VMOVAPSZ256rm:\n  case X86::VMOVAPSZ128rm_NOVLX:\n  case X86::VMOVAPSZ256rm_NOVLX:\n  case X86::VMOVAPSZrm:\n  case X86::VMOVDQA32Z128rm:\n  case X86::VMOVDQA32Z256rm:\n  case X86::VMOVDQA32Zrm:\n  case X86::VMOVDQA64Z128rm:\n  case X86::VMOVDQA64Z256rm:\n  case X86::VMOVDQA64Zrm:\n  case X86::VMOVDQU16Z128rm:\n  case X86::VMOVDQU16Z256rm:\n  case X86::VMOVDQU16Zrm:\n  case X86::VMOVDQU32Z128rm:\n  case X86::VMOVDQU32Z256rm:\n  case X86::VMOVDQU32Zrm:\n  case X86::VMOVDQU64Z128rm:\n  case X86::VMOVDQU64Z256rm:\n  case X86::VMOVDQU64Zrm:\n  case X86::VMOVDQU8Z128rm:\n  case X86::VMOVDQU8Z256rm:\n  case X86::VMOVDQU8Zrm:\n  case X86::VMOVUPDZ128rm:\n  case X86::VMOVUPDZ256rm:\n  case X86::VMOVUPDZrm:\n  case X86::VMOVUPSZ128rm:\n  case X86::VMOVUPSZ256rm:\n  case X86::VMOVUPSZ128rm_NOVLX:\n  case X86::VMOVUPSZ256rm_NOVLX:\n  case X86::VMOVUPSZrm: {\n    // Loads from constant pools are trivially rematerializable.\n    if (MI.getOperand(1 + X86::AddrBaseReg).isReg() &&\n        MI.getOperand(1 + X86::AddrScaleAmt).isImm() &&\n        MI.getOperand(1 + X86::AddrIndexReg).isReg() &&\n        MI.getOperand(1 + X86::AddrIndexReg).getReg() == 0 &&\n        MI.isDereferenceableInvariantLoad(AA)) {\n      Register BaseReg = MI.getOperand(1 + X86::AddrBaseReg).getReg();\n      if (BaseReg == 0 || BaseReg == X86::RIP)\n        return true;\n      // Allow re-materialization of PIC load.\n      if (!ReMatPICStubLoad && MI.getOperand(1 + X86::AddrDisp).isGlobal())\n        return false;\n      const MachineFunction &MF = *MI.getParent()->getParent();\n      const MachineRegisterInfo &MRI = MF.getRegInfo();\n      return regIsPICBase(BaseReg, MRI);\n    }\n    return false;\n  }\n\n  case X86::LEA32r:\n  case X86::LEA64r: {\n    if (MI.getOperand(1 + X86::AddrScaleAmt).isImm() &&\n        MI.getOperand(1 + X86::AddrIndexReg).isReg() &&\n        MI.getOperand(1 + X86::AddrIndexReg).getReg() == 0 &&\n        !MI.getOperand(1 + X86::AddrDisp).isReg()) {\n      // lea fi#, lea GV, etc. are all rematerializable.\n      if (!MI.getOperand(1 + X86::AddrBaseReg).isReg())\n        return true;\n      Register BaseReg = MI.getOperand(1 + X86::AddrBaseReg).getReg();\n      if (BaseReg == 0)\n        return true;\n      // Allow re-materialization of lea PICBase + x.\n      const MachineFunction &MF = *MI.getParent()->getParent();\n      const MachineRegisterInfo &MRI = MF.getRegInfo();\n      return regIsPICBase(BaseReg, MRI);\n    }\n    return false;\n  }\n  }\n}\n\nvoid X86InstrInfo::reMaterialize(MachineBasicBlock &MBB,\n                                 MachineBasicBlock::iterator I,\n                                 Register DestReg, unsigned SubIdx,\n                                 const MachineInstr &Orig,\n                                 const TargetRegisterInfo &TRI) const {\n  bool ClobbersEFLAGS = Orig.modifiesRegister(X86::EFLAGS, &TRI);\n  if (ClobbersEFLAGS && MBB.computeRegisterLiveness(&TRI, X86::EFLAGS, I) !=\n                            MachineBasicBlock::LQR_Dead) {\n    // The instruction clobbers EFLAGS. Re-materialize as MOV32ri to avoid side\n    // effects.\n    int Value;\n    switch (Orig.getOpcode()) {\n    case X86::MOV32r0:  Value = 0; break;\n    case X86::MOV32r1:  Value = 1; break;\n    case X86::MOV32r_1: Value = -1; break;\n    default:\n      llvm_unreachable(\"Unexpected instruction!\");\n    }\n\n    const DebugLoc &DL = Orig.getDebugLoc();\n    BuildMI(MBB, I, DL, get(X86::MOV32ri))\n        .add(Orig.getOperand(0))\n        .addImm(Value);\n  } else {\n    MachineInstr *MI = MBB.getParent()->CloneMachineInstr(&Orig);\n    MBB.insert(I, MI);\n  }\n\n  MachineInstr &NewMI = *std::prev(I);\n  NewMI.substituteRegister(Orig.getOperand(0).getReg(), DestReg, SubIdx, TRI);\n}\n\n/// True if MI has a condition code def, e.g. EFLAGS, that is not marked dead.\nbool X86InstrInfo::hasLiveCondCodeDef(MachineInstr &MI) const {\n  for (unsigned i = 0, e = MI.getNumOperands(); i != e; ++i) {\n    MachineOperand &MO = MI.getOperand(i);\n    if (MO.isReg() && MO.isDef() &&\n        MO.getReg() == X86::EFLAGS && !MO.isDead()) {\n      return true;\n    }\n  }\n  return false;\n}\n\n/// Check whether the shift count for a machine operand is non-zero.\ninline static unsigned getTruncatedShiftCount(const MachineInstr &MI,\n                                              unsigned ShiftAmtOperandIdx) {\n  // The shift count is six bits with the REX.W prefix and five bits without.\n  unsigned ShiftCountMask = (MI.getDesc().TSFlags & X86II::REX_W) ? 63 : 31;\n  unsigned Imm = MI.getOperand(ShiftAmtOperandIdx).getImm();\n  return Imm & ShiftCountMask;\n}\n\n/// Check whether the given shift count is appropriate\n/// can be represented by a LEA instruction.\ninline static bool isTruncatedShiftCountForLEA(unsigned ShAmt) {\n  // Left shift instructions can be transformed into load-effective-address\n  // instructions if we can encode them appropriately.\n  // A LEA instruction utilizes a SIB byte to encode its scale factor.\n  // The SIB.scale field is two bits wide which means that we can encode any\n  // shift amount less than 4.\n  return ShAmt < 4 && ShAmt > 0;\n}\n\nbool X86InstrInfo::classifyLEAReg(MachineInstr &MI, const MachineOperand &Src,\n                                  unsigned Opc, bool AllowSP, Register &NewSrc,\n                                  bool &isKill, MachineOperand &ImplicitOp,\n                                  LiveVariables *LV) const {\n  MachineFunction &MF = *MI.getParent()->getParent();\n  const TargetRegisterClass *RC;\n  if (AllowSP) {\n    RC = Opc != X86::LEA32r ? &X86::GR64RegClass : &X86::GR32RegClass;\n  } else {\n    RC = Opc != X86::LEA32r ?\n      &X86::GR64_NOSPRegClass : &X86::GR32_NOSPRegClass;\n  }\n  Register SrcReg = Src.getReg();\n\n  // For both LEA64 and LEA32 the register already has essentially the right\n  // type (32-bit or 64-bit) we may just need to forbid SP.\n  if (Opc != X86::LEA64_32r) {\n    NewSrc = SrcReg;\n    isKill = Src.isKill();\n    assert(!Src.isUndef() && \"Undef op doesn't need optimization\");\n\n    if (NewSrc.isVirtual() && !MF.getRegInfo().constrainRegClass(NewSrc, RC))\n      return false;\n\n    return true;\n  }\n\n  // This is for an LEA64_32r and incoming registers are 32-bit. One way or\n  // another we need to add 64-bit registers to the final MI.\n  if (SrcReg.isPhysical()) {\n    ImplicitOp = Src;\n    ImplicitOp.setImplicit();\n\n    NewSrc = getX86SubSuperRegister(Src.getReg(), 64);\n    isKill = Src.isKill();\n    assert(!Src.isUndef() && \"Undef op doesn't need optimization\");\n  } else {\n    // Virtual register of the wrong class, we have to create a temporary 64-bit\n    // vreg to feed into the LEA.\n    NewSrc = MF.getRegInfo().createVirtualRegister(RC);\n    MachineInstr *Copy =\n        BuildMI(*MI.getParent(), MI, MI.getDebugLoc(), get(TargetOpcode::COPY))\n            .addReg(NewSrc, RegState::Define | RegState::Undef, X86::sub_32bit)\n            .add(Src);\n\n    // Which is obviously going to be dead after we're done with it.\n    isKill = true;\n\n    if (LV)\n      LV->replaceKillInstruction(SrcReg, MI, *Copy);\n  }\n\n  // We've set all the parameters without issue.\n  return true;\n}\n\nMachineInstr *X86InstrInfo::convertToThreeAddressWithLEA(\n    unsigned MIOpc, MachineFunction::iterator &MFI, MachineInstr &MI,\n    LiveVariables *LV, bool Is8BitOp) const {\n  // We handle 8-bit adds and various 16-bit opcodes in the switch below.\n  MachineRegisterInfo &RegInfo = MFI->getParent()->getRegInfo();\n  assert((Is8BitOp || RegInfo.getTargetRegisterInfo()->getRegSizeInBits(\n              *RegInfo.getRegClass(MI.getOperand(0).getReg())) == 16) &&\n         \"Unexpected type for LEA transform\");\n\n  // TODO: For a 32-bit target, we need to adjust the LEA variables with\n  // something like this:\n  //   Opcode = X86::LEA32r;\n  //   InRegLEA = RegInfo.createVirtualRegister(&X86::GR32_NOSPRegClass);\n  //   OutRegLEA =\n  //       Is8BitOp ? RegInfo.createVirtualRegister(&X86::GR32ABCD_RegClass)\n  //                : RegInfo.createVirtualRegister(&X86::GR32RegClass);\n  if (!Subtarget.is64Bit())\n    return nullptr;\n\n  unsigned Opcode = X86::LEA64_32r;\n  Register InRegLEA = RegInfo.createVirtualRegister(&X86::GR64_NOSPRegClass);\n  Register OutRegLEA = RegInfo.createVirtualRegister(&X86::GR32RegClass);\n\n  // Build and insert into an implicit UNDEF value. This is OK because\n  // we will be shifting and then extracting the lower 8/16-bits.\n  // This has the potential to cause partial register stall. e.g.\n  //   movw    (%rbp,%rcx,2), %dx\n  //   leal    -65(%rdx), %esi\n  // But testing has shown this *does* help performance in 64-bit mode (at\n  // least on modern x86 machines).\n  MachineBasicBlock::iterator MBBI = MI.getIterator();\n  Register Dest = MI.getOperand(0).getReg();\n  Register Src = MI.getOperand(1).getReg();\n  bool IsDead = MI.getOperand(0).isDead();\n  bool IsKill = MI.getOperand(1).isKill();\n  unsigned SubReg = Is8BitOp ? X86::sub_8bit : X86::sub_16bit;\n  assert(!MI.getOperand(1).isUndef() && \"Undef op doesn't need optimization\");\n  BuildMI(*MFI, MBBI, MI.getDebugLoc(), get(X86::IMPLICIT_DEF), InRegLEA);\n  MachineInstr *InsMI =\n      BuildMI(*MFI, MBBI, MI.getDebugLoc(), get(TargetOpcode::COPY))\n          .addReg(InRegLEA, RegState::Define, SubReg)\n          .addReg(Src, getKillRegState(IsKill));\n\n  MachineInstrBuilder MIB =\n      BuildMI(*MFI, MBBI, MI.getDebugLoc(), get(Opcode), OutRegLEA);\n  switch (MIOpc) {\n  default: llvm_unreachable(\"Unreachable!\");\n  case X86::SHL8ri:\n  case X86::SHL16ri: {\n    unsigned ShAmt = MI.getOperand(2).getImm();\n    MIB.addReg(0).addImm(1ULL << ShAmt)\n       .addReg(InRegLEA, RegState::Kill).addImm(0).addReg(0);\n    break;\n  }\n  case X86::INC8r:\n  case X86::INC16r:\n    addRegOffset(MIB, InRegLEA, true, 1);\n    break;\n  case X86::DEC8r:\n  case X86::DEC16r:\n    addRegOffset(MIB, InRegLEA, true, -1);\n    break;\n  case X86::ADD8ri:\n  case X86::ADD8ri_DB:\n  case X86::ADD16ri:\n  case X86::ADD16ri8:\n  case X86::ADD16ri_DB:\n  case X86::ADD16ri8_DB:\n    addRegOffset(MIB, InRegLEA, true, MI.getOperand(2).getImm());\n    break;\n  case X86::ADD8rr:\n  case X86::ADD8rr_DB:\n  case X86::ADD16rr:\n  case X86::ADD16rr_DB: {\n    Register Src2 = MI.getOperand(2).getReg();\n    bool IsKill2 = MI.getOperand(2).isKill();\n    assert(!MI.getOperand(2).isUndef() && \"Undef op doesn't need optimization\");\n    unsigned InRegLEA2 = 0;\n    MachineInstr *InsMI2 = nullptr;\n    if (Src == Src2) {\n      // ADD8rr/ADD16rr killed %reg1028, %reg1028\n      // just a single insert_subreg.\n      addRegReg(MIB, InRegLEA, true, InRegLEA, false);\n    } else {\n      if (Subtarget.is64Bit())\n        InRegLEA2 = RegInfo.createVirtualRegister(&X86::GR64_NOSPRegClass);\n      else\n        InRegLEA2 = RegInfo.createVirtualRegister(&X86::GR32_NOSPRegClass);\n      // Build and insert into an implicit UNDEF value. This is OK because\n      // we will be shifting and then extracting the lower 8/16-bits.\n      BuildMI(*MFI, &*MIB, MI.getDebugLoc(), get(X86::IMPLICIT_DEF), InRegLEA2);\n      InsMI2 = BuildMI(*MFI, &*MIB, MI.getDebugLoc(), get(TargetOpcode::COPY))\n                   .addReg(InRegLEA2, RegState::Define, SubReg)\n                   .addReg(Src2, getKillRegState(IsKill2));\n      addRegReg(MIB, InRegLEA, true, InRegLEA2, true);\n    }\n    if (LV && IsKill2 && InsMI2)\n      LV->replaceKillInstruction(Src2, MI, *InsMI2);\n    break;\n  }\n  }\n\n  MachineInstr *NewMI = MIB;\n  MachineInstr *ExtMI =\n      BuildMI(*MFI, MBBI, MI.getDebugLoc(), get(TargetOpcode::COPY))\n          .addReg(Dest, RegState::Define | getDeadRegState(IsDead))\n          .addReg(OutRegLEA, RegState::Kill, SubReg);\n\n  if (LV) {\n    // Update live variables.\n    LV->getVarInfo(InRegLEA).Kills.push_back(NewMI);\n    LV->getVarInfo(OutRegLEA).Kills.push_back(ExtMI);\n    if (IsKill)\n      LV->replaceKillInstruction(Src, MI, *InsMI);\n    if (IsDead)\n      LV->replaceKillInstruction(Dest, MI, *ExtMI);\n  }\n\n  return ExtMI;\n}\n\n/// This method must be implemented by targets that\n/// set the M_CONVERTIBLE_TO_3_ADDR flag.  When this flag is set, the target\n/// may be able to convert a two-address instruction into a true\n/// three-address instruction on demand.  This allows the X86 target (for\n/// example) to convert ADD and SHL instructions into LEA instructions if they\n/// would require register copies due to two-addressness.\n///\n/// This method returns a null pointer if the transformation cannot be\n/// performed, otherwise it returns the new instruction.\n///\nMachineInstr *\nX86InstrInfo::convertToThreeAddress(MachineFunction::iterator &MFI,\n                                    MachineInstr &MI, LiveVariables *LV) const {\n  // The following opcodes also sets the condition code register(s). Only\n  // convert them to equivalent lea if the condition code register def's\n  // are dead!\n  if (hasLiveCondCodeDef(MI))\n    return nullptr;\n\n  MachineFunction &MF = *MI.getParent()->getParent();\n  // All instructions input are two-addr instructions.  Get the known operands.\n  const MachineOperand &Dest = MI.getOperand(0);\n  const MachineOperand &Src = MI.getOperand(1);\n\n  // Ideally, operations with undef should be folded before we get here, but we\n  // can't guarantee it. Bail out because optimizing undefs is a waste of time.\n  // Without this, we have to forward undef state to new register operands to\n  // avoid machine verifier errors.\n  if (Src.isUndef())\n    return nullptr;\n  if (MI.getNumOperands() > 2)\n    if (MI.getOperand(2).isReg() && MI.getOperand(2).isUndef())\n      return nullptr;\n\n  MachineInstr *NewMI = nullptr;\n  bool Is64Bit = Subtarget.is64Bit();\n\n  bool Is8BitOp = false;\n  unsigned MIOpc = MI.getOpcode();\n  switch (MIOpc) {\n  default: llvm_unreachable(\"Unreachable!\");\n  case X86::SHL64ri: {\n    assert(MI.getNumOperands() >= 3 && \"Unknown shift instruction!\");\n    unsigned ShAmt = getTruncatedShiftCount(MI, 2);\n    if (!isTruncatedShiftCountForLEA(ShAmt)) return nullptr;\n\n    // LEA can't handle RSP.\n    if (Src.getReg().isVirtual() && !MF.getRegInfo().constrainRegClass(\n                                        Src.getReg(), &X86::GR64_NOSPRegClass))\n      return nullptr;\n\n    NewMI = BuildMI(MF, MI.getDebugLoc(), get(X86::LEA64r))\n                .add(Dest)\n                .addReg(0)\n                .addImm(1ULL << ShAmt)\n                .add(Src)\n                .addImm(0)\n                .addReg(0);\n    break;\n  }\n  case X86::SHL32ri: {\n    assert(MI.getNumOperands() >= 3 && \"Unknown shift instruction!\");\n    unsigned ShAmt = getTruncatedShiftCount(MI, 2);\n    if (!isTruncatedShiftCountForLEA(ShAmt)) return nullptr;\n\n    unsigned Opc = Is64Bit ? X86::LEA64_32r : X86::LEA32r;\n\n    // LEA can't handle ESP.\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ false,\n                        SrcReg, isKill, ImplicitOp, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB =\n        BuildMI(MF, MI.getDebugLoc(), get(Opc))\n            .add(Dest)\n            .addReg(0)\n            .addImm(1ULL << ShAmt)\n            .addReg(SrcReg, getKillRegState(isKill))\n            .addImm(0)\n            .addReg(0);\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n    NewMI = MIB;\n\n    break;\n  }\n  case X86::SHL8ri:\n    Is8BitOp = true;\n    LLVM_FALLTHROUGH;\n  case X86::SHL16ri: {\n    assert(MI.getNumOperands() >= 3 && \"Unknown shift instruction!\");\n    unsigned ShAmt = getTruncatedShiftCount(MI, 2);\n    if (!isTruncatedShiftCountForLEA(ShAmt))\n      return nullptr;\n    return convertToThreeAddressWithLEA(MIOpc, MFI, MI, LV, Is8BitOp);\n  }\n  case X86::INC64r:\n  case X86::INC32r: {\n    assert(MI.getNumOperands() >= 2 && \"Unknown inc instruction!\");\n    unsigned Opc = MIOpc == X86::INC64r ? X86::LEA64r :\n        (Is64Bit ? X86::LEA64_32r : X86::LEA32r);\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ false, SrcReg, isKill,\n                        ImplicitOp, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB =\n        BuildMI(MF, MI.getDebugLoc(), get(Opc))\n            .add(Dest)\n            .addReg(SrcReg, getKillRegState(isKill));\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n\n    NewMI = addOffset(MIB, 1);\n    break;\n  }\n  case X86::DEC64r:\n  case X86::DEC32r: {\n    assert(MI.getNumOperands() >= 2 && \"Unknown dec instruction!\");\n    unsigned Opc = MIOpc == X86::DEC64r ? X86::LEA64r\n        : (Is64Bit ? X86::LEA64_32r : X86::LEA32r);\n\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ false, SrcReg, isKill,\n                        ImplicitOp, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB = BuildMI(MF, MI.getDebugLoc(), get(Opc))\n                                  .add(Dest)\n                                  .addReg(SrcReg, getKillRegState(isKill));\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n\n    NewMI = addOffset(MIB, -1);\n\n    break;\n  }\n  case X86::DEC8r:\n  case X86::INC8r:\n    Is8BitOp = true;\n    LLVM_FALLTHROUGH;\n  case X86::DEC16r:\n  case X86::INC16r:\n    return convertToThreeAddressWithLEA(MIOpc, MFI, MI, LV, Is8BitOp);\n  case X86::ADD64rr:\n  case X86::ADD64rr_DB:\n  case X86::ADD32rr:\n  case X86::ADD32rr_DB: {\n    assert(MI.getNumOperands() >= 3 && \"Unknown add instruction!\");\n    unsigned Opc;\n    if (MIOpc == X86::ADD64rr || MIOpc == X86::ADD64rr_DB)\n      Opc = X86::LEA64r;\n    else\n      Opc = Is64Bit ? X86::LEA64_32r : X86::LEA32r;\n\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ true,\n                        SrcReg, isKill, ImplicitOp, LV))\n      return nullptr;\n\n    const MachineOperand &Src2 = MI.getOperand(2);\n    bool isKill2;\n    Register SrcReg2;\n    MachineOperand ImplicitOp2 = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src2, Opc, /*AllowSP=*/ false,\n                        SrcReg2, isKill2, ImplicitOp2, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB = BuildMI(MF, MI.getDebugLoc(), get(Opc)).add(Dest);\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n    if (ImplicitOp2.getReg() != 0)\n      MIB.add(ImplicitOp2);\n\n    NewMI = addRegReg(MIB, SrcReg, isKill, SrcReg2, isKill2);\n    if (LV && Src2.isKill())\n      LV->replaceKillInstruction(SrcReg2, MI, *NewMI);\n    break;\n  }\n  case X86::ADD8rr:\n  case X86::ADD8rr_DB:\n    Is8BitOp = true;\n    LLVM_FALLTHROUGH;\n  case X86::ADD16rr:\n  case X86::ADD16rr_DB:\n    return convertToThreeAddressWithLEA(MIOpc, MFI, MI, LV, Is8BitOp);\n  case X86::ADD64ri32:\n  case X86::ADD64ri8:\n  case X86::ADD64ri32_DB:\n  case X86::ADD64ri8_DB:\n    assert(MI.getNumOperands() >= 3 && \"Unknown add instruction!\");\n    NewMI = addOffset(\n        BuildMI(MF, MI.getDebugLoc(), get(X86::LEA64r)).add(Dest).add(Src),\n        MI.getOperand(2));\n    break;\n  case X86::ADD32ri:\n  case X86::ADD32ri8:\n  case X86::ADD32ri_DB:\n  case X86::ADD32ri8_DB: {\n    assert(MI.getNumOperands() >= 3 && \"Unknown add instruction!\");\n    unsigned Opc = Is64Bit ? X86::LEA64_32r : X86::LEA32r;\n\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ true,\n                        SrcReg, isKill, ImplicitOp, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB = BuildMI(MF, MI.getDebugLoc(), get(Opc))\n                                  .add(Dest)\n                                  .addReg(SrcReg, getKillRegState(isKill));\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n\n    NewMI = addOffset(MIB, MI.getOperand(2));\n    break;\n  }\n  case X86::ADD8ri:\n  case X86::ADD8ri_DB:\n    Is8BitOp = true;\n    LLVM_FALLTHROUGH;\n  case X86::ADD16ri:\n  case X86::ADD16ri8:\n  case X86::ADD16ri_DB:\n  case X86::ADD16ri8_DB:\n    return convertToThreeAddressWithLEA(MIOpc, MFI, MI, LV, Is8BitOp);\n  case X86::SUB8ri:\n  case X86::SUB16ri8:\n  case X86::SUB16ri:\n    /// FIXME: Support these similar to ADD8ri/ADD16ri*.\n    return nullptr;\n  case X86::SUB32ri8:\n  case X86::SUB32ri: {\n    if (!MI.getOperand(2).isImm())\n      return nullptr;\n    int64_t Imm = MI.getOperand(2).getImm();\n    if (!isInt<32>(-Imm))\n      return nullptr;\n\n    assert(MI.getNumOperands() >= 3 && \"Unknown add instruction!\");\n    unsigned Opc = Is64Bit ? X86::LEA64_32r : X86::LEA32r;\n\n    bool isKill;\n    Register SrcReg;\n    MachineOperand ImplicitOp = MachineOperand::CreateReg(0, false);\n    if (!classifyLEAReg(MI, Src, Opc, /*AllowSP=*/ true,\n                        SrcReg, isKill, ImplicitOp, LV))\n      return nullptr;\n\n    MachineInstrBuilder MIB = BuildMI(MF, MI.getDebugLoc(), get(Opc))\n                                  .add(Dest)\n                                  .addReg(SrcReg, getKillRegState(isKill));\n    if (ImplicitOp.getReg() != 0)\n      MIB.add(ImplicitOp);\n\n    NewMI = addOffset(MIB, -Imm);\n    break;\n  }\n\n  case X86::SUB64ri8:\n  case X86::SUB64ri32: {\n    if (!MI.getOperand(2).isImm())\n      return nullptr;\n    int64_t Imm = MI.getOperand(2).getImm();\n    if (!isInt<32>(-Imm))\n      return nullptr;\n\n    assert(MI.getNumOperands() >= 3 && \"Unknown sub instruction!\");\n\n    MachineInstrBuilder MIB = BuildMI(MF, MI.getDebugLoc(),\n                                      get(X86::LEA64r)).add(Dest).add(Src);\n    NewMI = addOffset(MIB, -Imm);\n    break;\n  }\n\n  case X86::VMOVDQU8Z128rmk:\n  case X86::VMOVDQU8Z256rmk:\n  case X86::VMOVDQU8Zrmk:\n  case X86::VMOVDQU16Z128rmk:\n  case X86::VMOVDQU16Z256rmk:\n  case X86::VMOVDQU16Zrmk:\n  case X86::VMOVDQU32Z128rmk: case X86::VMOVDQA32Z128rmk:\n  case X86::VMOVDQU32Z256rmk: case X86::VMOVDQA32Z256rmk:\n  case X86::VMOVDQU32Zrmk:    case X86::VMOVDQA32Zrmk:\n  case X86::VMOVDQU64Z128rmk: case X86::VMOVDQA64Z128rmk:\n  case X86::VMOVDQU64Z256rmk: case X86::VMOVDQA64Z256rmk:\n  case X86::VMOVDQU64Zrmk:    case X86::VMOVDQA64Zrmk:\n  case X86::VMOVUPDZ128rmk:   case X86::VMOVAPDZ128rmk:\n  case X86::VMOVUPDZ256rmk:   case X86::VMOVAPDZ256rmk:\n  case X86::VMOVUPDZrmk:      case X86::VMOVAPDZrmk:\n  case X86::VMOVUPSZ128rmk:   case X86::VMOVAPSZ128rmk:\n  case X86::VMOVUPSZ256rmk:   case X86::VMOVAPSZ256rmk:\n  case X86::VMOVUPSZrmk:      case X86::VMOVAPSZrmk:\n  case X86::VBROADCASTSDZ256rmk:\n  case X86::VBROADCASTSDZrmk:\n  case X86::VBROADCASTSSZ128rmk:\n  case X86::VBROADCASTSSZ256rmk:\n  case X86::VBROADCASTSSZrmk:\n  case X86::VPBROADCASTDZ128rmk:\n  case X86::VPBROADCASTDZ256rmk:\n  case X86::VPBROADCASTDZrmk:\n  case X86::VPBROADCASTQZ128rmk:\n  case X86::VPBROADCASTQZ256rmk:\n  case X86::VPBROADCASTQZrmk: {\n    unsigned Opc;\n    switch (MIOpc) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::VMOVDQU8Z128rmk:     Opc = X86::VPBLENDMBZ128rmk; break;\n    case X86::VMOVDQU8Z256rmk:     Opc = X86::VPBLENDMBZ256rmk; break;\n    case X86::VMOVDQU8Zrmk:        Opc = X86::VPBLENDMBZrmk;    break;\n    case X86::VMOVDQU16Z128rmk:    Opc = X86::VPBLENDMWZ128rmk; break;\n    case X86::VMOVDQU16Z256rmk:    Opc = X86::VPBLENDMWZ256rmk; break;\n    case X86::VMOVDQU16Zrmk:       Opc = X86::VPBLENDMWZrmk;    break;\n    case X86::VMOVDQU32Z128rmk:    Opc = X86::VPBLENDMDZ128rmk; break;\n    case X86::VMOVDQU32Z256rmk:    Opc = X86::VPBLENDMDZ256rmk; break;\n    case X86::VMOVDQU32Zrmk:       Opc = X86::VPBLENDMDZrmk;    break;\n    case X86::VMOVDQU64Z128rmk:    Opc = X86::VPBLENDMQZ128rmk; break;\n    case X86::VMOVDQU64Z256rmk:    Opc = X86::VPBLENDMQZ256rmk; break;\n    case X86::VMOVDQU64Zrmk:       Opc = X86::VPBLENDMQZrmk;    break;\n    case X86::VMOVUPDZ128rmk:      Opc = X86::VBLENDMPDZ128rmk; break;\n    case X86::VMOVUPDZ256rmk:      Opc = X86::VBLENDMPDZ256rmk; break;\n    case X86::VMOVUPDZrmk:         Opc = X86::VBLENDMPDZrmk;    break;\n    case X86::VMOVUPSZ128rmk:      Opc = X86::VBLENDMPSZ128rmk; break;\n    case X86::VMOVUPSZ256rmk:      Opc = X86::VBLENDMPSZ256rmk; break;\n    case X86::VMOVUPSZrmk:         Opc = X86::VBLENDMPSZrmk;    break;\n    case X86::VMOVDQA32Z128rmk:    Opc = X86::VPBLENDMDZ128rmk; break;\n    case X86::VMOVDQA32Z256rmk:    Opc = X86::VPBLENDMDZ256rmk; break;\n    case X86::VMOVDQA32Zrmk:       Opc = X86::VPBLENDMDZrmk;    break;\n    case X86::VMOVDQA64Z128rmk:    Opc = X86::VPBLENDMQZ128rmk; break;\n    case X86::VMOVDQA64Z256rmk:    Opc = X86::VPBLENDMQZ256rmk; break;\n    case X86::VMOVDQA64Zrmk:       Opc = X86::VPBLENDMQZrmk;    break;\n    case X86::VMOVAPDZ128rmk:      Opc = X86::VBLENDMPDZ128rmk; break;\n    case X86::VMOVAPDZ256rmk:      Opc = X86::VBLENDMPDZ256rmk; break;\n    case X86::VMOVAPDZrmk:         Opc = X86::VBLENDMPDZrmk;    break;\n    case X86::VMOVAPSZ128rmk:      Opc = X86::VBLENDMPSZ128rmk; break;\n    case X86::VMOVAPSZ256rmk:      Opc = X86::VBLENDMPSZ256rmk; break;\n    case X86::VMOVAPSZrmk:         Opc = X86::VBLENDMPSZrmk;    break;\n    case X86::VBROADCASTSDZ256rmk: Opc = X86::VBLENDMPDZ256rmbk; break;\n    case X86::VBROADCASTSDZrmk:    Opc = X86::VBLENDMPDZrmbk;    break;\n    case X86::VBROADCASTSSZ128rmk: Opc = X86::VBLENDMPSZ128rmbk; break;\n    case X86::VBROADCASTSSZ256rmk: Opc = X86::VBLENDMPSZ256rmbk; break;\n    case X86::VBROADCASTSSZrmk:    Opc = X86::VBLENDMPSZrmbk;    break;\n    case X86::VPBROADCASTDZ128rmk: Opc = X86::VPBLENDMDZ128rmbk; break;\n    case X86::VPBROADCASTDZ256rmk: Opc = X86::VPBLENDMDZ256rmbk; break;\n    case X86::VPBROADCASTDZrmk:    Opc = X86::VPBLENDMDZrmbk;    break;\n    case X86::VPBROADCASTQZ128rmk: Opc = X86::VPBLENDMQZ128rmbk; break;\n    case X86::VPBROADCASTQZ256rmk: Opc = X86::VPBLENDMQZ256rmbk; break;\n    case X86::VPBROADCASTQZrmk:    Opc = X86::VPBLENDMQZrmbk;    break;\n    }\n\n    NewMI = BuildMI(MF, MI.getDebugLoc(), get(Opc))\n              .add(Dest)\n              .add(MI.getOperand(2))\n              .add(Src)\n              .add(MI.getOperand(3))\n              .add(MI.getOperand(4))\n              .add(MI.getOperand(5))\n              .add(MI.getOperand(6))\n              .add(MI.getOperand(7));\n    break;\n  }\n\n  case X86::VMOVDQU8Z128rrk:\n  case X86::VMOVDQU8Z256rrk:\n  case X86::VMOVDQU8Zrrk:\n  case X86::VMOVDQU16Z128rrk:\n  case X86::VMOVDQU16Z256rrk:\n  case X86::VMOVDQU16Zrrk:\n  case X86::VMOVDQU32Z128rrk: case X86::VMOVDQA32Z128rrk:\n  case X86::VMOVDQU32Z256rrk: case X86::VMOVDQA32Z256rrk:\n  case X86::VMOVDQU32Zrrk:    case X86::VMOVDQA32Zrrk:\n  case X86::VMOVDQU64Z128rrk: case X86::VMOVDQA64Z128rrk:\n  case X86::VMOVDQU64Z256rrk: case X86::VMOVDQA64Z256rrk:\n  case X86::VMOVDQU64Zrrk:    case X86::VMOVDQA64Zrrk:\n  case X86::VMOVUPDZ128rrk:   case X86::VMOVAPDZ128rrk:\n  case X86::VMOVUPDZ256rrk:   case X86::VMOVAPDZ256rrk:\n  case X86::VMOVUPDZrrk:      case X86::VMOVAPDZrrk:\n  case X86::VMOVUPSZ128rrk:   case X86::VMOVAPSZ128rrk:\n  case X86::VMOVUPSZ256rrk:   case X86::VMOVAPSZ256rrk:\n  case X86::VMOVUPSZrrk:      case X86::VMOVAPSZrrk: {\n    unsigned Opc;\n    switch (MIOpc) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::VMOVDQU8Z128rrk:  Opc = X86::VPBLENDMBZ128rrk; break;\n    case X86::VMOVDQU8Z256rrk:  Opc = X86::VPBLENDMBZ256rrk; break;\n    case X86::VMOVDQU8Zrrk:     Opc = X86::VPBLENDMBZrrk;    break;\n    case X86::VMOVDQU16Z128rrk: Opc = X86::VPBLENDMWZ128rrk; break;\n    case X86::VMOVDQU16Z256rrk: Opc = X86::VPBLENDMWZ256rrk; break;\n    case X86::VMOVDQU16Zrrk:    Opc = X86::VPBLENDMWZrrk;    break;\n    case X86::VMOVDQU32Z128rrk: Opc = X86::VPBLENDMDZ128rrk; break;\n    case X86::VMOVDQU32Z256rrk: Opc = X86::VPBLENDMDZ256rrk; break;\n    case X86::VMOVDQU32Zrrk:    Opc = X86::VPBLENDMDZrrk;    break;\n    case X86::VMOVDQU64Z128rrk: Opc = X86::VPBLENDMQZ128rrk; break;\n    case X86::VMOVDQU64Z256rrk: Opc = X86::VPBLENDMQZ256rrk; break;\n    case X86::VMOVDQU64Zrrk:    Opc = X86::VPBLENDMQZrrk;    break;\n    case X86::VMOVUPDZ128rrk:   Opc = X86::VBLENDMPDZ128rrk; break;\n    case X86::VMOVUPDZ256rrk:   Opc = X86::VBLENDMPDZ256rrk; break;\n    case X86::VMOVUPDZrrk:      Opc = X86::VBLENDMPDZrrk;    break;\n    case X86::VMOVUPSZ128rrk:   Opc = X86::VBLENDMPSZ128rrk; break;\n    case X86::VMOVUPSZ256rrk:   Opc = X86::VBLENDMPSZ256rrk; break;\n    case X86::VMOVUPSZrrk:      Opc = X86::VBLENDMPSZrrk;    break;\n    case X86::VMOVDQA32Z128rrk: Opc = X86::VPBLENDMDZ128rrk; break;\n    case X86::VMOVDQA32Z256rrk: Opc = X86::VPBLENDMDZ256rrk; break;\n    case X86::VMOVDQA32Zrrk:    Opc = X86::VPBLENDMDZrrk;    break;\n    case X86::VMOVDQA64Z128rrk: Opc = X86::VPBLENDMQZ128rrk; break;\n    case X86::VMOVDQA64Z256rrk: Opc = X86::VPBLENDMQZ256rrk; break;\n    case X86::VMOVDQA64Zrrk:    Opc = X86::VPBLENDMQZrrk;    break;\n    case X86::VMOVAPDZ128rrk:   Opc = X86::VBLENDMPDZ128rrk; break;\n    case X86::VMOVAPDZ256rrk:   Opc = X86::VBLENDMPDZ256rrk; break;\n    case X86::VMOVAPDZrrk:      Opc = X86::VBLENDMPDZrrk;    break;\n    case X86::VMOVAPSZ128rrk:   Opc = X86::VBLENDMPSZ128rrk; break;\n    case X86::VMOVAPSZ256rrk:   Opc = X86::VBLENDMPSZ256rrk; break;\n    case X86::VMOVAPSZrrk:      Opc = X86::VBLENDMPSZrrk;    break;\n    }\n\n    NewMI = BuildMI(MF, MI.getDebugLoc(), get(Opc))\n              .add(Dest)\n              .add(MI.getOperand(2))\n              .add(Src)\n              .add(MI.getOperand(3));\n    break;\n  }\n  }\n\n  if (!NewMI) return nullptr;\n\n  if (LV) {  // Update live variables\n    if (Src.isKill())\n      LV->replaceKillInstruction(Src.getReg(), MI, *NewMI);\n    if (Dest.isDead())\n      LV->replaceKillInstruction(Dest.getReg(), MI, *NewMI);\n  }\n\n  MFI->insert(MI.getIterator(), NewMI); // Insert the new inst\n  return NewMI;\n}\n\n/// This determines which of three possible cases of a three source commute\n/// the source indexes correspond to taking into account any mask operands.\n/// All prevents commuting a passthru operand. Returns -1 if the commute isn't\n/// possible.\n/// Case 0 - Possible to commute the first and second operands.\n/// Case 1 - Possible to commute the first and third operands.\n/// Case 2 - Possible to commute the second and third operands.\nstatic unsigned getThreeSrcCommuteCase(uint64_t TSFlags, unsigned SrcOpIdx1,\n                                       unsigned SrcOpIdx2) {\n  // Put the lowest index to SrcOpIdx1 to simplify the checks below.\n  if (SrcOpIdx1 > SrcOpIdx2)\n    std::swap(SrcOpIdx1, SrcOpIdx2);\n\n  unsigned Op1 = 1, Op2 = 2, Op3 = 3;\n  if (X86II::isKMasked(TSFlags)) {\n    Op2++;\n    Op3++;\n  }\n\n  if (SrcOpIdx1 == Op1 && SrcOpIdx2 == Op2)\n    return 0;\n  if (SrcOpIdx1 == Op1 && SrcOpIdx2 == Op3)\n    return 1;\n  if (SrcOpIdx1 == Op2 && SrcOpIdx2 == Op3)\n    return 2;\n  llvm_unreachable(\"Unknown three src commute case.\");\n}\n\nunsigned X86InstrInfo::getFMA3OpcodeToCommuteOperands(\n    const MachineInstr &MI, unsigned SrcOpIdx1, unsigned SrcOpIdx2,\n    const X86InstrFMA3Group &FMA3Group) const {\n\n  unsigned Opc = MI.getOpcode();\n\n  // TODO: Commuting the 1st operand of FMA*_Int requires some additional\n  // analysis. The commute optimization is legal only if all users of FMA*_Int\n  // use only the lowest element of the FMA*_Int instruction. Such analysis are\n  // not implemented yet. So, just return 0 in that case.\n  // When such analysis are available this place will be the right place for\n  // calling it.\n  assert(!(FMA3Group.isIntrinsic() && (SrcOpIdx1 == 1 || SrcOpIdx2 == 1)) &&\n         \"Intrinsic instructions can't commute operand 1\");\n\n  // Determine which case this commute is or if it can't be done.\n  unsigned Case = getThreeSrcCommuteCase(MI.getDesc().TSFlags, SrcOpIdx1,\n                                         SrcOpIdx2);\n  assert(Case < 3 && \"Unexpected case number!\");\n\n  // Define the FMA forms mapping array that helps to map input FMA form\n  // to output FMA form to preserve the operation semantics after\n  // commuting the operands.\n  const unsigned Form132Index = 0;\n  const unsigned Form213Index = 1;\n  const unsigned Form231Index = 2;\n  static const unsigned FormMapping[][3] = {\n    // 0: SrcOpIdx1 == 1 && SrcOpIdx2 == 2;\n    // FMA132 A, C, b; ==> FMA231 C, A, b;\n    // FMA213 B, A, c; ==> FMA213 A, B, c;\n    // FMA231 C, A, b; ==> FMA132 A, C, b;\n    { Form231Index, Form213Index, Form132Index },\n    // 1: SrcOpIdx1 == 1 && SrcOpIdx2 == 3;\n    // FMA132 A, c, B; ==> FMA132 B, c, A;\n    // FMA213 B, a, C; ==> FMA231 C, a, B;\n    // FMA231 C, a, B; ==> FMA213 B, a, C;\n    { Form132Index, Form231Index, Form213Index },\n    // 2: SrcOpIdx1 == 2 && SrcOpIdx2 == 3;\n    // FMA132 a, C, B; ==> FMA213 a, B, C;\n    // FMA213 b, A, C; ==> FMA132 b, C, A;\n    // FMA231 c, A, B; ==> FMA231 c, B, A;\n    { Form213Index, Form132Index, Form231Index }\n  };\n\n  unsigned FMAForms[3];\n  FMAForms[0] = FMA3Group.get132Opcode();\n  FMAForms[1] = FMA3Group.get213Opcode();\n  FMAForms[2] = FMA3Group.get231Opcode();\n  unsigned FormIndex;\n  for (FormIndex = 0; FormIndex < 3; FormIndex++)\n    if (Opc == FMAForms[FormIndex])\n      break;\n\n  // Everything is ready, just adjust the FMA opcode and return it.\n  FormIndex = FormMapping[Case][FormIndex];\n  return FMAForms[FormIndex];\n}\n\nstatic void commuteVPTERNLOG(MachineInstr &MI, unsigned SrcOpIdx1,\n                             unsigned SrcOpIdx2) {\n  // Determine which case this commute is or if it can't be done.\n  unsigned Case = getThreeSrcCommuteCase(MI.getDesc().TSFlags, SrcOpIdx1,\n                                         SrcOpIdx2);\n  assert(Case < 3 && \"Unexpected case value!\");\n\n  // For each case we need to swap two pairs of bits in the final immediate.\n  static const uint8_t SwapMasks[3][4] = {\n    { 0x04, 0x10, 0x08, 0x20 }, // Swap bits 2/4 and 3/5.\n    { 0x02, 0x10, 0x08, 0x40 }, // Swap bits 1/4 and 3/6.\n    { 0x02, 0x04, 0x20, 0x40 }, // Swap bits 1/2 and 5/6.\n  };\n\n  uint8_t Imm = MI.getOperand(MI.getNumOperands()-1).getImm();\n  // Clear out the bits we are swapping.\n  uint8_t NewImm = Imm & ~(SwapMasks[Case][0] | SwapMasks[Case][1] |\n                           SwapMasks[Case][2] | SwapMasks[Case][3]);\n  // If the immediate had a bit of the pair set, then set the opposite bit.\n  if (Imm & SwapMasks[Case][0]) NewImm |= SwapMasks[Case][1];\n  if (Imm & SwapMasks[Case][1]) NewImm |= SwapMasks[Case][0];\n  if (Imm & SwapMasks[Case][2]) NewImm |= SwapMasks[Case][3];\n  if (Imm & SwapMasks[Case][3]) NewImm |= SwapMasks[Case][2];\n  MI.getOperand(MI.getNumOperands()-1).setImm(NewImm);\n}\n\n// Returns true if this is a VPERMI2 or VPERMT2 instruction that can be\n// commuted.\nstatic bool isCommutableVPERMV3Instruction(unsigned Opcode) {\n#define VPERM_CASES(Suffix) \\\n  case X86::VPERMI2##Suffix##128rr:    case X86::VPERMT2##Suffix##128rr:    \\\n  case X86::VPERMI2##Suffix##256rr:    case X86::VPERMT2##Suffix##256rr:    \\\n  case X86::VPERMI2##Suffix##rr:       case X86::VPERMT2##Suffix##rr:       \\\n  case X86::VPERMI2##Suffix##128rm:    case X86::VPERMT2##Suffix##128rm:    \\\n  case X86::VPERMI2##Suffix##256rm:    case X86::VPERMT2##Suffix##256rm:    \\\n  case X86::VPERMI2##Suffix##rm:       case X86::VPERMT2##Suffix##rm:       \\\n  case X86::VPERMI2##Suffix##128rrkz:  case X86::VPERMT2##Suffix##128rrkz:  \\\n  case X86::VPERMI2##Suffix##256rrkz:  case X86::VPERMT2##Suffix##256rrkz:  \\\n  case X86::VPERMI2##Suffix##rrkz:     case X86::VPERMT2##Suffix##rrkz:     \\\n  case X86::VPERMI2##Suffix##128rmkz:  case X86::VPERMT2##Suffix##128rmkz:  \\\n  case X86::VPERMI2##Suffix##256rmkz:  case X86::VPERMT2##Suffix##256rmkz:  \\\n  case X86::VPERMI2##Suffix##rmkz:     case X86::VPERMT2##Suffix##rmkz:\n\n#define VPERM_CASES_BROADCAST(Suffix) \\\n  VPERM_CASES(Suffix) \\\n  case X86::VPERMI2##Suffix##128rmb:   case X86::VPERMT2##Suffix##128rmb:   \\\n  case X86::VPERMI2##Suffix##256rmb:   case X86::VPERMT2##Suffix##256rmb:   \\\n  case X86::VPERMI2##Suffix##rmb:      case X86::VPERMT2##Suffix##rmb:      \\\n  case X86::VPERMI2##Suffix##128rmbkz: case X86::VPERMT2##Suffix##128rmbkz: \\\n  case X86::VPERMI2##Suffix##256rmbkz: case X86::VPERMT2##Suffix##256rmbkz: \\\n  case X86::VPERMI2##Suffix##rmbkz:    case X86::VPERMT2##Suffix##rmbkz:\n\n  switch (Opcode) {\n  default: return false;\n  VPERM_CASES(B)\n  VPERM_CASES_BROADCAST(D)\n  VPERM_CASES_BROADCAST(PD)\n  VPERM_CASES_BROADCAST(PS)\n  VPERM_CASES_BROADCAST(Q)\n  VPERM_CASES(W)\n    return true;\n  }\n#undef VPERM_CASES_BROADCAST\n#undef VPERM_CASES\n}\n\n// Returns commuted opcode for VPERMI2 and VPERMT2 instructions by switching\n// from the I opcode to the T opcode and vice versa.\nstatic unsigned getCommutedVPERMV3Opcode(unsigned Opcode) {\n#define VPERM_CASES(Orig, New) \\\n  case X86::Orig##128rr:    return X86::New##128rr;   \\\n  case X86::Orig##128rrkz:  return X86::New##128rrkz; \\\n  case X86::Orig##128rm:    return X86::New##128rm;   \\\n  case X86::Orig##128rmkz:  return X86::New##128rmkz; \\\n  case X86::Orig##256rr:    return X86::New##256rr;   \\\n  case X86::Orig##256rrkz:  return X86::New##256rrkz; \\\n  case X86::Orig##256rm:    return X86::New##256rm;   \\\n  case X86::Orig##256rmkz:  return X86::New##256rmkz; \\\n  case X86::Orig##rr:       return X86::New##rr;      \\\n  case X86::Orig##rrkz:     return X86::New##rrkz;    \\\n  case X86::Orig##rm:       return X86::New##rm;      \\\n  case X86::Orig##rmkz:     return X86::New##rmkz;\n\n#define VPERM_CASES_BROADCAST(Orig, New) \\\n  VPERM_CASES(Orig, New) \\\n  case X86::Orig##128rmb:   return X86::New##128rmb;   \\\n  case X86::Orig##128rmbkz: return X86::New##128rmbkz; \\\n  case X86::Orig##256rmb:   return X86::New##256rmb;   \\\n  case X86::Orig##256rmbkz: return X86::New##256rmbkz; \\\n  case X86::Orig##rmb:      return X86::New##rmb;      \\\n  case X86::Orig##rmbkz:    return X86::New##rmbkz;\n\n  switch (Opcode) {\n  VPERM_CASES(VPERMI2B, VPERMT2B)\n  VPERM_CASES_BROADCAST(VPERMI2D,  VPERMT2D)\n  VPERM_CASES_BROADCAST(VPERMI2PD, VPERMT2PD)\n  VPERM_CASES_BROADCAST(VPERMI2PS, VPERMT2PS)\n  VPERM_CASES_BROADCAST(VPERMI2Q,  VPERMT2Q)\n  VPERM_CASES(VPERMI2W, VPERMT2W)\n  VPERM_CASES(VPERMT2B, VPERMI2B)\n  VPERM_CASES_BROADCAST(VPERMT2D,  VPERMI2D)\n  VPERM_CASES_BROADCAST(VPERMT2PD, VPERMI2PD)\n  VPERM_CASES_BROADCAST(VPERMT2PS, VPERMI2PS)\n  VPERM_CASES_BROADCAST(VPERMT2Q,  VPERMI2Q)\n  VPERM_CASES(VPERMT2W, VPERMI2W)\n  }\n\n  llvm_unreachable(\"Unreachable!\");\n#undef VPERM_CASES_BROADCAST\n#undef VPERM_CASES\n}\n\nMachineInstr *X86InstrInfo::commuteInstructionImpl(MachineInstr &MI, bool NewMI,\n                                                   unsigned OpIdx1,\n                                                   unsigned OpIdx2) const {\n  auto cloneIfNew = [NewMI](MachineInstr &MI) -> MachineInstr & {\n    if (NewMI)\n      return *MI.getParent()->getParent()->CloneMachineInstr(&MI);\n    return MI;\n  };\n\n  switch (MI.getOpcode()) {\n  case X86::SHRD16rri8: // A = SHRD16rri8 B, C, I -> A = SHLD16rri8 C, B, (16-I)\n  case X86::SHLD16rri8: // A = SHLD16rri8 B, C, I -> A = SHRD16rri8 C, B, (16-I)\n  case X86::SHRD32rri8: // A = SHRD32rri8 B, C, I -> A = SHLD32rri8 C, B, (32-I)\n  case X86::SHLD32rri8: // A = SHLD32rri8 B, C, I -> A = SHRD32rri8 C, B, (32-I)\n  case X86::SHRD64rri8: // A = SHRD64rri8 B, C, I -> A = SHLD64rri8 C, B, (64-I)\n  case X86::SHLD64rri8:{// A = SHLD64rri8 B, C, I -> A = SHRD64rri8 C, B, (64-I)\n    unsigned Opc;\n    unsigned Size;\n    switch (MI.getOpcode()) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::SHRD16rri8: Size = 16; Opc = X86::SHLD16rri8; break;\n    case X86::SHLD16rri8: Size = 16; Opc = X86::SHRD16rri8; break;\n    case X86::SHRD32rri8: Size = 32; Opc = X86::SHLD32rri8; break;\n    case X86::SHLD32rri8: Size = 32; Opc = X86::SHRD32rri8; break;\n    case X86::SHRD64rri8: Size = 64; Opc = X86::SHLD64rri8; break;\n    case X86::SHLD64rri8: Size = 64; Opc = X86::SHRD64rri8; break;\n    }\n    unsigned Amt = MI.getOperand(3).getImm();\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.setDesc(get(Opc));\n    WorkingMI.getOperand(3).setImm(Size - Amt);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::PFSUBrr:\n  case X86::PFSUBRrr: {\n    // PFSUB  x, y: x = x - y\n    // PFSUBR x, y: x = y - x\n    unsigned Opc =\n        (X86::PFSUBRrr == MI.getOpcode() ? X86::PFSUBrr : X86::PFSUBRrr);\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.setDesc(get(Opc));\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::BLENDPDrri:\n  case X86::BLENDPSrri:\n  case X86::VBLENDPDrri:\n  case X86::VBLENDPSrri:\n    // If we're optimizing for size, try to use MOVSD/MOVSS.\n    if (MI.getParent()->getParent()->getFunction().hasOptSize()) {\n      unsigned Mask, Opc;\n      switch (MI.getOpcode()) {\n      default: llvm_unreachable(\"Unreachable!\");\n      case X86::BLENDPDrri:  Opc = X86::MOVSDrr;  Mask = 0x03; break;\n      case X86::BLENDPSrri:  Opc = X86::MOVSSrr;  Mask = 0x0F; break;\n      case X86::VBLENDPDrri: Opc = X86::VMOVSDrr; Mask = 0x03; break;\n      case X86::VBLENDPSrri: Opc = X86::VMOVSSrr; Mask = 0x0F; break;\n      }\n      if ((MI.getOperand(3).getImm() ^ Mask) == 1) {\n        auto &WorkingMI = cloneIfNew(MI);\n        WorkingMI.setDesc(get(Opc));\n        WorkingMI.RemoveOperand(3);\n        return TargetInstrInfo::commuteInstructionImpl(WorkingMI,\n                                                       /*NewMI=*/false,\n                                                       OpIdx1, OpIdx2);\n      }\n    }\n    LLVM_FALLTHROUGH;\n  case X86::PBLENDWrri:\n  case X86::VBLENDPDYrri:\n  case X86::VBLENDPSYrri:\n  case X86::VPBLENDDrri:\n  case X86::VPBLENDWrri:\n  case X86::VPBLENDDYrri:\n  case X86::VPBLENDWYrri:{\n    int8_t Mask;\n    switch (MI.getOpcode()) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::BLENDPDrri:    Mask = (int8_t)0x03; break;\n    case X86::BLENDPSrri:    Mask = (int8_t)0x0F; break;\n    case X86::PBLENDWrri:    Mask = (int8_t)0xFF; break;\n    case X86::VBLENDPDrri:   Mask = (int8_t)0x03; break;\n    case X86::VBLENDPSrri:   Mask = (int8_t)0x0F; break;\n    case X86::VBLENDPDYrri:  Mask = (int8_t)0x0F; break;\n    case X86::VBLENDPSYrri:  Mask = (int8_t)0xFF; break;\n    case X86::VPBLENDDrri:   Mask = (int8_t)0x0F; break;\n    case X86::VPBLENDWrri:   Mask = (int8_t)0xFF; break;\n    case X86::VPBLENDDYrri:  Mask = (int8_t)0xFF; break;\n    case X86::VPBLENDWYrri:  Mask = (int8_t)0xFF; break;\n    }\n    // Only the least significant bits of Imm are used.\n    // Using int8_t to ensure it will be sign extended to the int64_t that\n    // setImm takes in order to match isel behavior.\n    int8_t Imm = MI.getOperand(3).getImm() & Mask;\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(3).setImm(Mask ^ Imm);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::INSERTPSrr:\n  case X86::VINSERTPSrr:\n  case X86::VINSERTPSZrr: {\n    unsigned Imm = MI.getOperand(MI.getNumOperands() - 1).getImm();\n    unsigned ZMask = Imm & 15;\n    unsigned DstIdx = (Imm >> 4) & 3;\n    unsigned SrcIdx = (Imm >> 6) & 3;\n\n    // We can commute insertps if we zero 2 of the elements, the insertion is\n    // \"inline\" and we don't override the insertion with a zero.\n    if (DstIdx == SrcIdx && (ZMask & (1 << DstIdx)) == 0 &&\n        countPopulation(ZMask) == 2) {\n      unsigned AltIdx = findFirstSet((ZMask | (1 << DstIdx)) ^ 15);\n      assert(AltIdx < 4 && \"Illegal insertion index\");\n      unsigned AltImm = (AltIdx << 6) | (AltIdx << 4) | ZMask;\n      auto &WorkingMI = cloneIfNew(MI);\n      WorkingMI.getOperand(MI.getNumOperands() - 1).setImm(AltImm);\n      return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                     OpIdx1, OpIdx2);\n    }\n    return nullptr;\n  }\n  case X86::MOVSDrr:\n  case X86::MOVSSrr:\n  case X86::VMOVSDrr:\n  case X86::VMOVSSrr:{\n    // On SSE41 or later we can commute a MOVSS/MOVSD to a BLENDPS/BLENDPD.\n    if (Subtarget.hasSSE41()) {\n      unsigned Mask, Opc;\n      switch (MI.getOpcode()) {\n      default: llvm_unreachable(\"Unreachable!\");\n      case X86::MOVSDrr:  Opc = X86::BLENDPDrri;  Mask = 0x02; break;\n      case X86::MOVSSrr:  Opc = X86::BLENDPSrri;  Mask = 0x0E; break;\n      case X86::VMOVSDrr: Opc = X86::VBLENDPDrri; Mask = 0x02; break;\n      case X86::VMOVSSrr: Opc = X86::VBLENDPSrri; Mask = 0x0E; break;\n      }\n\n      auto &WorkingMI = cloneIfNew(MI);\n      WorkingMI.setDesc(get(Opc));\n      WorkingMI.addOperand(MachineOperand::CreateImm(Mask));\n      return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                     OpIdx1, OpIdx2);\n    }\n\n    // Convert to SHUFPD.\n    assert(MI.getOpcode() == X86::MOVSDrr &&\n           \"Can only commute MOVSDrr without SSE4.1\");\n\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.setDesc(get(X86::SHUFPDrri));\n    WorkingMI.addOperand(MachineOperand::CreateImm(0x02));\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::SHUFPDrri: {\n    // Commute to MOVSD.\n    assert(MI.getOperand(3).getImm() == 0x02 && \"Unexpected immediate!\");\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.setDesc(get(X86::MOVSDrr));\n    WorkingMI.RemoveOperand(3);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::PCLMULQDQrr:\n  case X86::VPCLMULQDQrr:\n  case X86::VPCLMULQDQYrr:\n  case X86::VPCLMULQDQZrr:\n  case X86::VPCLMULQDQZ128rr:\n  case X86::VPCLMULQDQZ256rr: {\n    // SRC1 64bits = Imm[0] ? SRC1[127:64] : SRC1[63:0]\n    // SRC2 64bits = Imm[4] ? SRC2[127:64] : SRC2[63:0]\n    unsigned Imm = MI.getOperand(3).getImm();\n    unsigned Src1Hi = Imm & 0x01;\n    unsigned Src2Hi = Imm & 0x10;\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(3).setImm((Src1Hi << 4) | (Src2Hi >> 4));\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::VPCMPBZ128rri:  case X86::VPCMPUBZ128rri:\n  case X86::VPCMPBZ256rri:  case X86::VPCMPUBZ256rri:\n  case X86::VPCMPBZrri:     case X86::VPCMPUBZrri:\n  case X86::VPCMPDZ128rri:  case X86::VPCMPUDZ128rri:\n  case X86::VPCMPDZ256rri:  case X86::VPCMPUDZ256rri:\n  case X86::VPCMPDZrri:     case X86::VPCMPUDZrri:\n  case X86::VPCMPQZ128rri:  case X86::VPCMPUQZ128rri:\n  case X86::VPCMPQZ256rri:  case X86::VPCMPUQZ256rri:\n  case X86::VPCMPQZrri:     case X86::VPCMPUQZrri:\n  case X86::VPCMPWZ128rri:  case X86::VPCMPUWZ128rri:\n  case X86::VPCMPWZ256rri:  case X86::VPCMPUWZ256rri:\n  case X86::VPCMPWZrri:     case X86::VPCMPUWZrri:\n  case X86::VPCMPBZ128rrik: case X86::VPCMPUBZ128rrik:\n  case X86::VPCMPBZ256rrik: case X86::VPCMPUBZ256rrik:\n  case X86::VPCMPBZrrik:    case X86::VPCMPUBZrrik:\n  case X86::VPCMPDZ128rrik: case X86::VPCMPUDZ128rrik:\n  case X86::VPCMPDZ256rrik: case X86::VPCMPUDZ256rrik:\n  case X86::VPCMPDZrrik:    case X86::VPCMPUDZrrik:\n  case X86::VPCMPQZ128rrik: case X86::VPCMPUQZ128rrik:\n  case X86::VPCMPQZ256rrik: case X86::VPCMPUQZ256rrik:\n  case X86::VPCMPQZrrik:    case X86::VPCMPUQZrrik:\n  case X86::VPCMPWZ128rrik: case X86::VPCMPUWZ128rrik:\n  case X86::VPCMPWZ256rrik: case X86::VPCMPUWZ256rrik:\n  case X86::VPCMPWZrrik:    case X86::VPCMPUWZrrik: {\n    // Flip comparison mode immediate (if necessary).\n    unsigned Imm = MI.getOperand(MI.getNumOperands() - 1).getImm() & 0x7;\n    Imm = X86::getSwappedVPCMPImm(Imm);\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(MI.getNumOperands() - 1).setImm(Imm);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::VPCOMBri: case X86::VPCOMUBri:\n  case X86::VPCOMDri: case X86::VPCOMUDri:\n  case X86::VPCOMQri: case X86::VPCOMUQri:\n  case X86::VPCOMWri: case X86::VPCOMUWri: {\n    // Flip comparison mode immediate (if necessary).\n    unsigned Imm = MI.getOperand(3).getImm() & 0x7;\n    Imm = X86::getSwappedVPCOMImm(Imm);\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(3).setImm(Imm);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::VCMPSDZrr:\n  case X86::VCMPSSZrr:\n  case X86::VCMPPDZrri:\n  case X86::VCMPPSZrri:\n  case X86::VCMPPDZ128rri:\n  case X86::VCMPPSZ128rri:\n  case X86::VCMPPDZ256rri:\n  case X86::VCMPPSZ256rri:\n  case X86::VCMPPDZrrik:\n  case X86::VCMPPSZrrik:\n  case X86::VCMPPDZ128rrik:\n  case X86::VCMPPSZ128rrik:\n  case X86::VCMPPDZ256rrik:\n  case X86::VCMPPSZ256rrik: {\n    unsigned Imm =\n                MI.getOperand(MI.getNumExplicitOperands() - 1).getImm() & 0x1f;\n    Imm = X86::getSwappedVCMPImm(Imm);\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(MI.getNumExplicitOperands() - 1).setImm(Imm);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::VPERM2F128rr:\n  case X86::VPERM2I128rr: {\n    // Flip permute source immediate.\n    // Imm & 0x02: lo = if set, select Op1.lo/hi else Op0.lo/hi.\n    // Imm & 0x20: hi = if set, select Op1.lo/hi else Op0.lo/hi.\n    int8_t Imm = MI.getOperand(3).getImm() & 0xFF;\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.getOperand(3).setImm(Imm ^ 0x22);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::MOVHLPSrr:\n  case X86::UNPCKHPDrr:\n  case X86::VMOVHLPSrr:\n  case X86::VUNPCKHPDrr:\n  case X86::VMOVHLPSZrr:\n  case X86::VUNPCKHPDZ128rr: {\n    assert(Subtarget.hasSSE2() && \"Commuting MOVHLP/UNPCKHPD requires SSE2!\");\n\n    unsigned Opc = MI.getOpcode();\n    switch (Opc) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::MOVHLPSrr:       Opc = X86::UNPCKHPDrr;      break;\n    case X86::UNPCKHPDrr:      Opc = X86::MOVHLPSrr;       break;\n    case X86::VMOVHLPSrr:      Opc = X86::VUNPCKHPDrr;     break;\n    case X86::VUNPCKHPDrr:     Opc = X86::VMOVHLPSrr;      break;\n    case X86::VMOVHLPSZrr:     Opc = X86::VUNPCKHPDZ128rr; break;\n    case X86::VUNPCKHPDZ128rr: Opc = X86::VMOVHLPSZrr;     break;\n    }\n    auto &WorkingMI = cloneIfNew(MI);\n    WorkingMI.setDesc(get(Opc));\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::CMOV16rr:  case X86::CMOV32rr:  case X86::CMOV64rr: {\n    auto &WorkingMI = cloneIfNew(MI);\n    unsigned OpNo = MI.getDesc().getNumOperands() - 1;\n    X86::CondCode CC = static_cast<X86::CondCode>(MI.getOperand(OpNo).getImm());\n    WorkingMI.getOperand(OpNo).setImm(X86::GetOppositeBranchCondition(CC));\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  case X86::VPTERNLOGDZrri:      case X86::VPTERNLOGDZrmi:\n  case X86::VPTERNLOGDZ128rri:   case X86::VPTERNLOGDZ128rmi:\n  case X86::VPTERNLOGDZ256rri:   case X86::VPTERNLOGDZ256rmi:\n  case X86::VPTERNLOGQZrri:      case X86::VPTERNLOGQZrmi:\n  case X86::VPTERNLOGQZ128rri:   case X86::VPTERNLOGQZ128rmi:\n  case X86::VPTERNLOGQZ256rri:   case X86::VPTERNLOGQZ256rmi:\n  case X86::VPTERNLOGDZrrik:\n  case X86::VPTERNLOGDZ128rrik:\n  case X86::VPTERNLOGDZ256rrik:\n  case X86::VPTERNLOGQZrrik:\n  case X86::VPTERNLOGQZ128rrik:\n  case X86::VPTERNLOGQZ256rrik:\n  case X86::VPTERNLOGDZrrikz:    case X86::VPTERNLOGDZrmikz:\n  case X86::VPTERNLOGDZ128rrikz: case X86::VPTERNLOGDZ128rmikz:\n  case X86::VPTERNLOGDZ256rrikz: case X86::VPTERNLOGDZ256rmikz:\n  case X86::VPTERNLOGQZrrikz:    case X86::VPTERNLOGQZrmikz:\n  case X86::VPTERNLOGQZ128rrikz: case X86::VPTERNLOGQZ128rmikz:\n  case X86::VPTERNLOGQZ256rrikz: case X86::VPTERNLOGQZ256rmikz:\n  case X86::VPTERNLOGDZ128rmbi:\n  case X86::VPTERNLOGDZ256rmbi:\n  case X86::VPTERNLOGDZrmbi:\n  case X86::VPTERNLOGQZ128rmbi:\n  case X86::VPTERNLOGQZ256rmbi:\n  case X86::VPTERNLOGQZrmbi:\n  case X86::VPTERNLOGDZ128rmbikz:\n  case X86::VPTERNLOGDZ256rmbikz:\n  case X86::VPTERNLOGDZrmbikz:\n  case X86::VPTERNLOGQZ128rmbikz:\n  case X86::VPTERNLOGQZ256rmbikz:\n  case X86::VPTERNLOGQZrmbikz: {\n    auto &WorkingMI = cloneIfNew(MI);\n    commuteVPTERNLOG(WorkingMI, OpIdx1, OpIdx2);\n    return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                   OpIdx1, OpIdx2);\n  }\n  default: {\n    if (isCommutableVPERMV3Instruction(MI.getOpcode())) {\n      unsigned Opc = getCommutedVPERMV3Opcode(MI.getOpcode());\n      auto &WorkingMI = cloneIfNew(MI);\n      WorkingMI.setDesc(get(Opc));\n      return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                     OpIdx1, OpIdx2);\n    }\n\n    const X86InstrFMA3Group *FMA3Group = getFMA3Group(MI.getOpcode(),\n                                                      MI.getDesc().TSFlags);\n    if (FMA3Group) {\n      unsigned Opc =\n        getFMA3OpcodeToCommuteOperands(MI, OpIdx1, OpIdx2, *FMA3Group);\n      auto &WorkingMI = cloneIfNew(MI);\n      WorkingMI.setDesc(get(Opc));\n      return TargetInstrInfo::commuteInstructionImpl(WorkingMI, /*NewMI=*/false,\n                                                     OpIdx1, OpIdx2);\n    }\n\n    return TargetInstrInfo::commuteInstructionImpl(MI, NewMI, OpIdx1, OpIdx2);\n  }\n  }\n}\n\nbool\nX86InstrInfo::findThreeSrcCommutedOpIndices(const MachineInstr &MI,\n                                            unsigned &SrcOpIdx1,\n                                            unsigned &SrcOpIdx2,\n                                            bool IsIntrinsic) const {\n  uint64_t TSFlags = MI.getDesc().TSFlags;\n\n  unsigned FirstCommutableVecOp = 1;\n  unsigned LastCommutableVecOp = 3;\n  unsigned KMaskOp = -1U;\n  if (X86II::isKMasked(TSFlags)) {\n    // For k-zero-masked operations it is Ok to commute the first vector\n    // operand. Unless this is an intrinsic instruction.\n    // For regular k-masked operations a conservative choice is done as the\n    // elements of the first vector operand, for which the corresponding bit\n    // in the k-mask operand is set to 0, are copied to the result of the\n    // instruction.\n    // TODO/FIXME: The commute still may be legal if it is known that the\n    // k-mask operand is set to either all ones or all zeroes.\n    // It is also Ok to commute the 1st operand if all users of MI use only\n    // the elements enabled by the k-mask operand. For example,\n    //   v4 = VFMADD213PSZrk v1, k, v2, v3; // v1[i] = k[i] ? v2[i]*v1[i]+v3[i]\n    //                                                     : v1[i];\n    //   VMOVAPSZmrk <mem_addr>, k, v4; // this is the ONLY user of v4 ->\n    //                                  // Ok, to commute v1 in FMADD213PSZrk.\n\n    // The k-mask operand has index = 2 for masked and zero-masked operations.\n    KMaskOp = 2;\n\n    // The operand with index = 1 is used as a source for those elements for\n    // which the corresponding bit in the k-mask is set to 0.\n    if (X86II::isKMergeMasked(TSFlags) || IsIntrinsic)\n      FirstCommutableVecOp = 3;\n\n    LastCommutableVecOp++;\n  } else if (IsIntrinsic) {\n    // Commuting the first operand of an intrinsic instruction isn't possible\n    // unless we can prove that only the lowest element of the result is used.\n    FirstCommutableVecOp = 2;\n  }\n\n  if (isMem(MI, LastCommutableVecOp))\n    LastCommutableVecOp--;\n\n  // Only the first RegOpsNum operands are commutable.\n  // Also, the value 'CommuteAnyOperandIndex' is valid here as it means\n  // that the operand is not specified/fixed.\n  if (SrcOpIdx1 != CommuteAnyOperandIndex &&\n      (SrcOpIdx1 < FirstCommutableVecOp || SrcOpIdx1 > LastCommutableVecOp ||\n       SrcOpIdx1 == KMaskOp))\n    return false;\n  if (SrcOpIdx2 != CommuteAnyOperandIndex &&\n      (SrcOpIdx2 < FirstCommutableVecOp || SrcOpIdx2 > LastCommutableVecOp ||\n       SrcOpIdx2 == KMaskOp))\n    return false;\n\n  // Look for two different register operands assumed to be commutable\n  // regardless of the FMA opcode. The FMA opcode is adjusted later.\n  if (SrcOpIdx1 == CommuteAnyOperandIndex ||\n      SrcOpIdx2 == CommuteAnyOperandIndex) {\n    unsigned CommutableOpIdx2 = SrcOpIdx2;\n\n    // At least one of operands to be commuted is not specified and\n    // this method is free to choose appropriate commutable operands.\n    if (SrcOpIdx1 == SrcOpIdx2)\n      // Both of operands are not fixed. By default set one of commutable\n      // operands to the last register operand of the instruction.\n      CommutableOpIdx2 = LastCommutableVecOp;\n    else if (SrcOpIdx2 == CommuteAnyOperandIndex)\n      // Only one of operands is not fixed.\n      CommutableOpIdx2 = SrcOpIdx1;\n\n    // CommutableOpIdx2 is well defined now. Let's choose another commutable\n    // operand and assign its index to CommutableOpIdx1.\n    Register Op2Reg = MI.getOperand(CommutableOpIdx2).getReg();\n\n    unsigned CommutableOpIdx1;\n    for (CommutableOpIdx1 = LastCommutableVecOp;\n         CommutableOpIdx1 >= FirstCommutableVecOp; CommutableOpIdx1--) {\n      // Just ignore and skip the k-mask operand.\n      if (CommutableOpIdx1 == KMaskOp)\n        continue;\n\n      // The commuted operands must have different registers.\n      // Otherwise, the commute transformation does not change anything and\n      // is useless then.\n      if (Op2Reg != MI.getOperand(CommutableOpIdx1).getReg())\n        break;\n    }\n\n    // No appropriate commutable operands were found.\n    if (CommutableOpIdx1 < FirstCommutableVecOp)\n      return false;\n\n    // Assign the found pair of commutable indices to SrcOpIdx1 and SrcOpidx2\n    // to return those values.\n    if (!fixCommutedOpIndices(SrcOpIdx1, SrcOpIdx2,\n                              CommutableOpIdx1, CommutableOpIdx2))\n      return false;\n  }\n\n  return true;\n}\n\nbool X86InstrInfo::findCommutedOpIndices(const MachineInstr &MI,\n                                         unsigned &SrcOpIdx1,\n                                         unsigned &SrcOpIdx2) const {\n  const MCInstrDesc &Desc = MI.getDesc();\n  if (!Desc.isCommutable())\n    return false;\n\n  switch (MI.getOpcode()) {\n  case X86::CMPSDrr:\n  case X86::CMPSSrr:\n  case X86::CMPPDrri:\n  case X86::CMPPSrri:\n  case X86::VCMPSDrr:\n  case X86::VCMPSSrr:\n  case X86::VCMPPDrri:\n  case X86::VCMPPSrri:\n  case X86::VCMPPDYrri:\n  case X86::VCMPPSYrri:\n  case X86::VCMPSDZrr:\n  case X86::VCMPSSZrr:\n  case X86::VCMPPDZrri:\n  case X86::VCMPPSZrri:\n  case X86::VCMPPDZ128rri:\n  case X86::VCMPPSZ128rri:\n  case X86::VCMPPDZ256rri:\n  case X86::VCMPPSZ256rri:\n  case X86::VCMPPDZrrik:\n  case X86::VCMPPSZrrik:\n  case X86::VCMPPDZ128rrik:\n  case X86::VCMPPSZ128rrik:\n  case X86::VCMPPDZ256rrik:\n  case X86::VCMPPSZ256rrik: {\n    unsigned OpOffset = X86II::isKMasked(Desc.TSFlags) ? 1 : 0;\n\n    // Float comparison can be safely commuted for\n    // Ordered/Unordered/Equal/NotEqual tests\n    unsigned Imm = MI.getOperand(3 + OpOffset).getImm() & 0x7;\n    switch (Imm) {\n    default:\n      // EVEX versions can be commuted.\n      if ((Desc.TSFlags & X86II::EncodingMask) == X86II::EVEX)\n        break;\n      return false;\n    case 0x00: // EQUAL\n    case 0x03: // UNORDERED\n    case 0x04: // NOT EQUAL\n    case 0x07: // ORDERED\n      break;\n    }\n\n    // The indices of the commutable operands are 1 and 2 (or 2 and 3\n    // when masked).\n    // Assign them to the returned operand indices here.\n    return fixCommutedOpIndices(SrcOpIdx1, SrcOpIdx2, 1 + OpOffset,\n                                2 + OpOffset);\n  }\n  case X86::MOVSSrr:\n    // X86::MOVSDrr is always commutable. MOVSS is only commutable if we can\n    // form sse4.1 blend. We assume VMOVSSrr/VMOVSDrr is always commutable since\n    // AVX implies sse4.1.\n    if (Subtarget.hasSSE41())\n      return TargetInstrInfo::findCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);\n    return false;\n  case X86::SHUFPDrri:\n    // We can commute this to MOVSD.\n    if (MI.getOperand(3).getImm() == 0x02)\n      return TargetInstrInfo::findCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);\n    return false;\n  case X86::MOVHLPSrr:\n  case X86::UNPCKHPDrr:\n  case X86::VMOVHLPSrr:\n  case X86::VUNPCKHPDrr:\n  case X86::VMOVHLPSZrr:\n  case X86::VUNPCKHPDZ128rr:\n    if (Subtarget.hasSSE2())\n      return TargetInstrInfo::findCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);\n    return false;\n  case X86::VPTERNLOGDZrri:      case X86::VPTERNLOGDZrmi:\n  case X86::VPTERNLOGDZ128rri:   case X86::VPTERNLOGDZ128rmi:\n  case X86::VPTERNLOGDZ256rri:   case X86::VPTERNLOGDZ256rmi:\n  case X86::VPTERNLOGQZrri:      case X86::VPTERNLOGQZrmi:\n  case X86::VPTERNLOGQZ128rri:   case X86::VPTERNLOGQZ128rmi:\n  case X86::VPTERNLOGQZ256rri:   case X86::VPTERNLOGQZ256rmi:\n  case X86::VPTERNLOGDZrrik:\n  case X86::VPTERNLOGDZ128rrik:\n  case X86::VPTERNLOGDZ256rrik:\n  case X86::VPTERNLOGQZrrik:\n  case X86::VPTERNLOGQZ128rrik:\n  case X86::VPTERNLOGQZ256rrik:\n  case X86::VPTERNLOGDZrrikz:    case X86::VPTERNLOGDZrmikz:\n  case X86::VPTERNLOGDZ128rrikz: case X86::VPTERNLOGDZ128rmikz:\n  case X86::VPTERNLOGDZ256rrikz: case X86::VPTERNLOGDZ256rmikz:\n  case X86::VPTERNLOGQZrrikz:    case X86::VPTERNLOGQZrmikz:\n  case X86::VPTERNLOGQZ128rrikz: case X86::VPTERNLOGQZ128rmikz:\n  case X86::VPTERNLOGQZ256rrikz: case X86::VPTERNLOGQZ256rmikz:\n  case X86::VPTERNLOGDZ128rmbi:\n  case X86::VPTERNLOGDZ256rmbi:\n  case X86::VPTERNLOGDZrmbi:\n  case X86::VPTERNLOGQZ128rmbi:\n  case X86::VPTERNLOGQZ256rmbi:\n  case X86::VPTERNLOGQZrmbi:\n  case X86::VPTERNLOGDZ128rmbikz:\n  case X86::VPTERNLOGDZ256rmbikz:\n  case X86::VPTERNLOGDZrmbikz:\n  case X86::VPTERNLOGQZ128rmbikz:\n  case X86::VPTERNLOGQZ256rmbikz:\n  case X86::VPTERNLOGQZrmbikz:\n    return findThreeSrcCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);\n  case X86::VPDPWSSDYrr:\n  case X86::VPDPWSSDrr:\n  case X86::VPDPWSSDSYrr:\n  case X86::VPDPWSSDSrr:\n  case X86::VPDPWSSDZ128r:\n  case X86::VPDPWSSDZ128rk:\n  case X86::VPDPWSSDZ128rkz:\n  case X86::VPDPWSSDZ256r:\n  case X86::VPDPWSSDZ256rk:\n  case X86::VPDPWSSDZ256rkz:\n  case X86::VPDPWSSDZr:\n  case X86::VPDPWSSDZrk:\n  case X86::VPDPWSSDZrkz:\n  case X86::VPDPWSSDSZ128r:\n  case X86::VPDPWSSDSZ128rk:\n  case X86::VPDPWSSDSZ128rkz:\n  case X86::VPDPWSSDSZ256r:\n  case X86::VPDPWSSDSZ256rk:\n  case X86::VPDPWSSDSZ256rkz:\n  case X86::VPDPWSSDSZr:\n  case X86::VPDPWSSDSZrk:\n  case X86::VPDPWSSDSZrkz:\n  case X86::VPMADD52HUQZ128r:\n  case X86::VPMADD52HUQZ128rk:\n  case X86::VPMADD52HUQZ128rkz:\n  case X86::VPMADD52HUQZ256r:\n  case X86::VPMADD52HUQZ256rk:\n  case X86::VPMADD52HUQZ256rkz:\n  case X86::VPMADD52HUQZr:\n  case X86::VPMADD52HUQZrk:\n  case X86::VPMADD52HUQZrkz:\n  case X86::VPMADD52LUQZ128r:\n  case X86::VPMADD52LUQZ128rk:\n  case X86::VPMADD52LUQZ128rkz:\n  case X86::VPMADD52LUQZ256r:\n  case X86::VPMADD52LUQZ256rk:\n  case X86::VPMADD52LUQZ256rkz:\n  case X86::VPMADD52LUQZr:\n  case X86::VPMADD52LUQZrk:\n  case X86::VPMADD52LUQZrkz: {\n    unsigned CommutableOpIdx1 = 2;\n    unsigned CommutableOpIdx2 = 3;\n    if (X86II::isKMasked(Desc.TSFlags)) {\n      // Skip the mask register.\n      ++CommutableOpIdx1;\n      ++CommutableOpIdx2;\n    }\n    if (!fixCommutedOpIndices(SrcOpIdx1, SrcOpIdx2,\n                              CommutableOpIdx1, CommutableOpIdx2))\n      return false;\n    if (!MI.getOperand(SrcOpIdx1).isReg() ||\n        !MI.getOperand(SrcOpIdx2).isReg())\n      // No idea.\n      return false;\n    return true;\n  }\n\n  default:\n    const X86InstrFMA3Group *FMA3Group = getFMA3Group(MI.getOpcode(),\n                                                      MI.getDesc().TSFlags);\n    if (FMA3Group)\n      return findThreeSrcCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2,\n                                           FMA3Group->isIntrinsic());\n\n    // Handled masked instructions since we need to skip over the mask input\n    // and the preserved input.\n    if (X86II::isKMasked(Desc.TSFlags)) {\n      // First assume that the first input is the mask operand and skip past it.\n      unsigned CommutableOpIdx1 = Desc.getNumDefs() + 1;\n      unsigned CommutableOpIdx2 = Desc.getNumDefs() + 2;\n      // Check if the first input is tied. If there isn't one then we only\n      // need to skip the mask operand which we did above.\n      if ((MI.getDesc().getOperandConstraint(Desc.getNumDefs(),\n                                             MCOI::TIED_TO) != -1)) {\n        // If this is zero masking instruction with a tied operand, we need to\n        // move the first index back to the first input since this must\n        // be a 3 input instruction and we want the first two non-mask inputs.\n        // Otherwise this is a 2 input instruction with a preserved input and\n        // mask, so we need to move the indices to skip one more input.\n        if (X86II::isKMergeMasked(Desc.TSFlags)) {\n          ++CommutableOpIdx1;\n          ++CommutableOpIdx2;\n        } else {\n          --CommutableOpIdx1;\n        }\n      }\n\n      if (!fixCommutedOpIndices(SrcOpIdx1, SrcOpIdx2,\n                                CommutableOpIdx1, CommutableOpIdx2))\n        return false;\n\n      if (!MI.getOperand(SrcOpIdx1).isReg() ||\n          !MI.getOperand(SrcOpIdx2).isReg())\n        // No idea.\n        return false;\n      return true;\n    }\n\n    return TargetInstrInfo::findCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);\n  }\n  return false;\n}\n\nX86::CondCode X86::getCondFromBranch(const MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default: return X86::COND_INVALID;\n  case X86::JCC_1:\n    return static_cast<X86::CondCode>(\n        MI.getOperand(MI.getDesc().getNumOperands() - 1).getImm());\n  }\n}\n\n/// Return condition code of a SETCC opcode.\nX86::CondCode X86::getCondFromSETCC(const MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default: return X86::COND_INVALID;\n  case X86::SETCCr: case X86::SETCCm:\n    return static_cast<X86::CondCode>(\n        MI.getOperand(MI.getDesc().getNumOperands() - 1).getImm());\n  }\n}\n\n/// Return condition code of a CMov opcode.\nX86::CondCode X86::getCondFromCMov(const MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default: return X86::COND_INVALID;\n  case X86::CMOV16rr: case X86::CMOV32rr: case X86::CMOV64rr:\n  case X86::CMOV16rm: case X86::CMOV32rm: case X86::CMOV64rm:\n    return static_cast<X86::CondCode>(\n        MI.getOperand(MI.getDesc().getNumOperands() - 1).getImm());\n  }\n}\n\n/// Return the inverse of the specified condition,\n/// e.g. turning COND_E to COND_NE.\nX86::CondCode X86::GetOppositeBranchCondition(X86::CondCode CC) {\n  switch (CC) {\n  default: llvm_unreachable(\"Illegal condition code!\");\n  case X86::COND_E:  return X86::COND_NE;\n  case X86::COND_NE: return X86::COND_E;\n  case X86::COND_L:  return X86::COND_GE;\n  case X86::COND_LE: return X86::COND_G;\n  case X86::COND_G:  return X86::COND_LE;\n  case X86::COND_GE: return X86::COND_L;\n  case X86::COND_B:  return X86::COND_AE;\n  case X86::COND_BE: return X86::COND_A;\n  case X86::COND_A:  return X86::COND_BE;\n  case X86::COND_AE: return X86::COND_B;\n  case X86::COND_S:  return X86::COND_NS;\n  case X86::COND_NS: return X86::COND_S;\n  case X86::COND_P:  return X86::COND_NP;\n  case X86::COND_NP: return X86::COND_P;\n  case X86::COND_O:  return X86::COND_NO;\n  case X86::COND_NO: return X86::COND_O;\n  case X86::COND_NE_OR_P:  return X86::COND_E_AND_NP;\n  case X86::COND_E_AND_NP: return X86::COND_NE_OR_P;\n  }\n}\n\n/// Assuming the flags are set by MI(a,b), return the condition code if we\n/// modify the instructions such that flags are set by MI(b,a).\nstatic X86::CondCode getSwappedCondition(X86::CondCode CC) {\n  switch (CC) {\n  default: return X86::COND_INVALID;\n  case X86::COND_E:  return X86::COND_E;\n  case X86::COND_NE: return X86::COND_NE;\n  case X86::COND_L:  return X86::COND_G;\n  case X86::COND_LE: return X86::COND_GE;\n  case X86::COND_G:  return X86::COND_L;\n  case X86::COND_GE: return X86::COND_LE;\n  case X86::COND_B:  return X86::COND_A;\n  case X86::COND_BE: return X86::COND_AE;\n  case X86::COND_A:  return X86::COND_B;\n  case X86::COND_AE: return X86::COND_BE;\n  }\n}\n\nstd::pair<X86::CondCode, bool>\nX86::getX86ConditionCode(CmpInst::Predicate Predicate) {\n  X86::CondCode CC = X86::COND_INVALID;\n  bool NeedSwap = false;\n  switch (Predicate) {\n  default: break;\n  // Floating-point Predicates\n  case CmpInst::FCMP_UEQ: CC = X86::COND_E;       break;\n  case CmpInst::FCMP_OLT: NeedSwap = true;        LLVM_FALLTHROUGH;\n  case CmpInst::FCMP_OGT: CC = X86::COND_A;       break;\n  case CmpInst::FCMP_OLE: NeedSwap = true;        LLVM_FALLTHROUGH;\n  case CmpInst::FCMP_OGE: CC = X86::COND_AE;      break;\n  case CmpInst::FCMP_UGT: NeedSwap = true;        LLVM_FALLTHROUGH;\n  case CmpInst::FCMP_ULT: CC = X86::COND_B;       break;\n  case CmpInst::FCMP_UGE: NeedSwap = true;        LLVM_FALLTHROUGH;\n  case CmpInst::FCMP_ULE: CC = X86::COND_BE;      break;\n  case CmpInst::FCMP_ONE: CC = X86::COND_NE;      break;\n  case CmpInst::FCMP_UNO: CC = X86::COND_P;       break;\n  case CmpInst::FCMP_ORD: CC = X86::COND_NP;      break;\n  case CmpInst::FCMP_OEQ:                         LLVM_FALLTHROUGH;\n  case CmpInst::FCMP_UNE: CC = X86::COND_INVALID; break;\n\n  // Integer Predicates\n  case CmpInst::ICMP_EQ:  CC = X86::COND_E;       break;\n  case CmpInst::ICMP_NE:  CC = X86::COND_NE;      break;\n  case CmpInst::ICMP_UGT: CC = X86::COND_A;       break;\n  case CmpInst::ICMP_UGE: CC = X86::COND_AE;      break;\n  case CmpInst::ICMP_ULT: CC = X86::COND_B;       break;\n  case CmpInst::ICMP_ULE: CC = X86::COND_BE;      break;\n  case CmpInst::ICMP_SGT: CC = X86::COND_G;       break;\n  case CmpInst::ICMP_SGE: CC = X86::COND_GE;      break;\n  case CmpInst::ICMP_SLT: CC = X86::COND_L;       break;\n  case CmpInst::ICMP_SLE: CC = X86::COND_LE;      break;\n  }\n\n  return std::make_pair(CC, NeedSwap);\n}\n\n/// Return a setcc opcode based on whether it has memory operand.\nunsigned X86::getSETOpc(bool HasMemoryOperand) {\n  return HasMemoryOperand ? X86::SETCCr : X86::SETCCm;\n}\n\n/// Return a cmov opcode for the given register size in bytes, and operand type.\nunsigned X86::getCMovOpcode(unsigned RegBytes, bool HasMemoryOperand) {\n  switch(RegBytes) {\n  default: llvm_unreachable(\"Illegal register size!\");\n  case 2: return HasMemoryOperand ? X86::CMOV16rm : X86::CMOV16rr;\n  case 4: return HasMemoryOperand ? X86::CMOV32rm : X86::CMOV32rr;\n  case 8: return HasMemoryOperand ? X86::CMOV64rm : X86::CMOV64rr;\n  }\n}\n\n/// Get the VPCMP immediate for the given condition.\nunsigned X86::getVPCMPImmForCond(ISD::CondCode CC) {\n  switch (CC) {\n  default: llvm_unreachable(\"Unexpected SETCC condition\");\n  case ISD::SETNE:  return 4;\n  case ISD::SETEQ:  return 0;\n  case ISD::SETULT:\n  case ISD::SETLT: return 1;\n  case ISD::SETUGT:\n  case ISD::SETGT: return 6;\n  case ISD::SETUGE:\n  case ISD::SETGE: return 5;\n  case ISD::SETULE:\n  case ISD::SETLE: return 2;\n  }\n}\n\n/// Get the VPCMP immediate if the operands are swapped.\nunsigned X86::getSwappedVPCMPImm(unsigned Imm) {\n  switch (Imm) {\n  default: llvm_unreachable(\"Unreachable!\");\n  case 0x01: Imm = 0x06; break; // LT  -> NLE\n  case 0x02: Imm = 0x05; break; // LE  -> NLT\n  case 0x05: Imm = 0x02; break; // NLT -> LE\n  case 0x06: Imm = 0x01; break; // NLE -> LT\n  case 0x00: // EQ\n  case 0x03: // FALSE\n  case 0x04: // NE\n  case 0x07: // TRUE\n    break;\n  }\n\n  return Imm;\n}\n\n/// Get the VPCOM immediate if the operands are swapped.\nunsigned X86::getSwappedVPCOMImm(unsigned Imm) {\n  switch (Imm) {\n  default: llvm_unreachable(\"Unreachable!\");\n  case 0x00: Imm = 0x02; break; // LT -> GT\n  case 0x01: Imm = 0x03; break; // LE -> GE\n  case 0x02: Imm = 0x00; break; // GT -> LT\n  case 0x03: Imm = 0x01; break; // GE -> LE\n  case 0x04: // EQ\n  case 0x05: // NE\n  case 0x06: // FALSE\n  case 0x07: // TRUE\n    break;\n  }\n\n  return Imm;\n}\n\n/// Get the VCMP immediate if the operands are swapped.\nunsigned X86::getSwappedVCMPImm(unsigned Imm) {\n  // Only need the lower 2 bits to distinquish.\n  switch (Imm & 0x3) {\n  default: llvm_unreachable(\"Unreachable!\");\n  case 0x00: case 0x03:\n    // EQ/NE/TRUE/FALSE/ORD/UNORD don't change immediate when commuted.\n    break;\n  case 0x01: case 0x02:\n    // Need to toggle bits 3:0. Bit 4 stays the same.\n    Imm ^= 0xf;\n    break;\n  }\n\n  return Imm;\n}\n\nbool X86InstrInfo::isUnconditionalTailCall(const MachineInstr &MI) const {\n  switch (MI.getOpcode()) {\n  case X86::TCRETURNdi:\n  case X86::TCRETURNri:\n  case X86::TCRETURNmi:\n  case X86::TCRETURNdi64:\n  case X86::TCRETURNri64:\n  case X86::TCRETURNmi64:\n    return true;\n  default:\n    return false;\n  }\n}\n\nbool X86InstrInfo::canMakeTailCallConditional(\n    SmallVectorImpl<MachineOperand> &BranchCond,\n    const MachineInstr &TailCall) const {\n  if (TailCall.getOpcode() != X86::TCRETURNdi &&\n      TailCall.getOpcode() != X86::TCRETURNdi64) {\n    // Only direct calls can be done with a conditional branch.\n    return false;\n  }\n\n  const MachineFunction *MF = TailCall.getParent()->getParent();\n  if (Subtarget.isTargetWin64() && MF->hasWinCFI()) {\n    // Conditional tail calls confuse the Win64 unwinder.\n    return false;\n  }\n\n  assert(BranchCond.size() == 1);\n  if (BranchCond[0].getImm() > X86::LAST_VALID_COND) {\n    // Can't make a conditional tail call with this condition.\n    return false;\n  }\n\n  const X86MachineFunctionInfo *X86FI = MF->getInfo<X86MachineFunctionInfo>();\n  if (X86FI->getTCReturnAddrDelta() != 0 ||\n      TailCall.getOperand(1).getImm() != 0) {\n    // A conditional tail call cannot do any stack adjustment.\n    return false;\n  }\n\n  return true;\n}\n\nvoid X86InstrInfo::replaceBranchWithTailCall(\n    MachineBasicBlock &MBB, SmallVectorImpl<MachineOperand> &BranchCond,\n    const MachineInstr &TailCall) const {\n  assert(canMakeTailCallConditional(BranchCond, TailCall));\n\n  MachineBasicBlock::iterator I = MBB.end();\n  while (I != MBB.begin()) {\n    --I;\n    if (I->isDebugInstr())\n      continue;\n    if (!I->isBranch())\n      assert(0 && \"Can't find the branch to replace!\");\n\n    X86::CondCode CC = X86::getCondFromBranch(*I);\n    assert(BranchCond.size() == 1);\n    if (CC != BranchCond[0].getImm())\n      continue;\n\n    break;\n  }\n\n  unsigned Opc = TailCall.getOpcode() == X86::TCRETURNdi ? X86::TCRETURNdicc\n                                                         : X86::TCRETURNdi64cc;\n\n  auto MIB = BuildMI(MBB, I, MBB.findDebugLoc(I), get(Opc));\n  MIB->addOperand(TailCall.getOperand(0)); // Destination.\n  MIB.addImm(0); // Stack offset (not used).\n  MIB->addOperand(BranchCond[0]); // Condition.\n  MIB.copyImplicitOps(TailCall); // Regmask and (imp-used) parameters.\n\n  // Add implicit uses and defs of all live regs potentially clobbered by the\n  // call. This way they still appear live across the call.\n  LivePhysRegs LiveRegs(getRegisterInfo());\n  LiveRegs.addLiveOuts(MBB);\n  SmallVector<std::pair<MCPhysReg, const MachineOperand *>, 8> Clobbers;\n  LiveRegs.stepForward(*MIB, Clobbers);\n  for (const auto &C : Clobbers) {\n    MIB.addReg(C.first, RegState::Implicit);\n    MIB.addReg(C.first, RegState::Implicit | RegState::Define);\n  }\n\n  I->eraseFromParent();\n}\n\n// Given a MBB and its TBB, find the FBB which was a fallthrough MBB (it may\n// not be a fallthrough MBB now due to layout changes). Return nullptr if the\n// fallthrough MBB cannot be identified.\nstatic MachineBasicBlock *getFallThroughMBB(MachineBasicBlock *MBB,\n                                            MachineBasicBlock *TBB) {\n  // Look for non-EHPad successors other than TBB. If we find exactly one, it\n  // is the fallthrough MBB. If we find zero, then TBB is both the target MBB\n  // and fallthrough MBB. If we find more than one, we cannot identify the\n  // fallthrough MBB and should return nullptr.\n  MachineBasicBlock *FallthroughBB = nullptr;\n  for (auto SI = MBB->succ_begin(), SE = MBB->succ_end(); SI != SE; ++SI) {\n    if ((*SI)->isEHPad() || (*SI == TBB && FallthroughBB))\n      continue;\n    // Return a nullptr if we found more than one fallthrough successor.\n    if (FallthroughBB && FallthroughBB != TBB)\n      return nullptr;\n    FallthroughBB = *SI;\n  }\n  return FallthroughBB;\n}\n\nbool X86InstrInfo::AnalyzeBranchImpl(\n    MachineBasicBlock &MBB, MachineBasicBlock *&TBB, MachineBasicBlock *&FBB,\n    SmallVectorImpl<MachineOperand> &Cond,\n    SmallVectorImpl<MachineInstr *> &CondBranches, bool AllowModify) const {\n\n  // Start from the bottom of the block and work up, examining the\n  // terminator instructions.\n  MachineBasicBlock::iterator I = MBB.end();\n  MachineBasicBlock::iterator UnCondBrIter = MBB.end();\n  while (I != MBB.begin()) {\n    --I;\n    if (I->isDebugInstr())\n      continue;\n\n    // Working from the bottom, when we see a non-terminator instruction, we're\n    // done.\n    if (!isUnpredicatedTerminator(*I))\n      break;\n\n    // A terminator that isn't a branch can't easily be handled by this\n    // analysis.\n    if (!I->isBranch())\n      return true;\n\n    // Handle unconditional branches.\n    if (I->getOpcode() == X86::JMP_1) {\n      UnCondBrIter = I;\n\n      if (!AllowModify) {\n        TBB = I->getOperand(0).getMBB();\n        continue;\n      }\n\n      // If the block has any instructions after a JMP, delete them.\n      while (std::next(I) != MBB.end())\n        std::next(I)->eraseFromParent();\n\n      Cond.clear();\n      FBB = nullptr;\n\n      // Delete the JMP if it's equivalent to a fall-through.\n      if (MBB.isLayoutSuccessor(I->getOperand(0).getMBB())) {\n        TBB = nullptr;\n        I->eraseFromParent();\n        I = MBB.end();\n        UnCondBrIter = MBB.end();\n        continue;\n      }\n\n      // TBB is used to indicate the unconditional destination.\n      TBB = I->getOperand(0).getMBB();\n      continue;\n    }\n\n    // Handle conditional branches.\n    X86::CondCode BranchCode = X86::getCondFromBranch(*I);\n    if (BranchCode == X86::COND_INVALID)\n      return true;  // Can't handle indirect branch.\n\n    // In practice we should never have an undef eflags operand, if we do\n    // abort here as we are not prepared to preserve the flag.\n    if (I->findRegisterUseOperand(X86::EFLAGS)->isUndef())\n      return true;\n\n    // Working from the bottom, handle the first conditional branch.\n    if (Cond.empty()) {\n      MachineBasicBlock *TargetBB = I->getOperand(0).getMBB();\n      if (AllowModify && UnCondBrIter != MBB.end() &&\n          MBB.isLayoutSuccessor(TargetBB)) {\n        // If we can modify the code and it ends in something like:\n        //\n        //     jCC L1\n        //     jmp L2\n        //   L1:\n        //     ...\n        //   L2:\n        //\n        // Then we can change this to:\n        //\n        //     jnCC L2\n        //   L1:\n        //     ...\n        //   L2:\n        //\n        // Which is a bit more efficient.\n        // We conditionally jump to the fall-through block.\n        BranchCode = GetOppositeBranchCondition(BranchCode);\n        MachineBasicBlock::iterator OldInst = I;\n\n        BuildMI(MBB, UnCondBrIter, MBB.findDebugLoc(I), get(X86::JCC_1))\n          .addMBB(UnCondBrIter->getOperand(0).getMBB())\n          .addImm(BranchCode);\n        BuildMI(MBB, UnCondBrIter, MBB.findDebugLoc(I), get(X86::JMP_1))\n          .addMBB(TargetBB);\n\n        OldInst->eraseFromParent();\n        UnCondBrIter->eraseFromParent();\n\n        // Restart the analysis.\n        UnCondBrIter = MBB.end();\n        I = MBB.end();\n        continue;\n      }\n\n      FBB = TBB;\n      TBB = I->getOperand(0).getMBB();\n      Cond.push_back(MachineOperand::CreateImm(BranchCode));\n      CondBranches.push_back(&*I);\n      continue;\n    }\n\n    // Handle subsequent conditional branches. Only handle the case where all\n    // conditional branches branch to the same destination and their condition\n    // opcodes fit one of the special multi-branch idioms.\n    assert(Cond.size() == 1);\n    assert(TBB);\n\n    // If the conditions are the same, we can leave them alone.\n    X86::CondCode OldBranchCode = (X86::CondCode)Cond[0].getImm();\n    auto NewTBB = I->getOperand(0).getMBB();\n    if (OldBranchCode == BranchCode && TBB == NewTBB)\n      continue;\n\n    // If they differ, see if they fit one of the known patterns. Theoretically,\n    // we could handle more patterns here, but we shouldn't expect to see them\n    // if instruction selection has done a reasonable job.\n    if (TBB == NewTBB &&\n               ((OldBranchCode == X86::COND_P && BranchCode == X86::COND_NE) ||\n                (OldBranchCode == X86::COND_NE && BranchCode == X86::COND_P))) {\n      BranchCode = X86::COND_NE_OR_P;\n    } else if ((OldBranchCode == X86::COND_NP && BranchCode == X86::COND_NE) ||\n               (OldBranchCode == X86::COND_E && BranchCode == X86::COND_P)) {\n      if (NewTBB != (FBB ? FBB : getFallThroughMBB(&MBB, TBB)))\n        return true;\n\n      // X86::COND_E_AND_NP usually has two different branch destinations.\n      //\n      // JP B1\n      // JE B2\n      // JMP B1\n      // B1:\n      // B2:\n      //\n      // Here this condition branches to B2 only if NP && E. It has another\n      // equivalent form:\n      //\n      // JNE B1\n      // JNP B2\n      // JMP B1\n      // B1:\n      // B2:\n      //\n      // Similarly it branches to B2 only if E && NP. That is why this condition\n      // is named with COND_E_AND_NP.\n      BranchCode = X86::COND_E_AND_NP;\n    } else\n      return true;\n\n    // Update the MachineOperand.\n    Cond[0].setImm(BranchCode);\n    CondBranches.push_back(&*I);\n  }\n\n  return false;\n}\n\nbool X86InstrInfo::analyzeBranch(MachineBasicBlock &MBB,\n                                 MachineBasicBlock *&TBB,\n                                 MachineBasicBlock *&FBB,\n                                 SmallVectorImpl<MachineOperand> &Cond,\n                                 bool AllowModify) const {\n  SmallVector<MachineInstr *, 4> CondBranches;\n  return AnalyzeBranchImpl(MBB, TBB, FBB, Cond, CondBranches, AllowModify);\n}\n\nbool X86InstrInfo::analyzeBranchPredicate(MachineBasicBlock &MBB,\n                                          MachineBranchPredicate &MBP,\n                                          bool AllowModify) const {\n  using namespace std::placeholders;\n\n  SmallVector<MachineOperand, 4> Cond;\n  SmallVector<MachineInstr *, 4> CondBranches;\n  if (AnalyzeBranchImpl(MBB, MBP.TrueDest, MBP.FalseDest, Cond, CondBranches,\n                        AllowModify))\n    return true;\n\n  if (Cond.size() != 1)\n    return true;\n\n  assert(MBP.TrueDest && \"expected!\");\n\n  if (!MBP.FalseDest)\n    MBP.FalseDest = MBB.getNextNode();\n\n  const TargetRegisterInfo *TRI = &getRegisterInfo();\n\n  MachineInstr *ConditionDef = nullptr;\n  bool SingleUseCondition = true;\n\n  for (auto I = std::next(MBB.rbegin()), E = MBB.rend(); I != E; ++I) {\n    if (I->modifiesRegister(X86::EFLAGS, TRI)) {\n      ConditionDef = &*I;\n      break;\n    }\n\n    if (I->readsRegister(X86::EFLAGS, TRI))\n      SingleUseCondition = false;\n  }\n\n  if (!ConditionDef)\n    return true;\n\n  if (SingleUseCondition) {\n    for (auto *Succ : MBB.successors())\n      if (Succ->isLiveIn(X86::EFLAGS))\n        SingleUseCondition = false;\n  }\n\n  MBP.ConditionDef = ConditionDef;\n  MBP.SingleUseCondition = SingleUseCondition;\n\n  // Currently we only recognize the simple pattern:\n  //\n  //   test %reg, %reg\n  //   je %label\n  //\n  const unsigned TestOpcode =\n      Subtarget.is64Bit() ? X86::TEST64rr : X86::TEST32rr;\n\n  if (ConditionDef->getOpcode() == TestOpcode &&\n      ConditionDef->getNumOperands() == 3 &&\n      ConditionDef->getOperand(0).isIdenticalTo(ConditionDef->getOperand(1)) &&\n      (Cond[0].getImm() == X86::COND_NE || Cond[0].getImm() == X86::COND_E)) {\n    MBP.LHS = ConditionDef->getOperand(0);\n    MBP.RHS = MachineOperand::CreateImm(0);\n    MBP.Predicate = Cond[0].getImm() == X86::COND_NE\n                        ? MachineBranchPredicate::PRED_NE\n                        : MachineBranchPredicate::PRED_EQ;\n    return false;\n  }\n\n  return true;\n}\n\nunsigned X86InstrInfo::removeBranch(MachineBasicBlock &MBB,\n                                    int *BytesRemoved) const {\n  assert(!BytesRemoved && \"code size not handled\");\n\n  MachineBasicBlock::iterator I = MBB.end();\n  unsigned Count = 0;\n\n  while (I != MBB.begin()) {\n    --I;\n    if (I->isDebugInstr())\n      continue;\n    if (I->getOpcode() != X86::JMP_1 &&\n        X86::getCondFromBranch(*I) == X86::COND_INVALID)\n      break;\n    // Remove the branch.\n    I->eraseFromParent();\n    I = MBB.end();\n    ++Count;\n  }\n\n  return Count;\n}\n\nunsigned X86InstrInfo::insertBranch(MachineBasicBlock &MBB,\n                                    MachineBasicBlock *TBB,\n                                    MachineBasicBlock *FBB,\n                                    ArrayRef<MachineOperand> Cond,\n                                    const DebugLoc &DL,\n                                    int *BytesAdded) const {\n  // Shouldn't be a fall through.\n  assert(TBB && \"insertBranch must not be told to insert a fallthrough\");\n  assert((Cond.size() == 1 || Cond.size() == 0) &&\n         \"X86 branch conditions have one component!\");\n  assert(!BytesAdded && \"code size not handled\");\n\n  if (Cond.empty()) {\n    // Unconditional branch?\n    assert(!FBB && \"Unconditional branch with multiple successors!\");\n    BuildMI(&MBB, DL, get(X86::JMP_1)).addMBB(TBB);\n    return 1;\n  }\n\n  // If FBB is null, it is implied to be a fall-through block.\n  bool FallThru = FBB == nullptr;\n\n  // Conditional branch.\n  unsigned Count = 0;\n  X86::CondCode CC = (X86::CondCode)Cond[0].getImm();\n  switch (CC) {\n  case X86::COND_NE_OR_P:\n    // Synthesize NE_OR_P with two branches.\n    BuildMI(&MBB, DL, get(X86::JCC_1)).addMBB(TBB).addImm(X86::COND_NE);\n    ++Count;\n    BuildMI(&MBB, DL, get(X86::JCC_1)).addMBB(TBB).addImm(X86::COND_P);\n    ++Count;\n    break;\n  case X86::COND_E_AND_NP:\n    // Use the next block of MBB as FBB if it is null.\n    if (FBB == nullptr) {\n      FBB = getFallThroughMBB(&MBB, TBB);\n      assert(FBB && \"MBB cannot be the last block in function when the false \"\n                    \"body is a fall-through.\");\n    }\n    // Synthesize COND_E_AND_NP with two branches.\n    BuildMI(&MBB, DL, get(X86::JCC_1)).addMBB(FBB).addImm(X86::COND_NE);\n    ++Count;\n    BuildMI(&MBB, DL, get(X86::JCC_1)).addMBB(TBB).addImm(X86::COND_NP);\n    ++Count;\n    break;\n  default: {\n    BuildMI(&MBB, DL, get(X86::JCC_1)).addMBB(TBB).addImm(CC);\n    ++Count;\n  }\n  }\n  if (!FallThru) {\n    // Two-way Conditional branch. Insert the second branch.\n    BuildMI(&MBB, DL, get(X86::JMP_1)).addMBB(FBB);\n    ++Count;\n  }\n  return Count;\n}\n\nbool X86InstrInfo::canInsertSelect(const MachineBasicBlock &MBB,\n                                   ArrayRef<MachineOperand> Cond,\n                                   Register DstReg, Register TrueReg,\n                                   Register FalseReg, int &CondCycles,\n                                   int &TrueCycles, int &FalseCycles) const {\n  // Not all subtargets have cmov instructions.\n  if (!Subtarget.hasCMov())\n    return false;\n  if (Cond.size() != 1)\n    return false;\n  // We cannot do the composite conditions, at least not in SSA form.\n  if ((X86::CondCode)Cond[0].getImm() > X86::LAST_VALID_COND)\n    return false;\n\n  // Check register classes.\n  const MachineRegisterInfo &MRI = MBB.getParent()->getRegInfo();\n  const TargetRegisterClass *RC =\n    RI.getCommonSubClass(MRI.getRegClass(TrueReg), MRI.getRegClass(FalseReg));\n  if (!RC)\n    return false;\n\n  // We have cmov instructions for 16, 32, and 64 bit general purpose registers.\n  if (X86::GR16RegClass.hasSubClassEq(RC) ||\n      X86::GR32RegClass.hasSubClassEq(RC) ||\n      X86::GR64RegClass.hasSubClassEq(RC)) {\n    // This latency applies to Pentium M, Merom, Wolfdale, Nehalem, and Sandy\n    // Bridge. Probably Ivy Bridge as well.\n    CondCycles = 2;\n    TrueCycles = 2;\n    FalseCycles = 2;\n    return true;\n  }\n\n  // Can't do vectors.\n  return false;\n}\n\nvoid X86InstrInfo::insertSelect(MachineBasicBlock &MBB,\n                                MachineBasicBlock::iterator I,\n                                const DebugLoc &DL, Register DstReg,\n                                ArrayRef<MachineOperand> Cond, Register TrueReg,\n                                Register FalseReg) const {\n  MachineRegisterInfo &MRI = MBB.getParent()->getRegInfo();\n  const TargetRegisterInfo &TRI = *MRI.getTargetRegisterInfo();\n  const TargetRegisterClass &RC = *MRI.getRegClass(DstReg);\n  assert(Cond.size() == 1 && \"Invalid Cond array\");\n  unsigned Opc = X86::getCMovOpcode(TRI.getRegSizeInBits(RC) / 8,\n                                    false /*HasMemoryOperand*/);\n  BuildMI(MBB, I, DL, get(Opc), DstReg)\n      .addReg(FalseReg)\n      .addReg(TrueReg)\n      .addImm(Cond[0].getImm());\n}\n\n/// Test if the given register is a physical h register.\nstatic bool isHReg(unsigned Reg) {\n  return X86::GR8_ABCD_HRegClass.contains(Reg);\n}\n\n// Try and copy between VR128/VR64 and GR64 registers.\nstatic unsigned CopyToFromAsymmetricReg(unsigned DestReg, unsigned SrcReg,\n                                        const X86Subtarget &Subtarget) {\n  bool HasAVX = Subtarget.hasAVX();\n  bool HasAVX512 = Subtarget.hasAVX512();\n\n  // SrcReg(MaskReg) -> DestReg(GR64)\n  // SrcReg(MaskReg) -> DestReg(GR32)\n\n  // All KMASK RegClasses hold the same k registers, can be tested against anyone.\n  if (X86::VK16RegClass.contains(SrcReg)) {\n    if (X86::GR64RegClass.contains(DestReg)) {\n      assert(Subtarget.hasBWI());\n      return X86::KMOVQrk;\n    }\n    if (X86::GR32RegClass.contains(DestReg))\n      return Subtarget.hasBWI() ? X86::KMOVDrk : X86::KMOVWrk;\n  }\n\n  // SrcReg(GR64) -> DestReg(MaskReg)\n  // SrcReg(GR32) -> DestReg(MaskReg)\n\n  // All KMASK RegClasses hold the same k registers, can be tested against anyone.\n  if (X86::VK16RegClass.contains(DestReg)) {\n    if (X86::GR64RegClass.contains(SrcReg)) {\n      assert(Subtarget.hasBWI());\n      return X86::KMOVQkr;\n    }\n    if (X86::GR32RegClass.contains(SrcReg))\n      return Subtarget.hasBWI() ? X86::KMOVDkr : X86::KMOVWkr;\n  }\n\n\n  // SrcReg(VR128) -> DestReg(GR64)\n  // SrcReg(VR64)  -> DestReg(GR64)\n  // SrcReg(GR64)  -> DestReg(VR128)\n  // SrcReg(GR64)  -> DestReg(VR64)\n\n  if (X86::GR64RegClass.contains(DestReg)) {\n    if (X86::VR128XRegClass.contains(SrcReg))\n      // Copy from a VR128 register to a GR64 register.\n      return HasAVX512 ? X86::VMOVPQIto64Zrr :\n             HasAVX    ? X86::VMOVPQIto64rr  :\n                         X86::MOVPQIto64rr;\n    if (X86::VR64RegClass.contains(SrcReg))\n      // Copy from a VR64 register to a GR64 register.\n      return X86::MMX_MOVD64from64rr;\n  } else if (X86::GR64RegClass.contains(SrcReg)) {\n    // Copy from a GR64 register to a VR128 register.\n    if (X86::VR128XRegClass.contains(DestReg))\n      return HasAVX512 ? X86::VMOV64toPQIZrr :\n             HasAVX    ? X86::VMOV64toPQIrr  :\n                         X86::MOV64toPQIrr;\n    // Copy from a GR64 register to a VR64 register.\n    if (X86::VR64RegClass.contains(DestReg))\n      return X86::MMX_MOVD64to64rr;\n  }\n\n  // SrcReg(VR128) -> DestReg(GR32)\n  // SrcReg(GR32)  -> DestReg(VR128)\n\n  if (X86::GR32RegClass.contains(DestReg) &&\n      X86::VR128XRegClass.contains(SrcReg))\n    // Copy from a VR128 register to a GR32 register.\n    return HasAVX512 ? X86::VMOVPDI2DIZrr :\n           HasAVX    ? X86::VMOVPDI2DIrr  :\n                       X86::MOVPDI2DIrr;\n\n  if (X86::VR128XRegClass.contains(DestReg) &&\n      X86::GR32RegClass.contains(SrcReg))\n    // Copy from a VR128 register to a VR128 register.\n    return HasAVX512 ? X86::VMOVDI2PDIZrr :\n           HasAVX    ? X86::VMOVDI2PDIrr  :\n                       X86::MOVDI2PDIrr;\n  return 0;\n}\n\nvoid X86InstrInfo::copyPhysReg(MachineBasicBlock &MBB,\n                               MachineBasicBlock::iterator MI,\n                               const DebugLoc &DL, MCRegister DestReg,\n                               MCRegister SrcReg, bool KillSrc) const {\n  // First deal with the normal symmetric copies.\n  bool HasAVX = Subtarget.hasAVX();\n  bool HasVLX = Subtarget.hasVLX();\n  unsigned Opc = 0;\n  if (X86::GR64RegClass.contains(DestReg, SrcReg))\n    Opc = X86::MOV64rr;\n  else if (X86::GR32RegClass.contains(DestReg, SrcReg))\n    Opc = X86::MOV32rr;\n  else if (X86::GR16RegClass.contains(DestReg, SrcReg))\n    Opc = X86::MOV16rr;\n  else if (X86::GR8RegClass.contains(DestReg, SrcReg)) {\n    // Copying to or from a physical H register on x86-64 requires a NOREX\n    // move.  Otherwise use a normal move.\n    if ((isHReg(DestReg) || isHReg(SrcReg)) &&\n        Subtarget.is64Bit()) {\n      Opc = X86::MOV8rr_NOREX;\n      // Both operands must be encodable without an REX prefix.\n      assert(X86::GR8_NOREXRegClass.contains(SrcReg, DestReg) &&\n             \"8-bit H register can not be copied outside GR8_NOREX\");\n    } else\n      Opc = X86::MOV8rr;\n  }\n  else if (X86::VR64RegClass.contains(DestReg, SrcReg))\n    Opc = X86::MMX_MOVQ64rr;\n  else if (X86::VR128XRegClass.contains(DestReg, SrcReg)) {\n    if (HasVLX)\n      Opc = X86::VMOVAPSZ128rr;\n    else if (X86::VR128RegClass.contains(DestReg, SrcReg))\n      Opc = HasAVX ? X86::VMOVAPSrr : X86::MOVAPSrr;\n    else {\n      // If this an extended register and we don't have VLX we need to use a\n      // 512-bit move.\n      Opc = X86::VMOVAPSZrr;\n      const TargetRegisterInfo *TRI = &getRegisterInfo();\n      DestReg = TRI->getMatchingSuperReg(DestReg, X86::sub_xmm,\n                                         &X86::VR512RegClass);\n      SrcReg = TRI->getMatchingSuperReg(SrcReg, X86::sub_xmm,\n                                        &X86::VR512RegClass);\n    }\n  } else if (X86::VR256XRegClass.contains(DestReg, SrcReg)) {\n    if (HasVLX)\n      Opc = X86::VMOVAPSZ256rr;\n    else if (X86::VR256RegClass.contains(DestReg, SrcReg))\n      Opc = X86::VMOVAPSYrr;\n    else {\n      // If this an extended register and we don't have VLX we need to use a\n      // 512-bit move.\n      Opc = X86::VMOVAPSZrr;\n      const TargetRegisterInfo *TRI = &getRegisterInfo();\n      DestReg = TRI->getMatchingSuperReg(DestReg, X86::sub_ymm,\n                                         &X86::VR512RegClass);\n      SrcReg = TRI->getMatchingSuperReg(SrcReg, X86::sub_ymm,\n                                        &X86::VR512RegClass);\n    }\n  } else if (X86::VR512RegClass.contains(DestReg, SrcReg))\n    Opc = X86::VMOVAPSZrr;\n  // All KMASK RegClasses hold the same k registers, can be tested against anyone.\n  else if (X86::VK16RegClass.contains(DestReg, SrcReg))\n    Opc = Subtarget.hasBWI() ? X86::KMOVQkk : X86::KMOVWkk;\n  if (!Opc)\n    Opc = CopyToFromAsymmetricReg(DestReg, SrcReg, Subtarget);\n\n  if (Opc) {\n    BuildMI(MBB, MI, DL, get(Opc), DestReg)\n      .addReg(SrcReg, getKillRegState(KillSrc));\n    return;\n  }\n\n  if (SrcReg == X86::EFLAGS || DestReg == X86::EFLAGS) {\n    // FIXME: We use a fatal error here because historically LLVM has tried\n    // lower some of these physreg copies and we want to ensure we get\n    // reasonable bug reports if someone encounters a case no other testing\n    // found. This path should be removed after the LLVM 7 release.\n    report_fatal_error(\"Unable to copy EFLAGS physical register!\");\n  }\n\n  LLVM_DEBUG(dbgs() << \"Cannot copy \" << RI.getName(SrcReg) << \" to \"\n                    << RI.getName(DestReg) << '\\n');\n  report_fatal_error(\"Cannot emit physreg copy instruction\");\n}\n\nOptional<DestSourcePair>\nX86InstrInfo::isCopyInstrImpl(const MachineInstr &MI) const {\n  if (MI.isMoveReg())\n    return DestSourcePair{MI.getOperand(0), MI.getOperand(1)};\n  return None;\n}\n\nstatic unsigned getLoadStoreRegOpcode(Register Reg,\n                                      const TargetRegisterClass *RC,\n                                      bool IsStackAligned,\n                                      const X86Subtarget &STI, bool load) {\n  bool HasAVX = STI.hasAVX();\n  bool HasAVX512 = STI.hasAVX512();\n  bool HasVLX = STI.hasVLX();\n\n  switch (STI.getRegisterInfo()->getSpillSize(*RC)) {\n  default:\n    llvm_unreachable(\"Unknown spill size\");\n  case 1:\n    assert(X86::GR8RegClass.hasSubClassEq(RC) && \"Unknown 1-byte regclass\");\n    if (STI.is64Bit())\n      // Copying to or from a physical H register on x86-64 requires a NOREX\n      // move.  Otherwise use a normal move.\n      if (isHReg(Reg) || X86::GR8_ABCD_HRegClass.hasSubClassEq(RC))\n        return load ? X86::MOV8rm_NOREX : X86::MOV8mr_NOREX;\n    return load ? X86::MOV8rm : X86::MOV8mr;\n  case 2:\n    if (X86::VK16RegClass.hasSubClassEq(RC))\n      return load ? X86::KMOVWkm : X86::KMOVWmk;\n    assert(X86::GR16RegClass.hasSubClassEq(RC) && \"Unknown 2-byte regclass\");\n    return load ? X86::MOV16rm : X86::MOV16mr;\n  case 4:\n    if (X86::GR32RegClass.hasSubClassEq(RC))\n      return load ? X86::MOV32rm : X86::MOV32mr;\n    if (X86::FR32XRegClass.hasSubClassEq(RC))\n      return load ?\n        (HasAVX512 ? X86::VMOVSSZrm_alt :\n         HasAVX    ? X86::VMOVSSrm_alt :\n                     X86::MOVSSrm_alt) :\n        (HasAVX512 ? X86::VMOVSSZmr :\n         HasAVX    ? X86::VMOVSSmr :\n                     X86::MOVSSmr);\n    if (X86::RFP32RegClass.hasSubClassEq(RC))\n      return load ? X86::LD_Fp32m : X86::ST_Fp32m;\n    if (X86::VK32RegClass.hasSubClassEq(RC)) {\n      assert(STI.hasBWI() && \"KMOVD requires BWI\");\n      return load ? X86::KMOVDkm : X86::KMOVDmk;\n    }\n    // All of these mask pair classes have the same spill size, the same kind\n    // of kmov instructions can be used with all of them.\n    if (X86::VK1PAIRRegClass.hasSubClassEq(RC) ||\n        X86::VK2PAIRRegClass.hasSubClassEq(RC) ||\n        X86::VK4PAIRRegClass.hasSubClassEq(RC) ||\n        X86::VK8PAIRRegClass.hasSubClassEq(RC) ||\n        X86::VK16PAIRRegClass.hasSubClassEq(RC))\n      return load ? X86::MASKPAIR16LOAD : X86::MASKPAIR16STORE;\n    llvm_unreachable(\"Unknown 4-byte regclass\");\n  case 8:\n    if (X86::GR64RegClass.hasSubClassEq(RC))\n      return load ? X86::MOV64rm : X86::MOV64mr;\n    if (X86::FR64XRegClass.hasSubClassEq(RC))\n      return load ?\n        (HasAVX512 ? X86::VMOVSDZrm_alt :\n         HasAVX    ? X86::VMOVSDrm_alt :\n                     X86::MOVSDrm_alt) :\n        (HasAVX512 ? X86::VMOVSDZmr :\n         HasAVX    ? X86::VMOVSDmr :\n                     X86::MOVSDmr);\n    if (X86::VR64RegClass.hasSubClassEq(RC))\n      return load ? X86::MMX_MOVQ64rm : X86::MMX_MOVQ64mr;\n    if (X86::RFP64RegClass.hasSubClassEq(RC))\n      return load ? X86::LD_Fp64m : X86::ST_Fp64m;\n    if (X86::VK64RegClass.hasSubClassEq(RC)) {\n      assert(STI.hasBWI() && \"KMOVQ requires BWI\");\n      return load ? X86::KMOVQkm : X86::KMOVQmk;\n    }\n    llvm_unreachable(\"Unknown 8-byte regclass\");\n  case 10:\n    assert(X86::RFP80RegClass.hasSubClassEq(RC) && \"Unknown 10-byte regclass\");\n    return load ? X86::LD_Fp80m : X86::ST_FpP80m;\n  case 16: {\n    if (X86::VR128XRegClass.hasSubClassEq(RC)) {\n      // If stack is realigned we can use aligned stores.\n      if (IsStackAligned)\n        return load ?\n          (HasVLX    ? X86::VMOVAPSZ128rm :\n           HasAVX512 ? X86::VMOVAPSZ128rm_NOVLX :\n           HasAVX    ? X86::VMOVAPSrm :\n                       X86::MOVAPSrm):\n          (HasVLX    ? X86::VMOVAPSZ128mr :\n           HasAVX512 ? X86::VMOVAPSZ128mr_NOVLX :\n           HasAVX    ? X86::VMOVAPSmr :\n                       X86::MOVAPSmr);\n      else\n        return load ?\n          (HasVLX    ? X86::VMOVUPSZ128rm :\n           HasAVX512 ? X86::VMOVUPSZ128rm_NOVLX :\n           HasAVX    ? X86::VMOVUPSrm :\n                       X86::MOVUPSrm):\n          (HasVLX    ? X86::VMOVUPSZ128mr :\n           HasAVX512 ? X86::VMOVUPSZ128mr_NOVLX :\n           HasAVX    ? X86::VMOVUPSmr :\n                       X86::MOVUPSmr);\n    }\n    if (X86::BNDRRegClass.hasSubClassEq(RC)) {\n      if (STI.is64Bit())\n        return load ? X86::BNDMOV64rm : X86::BNDMOV64mr;\n      else\n        return load ? X86::BNDMOV32rm : X86::BNDMOV32mr;\n    }\n    llvm_unreachable(\"Unknown 16-byte regclass\");\n  }\n  case 32:\n    assert(X86::VR256XRegClass.hasSubClassEq(RC) && \"Unknown 32-byte regclass\");\n    // If stack is realigned we can use aligned stores.\n    if (IsStackAligned)\n      return load ?\n        (HasVLX    ? X86::VMOVAPSZ256rm :\n         HasAVX512 ? X86::VMOVAPSZ256rm_NOVLX :\n                     X86::VMOVAPSYrm) :\n        (HasVLX    ? X86::VMOVAPSZ256mr :\n         HasAVX512 ? X86::VMOVAPSZ256mr_NOVLX :\n                     X86::VMOVAPSYmr);\n    else\n      return load ?\n        (HasVLX    ? X86::VMOVUPSZ256rm :\n         HasAVX512 ? X86::VMOVUPSZ256rm_NOVLX :\n                     X86::VMOVUPSYrm) :\n        (HasVLX    ? X86::VMOVUPSZ256mr :\n         HasAVX512 ? X86::VMOVUPSZ256mr_NOVLX :\n                     X86::VMOVUPSYmr);\n  case 64:\n    assert(X86::VR512RegClass.hasSubClassEq(RC) && \"Unknown 64-byte regclass\");\n    assert(STI.hasAVX512() && \"Using 512-bit register requires AVX512\");\n    if (IsStackAligned)\n      return load ? X86::VMOVAPSZrm : X86::VMOVAPSZmr;\n    else\n      return load ? X86::VMOVUPSZrm : X86::VMOVUPSZmr;\n  }\n}\n\nOptional<ExtAddrMode>\nX86InstrInfo::getAddrModeFromMemoryOp(const MachineInstr &MemI,\n                                      const TargetRegisterInfo *TRI) const {\n  const MCInstrDesc &Desc = MemI.getDesc();\n  int MemRefBegin = X86II::getMemoryOperandNo(Desc.TSFlags);\n  if (MemRefBegin < 0)\n    return None;\n\n  MemRefBegin += X86II::getOperandBias(Desc);\n\n  auto &BaseOp = MemI.getOperand(MemRefBegin + X86::AddrBaseReg);\n  if (!BaseOp.isReg()) // Can be an MO_FrameIndex\n    return None;\n\n  const MachineOperand &DispMO = MemI.getOperand(MemRefBegin + X86::AddrDisp);\n  // Displacement can be symbolic\n  if (!DispMO.isImm())\n    return None;\n\n  ExtAddrMode AM;\n  AM.BaseReg = BaseOp.getReg();\n  AM.ScaledReg = MemI.getOperand(MemRefBegin + X86::AddrIndexReg).getReg();\n  AM.Scale = MemI.getOperand(MemRefBegin + X86::AddrScaleAmt).getImm();\n  AM.Displacement = DispMO.getImm();\n  return AM;\n}\n\nbool X86InstrInfo::getConstValDefinedInReg(const MachineInstr &MI,\n                                           const Register Reg,\n                                           int64_t &ImmVal) const {\n  if (MI.getOpcode() != X86::MOV32ri && MI.getOpcode() != X86::MOV64ri)\n    return false;\n  // Mov Src can be a global address.\n  if (!MI.getOperand(1).isImm() || MI.getOperand(0).getReg() != Reg)\n    return false;\n  ImmVal = MI.getOperand(1).getImm();\n  return true;\n}\n\nbool X86InstrInfo::preservesZeroValueInReg(\n    const MachineInstr *MI, const Register NullValueReg,\n    const TargetRegisterInfo *TRI) const {\n  if (!MI->modifiesRegister(NullValueReg, TRI))\n    return true;\n  switch (MI->getOpcode()) {\n  // Shift right/left of a null unto itself is still a null, i.e. rax = shl rax\n  // X.\n  case X86::SHR64ri:\n  case X86::SHR32ri:\n  case X86::SHL64ri:\n  case X86::SHL32ri:\n    assert(MI->getOperand(0).isDef() && MI->getOperand(1).isUse() &&\n           \"expected for shift opcode!\");\n    return MI->getOperand(0).getReg() == NullValueReg &&\n           MI->getOperand(1).getReg() == NullValueReg;\n  // Zero extend of a sub-reg of NullValueReg into itself does not change the\n  // null value.\n  case X86::MOV32rr:\n    return llvm::all_of(MI->operands(), [&](const MachineOperand &MO) {\n      return TRI->isSubRegisterEq(NullValueReg, MO.getReg());\n    });\n  default:\n    return false;\n  }\n  llvm_unreachable(\"Should be handled above!\");\n}\n\nbool X86InstrInfo::getMemOperandsWithOffsetWidth(\n    const MachineInstr &MemOp, SmallVectorImpl<const MachineOperand *> &BaseOps,\n    int64_t &Offset, bool &OffsetIsScalable, unsigned &Width,\n    const TargetRegisterInfo *TRI) const {\n  const MCInstrDesc &Desc = MemOp.getDesc();\n  int MemRefBegin = X86II::getMemoryOperandNo(Desc.TSFlags);\n  if (MemRefBegin < 0)\n    return false;\n\n  MemRefBegin += X86II::getOperandBias(Desc);\n\n  const MachineOperand *BaseOp =\n      &MemOp.getOperand(MemRefBegin + X86::AddrBaseReg);\n  if (!BaseOp->isReg()) // Can be an MO_FrameIndex\n    return false;\n\n  if (MemOp.getOperand(MemRefBegin + X86::AddrScaleAmt).getImm() != 1)\n    return false;\n\n  if (MemOp.getOperand(MemRefBegin + X86::AddrIndexReg).getReg() !=\n      X86::NoRegister)\n    return false;\n\n  const MachineOperand &DispMO = MemOp.getOperand(MemRefBegin + X86::AddrDisp);\n\n  // Displacement can be symbolic\n  if (!DispMO.isImm())\n    return false;\n\n  Offset = DispMO.getImm();\n\n  if (!BaseOp->isReg())\n    return false;\n\n  OffsetIsScalable = false;\n  // FIXME: Relying on memoperands() may not be right thing to do here. Check\n  // with X86 maintainers, and fix it accordingly. For now, it is ok, since\n  // there is no use of `Width` for X86 back-end at the moment.\n  Width =\n      !MemOp.memoperands_empty() ? MemOp.memoperands().front()->getSize() : 0;\n  BaseOps.push_back(BaseOp);\n  return true;\n}\n\nstatic unsigned getStoreRegOpcode(Register SrcReg,\n                                  const TargetRegisterClass *RC,\n                                  bool IsStackAligned,\n                                  const X86Subtarget &STI) {\n  return getLoadStoreRegOpcode(SrcReg, RC, IsStackAligned, STI, false);\n}\n\nstatic unsigned getLoadRegOpcode(Register DestReg,\n                                 const TargetRegisterClass *RC,\n                                 bool IsStackAligned, const X86Subtarget &STI) {\n  return getLoadStoreRegOpcode(DestReg, RC, IsStackAligned, STI, true);\n}\n\nvoid X86InstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,\n                                       MachineBasicBlock::iterator MI,\n                                       Register SrcReg, bool isKill, int FrameIdx,\n                                       const TargetRegisterClass *RC,\n                                       const TargetRegisterInfo *TRI) const {\n  const MachineFunction &MF = *MBB.getParent();\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  assert(MFI.getObjectSize(FrameIdx) >= TRI->getSpillSize(*RC) &&\n         \"Stack slot too small for store\");\n  if (RC->getID() == X86::TILERegClassID) {\n    unsigned Opc = X86::TILESTORED;\n    // tilestored %tmm, (%sp, %idx)\n    MachineRegisterInfo &RegInfo = MBB.getParent()->getRegInfo();\n    Register VirtReg = RegInfo.createVirtualRegister(&X86::GR64_NOSPRegClass);\n    BuildMI(MBB, MI, DebugLoc(), get(X86::MOV64ri), VirtReg).addImm(64);\n    MachineInstr *NewMI =\n        addFrameReference(BuildMI(MBB, MI, DebugLoc(), get(Opc)), FrameIdx)\n            .addReg(SrcReg, getKillRegState(isKill));\n    MachineOperand &MO = NewMI->getOperand(2);\n    MO.setReg(VirtReg);\n    MO.setIsKill(true);\n  } else {\n    unsigned Alignment = std::max<uint32_t>(TRI->getSpillSize(*RC), 16);\n    bool isAligned =\n        (Subtarget.getFrameLowering()->getStackAlign() >= Alignment) ||\n        (RI.canRealignStack(MF) && !MFI.isFixedObjectIndex(FrameIdx));\n    unsigned Opc = getStoreRegOpcode(SrcReg, RC, isAligned, Subtarget);\n    addFrameReference(BuildMI(MBB, MI, DebugLoc(), get(Opc)), FrameIdx)\n        .addReg(SrcReg, getKillRegState(isKill));\n  }\n}\n\nvoid X86InstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,\n                                        MachineBasicBlock::iterator MI,\n                                        Register DestReg, int FrameIdx,\n                                        const TargetRegisterClass *RC,\n                                        const TargetRegisterInfo *TRI) const {\n  if (RC->getID() == X86::TILERegClassID) {\n    unsigned Opc = X86::TILELOADD;\n    // tileloadd (%sp, %idx), %tmm\n    MachineRegisterInfo &RegInfo = MBB.getParent()->getRegInfo();\n    Register VirtReg = RegInfo.createVirtualRegister(&X86::GR64_NOSPRegClass);\n    MachineInstr *NewMI =\n        BuildMI(MBB, MI, DebugLoc(), get(X86::MOV64ri), VirtReg).addImm(64);\n    NewMI = addFrameReference(BuildMI(MBB, MI, DebugLoc(), get(Opc), DestReg),\n                              FrameIdx);\n    MachineOperand &MO = NewMI->getOperand(3);\n    MO.setReg(VirtReg);\n    MO.setIsKill(true);\n  } else {\n    const MachineFunction &MF = *MBB.getParent();\n    const MachineFrameInfo &MFI = MF.getFrameInfo();\n    unsigned Alignment = std::max<uint32_t>(TRI->getSpillSize(*RC), 16);\n    bool isAligned =\n        (Subtarget.getFrameLowering()->getStackAlign() >= Alignment) ||\n        (RI.canRealignStack(MF) && !MFI.isFixedObjectIndex(FrameIdx));\n    unsigned Opc = getLoadRegOpcode(DestReg, RC, isAligned, Subtarget);\n    addFrameReference(BuildMI(MBB, MI, DebugLoc(), get(Opc), DestReg),\n                      FrameIdx);\n  }\n}\n\nbool X86InstrInfo::analyzeCompare(const MachineInstr &MI, Register &SrcReg,\n                                  Register &SrcReg2, int &CmpMask,\n                                  int &CmpValue) const {\n  switch (MI.getOpcode()) {\n  default: break;\n  case X86::CMP64ri32:\n  case X86::CMP64ri8:\n  case X86::CMP32ri:\n  case X86::CMP32ri8:\n  case X86::CMP16ri:\n  case X86::CMP16ri8:\n  case X86::CMP8ri:\n    SrcReg = MI.getOperand(0).getReg();\n    SrcReg2 = 0;\n    if (MI.getOperand(1).isImm()) {\n      CmpMask = ~0;\n      CmpValue = MI.getOperand(1).getImm();\n    } else {\n      CmpMask = CmpValue = 0;\n    }\n    return true;\n  // A SUB can be used to perform comparison.\n  case X86::SUB64rm:\n  case X86::SUB32rm:\n  case X86::SUB16rm:\n  case X86::SUB8rm:\n    SrcReg = MI.getOperand(1).getReg();\n    SrcReg2 = 0;\n    CmpMask = 0;\n    CmpValue = 0;\n    return true;\n  case X86::SUB64rr:\n  case X86::SUB32rr:\n  case X86::SUB16rr:\n  case X86::SUB8rr:\n    SrcReg = MI.getOperand(1).getReg();\n    SrcReg2 = MI.getOperand(2).getReg();\n    CmpMask = 0;\n    CmpValue = 0;\n    return true;\n  case X86::SUB64ri32:\n  case X86::SUB64ri8:\n  case X86::SUB32ri:\n  case X86::SUB32ri8:\n  case X86::SUB16ri:\n  case X86::SUB16ri8:\n  case X86::SUB8ri:\n    SrcReg = MI.getOperand(1).getReg();\n    SrcReg2 = 0;\n    if (MI.getOperand(2).isImm()) {\n      CmpMask = ~0;\n      CmpValue = MI.getOperand(2).getImm();\n    } else {\n      CmpMask = CmpValue = 0;\n    }\n    return true;\n  case X86::CMP64rr:\n  case X86::CMP32rr:\n  case X86::CMP16rr:\n  case X86::CMP8rr:\n    SrcReg = MI.getOperand(0).getReg();\n    SrcReg2 = MI.getOperand(1).getReg();\n    CmpMask = 0;\n    CmpValue = 0;\n    return true;\n  case X86::TEST8rr:\n  case X86::TEST16rr:\n  case X86::TEST32rr:\n  case X86::TEST64rr:\n    SrcReg = MI.getOperand(0).getReg();\n    if (MI.getOperand(1).getReg() != SrcReg)\n      return false;\n    // Compare against zero.\n    SrcReg2 = 0;\n    CmpMask = ~0;\n    CmpValue = 0;\n    return true;\n  }\n  return false;\n}\n\n/// Check whether the first instruction, whose only\n/// purpose is to update flags, can be made redundant.\n/// CMPrr can be made redundant by SUBrr if the operands are the same.\n/// This function can be extended later on.\n/// SrcReg, SrcRegs: register operands for FlagI.\n/// ImmValue: immediate for FlagI if it takes an immediate.\ninline static bool isRedundantFlagInstr(const MachineInstr &FlagI,\n                                        Register SrcReg, Register SrcReg2,\n                                        int ImmMask, int ImmValue,\n                                        const MachineInstr &OI) {\n  if (((FlagI.getOpcode() == X86::CMP64rr && OI.getOpcode() == X86::SUB64rr) ||\n       (FlagI.getOpcode() == X86::CMP32rr && OI.getOpcode() == X86::SUB32rr) ||\n       (FlagI.getOpcode() == X86::CMP16rr && OI.getOpcode() == X86::SUB16rr) ||\n       (FlagI.getOpcode() == X86::CMP8rr && OI.getOpcode() == X86::SUB8rr)) &&\n      ((OI.getOperand(1).getReg() == SrcReg &&\n        OI.getOperand(2).getReg() == SrcReg2) ||\n       (OI.getOperand(1).getReg() == SrcReg2 &&\n        OI.getOperand(2).getReg() == SrcReg)))\n    return true;\n\n  if (ImmMask != 0 &&\n      ((FlagI.getOpcode() == X86::CMP64ri32 &&\n        OI.getOpcode() == X86::SUB64ri32) ||\n       (FlagI.getOpcode() == X86::CMP64ri8 &&\n        OI.getOpcode() == X86::SUB64ri8) ||\n       (FlagI.getOpcode() == X86::CMP32ri && OI.getOpcode() == X86::SUB32ri) ||\n       (FlagI.getOpcode() == X86::CMP32ri8 &&\n        OI.getOpcode() == X86::SUB32ri8) ||\n       (FlagI.getOpcode() == X86::CMP16ri && OI.getOpcode() == X86::SUB16ri) ||\n       (FlagI.getOpcode() == X86::CMP16ri8 &&\n        OI.getOpcode() == X86::SUB16ri8) ||\n       (FlagI.getOpcode() == X86::CMP8ri && OI.getOpcode() == X86::SUB8ri)) &&\n      OI.getOperand(1).getReg() == SrcReg &&\n      OI.getOperand(2).getImm() == ImmValue)\n    return true;\n  return false;\n}\n\n/// Check whether the definition can be converted\n/// to remove a comparison against zero.\ninline static bool isDefConvertible(const MachineInstr &MI, bool &NoSignFlag) {\n  NoSignFlag = false;\n\n  switch (MI.getOpcode()) {\n  default: return false;\n\n  // The shift instructions only modify ZF if their shift count is non-zero.\n  // N.B.: The processor truncates the shift count depending on the encoding.\n  case X86::SAR8ri:    case X86::SAR16ri:  case X86::SAR32ri:case X86::SAR64ri:\n  case X86::SHR8ri:    case X86::SHR16ri:  case X86::SHR32ri:case X86::SHR64ri:\n     return getTruncatedShiftCount(MI, 2) != 0;\n\n  // Some left shift instructions can be turned into LEA instructions but only\n  // if their flags aren't used. Avoid transforming such instructions.\n  case X86::SHL8ri:    case X86::SHL16ri:  case X86::SHL32ri:case X86::SHL64ri:{\n    unsigned ShAmt = getTruncatedShiftCount(MI, 2);\n    if (isTruncatedShiftCountForLEA(ShAmt)) return false;\n    return ShAmt != 0;\n  }\n\n  case X86::SHRD16rri8:case X86::SHRD32rri8:case X86::SHRD64rri8:\n  case X86::SHLD16rri8:case X86::SHLD32rri8:case X86::SHLD64rri8:\n     return getTruncatedShiftCount(MI, 3) != 0;\n\n  case X86::SUB64ri32: case X86::SUB64ri8: case X86::SUB32ri:\n  case X86::SUB32ri8:  case X86::SUB16ri:  case X86::SUB16ri8:\n  case X86::SUB8ri:    case X86::SUB64rr:  case X86::SUB32rr:\n  case X86::SUB16rr:   case X86::SUB8rr:   case X86::SUB64rm:\n  case X86::SUB32rm:   case X86::SUB16rm:  case X86::SUB8rm:\n  case X86::DEC64r:    case X86::DEC32r:   case X86::DEC16r: case X86::DEC8r:\n  case X86::ADD64ri32: case X86::ADD64ri8: case X86::ADD32ri:\n  case X86::ADD32ri8:  case X86::ADD16ri:  case X86::ADD16ri8:\n  case X86::ADD8ri:    case X86::ADD64rr:  case X86::ADD32rr:\n  case X86::ADD16rr:   case X86::ADD8rr:   case X86::ADD64rm:\n  case X86::ADD32rm:   case X86::ADD16rm:  case X86::ADD8rm:\n  case X86::INC64r:    case X86::INC32r:   case X86::INC16r: case X86::INC8r:\n  case X86::AND64ri32: case X86::AND64ri8: case X86::AND32ri:\n  case X86::AND32ri8:  case X86::AND16ri:  case X86::AND16ri8:\n  case X86::AND8ri:    case X86::AND64rr:  case X86::AND32rr:\n  case X86::AND16rr:   case X86::AND8rr:   case X86::AND64rm:\n  case X86::AND32rm:   case X86::AND16rm:  case X86::AND8rm:\n  case X86::XOR64ri32: case X86::XOR64ri8: case X86::XOR32ri:\n  case X86::XOR32ri8:  case X86::XOR16ri:  case X86::XOR16ri8:\n  case X86::XOR8ri:    case X86::XOR64rr:  case X86::XOR32rr:\n  case X86::XOR16rr:   case X86::XOR8rr:   case X86::XOR64rm:\n  case X86::XOR32rm:   case X86::XOR16rm:  case X86::XOR8rm:\n  case X86::OR64ri32:  case X86::OR64ri8:  case X86::OR32ri:\n  case X86::OR32ri8:   case X86::OR16ri:   case X86::OR16ri8:\n  case X86::OR8ri:     case X86::OR64rr:   case X86::OR32rr:\n  case X86::OR16rr:    case X86::OR8rr:    case X86::OR64rm:\n  case X86::OR32rm:    case X86::OR16rm:   case X86::OR8rm:\n  case X86::ADC64ri32: case X86::ADC64ri8: case X86::ADC32ri:\n  case X86::ADC32ri8:  case X86::ADC16ri:  case X86::ADC16ri8:\n  case X86::ADC8ri:    case X86::ADC64rr:  case X86::ADC32rr:\n  case X86::ADC16rr:   case X86::ADC8rr:   case X86::ADC64rm:\n  case X86::ADC32rm:   case X86::ADC16rm:  case X86::ADC8rm:\n  case X86::SBB64ri32: case X86::SBB64ri8: case X86::SBB32ri:\n  case X86::SBB32ri8:  case X86::SBB16ri:  case X86::SBB16ri8:\n  case X86::SBB8ri:    case X86::SBB64rr:  case X86::SBB32rr:\n  case X86::SBB16rr:   case X86::SBB8rr:   case X86::SBB64rm:\n  case X86::SBB32rm:   case X86::SBB16rm:  case X86::SBB8rm:\n  case X86::NEG8r:     case X86::NEG16r:   case X86::NEG32r: case X86::NEG64r:\n  case X86::SAR8r1:    case X86::SAR16r1:  case X86::SAR32r1:case X86::SAR64r1:\n  case X86::SHR8r1:    case X86::SHR16r1:  case X86::SHR32r1:case X86::SHR64r1:\n  case X86::SHL8r1:    case X86::SHL16r1:  case X86::SHL32r1:case X86::SHL64r1:\n  case X86::ANDN32rr:  case X86::ANDN32rm:\n  case X86::ANDN64rr:  case X86::ANDN64rm:\n  case X86::BLSI32rr:  case X86::BLSI32rm:\n  case X86::BLSI64rr:  case X86::BLSI64rm:\n  case X86::BLSMSK32rr:case X86::BLSMSK32rm:\n  case X86::BLSMSK64rr:case X86::BLSMSK64rm:\n  case X86::BLSR32rr:  case X86::BLSR32rm:\n  case X86::BLSR64rr:  case X86::BLSR64rm:\n  case X86::BZHI32rr:  case X86::BZHI32rm:\n  case X86::BZHI64rr:  case X86::BZHI64rm:\n  case X86::LZCNT16rr: case X86::LZCNT16rm:\n  case X86::LZCNT32rr: case X86::LZCNT32rm:\n  case X86::LZCNT64rr: case X86::LZCNT64rm:\n  case X86::POPCNT16rr:case X86::POPCNT16rm:\n  case X86::POPCNT32rr:case X86::POPCNT32rm:\n  case X86::POPCNT64rr:case X86::POPCNT64rm:\n  case X86::TZCNT16rr: case X86::TZCNT16rm:\n  case X86::TZCNT32rr: case X86::TZCNT32rm:\n  case X86::TZCNT64rr: case X86::TZCNT64rm:\n  case X86::BLCFILL32rr: case X86::BLCFILL32rm:\n  case X86::BLCFILL64rr: case X86::BLCFILL64rm:\n  case X86::BLCI32rr:    case X86::BLCI32rm:\n  case X86::BLCI64rr:    case X86::BLCI64rm:\n  case X86::BLCIC32rr:   case X86::BLCIC32rm:\n  case X86::BLCIC64rr:   case X86::BLCIC64rm:\n  case X86::BLCMSK32rr:  case X86::BLCMSK32rm:\n  case X86::BLCMSK64rr:  case X86::BLCMSK64rm:\n  case X86::BLCS32rr:    case X86::BLCS32rm:\n  case X86::BLCS64rr:    case X86::BLCS64rm:\n  case X86::BLSFILL32rr: case X86::BLSFILL32rm:\n  case X86::BLSFILL64rr: case X86::BLSFILL64rm:\n  case X86::BLSIC32rr:   case X86::BLSIC32rm:\n  case X86::BLSIC64rr:   case X86::BLSIC64rm:\n  case X86::T1MSKC32rr:  case X86::T1MSKC32rm:\n  case X86::T1MSKC64rr:  case X86::T1MSKC64rm:\n  case X86::TZMSK32rr:   case X86::TZMSK32rm:\n  case X86::TZMSK64rr:   case X86::TZMSK64rm:\n    return true;\n  case X86::BEXTR32rr:   case X86::BEXTR64rr:\n  case X86::BEXTR32rm:   case X86::BEXTR64rm:\n  case X86::BEXTRI32ri:  case X86::BEXTRI32mi:\n  case X86::BEXTRI64ri:  case X86::BEXTRI64mi:\n    // BEXTR doesn't update the sign flag so we can't use it.\n    NoSignFlag = true;\n    return true;\n  }\n}\n\n/// Check whether the use can be converted to remove a comparison against zero.\nstatic X86::CondCode isUseDefConvertible(const MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default: return X86::COND_INVALID;\n  case X86::NEG8r:\n  case X86::NEG16r:\n  case X86::NEG32r:\n  case X86::NEG64r:\n    return X86::COND_AE;\n  case X86::LZCNT16rr:\n  case X86::LZCNT32rr:\n  case X86::LZCNT64rr:\n    return X86::COND_B;\n  case X86::POPCNT16rr:\n  case X86::POPCNT32rr:\n  case X86::POPCNT64rr:\n    return X86::COND_E;\n  case X86::TZCNT16rr:\n  case X86::TZCNT32rr:\n  case X86::TZCNT64rr:\n    return X86::COND_B;\n  case X86::BSF16rr:\n  case X86::BSF32rr:\n  case X86::BSF64rr:\n  case X86::BSR16rr:\n  case X86::BSR32rr:\n  case X86::BSR64rr:\n    return X86::COND_E;\n  case X86::BLSI32rr:\n  case X86::BLSI64rr:\n    return X86::COND_AE;\n  case X86::BLSR32rr:\n  case X86::BLSR64rr:\n  case X86::BLSMSK32rr:\n  case X86::BLSMSK64rr:\n    return X86::COND_B;\n  // TODO: TBM instructions.\n  }\n}\n\n/// Check if there exists an earlier instruction that\n/// operates on the same source operands and sets flags in the same way as\n/// Compare; remove Compare if possible.\nbool X86InstrInfo::optimizeCompareInstr(MachineInstr &CmpInstr, Register SrcReg,\n                                        Register SrcReg2, int CmpMask,\n                                        int CmpValue,\n                                        const MachineRegisterInfo *MRI) const {\n  // Check whether we can replace SUB with CMP.\n  switch (CmpInstr.getOpcode()) {\n  default: break;\n  case X86::SUB64ri32:\n  case X86::SUB64ri8:\n  case X86::SUB32ri:\n  case X86::SUB32ri8:\n  case X86::SUB16ri:\n  case X86::SUB16ri8:\n  case X86::SUB8ri:\n  case X86::SUB64rm:\n  case X86::SUB32rm:\n  case X86::SUB16rm:\n  case X86::SUB8rm:\n  case X86::SUB64rr:\n  case X86::SUB32rr:\n  case X86::SUB16rr:\n  case X86::SUB8rr: {\n    if (!MRI->use_nodbg_empty(CmpInstr.getOperand(0).getReg()))\n      return false;\n    // There is no use of the destination register, we can replace SUB with CMP.\n    unsigned NewOpcode = 0;\n    switch (CmpInstr.getOpcode()) {\n    default: llvm_unreachable(\"Unreachable!\");\n    case X86::SUB64rm:   NewOpcode = X86::CMP64rm;   break;\n    case X86::SUB32rm:   NewOpcode = X86::CMP32rm;   break;\n    case X86::SUB16rm:   NewOpcode = X86::CMP16rm;   break;\n    case X86::SUB8rm:    NewOpcode = X86::CMP8rm;    break;\n    case X86::SUB64rr:   NewOpcode = X86::CMP64rr;   break;\n    case X86::SUB32rr:   NewOpcode = X86::CMP32rr;   break;\n    case X86::SUB16rr:   NewOpcode = X86::CMP16rr;   break;\n    case X86::SUB8rr:    NewOpcode = X86::CMP8rr;    break;\n    case X86::SUB64ri32: NewOpcode = X86::CMP64ri32; break;\n    case X86::SUB64ri8:  NewOpcode = X86::CMP64ri8;  break;\n    case X86::SUB32ri:   NewOpcode = X86::CMP32ri;   break;\n    case X86::SUB32ri8:  NewOpcode = X86::CMP32ri8;  break;\n    case X86::SUB16ri:   NewOpcode = X86::CMP16ri;   break;\n    case X86::SUB16ri8:  NewOpcode = X86::CMP16ri8;  break;\n    case X86::SUB8ri:    NewOpcode = X86::CMP8ri;    break;\n    }\n    CmpInstr.setDesc(get(NewOpcode));\n    CmpInstr.RemoveOperand(0);\n    // Fall through to optimize Cmp if Cmp is CMPrr or CMPri.\n    if (NewOpcode == X86::CMP64rm || NewOpcode == X86::CMP32rm ||\n        NewOpcode == X86::CMP16rm || NewOpcode == X86::CMP8rm)\n      return false;\n  }\n  }\n\n  // Get the unique definition of SrcReg.\n  MachineInstr *MI = MRI->getUniqueVRegDef(SrcReg);\n  if (!MI) return false;\n\n  // CmpInstr is the first instruction of the BB.\n  MachineBasicBlock::iterator I = CmpInstr, Def = MI;\n\n  // If we are comparing against zero, check whether we can use MI to update\n  // EFLAGS. If MI is not in the same BB as CmpInstr, do not optimize.\n  bool IsCmpZero = (CmpMask != 0 && CmpValue == 0);\n  if (IsCmpZero && MI->getParent() != CmpInstr.getParent())\n    return false;\n\n  // If we have a use of the source register between the def and our compare\n  // instruction we can eliminate the compare iff the use sets EFLAGS in the\n  // right way.\n  bool ShouldUpdateCC = false;\n  bool NoSignFlag = false;\n  X86::CondCode NewCC = X86::COND_INVALID;\n  if (IsCmpZero && !isDefConvertible(*MI, NoSignFlag)) {\n    // Scan forward from the use until we hit the use we're looking for or the\n    // compare instruction.\n    for (MachineBasicBlock::iterator J = MI;; ++J) {\n      // Do we have a convertible instruction?\n      NewCC = isUseDefConvertible(*J);\n      if (NewCC != X86::COND_INVALID && J->getOperand(1).isReg() &&\n          J->getOperand(1).getReg() == SrcReg) {\n        assert(J->definesRegister(X86::EFLAGS) && \"Must be an EFLAGS def!\");\n        ShouldUpdateCC = true; // Update CC later on.\n        // This is not a def of SrcReg, but still a def of EFLAGS. Keep going\n        // with the new def.\n        Def = J;\n        MI = &*Def;\n        break;\n      }\n\n      if (J == I)\n        return false;\n    }\n  }\n\n  // We are searching for an earlier instruction that can make CmpInstr\n  // redundant and that instruction will be saved in Sub.\n  MachineInstr *Sub = nullptr;\n  const TargetRegisterInfo *TRI = &getRegisterInfo();\n\n  // We iterate backward, starting from the instruction before CmpInstr and\n  // stop when reaching the definition of a source register or done with the BB.\n  // RI points to the instruction before CmpInstr.\n  // If the definition is in this basic block, RE points to the definition;\n  // otherwise, RE is the rend of the basic block.\n  MachineBasicBlock::reverse_iterator\n      RI = ++I.getReverse(),\n      RE = CmpInstr.getParent() == MI->getParent()\n               ? Def.getReverse() /* points to MI */\n               : CmpInstr.getParent()->rend();\n  MachineInstr *Movr0Inst = nullptr;\n  for (; RI != RE; ++RI) {\n    MachineInstr &Instr = *RI;\n    // Check whether CmpInstr can be made redundant by the current instruction.\n    if (!IsCmpZero && isRedundantFlagInstr(CmpInstr, SrcReg, SrcReg2, CmpMask,\n                                           CmpValue, Instr)) {\n      Sub = &Instr;\n      break;\n    }\n\n    if (Instr.modifiesRegister(X86::EFLAGS, TRI) ||\n        Instr.readsRegister(X86::EFLAGS, TRI)) {\n      // This instruction modifies or uses EFLAGS.\n\n      // MOV32r0 etc. are implemented with xor which clobbers condition code.\n      // They are safe to move up, if the definition to EFLAGS is dead and\n      // earlier instructions do not read or write EFLAGS.\n      if (!Movr0Inst && Instr.getOpcode() == X86::MOV32r0 &&\n          Instr.registerDefIsDead(X86::EFLAGS, TRI)) {\n        Movr0Inst = &Instr;\n        continue;\n      }\n\n      // We can't remove CmpInstr.\n      return false;\n    }\n  }\n\n  // Return false if no candidates exist.\n  if (!IsCmpZero && !Sub)\n    return false;\n\n  bool IsSwapped =\n      (SrcReg2 != 0 && Sub && Sub->getOperand(1).getReg() == SrcReg2 &&\n       Sub->getOperand(2).getReg() == SrcReg);\n\n  // Scan forward from the instruction after CmpInstr for uses of EFLAGS.\n  // It is safe to remove CmpInstr if EFLAGS is redefined or killed.\n  // If we are done with the basic block, we need to check whether EFLAGS is\n  // live-out.\n  bool IsSafe = false;\n  SmallVector<std::pair<MachineInstr*, X86::CondCode>, 4> OpsToUpdate;\n  MachineBasicBlock::iterator E = CmpInstr.getParent()->end();\n  for (++I; I != E; ++I) {\n    const MachineInstr &Instr = *I;\n    bool ModifyEFLAGS = Instr.modifiesRegister(X86::EFLAGS, TRI);\n    bool UseEFLAGS = Instr.readsRegister(X86::EFLAGS, TRI);\n    // We should check the usage if this instruction uses and updates EFLAGS.\n    if (!UseEFLAGS && ModifyEFLAGS) {\n      // It is safe to remove CmpInstr if EFLAGS is updated again.\n      IsSafe = true;\n      break;\n    }\n    if (!UseEFLAGS && !ModifyEFLAGS)\n      continue;\n\n    // EFLAGS is used by this instruction.\n    X86::CondCode OldCC = X86::COND_INVALID;\n    if (IsCmpZero || IsSwapped) {\n      // We decode the condition code from opcode.\n      if (Instr.isBranch())\n        OldCC = X86::getCondFromBranch(Instr);\n      else {\n        OldCC = X86::getCondFromSETCC(Instr);\n        if (OldCC == X86::COND_INVALID)\n          OldCC = X86::getCondFromCMov(Instr);\n      }\n      if (OldCC == X86::COND_INVALID) return false;\n    }\n    X86::CondCode ReplacementCC = X86::COND_INVALID;\n    if (IsCmpZero) {\n      switch (OldCC) {\n      default: break;\n      case X86::COND_A: case X86::COND_AE:\n      case X86::COND_B: case X86::COND_BE:\n      case X86::COND_G: case X86::COND_GE:\n      case X86::COND_L: case X86::COND_LE:\n      case X86::COND_O: case X86::COND_NO:\n        // CF and OF are used, we can't perform this optimization.\n        return false;\n      case X86::COND_S: case X86::COND_NS:\n        // If SF is used, but the instruction doesn't update the SF, then we\n        // can't do the optimization.\n        if (NoSignFlag)\n          return false;\n        break;\n      }\n\n      // If we're updating the condition code check if we have to reverse the\n      // condition.\n      if (ShouldUpdateCC)\n        switch (OldCC) {\n        default:\n          return false;\n        case X86::COND_E:\n          ReplacementCC = NewCC;\n          break;\n        case X86::COND_NE:\n          ReplacementCC = GetOppositeBranchCondition(NewCC);\n          break;\n        }\n    } else if (IsSwapped) {\n      // If we have SUB(r1, r2) and CMP(r2, r1), the condition code needs\n      // to be changed from r2 > r1 to r1 < r2, from r2 < r1 to r1 > r2, etc.\n      // We swap the condition code and synthesize the new opcode.\n      ReplacementCC = getSwappedCondition(OldCC);\n      if (ReplacementCC == X86::COND_INVALID) return false;\n    }\n\n    if ((ShouldUpdateCC || IsSwapped) && ReplacementCC != OldCC) {\n      // Push the MachineInstr to OpsToUpdate.\n      // If it is safe to remove CmpInstr, the condition code of these\n      // instructions will be modified.\n      OpsToUpdate.push_back(std::make_pair(&*I, ReplacementCC));\n    }\n    if (ModifyEFLAGS || Instr.killsRegister(X86::EFLAGS, TRI)) {\n      // It is safe to remove CmpInstr if EFLAGS is updated again or killed.\n      IsSafe = true;\n      break;\n    }\n  }\n\n  // If EFLAGS is not killed nor re-defined, we should check whether it is\n  // live-out. If it is live-out, do not optimize.\n  if ((IsCmpZero || IsSwapped) && !IsSafe) {\n    MachineBasicBlock *MBB = CmpInstr.getParent();\n    for (MachineBasicBlock *Successor : MBB->successors())\n      if (Successor->isLiveIn(X86::EFLAGS))\n        return false;\n  }\n\n  // The instruction to be updated is either Sub or MI.\n  Sub = IsCmpZero ? MI : Sub;\n  // Move Movr0Inst to the appropriate place before Sub.\n  if (Movr0Inst) {\n    // Look backwards until we find a def that doesn't use the current EFLAGS.\n    Def = Sub;\n    MachineBasicBlock::reverse_iterator InsertI = Def.getReverse(),\n                                        InsertE = Sub->getParent()->rend();\n    for (; InsertI != InsertE; ++InsertI) {\n      MachineInstr *Instr = &*InsertI;\n      if (!Instr->readsRegister(X86::EFLAGS, TRI) &&\n          Instr->modifiesRegister(X86::EFLAGS, TRI)) {\n        Sub->getParent()->remove(Movr0Inst);\n        Instr->getParent()->insert(MachineBasicBlock::iterator(Instr),\n                                   Movr0Inst);\n        break;\n      }\n    }\n    if (InsertI == InsertE)\n      return false;\n  }\n\n  // Make sure Sub instruction defines EFLAGS and mark the def live.\n  MachineOperand *FlagDef = Sub->findRegisterDefOperand(X86::EFLAGS);\n  assert(FlagDef && \"Unable to locate a def EFLAGS operand\");\n  FlagDef->setIsDead(false);\n\n  CmpInstr.eraseFromParent();\n\n  // Modify the condition code of instructions in OpsToUpdate.\n  for (auto &Op : OpsToUpdate) {\n    Op.first->getOperand(Op.first->getDesc().getNumOperands() - 1)\n        .setImm(Op.second);\n  }\n  return true;\n}\n\n/// Try to remove the load by folding it to a register\n/// operand at the use. We fold the load instructions if load defines a virtual\n/// register, the virtual register is used once in the same BB, and the\n/// instructions in-between do not load or store, and have no side effects.\nMachineInstr *X86InstrInfo::optimizeLoadInstr(MachineInstr &MI,\n                                              const MachineRegisterInfo *MRI,\n                                              Register &FoldAsLoadDefReg,\n                                              MachineInstr *&DefMI) const {\n  // Check whether we can move DefMI here.\n  DefMI = MRI->getVRegDef(FoldAsLoadDefReg);\n  assert(DefMI);\n  bool SawStore = false;\n  if (!DefMI->isSafeToMove(nullptr, SawStore))\n    return nullptr;\n\n  // Collect information about virtual register operands of MI.\n  SmallVector<unsigned, 1> SrcOperandIds;\n  for (unsigned i = 0, e = MI.getNumOperands(); i != e; ++i) {\n    MachineOperand &MO = MI.getOperand(i);\n    if (!MO.isReg())\n      continue;\n    Register Reg = MO.getReg();\n    if (Reg != FoldAsLoadDefReg)\n      continue;\n    // Do not fold if we have a subreg use or a def.\n    if (MO.getSubReg() || MO.isDef())\n      return nullptr;\n    SrcOperandIds.push_back(i);\n  }\n  if (SrcOperandIds.empty())\n    return nullptr;\n\n  // Check whether we can fold the def into SrcOperandId.\n  if (MachineInstr *FoldMI = foldMemoryOperand(MI, SrcOperandIds, *DefMI)) {\n    FoldAsLoadDefReg = 0;\n    return FoldMI;\n  }\n\n  return nullptr;\n}\n\n/// Expand a single-def pseudo instruction to a two-addr\n/// instruction with two undef reads of the register being defined.\n/// This is used for mapping:\n///   %xmm4 = V_SET0\n/// to:\n///   %xmm4 = PXORrr undef %xmm4, undef %xmm4\n///\nstatic bool Expand2AddrUndef(MachineInstrBuilder &MIB,\n                             const MCInstrDesc &Desc) {\n  assert(Desc.getNumOperands() == 3 && \"Expected two-addr instruction.\");\n  Register Reg = MIB.getReg(0);\n  MIB->setDesc(Desc);\n\n  // MachineInstr::addOperand() will insert explicit operands before any\n  // implicit operands.\n  MIB.addReg(Reg, RegState::Undef).addReg(Reg, RegState::Undef);\n  // But we don't trust that.\n  assert(MIB.getReg(1) == Reg &&\n         MIB.getReg(2) == Reg && \"Misplaced operand\");\n  return true;\n}\n\n/// Expand a single-def pseudo instruction to a two-addr\n/// instruction with two %k0 reads.\n/// This is used for mapping:\n///   %k4 = K_SET1\n/// to:\n///   %k4 = KXNORrr %k0, %k0\nstatic bool Expand2AddrKreg(MachineInstrBuilder &MIB, const MCInstrDesc &Desc,\n                            Register Reg) {\n  assert(Desc.getNumOperands() == 3 && \"Expected two-addr instruction.\");\n  MIB->setDesc(Desc);\n  MIB.addReg(Reg, RegState::Undef).addReg(Reg, RegState::Undef);\n  return true;\n}\n\nstatic bool expandMOV32r1(MachineInstrBuilder &MIB, const TargetInstrInfo &TII,\n                          bool MinusOne) {\n  MachineBasicBlock &MBB = *MIB->getParent();\n  DebugLoc DL = MIB->getDebugLoc();\n  Register Reg = MIB.getReg(0);\n\n  // Insert the XOR.\n  BuildMI(MBB, MIB.getInstr(), DL, TII.get(X86::XOR32rr), Reg)\n      .addReg(Reg, RegState::Undef)\n      .addReg(Reg, RegState::Undef);\n\n  // Turn the pseudo into an INC or DEC.\n  MIB->setDesc(TII.get(MinusOne ? X86::DEC32r : X86::INC32r));\n  MIB.addReg(Reg);\n\n  return true;\n}\n\nstatic bool ExpandMOVImmSExti8(MachineInstrBuilder &MIB,\n                               const TargetInstrInfo &TII,\n                               const X86Subtarget &Subtarget) {\n  MachineBasicBlock &MBB = *MIB->getParent();\n  DebugLoc DL = MIB->getDebugLoc();\n  int64_t Imm = MIB->getOperand(1).getImm();\n  assert(Imm != 0 && \"Using push/pop for 0 is not efficient.\");\n  MachineBasicBlock::iterator I = MIB.getInstr();\n\n  int StackAdjustment;\n\n  if (Subtarget.is64Bit()) {\n    assert(MIB->getOpcode() == X86::MOV64ImmSExti8 ||\n           MIB->getOpcode() == X86::MOV32ImmSExti8);\n\n    // Can't use push/pop lowering if the function might write to the red zone.\n    X86MachineFunctionInfo *X86FI =\n        MBB.getParent()->getInfo<X86MachineFunctionInfo>();\n    if (X86FI->getUsesRedZone()) {\n      MIB->setDesc(TII.get(MIB->getOpcode() ==\n                           X86::MOV32ImmSExti8 ? X86::MOV32ri : X86::MOV64ri));\n      return true;\n    }\n\n    // 64-bit mode doesn't have 32-bit push/pop, so use 64-bit operations and\n    // widen the register if necessary.\n    StackAdjustment = 8;\n    BuildMI(MBB, I, DL, TII.get(X86::PUSH64i8)).addImm(Imm);\n    MIB->setDesc(TII.get(X86::POP64r));\n    MIB->getOperand(0)\n        .setReg(getX86SubSuperRegister(MIB.getReg(0), 64));\n  } else {\n    assert(MIB->getOpcode() == X86::MOV32ImmSExti8);\n    StackAdjustment = 4;\n    BuildMI(MBB, I, DL, TII.get(X86::PUSH32i8)).addImm(Imm);\n    MIB->setDesc(TII.get(X86::POP32r));\n  }\n  MIB->RemoveOperand(1);\n  MIB->addImplicitDefUseOperands(*MBB.getParent());\n\n  // Build CFI if necessary.\n  MachineFunction &MF = *MBB.getParent();\n  const X86FrameLowering *TFL = Subtarget.getFrameLowering();\n  bool IsWin64Prologue = MF.getTarget().getMCAsmInfo()->usesWindowsCFI();\n  bool NeedsDwarfCFI = !IsWin64Prologue && MF.needsFrameMoves();\n  bool EmitCFI = !TFL->hasFP(MF) && NeedsDwarfCFI;\n  if (EmitCFI) {\n    TFL->BuildCFI(MBB, I, DL,\n        MCCFIInstruction::createAdjustCfaOffset(nullptr, StackAdjustment));\n    TFL->BuildCFI(MBB, std::next(I), DL,\n        MCCFIInstruction::createAdjustCfaOffset(nullptr, -StackAdjustment));\n  }\n\n  return true;\n}\n\n// LoadStackGuard has so far only been implemented for 64-bit MachO. Different\n// code sequence is needed for other targets.\nstatic void expandLoadStackGuard(MachineInstrBuilder &MIB,\n                                 const TargetInstrInfo &TII) {\n  MachineBasicBlock &MBB = *MIB->getParent();\n  DebugLoc DL = MIB->getDebugLoc();\n  Register Reg = MIB.getReg(0);\n  const GlobalValue *GV =\n      cast<GlobalValue>((*MIB->memoperands_begin())->getValue());\n  auto Flags = MachineMemOperand::MOLoad |\n               MachineMemOperand::MODereferenceable |\n               MachineMemOperand::MOInvariant;\n  MachineMemOperand *MMO = MBB.getParent()->getMachineMemOperand(\n      MachinePointerInfo::getGOT(*MBB.getParent()), Flags, 8, Align(8));\n  MachineBasicBlock::iterator I = MIB.getInstr();\n\n  BuildMI(MBB, I, DL, TII.get(X86::MOV64rm), Reg).addReg(X86::RIP).addImm(1)\n      .addReg(0).addGlobalAddress(GV, 0, X86II::MO_GOTPCREL).addReg(0)\n      .addMemOperand(MMO);\n  MIB->setDebugLoc(DL);\n  MIB->setDesc(TII.get(X86::MOV64rm));\n  MIB.addReg(Reg, RegState::Kill).addImm(1).addReg(0).addImm(0).addReg(0);\n}\n\nstatic bool expandXorFP(MachineInstrBuilder &MIB, const TargetInstrInfo &TII) {\n  MachineBasicBlock &MBB = *MIB->getParent();\n  MachineFunction &MF = *MBB.getParent();\n  const X86Subtarget &Subtarget = MF.getSubtarget<X86Subtarget>();\n  const X86RegisterInfo *TRI = Subtarget.getRegisterInfo();\n  unsigned XorOp =\n      MIB->getOpcode() == X86::XOR64_FP ? X86::XOR64rr : X86::XOR32rr;\n  MIB->setDesc(TII.get(XorOp));\n  MIB.addReg(TRI->getFrameRegister(MF), RegState::Undef);\n  return true;\n}\n\n// This is used to handle spills for 128/256-bit registers when we have AVX512,\n// but not VLX. If it uses an extended register we need to use an instruction\n// that loads the lower 128/256-bit, but is available with only AVX512F.\nstatic bool expandNOVLXLoad(MachineInstrBuilder &MIB,\n                            const TargetRegisterInfo *TRI,\n                            const MCInstrDesc &LoadDesc,\n                            const MCInstrDesc &BroadcastDesc,\n                            unsigned SubIdx) {\n  Register DestReg = MIB.getReg(0);\n  // Check if DestReg is XMM16-31 or YMM16-31.\n  if (TRI->getEncodingValue(DestReg) < 16) {\n    // We can use a normal VEX encoded load.\n    MIB->setDesc(LoadDesc);\n  } else {\n    // Use a 128/256-bit VBROADCAST instruction.\n    MIB->setDesc(BroadcastDesc);\n    // Change the destination to a 512-bit register.\n    DestReg = TRI->getMatchingSuperReg(DestReg, SubIdx, &X86::VR512RegClass);\n    MIB->getOperand(0).setReg(DestReg);\n  }\n  return true;\n}\n\n// This is used to handle spills for 128/256-bit registers when we have AVX512,\n// but not VLX. If it uses an extended register we need to use an instruction\n// that stores the lower 128/256-bit, but is available with only AVX512F.\nstatic bool expandNOVLXStore(MachineInstrBuilder &MIB,\n                             const TargetRegisterInfo *TRI,\n                             const MCInstrDesc &StoreDesc,\n                             const MCInstrDesc &ExtractDesc,\n                             unsigned SubIdx) {\n  Register SrcReg = MIB.getReg(X86::AddrNumOperands);\n  // Check if DestReg is XMM16-31 or YMM16-31.\n  if (TRI->getEncodingValue(SrcReg) < 16) {\n    // We can use a normal VEX encoded store.\n    MIB->setDesc(StoreDesc);\n  } else {\n    // Use a VEXTRACTF instruction.\n    MIB->setDesc(ExtractDesc);\n    // Change the destination to a 512-bit register.\n    SrcReg = TRI->getMatchingSuperReg(SrcReg, SubIdx, &X86::VR512RegClass);\n    MIB->getOperand(X86::AddrNumOperands).setReg(SrcReg);\n    MIB.addImm(0x0); // Append immediate to extract from the lower bits.\n  }\n\n  return true;\n}\n\nstatic bool expandSHXDROT(MachineInstrBuilder &MIB, const MCInstrDesc &Desc) {\n  MIB->setDesc(Desc);\n  int64_t ShiftAmt = MIB->getOperand(2).getImm();\n  // Temporarily remove the immediate so we can add another source register.\n  MIB->RemoveOperand(2);\n  // Add the register. Don't copy the kill flag if there is one.\n  MIB.addReg(MIB.getReg(1),\n             getUndefRegState(MIB->getOperand(1).isUndef()));\n  // Add back the immediate.\n  MIB.addImm(ShiftAmt);\n  return true;\n}\n\nbool X86InstrInfo::expandPostRAPseudo(MachineInstr &MI) const {\n  bool HasAVX = Subtarget.hasAVX();\n  MachineInstrBuilder MIB(*MI.getParent()->getParent(), MI);\n  switch (MI.getOpcode()) {\n  case X86::MOV32r0:\n    return Expand2AddrUndef(MIB, get(X86::XOR32rr));\n  case X86::MOV32r1:\n    return expandMOV32r1(MIB, *this, /*MinusOne=*/ false);\n  case X86::MOV32r_1:\n    return expandMOV32r1(MIB, *this, /*MinusOne=*/ true);\n  case X86::MOV32ImmSExti8:\n  case X86::MOV64ImmSExti8:\n    return ExpandMOVImmSExti8(MIB, *this, Subtarget);\n  case X86::SETB_C32r:\n    return Expand2AddrUndef(MIB, get(X86::SBB32rr));\n  case X86::SETB_C64r:\n    return Expand2AddrUndef(MIB, get(X86::SBB64rr));\n  case X86::MMX_SET0:\n    return Expand2AddrUndef(MIB, get(X86::MMX_PXORirr));\n  case X86::V_SET0:\n  case X86::FsFLD0SS:\n  case X86::FsFLD0SD:\n  case X86::FsFLD0F128:\n    return Expand2AddrUndef(MIB, get(HasAVX ? X86::VXORPSrr : X86::XORPSrr));\n  case X86::AVX_SET0: {\n    assert(HasAVX && \"AVX not supported\");\n    const TargetRegisterInfo *TRI = &getRegisterInfo();\n    Register SrcReg = MIB.getReg(0);\n    Register XReg = TRI->getSubReg(SrcReg, X86::sub_xmm);\n    MIB->getOperand(0).setReg(XReg);\n    Expand2AddrUndef(MIB, get(X86::VXORPSrr));\n    MIB.addReg(SrcReg, RegState::ImplicitDefine);\n    return true;\n  }\n  case X86::AVX512_128_SET0:\n  case X86::AVX512_FsFLD0SS:\n  case X86::AVX512_FsFLD0SD:\n  case X86::AVX512_FsFLD0F128: {\n    bool HasVLX = Subtarget.hasVLX();\n    Register SrcReg = MIB.getReg(0);\n    const TargetRegisterInfo *TRI = &getRegisterInfo();\n    if (HasVLX || TRI->getEncodingValue(SrcReg) < 16)\n      return Expand2AddrUndef(MIB,\n                              get(HasVLX ? X86::VPXORDZ128rr : X86::VXORPSrr));\n    // Extended register without VLX. Use a larger XOR.\n    SrcReg =\n        TRI->getMatchingSuperReg(SrcReg, X86::sub_xmm, &X86::VR512RegClass);\n    MIB->getOperand(0).setReg(SrcReg);\n    return Expand2AddrUndef(MIB, get(X86::VPXORDZrr));\n  }\n  case X86::AVX512_256_SET0:\n  case X86::AVX512_512_SET0: {\n    bool HasVLX = Subtarget.hasVLX();\n    Register SrcReg = MIB.getReg(0);\n    const TargetRegisterInfo *TRI = &getRegisterInfo();\n    if (HasVLX || TRI->getEncodingValue(SrcReg) < 16) {\n      Register XReg = TRI->getSubReg(SrcReg, X86::sub_xmm);\n      MIB->getOperand(0).setReg(XReg);\n      Expand2AddrUndef(MIB,\n                       get(HasVLX ? X86::VPXORDZ128rr : X86::VXORPSrr));\n      MIB.addReg(SrcReg, RegState::ImplicitDefine);\n      return true;\n    }\n    if (MI.getOpcode() == X86::AVX512_256_SET0) {\n      // No VLX so we must reference a zmm.\n      unsigned ZReg =\n        TRI->getMatchingSuperReg(SrcReg, X86::sub_ymm, &X86::VR512RegClass);\n      MIB->getOperand(0).setReg(ZReg);\n    }\n    return Expand2AddrUndef(MIB, get(X86::VPXORDZrr));\n  }\n  case X86::V_SETALLONES:\n    return Expand2AddrUndef(MIB, get(HasAVX ? X86::VPCMPEQDrr : X86::PCMPEQDrr));\n  case X86::AVX2_SETALLONES:\n    return Expand2AddrUndef(MIB, get(X86::VPCMPEQDYrr));\n  case X86::AVX1_SETALLONES: {\n    Register Reg = MIB.getReg(0);\n    // VCMPPSYrri with an immediate 0xf should produce VCMPTRUEPS.\n    MIB->setDesc(get(X86::VCMPPSYrri));\n    MIB.addReg(Reg, RegState::Undef).addReg(Reg, RegState::Undef).addImm(0xf);\n    return true;\n  }\n  case X86::AVX512_512_SETALLONES: {\n    Register Reg = MIB.getReg(0);\n    MIB->setDesc(get(X86::VPTERNLOGDZrri));\n    // VPTERNLOGD needs 3 register inputs and an immediate.\n    // 0xff will return 1s for any input.\n    MIB.addReg(Reg, RegState::Undef).addReg(Reg, RegState::Undef)\n       .addReg(Reg, RegState::Undef).addImm(0xff);\n    return true;\n  }\n  case X86::AVX512_512_SEXT_MASK_32:\n  case X86::AVX512_512_SEXT_MASK_64: {\n    Register Reg = MIB.getReg(0);\n    Register MaskReg = MIB.getReg(1);\n    unsigned MaskState = getRegState(MIB->getOperand(1));\n    unsigned Opc = (MI.getOpcode() == X86::AVX512_512_SEXT_MASK_64) ?\n                   X86::VPTERNLOGQZrrikz : X86::VPTERNLOGDZrrikz;\n    MI.RemoveOperand(1);\n    MIB->setDesc(get(Opc));\n    // VPTERNLOG needs 3 register inputs and an immediate.\n    // 0xff will return 1s for any input.\n    MIB.addReg(Reg, RegState::Undef).addReg(MaskReg, MaskState)\n       .addReg(Reg, RegState::Undef).addReg(Reg, RegState::Undef).addImm(0xff);\n    return true;\n  }\n  case X86::VMOVAPSZ128rm_NOVLX:\n    return expandNOVLXLoad(MIB, &getRegisterInfo(), get(X86::VMOVAPSrm),\n                           get(X86::VBROADCASTF32X4rm), X86::sub_xmm);\n  case X86::VMOVUPSZ128rm_NOVLX:\n    return expandNOVLXLoad(MIB, &getRegisterInfo(), get(X86::VMOVUPSrm),\n                           get(X86::VBROADCASTF32X4rm), X86::sub_xmm);\n  case X86::VMOVAPSZ256rm_NOVLX:\n    return expandNOVLXLoad(MIB, &getRegisterInfo(), get(X86::VMOVAPSYrm),\n                           get(X86::VBROADCASTF64X4rm), X86::sub_ymm);\n  case X86::VMOVUPSZ256rm_NOVLX:\n    return expandNOVLXLoad(MIB, &getRegisterInfo(), get(X86::VMOVUPSYrm),\n                           get(X86::VBROADCASTF64X4rm), X86::sub_ymm);\n  case X86::VMOVAPSZ128mr_NOVLX:\n    return expandNOVLXStore(MIB, &getRegisterInfo(), get(X86::VMOVAPSmr),\n                            get(X86::VEXTRACTF32x4Zmr), X86::sub_xmm);\n  case X86::VMOVUPSZ128mr_NOVLX:\n    return expandNOVLXStore(MIB, &getRegisterInfo(), get(X86::VMOVUPSmr),\n                            get(X86::VEXTRACTF32x4Zmr), X86::sub_xmm);\n  case X86::VMOVAPSZ256mr_NOVLX:\n    return expandNOVLXStore(MIB, &getRegisterInfo(), get(X86::VMOVAPSYmr),\n                            get(X86::VEXTRACTF64x4Zmr), X86::sub_ymm);\n  case X86::VMOVUPSZ256mr_NOVLX:\n    return expandNOVLXStore(MIB, &getRegisterInfo(), get(X86::VMOVUPSYmr),\n                            get(X86::VEXTRACTF64x4Zmr), X86::sub_ymm);\n  case X86::MOV32ri64: {\n    Register Reg = MIB.getReg(0);\n    Register Reg32 = RI.getSubReg(Reg, X86::sub_32bit);\n    MI.setDesc(get(X86::MOV32ri));\n    MIB->getOperand(0).setReg(Reg32);\n    MIB.addReg(Reg, RegState::ImplicitDefine);\n    return true;\n  }\n\n  // KNL does not recognize dependency-breaking idioms for mask registers,\n  // so kxnor %k1, %k1, %k2 has a RAW dependence on %k1.\n  // Using %k0 as the undef input register is a performance heuristic based\n  // on the assumption that %k0 is used less frequently than the other mask\n  // registers, since it is not usable as a write mask.\n  // FIXME: A more advanced approach would be to choose the best input mask\n  // register based on context.\n  case X86::KSET0W: return Expand2AddrKreg(MIB, get(X86::KXORWrr), X86::K0);\n  case X86::KSET0D: return Expand2AddrKreg(MIB, get(X86::KXORDrr), X86::K0);\n  case X86::KSET0Q: return Expand2AddrKreg(MIB, get(X86::KXORQrr), X86::K0);\n  case X86::KSET1W: return Expand2AddrKreg(MIB, get(X86::KXNORWrr), X86::K0);\n  case X86::KSET1D: return Expand2AddrKreg(MIB, get(X86::KXNORDrr), X86::K0);\n  case X86::KSET1Q: return Expand2AddrKreg(MIB, get(X86::KXNORQrr), X86::K0);\n  case TargetOpcode::LOAD_STACK_GUARD:\n    expandLoadStackGuard(MIB, *this);\n    return true;\n  case X86::XOR64_FP:\n  case X86::XOR32_FP:\n    return expandXorFP(MIB, *this);\n  case X86::SHLDROT32ri: return expandSHXDROT(MIB, get(X86::SHLD32rri8));\n  case X86::SHLDROT64ri: return expandSHXDROT(MIB, get(X86::SHLD64rri8));\n  case X86::SHRDROT32ri: return expandSHXDROT(MIB, get(X86::SHRD32rri8));\n  case X86::SHRDROT64ri: return expandSHXDROT(MIB, get(X86::SHRD64rri8));\n  case X86::ADD8rr_DB:    MIB->setDesc(get(X86::OR8rr));    break;\n  case X86::ADD16rr_DB:   MIB->setDesc(get(X86::OR16rr));   break;\n  case X86::ADD32rr_DB:   MIB->setDesc(get(X86::OR32rr));   break;\n  case X86::ADD64rr_DB:   MIB->setDesc(get(X86::OR64rr));   break;\n  case X86::ADD8ri_DB:    MIB->setDesc(get(X86::OR8ri));    break;\n  case X86::ADD16ri_DB:   MIB->setDesc(get(X86::OR16ri));   break;\n  case X86::ADD32ri_DB:   MIB->setDesc(get(X86::OR32ri));   break;\n  case X86::ADD64ri32_DB: MIB->setDesc(get(X86::OR64ri32)); break;\n  case X86::ADD16ri8_DB:  MIB->setDesc(get(X86::OR16ri8));  break;\n  case X86::ADD32ri8_DB:  MIB->setDesc(get(X86::OR32ri8));  break;\n  case X86::ADD64ri8_DB:  MIB->setDesc(get(X86::OR64ri8));  break;\n  }\n  return false;\n}\n\n/// Return true for all instructions that only update\n/// the first 32 or 64-bits of the destination register and leave the rest\n/// unmodified. This can be used to avoid folding loads if the instructions\n/// only update part of the destination register, and the non-updated part is\n/// not needed. e.g. cvtss2sd, sqrtss. Unfolding the load from these\n/// instructions breaks the partial register dependency and it can improve\n/// performance. e.g.:\n///\n///   movss (%rdi), %xmm0\n///   cvtss2sd %xmm0, %xmm0\n///\n/// Instead of\n///   cvtss2sd (%rdi), %xmm0\n///\n/// FIXME: This should be turned into a TSFlags.\n///\nstatic bool hasPartialRegUpdate(unsigned Opcode,\n                                const X86Subtarget &Subtarget,\n                                bool ForLoadFold = false) {\n  switch (Opcode) {\n  case X86::CVTSI2SSrr:\n  case X86::CVTSI2SSrm:\n  case X86::CVTSI642SSrr:\n  case X86::CVTSI642SSrm:\n  case X86::CVTSI2SDrr:\n  case X86::CVTSI2SDrm:\n  case X86::CVTSI642SDrr:\n  case X86::CVTSI642SDrm:\n    // Load folding won't effect the undef register update since the input is\n    // a GPR.\n    return !ForLoadFold;\n  case X86::CVTSD2SSrr:\n  case X86::CVTSD2SSrm:\n  case X86::CVTSS2SDrr:\n  case X86::CVTSS2SDrm:\n  case X86::MOVHPDrm:\n  case X86::MOVHPSrm:\n  case X86::MOVLPDrm:\n  case X86::MOVLPSrm:\n  case X86::RCPSSr:\n  case X86::RCPSSm:\n  case X86::RCPSSr_Int:\n  case X86::RCPSSm_Int:\n  case X86::ROUNDSDr:\n  case X86::ROUNDSDm:\n  case X86::ROUNDSSr:\n  case X86::ROUNDSSm:\n  case X86::RSQRTSSr:\n  case X86::RSQRTSSm:\n  case X86::RSQRTSSr_Int:\n  case X86::RSQRTSSm_Int:\n  case X86::SQRTSSr:\n  case X86::SQRTSSm:\n  case X86::SQRTSSr_Int:\n  case X86::SQRTSSm_Int:\n  case X86::SQRTSDr:\n  case X86::SQRTSDm:\n  case X86::SQRTSDr_Int:\n  case X86::SQRTSDm_Int:\n    return true;\n  // GPR\n  case X86::POPCNT32rm:\n  case X86::POPCNT32rr:\n  case X86::POPCNT64rm:\n  case X86::POPCNT64rr:\n    return Subtarget.hasPOPCNTFalseDeps();\n  case X86::LZCNT32rm:\n  case X86::LZCNT32rr:\n  case X86::LZCNT64rm:\n  case X86::LZCNT64rr:\n  case X86::TZCNT32rm:\n  case X86::TZCNT32rr:\n  case X86::TZCNT64rm:\n  case X86::TZCNT64rr:\n    return Subtarget.hasLZCNTFalseDeps();\n  }\n\n  return false;\n}\n\n/// Inform the BreakFalseDeps pass how many idle\n/// instructions we would like before a partial register update.\nunsigned X86InstrInfo::getPartialRegUpdateClearance(\n    const MachineInstr &MI, unsigned OpNum,\n    const TargetRegisterInfo *TRI) const {\n  if (OpNum != 0 || !hasPartialRegUpdate(MI.getOpcode(), Subtarget))\n    return 0;\n\n  // If MI is marked as reading Reg, the partial register update is wanted.\n  const MachineOperand &MO = MI.getOperand(0);\n  Register Reg = MO.getReg();\n  if (Reg.isVirtual()) {\n    if (MO.readsReg() || MI.readsVirtualRegister(Reg))\n      return 0;\n  } else {\n    if (MI.readsRegister(Reg, TRI))\n      return 0;\n  }\n\n  // If any instructions in the clearance range are reading Reg, insert a\n  // dependency breaking instruction, which is inexpensive and is likely to\n  // be hidden in other instruction's cycles.\n  return PartialRegUpdateClearance;\n}\n\n// Return true for any instruction the copies the high bits of the first source\n// operand into the unused high bits of the destination operand.\n// Also returns true for instructions that have two inputs where one may\n// be undef and we want it to use the same register as the other input.\nstatic bool hasUndefRegUpdate(unsigned Opcode, unsigned OpNum,\n                              bool ForLoadFold = false) {\n  // Set the OpNum parameter to the first source operand.\n  switch (Opcode) {\n  case X86::MMX_PUNPCKHBWirr:\n  case X86::MMX_PUNPCKHWDirr:\n  case X86::MMX_PUNPCKHDQirr:\n  case X86::MMX_PUNPCKLBWirr:\n  case X86::MMX_PUNPCKLWDirr:\n  case X86::MMX_PUNPCKLDQirr:\n  case X86::MOVHLPSrr:\n  case X86::PACKSSWBrr:\n  case X86::PACKUSWBrr:\n  case X86::PACKSSDWrr:\n  case X86::PACKUSDWrr:\n  case X86::PUNPCKHBWrr:\n  case X86::PUNPCKLBWrr:\n  case X86::PUNPCKHWDrr:\n  case X86::PUNPCKLWDrr:\n  case X86::PUNPCKHDQrr:\n  case X86::PUNPCKLDQrr:\n  case X86::PUNPCKHQDQrr:\n  case X86::PUNPCKLQDQrr:\n  case X86::SHUFPDrri:\n  case X86::SHUFPSrri:\n    // These instructions are sometimes used with an undef first or second\n    // source. Return true here so BreakFalseDeps will assign this source to the\n    // same register as the first source to avoid a false dependency.\n    // Operand 1 of these instructions is tied so they're separate from their\n    // VEX counterparts.\n    return OpNum == 2 && !ForLoadFold;\n\n  case X86::VMOVLHPSrr:\n  case X86::VMOVLHPSZrr:\n  case X86::VPACKSSWBrr:\n  case X86::VPACKUSWBrr:\n  case X86::VPACKSSDWrr:\n  case X86::VPACKUSDWrr:\n  case X86::VPACKSSWBZ128rr:\n  case X86::VPACKUSWBZ128rr:\n  case X86::VPACKSSDWZ128rr:\n  case X86::VPACKUSDWZ128rr:\n  case X86::VPERM2F128rr:\n  case X86::VPERM2I128rr:\n  case X86::VSHUFF32X4Z256rri:\n  case X86::VSHUFF32X4Zrri:\n  case X86::VSHUFF64X2Z256rri:\n  case X86::VSHUFF64X2Zrri:\n  case X86::VSHUFI32X4Z256rri:\n  case X86::VSHUFI32X4Zrri:\n  case X86::VSHUFI64X2Z256rri:\n  case X86::VSHUFI64X2Zrri:\n  case X86::VPUNPCKHBWrr:\n  case X86::VPUNPCKLBWrr:\n  case X86::VPUNPCKHBWYrr:\n  case X86::VPUNPCKLBWYrr:\n  case X86::VPUNPCKHBWZ128rr:\n  case X86::VPUNPCKLBWZ128rr:\n  case X86::VPUNPCKHBWZ256rr:\n  case X86::VPUNPCKLBWZ256rr:\n  case X86::VPUNPCKHBWZrr:\n  case X86::VPUNPCKLBWZrr:\n  case X86::VPUNPCKHWDrr:\n  case X86::VPUNPCKLWDrr:\n  case X86::VPUNPCKHWDYrr:\n  case X86::VPUNPCKLWDYrr:\n  case X86::VPUNPCKHWDZ128rr:\n  case X86::VPUNPCKLWDZ128rr:\n  case X86::VPUNPCKHWDZ256rr:\n  case X86::VPUNPCKLWDZ256rr:\n  case X86::VPUNPCKHWDZrr:\n  case X86::VPUNPCKLWDZrr:\n  case X86::VPUNPCKHDQrr:\n  case X86::VPUNPCKLDQrr:\n  case X86::VPUNPCKHDQYrr:\n  case X86::VPUNPCKLDQYrr:\n  case X86::VPUNPCKHDQZ128rr:\n  case X86::VPUNPCKLDQZ128rr:\n  case X86::VPUNPCKHDQZ256rr:\n  case X86::VPUNPCKLDQZ256rr:\n  case X86::VPUNPCKHDQZrr:\n  case X86::VPUNPCKLDQZrr:\n  case X86::VPUNPCKHQDQrr:\n  case X86::VPUNPCKLQDQrr:\n  case X86::VPUNPCKHQDQYrr:\n  case X86::VPUNPCKLQDQYrr:\n  case X86::VPUNPCKHQDQZ128rr:\n  case X86::VPUNPCKLQDQZ128rr:\n  case X86::VPUNPCKHQDQZ256rr:\n  case X86::VPUNPCKLQDQZ256rr:\n  case X86::VPUNPCKHQDQZrr:\n  case X86::VPUNPCKLQDQZrr:\n    // These instructions are sometimes used with an undef first or second\n    // source. Return true here so BreakFalseDeps will assign this source to the\n    // same register as the first source to avoid a false dependency.\n    return (OpNum == 1 || OpNum == 2) && !ForLoadFold;\n\n  case X86::VCVTSI2SSrr:\n  case X86::VCVTSI2SSrm:\n  case X86::VCVTSI2SSrr_Int:\n  case X86::VCVTSI2SSrm_Int:\n  case X86::VCVTSI642SSrr:\n  case X86::VCVTSI642SSrm:\n  case X86::VCVTSI642SSrr_Int:\n  case X86::VCVTSI642SSrm_Int:\n  case X86::VCVTSI2SDrr:\n  case X86::VCVTSI2SDrm:\n  case X86::VCVTSI2SDrr_Int:\n  case X86::VCVTSI2SDrm_Int:\n  case X86::VCVTSI642SDrr:\n  case X86::VCVTSI642SDrm:\n  case X86::VCVTSI642SDrr_Int:\n  case X86::VCVTSI642SDrm_Int:\n  // AVX-512\n  case X86::VCVTSI2SSZrr:\n  case X86::VCVTSI2SSZrm:\n  case X86::VCVTSI2SSZrr_Int:\n  case X86::VCVTSI2SSZrrb_Int:\n  case X86::VCVTSI2SSZrm_Int:\n  case X86::VCVTSI642SSZrr:\n  case X86::VCVTSI642SSZrm:\n  case X86::VCVTSI642SSZrr_Int:\n  case X86::VCVTSI642SSZrrb_Int:\n  case X86::VCVTSI642SSZrm_Int:\n  case X86::VCVTSI2SDZrr:\n  case X86::VCVTSI2SDZrm:\n  case X86::VCVTSI2SDZrr_Int:\n  case X86::VCVTSI2SDZrm_Int:\n  case X86::VCVTSI642SDZrr:\n  case X86::VCVTSI642SDZrm:\n  case X86::VCVTSI642SDZrr_Int:\n  case X86::VCVTSI642SDZrrb_Int:\n  case X86::VCVTSI642SDZrm_Int:\n  case X86::VCVTUSI2SSZrr:\n  case X86::VCVTUSI2SSZrm:\n  case X86::VCVTUSI2SSZrr_Int:\n  case X86::VCVTUSI2SSZrrb_Int:\n  case X86::VCVTUSI2SSZrm_Int:\n  case X86::VCVTUSI642SSZrr:\n  case X86::VCVTUSI642SSZrm:\n  case X86::VCVTUSI642SSZrr_Int:\n  case X86::VCVTUSI642SSZrrb_Int:\n  case X86::VCVTUSI642SSZrm_Int:\n  case X86::VCVTUSI2SDZrr:\n  case X86::VCVTUSI2SDZrm:\n  case X86::VCVTUSI2SDZrr_Int:\n  case X86::VCVTUSI2SDZrm_Int:\n  case X86::VCVTUSI642SDZrr:\n  case X86::VCVTUSI642SDZrm:\n  case X86::VCVTUSI642SDZrr_Int:\n  case X86::VCVTUSI642SDZrrb_Int:\n  case X86::VCVTUSI642SDZrm_Int:\n    // Load folding won't effect the undef register update since the input is\n    // a GPR.\n    return OpNum == 1 && !ForLoadFold;\n  case X86::VCVTSD2SSrr:\n  case X86::VCVTSD2SSrm:\n  case X86::VCVTSD2SSrr_Int:\n  case X86::VCVTSD2SSrm_Int:\n  case X86::VCVTSS2SDrr:\n  case X86::VCVTSS2SDrm:\n  case X86::VCVTSS2SDrr_Int:\n  case X86::VCVTSS2SDrm_Int:\n  case X86::VRCPSSr:\n  case X86::VRCPSSr_Int:\n  case X86::VRCPSSm:\n  case X86::VRCPSSm_Int:\n  case X86::VROUNDSDr:\n  case X86::VROUNDSDm:\n  case X86::VROUNDSDr_Int:\n  case X86::VROUNDSDm_Int:\n  case X86::VROUNDSSr:\n  case X86::VROUNDSSm:\n  case X86::VROUNDSSr_Int:\n  case X86::VROUNDSSm_Int:\n  case X86::VRSQRTSSr:\n  case X86::VRSQRTSSr_Int:\n  case X86::VRSQRTSSm:\n  case X86::VRSQRTSSm_Int:\n  case X86::VSQRTSSr:\n  case X86::VSQRTSSr_Int:\n  case X86::VSQRTSSm:\n  case X86::VSQRTSSm_Int:\n  case X86::VSQRTSDr:\n  case X86::VSQRTSDr_Int:\n  case X86::VSQRTSDm:\n  case X86::VSQRTSDm_Int:\n  // AVX-512\n  case X86::VCVTSD2SSZrr:\n  case X86::VCVTSD2SSZrr_Int:\n  case X86::VCVTSD2SSZrrb_Int:\n  case X86::VCVTSD2SSZrm:\n  case X86::VCVTSD2SSZrm_Int:\n  case X86::VCVTSS2SDZrr:\n  case X86::VCVTSS2SDZrr_Int:\n  case X86::VCVTSS2SDZrrb_Int:\n  case X86::VCVTSS2SDZrm:\n  case X86::VCVTSS2SDZrm_Int:\n  case X86::VGETEXPSDZr:\n  case X86::VGETEXPSDZrb:\n  case X86::VGETEXPSDZm:\n  case X86::VGETEXPSSZr:\n  case X86::VGETEXPSSZrb:\n  case X86::VGETEXPSSZm:\n  case X86::VGETMANTSDZrri:\n  case X86::VGETMANTSDZrrib:\n  case X86::VGETMANTSDZrmi:\n  case X86::VGETMANTSSZrri:\n  case X86::VGETMANTSSZrrib:\n  case X86::VGETMANTSSZrmi:\n  case X86::VRNDSCALESDZr:\n  case X86::VRNDSCALESDZr_Int:\n  case X86::VRNDSCALESDZrb_Int:\n  case X86::VRNDSCALESDZm:\n  case X86::VRNDSCALESDZm_Int:\n  case X86::VRNDSCALESSZr:\n  case X86::VRNDSCALESSZr_Int:\n  case X86::VRNDSCALESSZrb_Int:\n  case X86::VRNDSCALESSZm:\n  case X86::VRNDSCALESSZm_Int:\n  case X86::VRCP14SDZrr:\n  case X86::VRCP14SDZrm:\n  case X86::VRCP14SSZrr:\n  case X86::VRCP14SSZrm:\n  case X86::VRCP28SDZr:\n  case X86::VRCP28SDZrb:\n  case X86::VRCP28SDZm:\n  case X86::VRCP28SSZr:\n  case X86::VRCP28SSZrb:\n  case X86::VRCP28SSZm:\n  case X86::VREDUCESSZrmi:\n  case X86::VREDUCESSZrri:\n  case X86::VREDUCESSZrrib:\n  case X86::VRSQRT14SDZrr:\n  case X86::VRSQRT14SDZrm:\n  case X86::VRSQRT14SSZrr:\n  case X86::VRSQRT14SSZrm:\n  case X86::VRSQRT28SDZr:\n  case X86::VRSQRT28SDZrb:\n  case X86::VRSQRT28SDZm:\n  case X86::VRSQRT28SSZr:\n  case X86::VRSQRT28SSZrb:\n  case X86::VRSQRT28SSZm:\n  case X86::VSQRTSSZr:\n  case X86::VSQRTSSZr_Int:\n  case X86::VSQRTSSZrb_Int:\n  case X86::VSQRTSSZm:\n  case X86::VSQRTSSZm_Int:\n  case X86::VSQRTSDZr:\n  case X86::VSQRTSDZr_Int:\n  case X86::VSQRTSDZrb_Int:\n  case X86::VSQRTSDZm:\n  case X86::VSQRTSDZm_Int:\n    return OpNum == 1;\n  case X86::VMOVSSZrrk:\n  case X86::VMOVSDZrrk:\n    return OpNum == 3 && !ForLoadFold;\n  case X86::VMOVSSZrrkz:\n  case X86::VMOVSDZrrkz:\n    return OpNum == 2 && !ForLoadFold;\n  }\n\n  return false;\n}\n\n/// Inform the BreakFalseDeps pass how many idle instructions we would like\n/// before certain undef register reads.\n///\n/// This catches the VCVTSI2SD family of instructions:\n///\n/// vcvtsi2sdq %rax, undef %xmm0, %xmm14\n///\n/// We should to be careful *not* to catch VXOR idioms which are presumably\n/// handled specially in the pipeline:\n///\n/// vxorps undef %xmm1, undef %xmm1, %xmm1\n///\n/// Like getPartialRegUpdateClearance, this makes a strong assumption that the\n/// high bits that are passed-through are not live.\nunsigned\nX86InstrInfo::getUndefRegClearance(const MachineInstr &MI, unsigned OpNum,\n                                   const TargetRegisterInfo *TRI) const {\n  const MachineOperand &MO = MI.getOperand(OpNum);\n  if (Register::isPhysicalRegister(MO.getReg()) &&\n      hasUndefRegUpdate(MI.getOpcode(), OpNum))\n    return UndefRegClearance;\n\n  return 0;\n}\n\nvoid X86InstrInfo::breakPartialRegDependency(\n    MachineInstr &MI, unsigned OpNum, const TargetRegisterInfo *TRI) const {\n  Register Reg = MI.getOperand(OpNum).getReg();\n  // If MI kills this register, the false dependence is already broken.\n  if (MI.killsRegister(Reg, TRI))\n    return;\n\n  if (X86::VR128RegClass.contains(Reg)) {\n    // These instructions are all floating point domain, so xorps is the best\n    // choice.\n    unsigned Opc = Subtarget.hasAVX() ? X86::VXORPSrr : X86::XORPSrr;\n    BuildMI(*MI.getParent(), MI, MI.getDebugLoc(), get(Opc), Reg)\n        .addReg(Reg, RegState::Undef)\n        .addReg(Reg, RegState::Undef);\n    MI.addRegisterKilled(Reg, TRI, true);\n  } else if (X86::VR256RegClass.contains(Reg)) {\n    // Use vxorps to clear the full ymm register.\n    // It wants to read and write the xmm sub-register.\n    Register XReg = TRI->getSubReg(Reg, X86::sub_xmm);\n    BuildMI(*MI.getParent(), MI, MI.getDebugLoc(), get(X86::VXORPSrr), XReg)\n        .addReg(XReg, RegState::Undef)\n        .addReg(XReg, RegState::Undef)\n        .addReg(Reg, RegState::ImplicitDefine);\n    MI.addRegisterKilled(Reg, TRI, true);\n  } else if (X86::GR64RegClass.contains(Reg)) {\n    // Using XOR32rr because it has shorter encoding and zeros up the upper bits\n    // as well.\n    Register XReg = TRI->getSubReg(Reg, X86::sub_32bit);\n    BuildMI(*MI.getParent(), MI, MI.getDebugLoc(), get(X86::XOR32rr), XReg)\n        .addReg(XReg, RegState::Undef)\n        .addReg(XReg, RegState::Undef)\n        .addReg(Reg, RegState::ImplicitDefine);\n    MI.addRegisterKilled(Reg, TRI, true);\n  } else if (X86::GR32RegClass.contains(Reg)) {\n    BuildMI(*MI.getParent(), MI, MI.getDebugLoc(), get(X86::XOR32rr), Reg)\n        .addReg(Reg, RegState::Undef)\n        .addReg(Reg, RegState::Undef);\n    MI.addRegisterKilled(Reg, TRI, true);\n  }\n}\n\nstatic void addOperands(MachineInstrBuilder &MIB, ArrayRef<MachineOperand> MOs,\n                        int PtrOffset = 0) {\n  unsigned NumAddrOps = MOs.size();\n\n  if (NumAddrOps < 4) {\n    // FrameIndex only - add an immediate offset (whether its zero or not).\n    for (unsigned i = 0; i != NumAddrOps; ++i)\n      MIB.add(MOs[i]);\n    addOffset(MIB, PtrOffset);\n  } else {\n    // General Memory Addressing - we need to add any offset to an existing\n    // offset.\n    assert(MOs.size() == 5 && \"Unexpected memory operand list length\");\n    for (unsigned i = 0; i != NumAddrOps; ++i) {\n      const MachineOperand &MO = MOs[i];\n      if (i == 3 && PtrOffset != 0) {\n        MIB.addDisp(MO, PtrOffset);\n      } else {\n        MIB.add(MO);\n      }\n    }\n  }\n}\n\nstatic void updateOperandRegConstraints(MachineFunction &MF,\n                                        MachineInstr &NewMI,\n                                        const TargetInstrInfo &TII) {\n  MachineRegisterInfo &MRI = MF.getRegInfo();\n  const TargetRegisterInfo &TRI = *MRI.getTargetRegisterInfo();\n\n  for (int Idx : llvm::seq<int>(0, NewMI.getNumOperands())) {\n    MachineOperand &MO = NewMI.getOperand(Idx);\n    // We only need to update constraints on virtual register operands.\n    if (!MO.isReg())\n      continue;\n    Register Reg = MO.getReg();\n    if (!Reg.isVirtual())\n      continue;\n\n    auto *NewRC = MRI.constrainRegClass(\n        Reg, TII.getRegClass(NewMI.getDesc(), Idx, &TRI, MF));\n    if (!NewRC) {\n      LLVM_DEBUG(\n          dbgs() << \"WARNING: Unable to update register constraint for operand \"\n                 << Idx << \" of instruction:\\n\";\n          NewMI.dump(); dbgs() << \"\\n\");\n    }\n  }\n}\n\nstatic MachineInstr *FuseTwoAddrInst(MachineFunction &MF, unsigned Opcode,\n                                     ArrayRef<MachineOperand> MOs,\n                                     MachineBasicBlock::iterator InsertPt,\n                                     MachineInstr &MI,\n                                     const TargetInstrInfo &TII) {\n  // Create the base instruction with the memory operand as the first part.\n  // Omit the implicit operands, something BuildMI can't do.\n  MachineInstr *NewMI =\n      MF.CreateMachineInstr(TII.get(Opcode), MI.getDebugLoc(), true);\n  MachineInstrBuilder MIB(MF, NewMI);\n  addOperands(MIB, MOs);\n\n  // Loop over the rest of the ri operands, converting them over.\n  unsigned NumOps = MI.getDesc().getNumOperands() - 2;\n  for (unsigned i = 0; i != NumOps; ++i) {\n    MachineOperand &MO = MI.getOperand(i + 2);\n    MIB.add(MO);\n  }\n  for (unsigned i = NumOps + 2, e = MI.getNumOperands(); i != e; ++i) {\n    MachineOperand &MO = MI.getOperand(i);\n    MIB.add(MO);\n  }\n\n  updateOperandRegConstraints(MF, *NewMI, TII);\n\n  MachineBasicBlock *MBB = InsertPt->getParent();\n  MBB->insert(InsertPt, NewMI);\n\n  return MIB;\n}\n\nstatic MachineInstr *FuseInst(MachineFunction &MF, unsigned Opcode,\n                              unsigned OpNo, ArrayRef<MachineOperand> MOs,\n                              MachineBasicBlock::iterator InsertPt,\n                              MachineInstr &MI, const TargetInstrInfo &TII,\n                              int PtrOffset = 0) {\n  // Omit the implicit operands, something BuildMI can't do.\n  MachineInstr *NewMI =\n      MF.CreateMachineInstr(TII.get(Opcode), MI.getDebugLoc(), true);\n  MachineInstrBuilder MIB(MF, NewMI);\n\n  for (unsigned i = 0, e = MI.getNumOperands(); i != e; ++i) {\n    MachineOperand &MO = MI.getOperand(i);\n    if (i == OpNo) {\n      assert(MO.isReg() && \"Expected to fold into reg operand!\");\n      addOperands(MIB, MOs, PtrOffset);\n    } else {\n      MIB.add(MO);\n    }\n  }\n\n  updateOperandRegConstraints(MF, *NewMI, TII);\n\n  // Copy the NoFPExcept flag from the instruction we're fusing.\n  if (MI.getFlag(MachineInstr::MIFlag::NoFPExcept))\n    NewMI->setFlag(MachineInstr::MIFlag::NoFPExcept);\n\n  MachineBasicBlock *MBB = InsertPt->getParent();\n  MBB->insert(InsertPt, NewMI);\n\n  return MIB;\n}\n\nstatic MachineInstr *MakeM0Inst(const TargetInstrInfo &TII, unsigned Opcode,\n                                ArrayRef<MachineOperand> MOs,\n                                MachineBasicBlock::iterator InsertPt,\n                                MachineInstr &MI) {\n  MachineInstrBuilder MIB = BuildMI(*InsertPt->getParent(), InsertPt,\n                                    MI.getDebugLoc(), TII.get(Opcode));\n  addOperands(MIB, MOs);\n  return MIB.addImm(0);\n}\n\nMachineInstr *X86InstrInfo::foldMemoryOperandCustom(\n    MachineFunction &MF, MachineInstr &MI, unsigned OpNum,\n    ArrayRef<MachineOperand> MOs, MachineBasicBlock::iterator InsertPt,\n    unsigned Size, Align Alignment) const {\n  switch (MI.getOpcode()) {\n  case X86::INSERTPSrr:\n  case X86::VINSERTPSrr:\n  case X86::VINSERTPSZrr:\n    // Attempt to convert the load of inserted vector into a fold load\n    // of a single float.\n    if (OpNum == 2) {\n      unsigned Imm = MI.getOperand(MI.getNumOperands() - 1).getImm();\n      unsigned ZMask = Imm & 15;\n      unsigned DstIdx = (Imm >> 4) & 3;\n      unsigned SrcIdx = (Imm >> 6) & 3;\n\n      const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n      const TargetRegisterClass *RC = getRegClass(MI.getDesc(), OpNum, &RI, MF);\n      unsigned RCSize = TRI.getRegSizeInBits(*RC) / 8;\n      if ((Size == 0 || Size >= 16) && RCSize >= 16 && Alignment >= Align(4)) {\n        int PtrOffset = SrcIdx * 4;\n        unsigned NewImm = (DstIdx << 4) | ZMask;\n        unsigned NewOpCode =\n            (MI.getOpcode() == X86::VINSERTPSZrr) ? X86::VINSERTPSZrm :\n            (MI.getOpcode() == X86::VINSERTPSrr)  ? X86::VINSERTPSrm  :\n                                                    X86::INSERTPSrm;\n        MachineInstr *NewMI =\n            FuseInst(MF, NewOpCode, OpNum, MOs, InsertPt, MI, *this, PtrOffset);\n        NewMI->getOperand(NewMI->getNumOperands() - 1).setImm(NewImm);\n        return NewMI;\n      }\n    }\n    break;\n  case X86::MOVHLPSrr:\n  case X86::VMOVHLPSrr:\n  case X86::VMOVHLPSZrr:\n    // Move the upper 64-bits of the second operand to the lower 64-bits.\n    // To fold the load, adjust the pointer to the upper and use (V)MOVLPS.\n    // TODO: In most cases AVX doesn't have a 8-byte alignment requirement.\n    if (OpNum == 2) {\n      const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n      const TargetRegisterClass *RC = getRegClass(MI.getDesc(), OpNum, &RI, MF);\n      unsigned RCSize = TRI.getRegSizeInBits(*RC) / 8;\n      if ((Size == 0 || Size >= 16) && RCSize >= 16 && Alignment >= Align(8)) {\n        unsigned NewOpCode =\n            (MI.getOpcode() == X86::VMOVHLPSZrr) ? X86::VMOVLPSZ128rm :\n            (MI.getOpcode() == X86::VMOVHLPSrr)  ? X86::VMOVLPSrm     :\n                                                   X86::MOVLPSrm;\n        MachineInstr *NewMI =\n            FuseInst(MF, NewOpCode, OpNum, MOs, InsertPt, MI, *this, 8);\n        return NewMI;\n      }\n    }\n    break;\n  case X86::UNPCKLPDrr:\n    // If we won't be able to fold this to the memory form of UNPCKL, use\n    // MOVHPD instead. Done as custom because we can't have this in the load\n    // table twice.\n    if (OpNum == 2) {\n      const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n      const TargetRegisterClass *RC = getRegClass(MI.getDesc(), OpNum, &RI, MF);\n      unsigned RCSize = TRI.getRegSizeInBits(*RC) / 8;\n      if ((Size == 0 || Size >= 16) && RCSize >= 16 && Alignment < Align(16)) {\n        MachineInstr *NewMI =\n            FuseInst(MF, X86::MOVHPDrm, OpNum, MOs, InsertPt, MI, *this);\n        return NewMI;\n      }\n    }\n    break;\n  }\n\n  return nullptr;\n}\n\nstatic bool shouldPreventUndefRegUpdateMemFold(MachineFunction &MF,\n                                               MachineInstr &MI) {\n  if (!hasUndefRegUpdate(MI.getOpcode(), 1, /*ForLoadFold*/true) ||\n      !MI.getOperand(1).isReg())\n    return false;\n\n  // The are two cases we need to handle depending on where in the pipeline\n  // the folding attempt is being made.\n  // -Register has the undef flag set.\n  // -Register is produced by the IMPLICIT_DEF instruction.\n\n  if (MI.getOperand(1).isUndef())\n    return true;\n\n  MachineRegisterInfo &RegInfo = MF.getRegInfo();\n  MachineInstr *VRegDef = RegInfo.getUniqueVRegDef(MI.getOperand(1).getReg());\n  return VRegDef && VRegDef->isImplicitDef();\n}\n\nMachineInstr *X86InstrInfo::foldMemoryOperandImpl(\n    MachineFunction &MF, MachineInstr &MI, unsigned OpNum,\n    ArrayRef<MachineOperand> MOs, MachineBasicBlock::iterator InsertPt,\n    unsigned Size, Align Alignment, bool AllowCommute) const {\n  bool isSlowTwoMemOps = Subtarget.slowTwoMemOps();\n  bool isTwoAddrFold = false;\n\n  // For CPUs that favor the register form of a call or push,\n  // do not fold loads into calls or pushes, unless optimizing for size\n  // aggressively.\n  if (isSlowTwoMemOps && !MF.getFunction().hasMinSize() &&\n      (MI.getOpcode() == X86::CALL32r || MI.getOpcode() == X86::CALL64r ||\n       MI.getOpcode() == X86::PUSH16r || MI.getOpcode() == X86::PUSH32r ||\n       MI.getOpcode() == X86::PUSH64r))\n    return nullptr;\n\n  // Avoid partial and undef register update stalls unless optimizing for size.\n  if (!MF.getFunction().hasOptSize() &&\n      (hasPartialRegUpdate(MI.getOpcode(), Subtarget, /*ForLoadFold*/true) ||\n       shouldPreventUndefRegUpdateMemFold(MF, MI)))\n    return nullptr;\n\n  unsigned NumOps = MI.getDesc().getNumOperands();\n  bool isTwoAddr =\n      NumOps > 1 && MI.getDesc().getOperandConstraint(1, MCOI::TIED_TO) != -1;\n\n  // FIXME: AsmPrinter doesn't know how to handle\n  // X86II::MO_GOT_ABSOLUTE_ADDRESS after folding.\n  if (MI.getOpcode() == X86::ADD32ri &&\n      MI.getOperand(2).getTargetFlags() == X86II::MO_GOT_ABSOLUTE_ADDRESS)\n    return nullptr;\n\n  // GOTTPOFF relocation loads can only be folded into add instructions.\n  // FIXME: Need to exclude other relocations that only support specific\n  // instructions.\n  if (MOs.size() == X86::AddrNumOperands &&\n      MOs[X86::AddrDisp].getTargetFlags() == X86II::MO_GOTTPOFF &&\n      MI.getOpcode() != X86::ADD64rr)\n    return nullptr;\n\n  MachineInstr *NewMI = nullptr;\n\n  // Attempt to fold any custom cases we have.\n  if (MachineInstr *CustomMI = foldMemoryOperandCustom(\n          MF, MI, OpNum, MOs, InsertPt, Size, Alignment))\n    return CustomMI;\n\n  const X86MemoryFoldTableEntry *I = nullptr;\n\n  // Folding a memory location into the two-address part of a two-address\n  // instruction is different than folding it other places.  It requires\n  // replacing the *two* registers with the memory location.\n  if (isTwoAddr && NumOps >= 2 && OpNum < 2 && MI.getOperand(0).isReg() &&\n      MI.getOperand(1).isReg() &&\n      MI.getOperand(0).getReg() == MI.getOperand(1).getReg()) {\n    I = lookupTwoAddrFoldTable(MI.getOpcode());\n    isTwoAddrFold = true;\n  } else {\n    if (OpNum == 0) {\n      if (MI.getOpcode() == X86::MOV32r0) {\n        NewMI = MakeM0Inst(*this, X86::MOV32mi, MOs, InsertPt, MI);\n        if (NewMI)\n          return NewMI;\n      }\n    }\n\n    I = lookupFoldTable(MI.getOpcode(), OpNum);\n  }\n\n  if (I != nullptr) {\n    unsigned Opcode = I->DstOp;\n    bool FoldedLoad =\n        isTwoAddrFold || (OpNum == 0 && I->Flags & TB_FOLDED_LOAD) || OpNum > 0;\n    bool FoldedStore =\n        isTwoAddrFold || (OpNum == 0 && I->Flags & TB_FOLDED_STORE);\n    MaybeAlign MinAlign =\n        decodeMaybeAlign((I->Flags & TB_ALIGN_MASK) >> TB_ALIGN_SHIFT);\n    if (MinAlign && Alignment < *MinAlign)\n      return nullptr;\n    bool NarrowToMOV32rm = false;\n    if (Size) {\n      const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n      const TargetRegisterClass *RC = getRegClass(MI.getDesc(), OpNum,\n                                                  &RI, MF);\n      unsigned RCSize = TRI.getRegSizeInBits(*RC) / 8;\n      // Check if it's safe to fold the load. If the size of the object is\n      // narrower than the load width, then it's not.\n      // FIXME: Allow scalar intrinsic instructions like ADDSSrm_Int.\n      if (FoldedLoad && Size < RCSize) {\n        // If this is a 64-bit load, but the spill slot is 32, then we can do\n        // a 32-bit load which is implicitly zero-extended. This likely is\n        // due to live interval analysis remat'ing a load from stack slot.\n        if (Opcode != X86::MOV64rm || RCSize != 8 || Size != 4)\n          return nullptr;\n        if (MI.getOperand(0).getSubReg() || MI.getOperand(1).getSubReg())\n          return nullptr;\n        Opcode = X86::MOV32rm;\n        NarrowToMOV32rm = true;\n      }\n      // For stores, make sure the size of the object is equal to the size of\n      // the store. If the object is larger, the extra bits would be garbage. If\n      // the object is smaller we might overwrite another object or fault.\n      if (FoldedStore && Size != RCSize)\n        return nullptr;\n    }\n\n    if (isTwoAddrFold)\n      NewMI = FuseTwoAddrInst(MF, Opcode, MOs, InsertPt, MI, *this);\n    else\n      NewMI = FuseInst(MF, Opcode, OpNum, MOs, InsertPt, MI, *this);\n\n    if (NarrowToMOV32rm) {\n      // If this is the special case where we use a MOV32rm to load a 32-bit\n      // value and zero-extend the top bits. Change the destination register\n      // to a 32-bit one.\n      Register DstReg = NewMI->getOperand(0).getReg();\n      if (DstReg.isPhysical())\n        NewMI->getOperand(0).setReg(RI.getSubReg(DstReg, X86::sub_32bit));\n      else\n        NewMI->getOperand(0).setSubReg(X86::sub_32bit);\n    }\n    return NewMI;\n  }\n\n  // If the instruction and target operand are commutable, commute the\n  // instruction and try again.\n  if (AllowCommute) {\n    unsigned CommuteOpIdx1 = OpNum, CommuteOpIdx2 = CommuteAnyOperandIndex;\n    if (findCommutedOpIndices(MI, CommuteOpIdx1, CommuteOpIdx2)) {\n      bool HasDef = MI.getDesc().getNumDefs();\n      Register Reg0 = HasDef ? MI.getOperand(0).getReg() : Register();\n      Register Reg1 = MI.getOperand(CommuteOpIdx1).getReg();\n      Register Reg2 = MI.getOperand(CommuteOpIdx2).getReg();\n      bool Tied1 =\n          0 == MI.getDesc().getOperandConstraint(CommuteOpIdx1, MCOI::TIED_TO);\n      bool Tied2 =\n          0 == MI.getDesc().getOperandConstraint(CommuteOpIdx2, MCOI::TIED_TO);\n\n      // If either of the commutable operands are tied to the destination\n      // then we can not commute + fold.\n      if ((HasDef && Reg0 == Reg1 && Tied1) ||\n          (HasDef && Reg0 == Reg2 && Tied2))\n        return nullptr;\n\n      MachineInstr *CommutedMI =\n          commuteInstruction(MI, false, CommuteOpIdx1, CommuteOpIdx2);\n      if (!CommutedMI) {\n        // Unable to commute.\n        return nullptr;\n      }\n      if (CommutedMI != &MI) {\n        // New instruction. We can't fold from this.\n        CommutedMI->eraseFromParent();\n        return nullptr;\n      }\n\n      // Attempt to fold with the commuted version of the instruction.\n      NewMI = foldMemoryOperandImpl(MF, MI, CommuteOpIdx2, MOs, InsertPt, Size,\n                                    Alignment, /*AllowCommute=*/false);\n      if (NewMI)\n        return NewMI;\n\n      // Folding failed again - undo the commute before returning.\n      MachineInstr *UncommutedMI =\n          commuteInstruction(MI, false, CommuteOpIdx1, CommuteOpIdx2);\n      if (!UncommutedMI) {\n        // Unable to commute.\n        return nullptr;\n      }\n      if (UncommutedMI != &MI) {\n        // New instruction. It doesn't need to be kept.\n        UncommutedMI->eraseFromParent();\n        return nullptr;\n      }\n\n      // Return here to prevent duplicate fuse failure report.\n      return nullptr;\n    }\n  }\n\n  // No fusion\n  if (PrintFailedFusing && !MI.isCopy())\n    dbgs() << \"We failed to fuse operand \" << OpNum << \" in \" << MI;\n  return nullptr;\n}\n\nMachineInstr *\nX86InstrInfo::foldMemoryOperandImpl(MachineFunction &MF, MachineInstr &MI,\n                                    ArrayRef<unsigned> Ops,\n                                    MachineBasicBlock::iterator InsertPt,\n                                    int FrameIndex, LiveIntervals *LIS,\n                                    VirtRegMap *VRM) const {\n  // Check switch flag\n  if (NoFusing)\n    return nullptr;\n\n  // Avoid partial and undef register update stalls unless optimizing for size.\n  if (!MF.getFunction().hasOptSize() &&\n      (hasPartialRegUpdate(MI.getOpcode(), Subtarget, /*ForLoadFold*/true) ||\n       shouldPreventUndefRegUpdateMemFold(MF, MI)))\n    return nullptr;\n\n  // Don't fold subreg spills, or reloads that use a high subreg.\n  for (auto Op : Ops) {\n    MachineOperand &MO = MI.getOperand(Op);\n    auto SubReg = MO.getSubReg();\n    if (SubReg && (MO.isDef() || SubReg == X86::sub_8bit_hi))\n      return nullptr;\n  }\n\n  const MachineFrameInfo &MFI = MF.getFrameInfo();\n  unsigned Size = MFI.getObjectSize(FrameIndex);\n  Align Alignment = MFI.getObjectAlign(FrameIndex);\n  // If the function stack isn't realigned we don't want to fold instructions\n  // that need increased alignment.\n  if (!RI.needsStackRealignment(MF))\n    Alignment =\n        std::min(Alignment, Subtarget.getFrameLowering()->getStackAlign());\n  if (Ops.size() == 2 && Ops[0] == 0 && Ops[1] == 1) {\n    unsigned NewOpc = 0;\n    unsigned RCSize = 0;\n    switch (MI.getOpcode()) {\n    default: return nullptr;\n    case X86::TEST8rr:  NewOpc = X86::CMP8ri; RCSize = 1; break;\n    case X86::TEST16rr: NewOpc = X86::CMP16ri8; RCSize = 2; break;\n    case X86::TEST32rr: NewOpc = X86::CMP32ri8; RCSize = 4; break;\n    case X86::TEST64rr: NewOpc = X86::CMP64ri8; RCSize = 8; break;\n    }\n    // Check if it's safe to fold the load. If the size of the object is\n    // narrower than the load width, then it's not.\n    if (Size < RCSize)\n      return nullptr;\n    // Change to CMPXXri r, 0 first.\n    MI.setDesc(get(NewOpc));\n    MI.getOperand(1).ChangeToImmediate(0);\n  } else if (Ops.size() != 1)\n    return nullptr;\n\n  return foldMemoryOperandImpl(MF, MI, Ops[0],\n                               MachineOperand::CreateFI(FrameIndex), InsertPt,\n                               Size, Alignment, /*AllowCommute=*/true);\n}\n\n/// Check if \\p LoadMI is a partial register load that we can't fold into \\p MI\n/// because the latter uses contents that wouldn't be defined in the folded\n/// version.  For instance, this transformation isn't legal:\n///   movss (%rdi), %xmm0\n///   addps %xmm0, %xmm0\n/// ->\n///   addps (%rdi), %xmm0\n///\n/// But this one is:\n///   movss (%rdi), %xmm0\n///   addss %xmm0, %xmm0\n/// ->\n///   addss (%rdi), %xmm0\n///\nstatic bool isNonFoldablePartialRegisterLoad(const MachineInstr &LoadMI,\n                                             const MachineInstr &UserMI,\n                                             const MachineFunction &MF) {\n  unsigned Opc = LoadMI.getOpcode();\n  unsigned UserOpc = UserMI.getOpcode();\n  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n  const TargetRegisterClass *RC =\n      MF.getRegInfo().getRegClass(LoadMI.getOperand(0).getReg());\n  unsigned RegSize = TRI.getRegSizeInBits(*RC);\n\n  if ((Opc == X86::MOVSSrm || Opc == X86::VMOVSSrm || Opc == X86::VMOVSSZrm ||\n       Opc == X86::MOVSSrm_alt || Opc == X86::VMOVSSrm_alt ||\n       Opc == X86::VMOVSSZrm_alt) &&\n      RegSize > 32) {\n    // These instructions only load 32 bits, we can't fold them if the\n    // destination register is wider than 32 bits (4 bytes), and its user\n    // instruction isn't scalar (SS).\n    switch (UserOpc) {\n    case X86::CVTSS2SDrr_Int:\n    case X86::VCVTSS2SDrr_Int:\n    case X86::VCVTSS2SDZrr_Int:\n    case X86::VCVTSS2SDZrr_Intk:\n    case X86::VCVTSS2SDZrr_Intkz:\n    case X86::CVTSS2SIrr_Int:     case X86::CVTSS2SI64rr_Int:\n    case X86::VCVTSS2SIrr_Int:    case X86::VCVTSS2SI64rr_Int:\n    case X86::VCVTSS2SIZrr_Int:   case X86::VCVTSS2SI64Zrr_Int:\n    case X86::CVTTSS2SIrr_Int:    case X86::CVTTSS2SI64rr_Int:\n    case X86::VCVTTSS2SIrr_Int:   case X86::VCVTTSS2SI64rr_Int:\n    case X86::VCVTTSS2SIZrr_Int:  case X86::VCVTTSS2SI64Zrr_Int:\n    case X86::VCVTSS2USIZrr_Int:  case X86::VCVTSS2USI64Zrr_Int:\n    case X86::VCVTTSS2USIZrr_Int: case X86::VCVTTSS2USI64Zrr_Int:\n    case X86::RCPSSr_Int:   case X86::VRCPSSr_Int:\n    case X86::RSQRTSSr_Int: case X86::VRSQRTSSr_Int:\n    case X86::ROUNDSSr_Int: case X86::VROUNDSSr_Int:\n    case X86::COMISSrr_Int: case X86::VCOMISSrr_Int: case X86::VCOMISSZrr_Int:\n    case X86::UCOMISSrr_Int:case X86::VUCOMISSrr_Int:case X86::VUCOMISSZrr_Int:\n    case X86::ADDSSrr_Int: case X86::VADDSSrr_Int: case X86::VADDSSZrr_Int:\n    case X86::CMPSSrr_Int: case X86::VCMPSSrr_Int: case X86::VCMPSSZrr_Int:\n    case X86::DIVSSrr_Int: case X86::VDIVSSrr_Int: case X86::VDIVSSZrr_Int:\n    case X86::MAXSSrr_Int: case X86::VMAXSSrr_Int: case X86::VMAXSSZrr_Int:\n    case X86::MINSSrr_Int: case X86::VMINSSrr_Int: case X86::VMINSSZrr_Int:\n    case X86::MULSSrr_Int: case X86::VMULSSrr_Int: case X86::VMULSSZrr_Int:\n    case X86::SQRTSSr_Int: case X86::VSQRTSSr_Int: case X86::VSQRTSSZr_Int:\n    case X86::SUBSSrr_Int: case X86::VSUBSSrr_Int: case X86::VSUBSSZrr_Int:\n    case X86::VADDSSZrr_Intk: case X86::VADDSSZrr_Intkz:\n    case X86::VCMPSSZrr_Intk:\n    case X86::VDIVSSZrr_Intk: case X86::VDIVSSZrr_Intkz:\n    case X86::VMAXSSZrr_Intk: case X86::VMAXSSZrr_Intkz:\n    case X86::VMINSSZrr_Intk: case X86::VMINSSZrr_Intkz:\n    case X86::VMULSSZrr_Intk: case X86::VMULSSZrr_Intkz:\n    case X86::VSQRTSSZr_Intk: case X86::VSQRTSSZr_Intkz:\n    case X86::VSUBSSZrr_Intk: case X86::VSUBSSZrr_Intkz:\n    case X86::VFMADDSS4rr_Int:   case X86::VFNMADDSS4rr_Int:\n    case X86::VFMSUBSS4rr_Int:   case X86::VFNMSUBSS4rr_Int:\n    case X86::VFMADD132SSr_Int:  case X86::VFNMADD132SSr_Int:\n    case X86::VFMADD213SSr_Int:  case X86::VFNMADD213SSr_Int:\n    case X86::VFMADD231SSr_Int:  case X86::VFNMADD231SSr_Int:\n    case X86::VFMSUB132SSr_Int:  case X86::VFNMSUB132SSr_Int:\n    case X86::VFMSUB213SSr_Int:  case X86::VFNMSUB213SSr_Int:\n    case X86::VFMSUB231SSr_Int:  case X86::VFNMSUB231SSr_Int:\n    case X86::VFMADD132SSZr_Int: case X86::VFNMADD132SSZr_Int:\n    case X86::VFMADD213SSZr_Int: case X86::VFNMADD213SSZr_Int:\n    case X86::VFMADD231SSZr_Int: case X86::VFNMADD231SSZr_Int:\n    case X86::VFMSUB132SSZr_Int: case X86::VFNMSUB132SSZr_Int:\n    case X86::VFMSUB213SSZr_Int: case X86::VFNMSUB213SSZr_Int:\n    case X86::VFMSUB231SSZr_Int: case X86::VFNMSUB231SSZr_Int:\n    case X86::VFMADD132SSZr_Intk: case X86::VFNMADD132SSZr_Intk:\n    case X86::VFMADD213SSZr_Intk: case X86::VFNMADD213SSZr_Intk:\n    case X86::VFMADD231SSZr_Intk: case X86::VFNMADD231SSZr_Intk:\n    case X86::VFMSUB132SSZr_Intk: case X86::VFNMSUB132SSZr_Intk:\n    case X86::VFMSUB213SSZr_Intk: case X86::VFNMSUB213SSZr_Intk:\n    case X86::VFMSUB231SSZr_Intk: case X86::VFNMSUB231SSZr_Intk:\n    case X86::VFMADD132SSZr_Intkz: case X86::VFNMADD132SSZr_Intkz:\n    case X86::VFMADD213SSZr_Intkz: case X86::VFNMADD213SSZr_Intkz:\n    case X86::VFMADD231SSZr_Intkz: case X86::VFNMADD231SSZr_Intkz:\n    case X86::VFMSUB132SSZr_Intkz: case X86::VFNMSUB132SSZr_Intkz:\n    case X86::VFMSUB213SSZr_Intkz: case X86::VFNMSUB213SSZr_Intkz:\n    case X86::VFMSUB231SSZr_Intkz: case X86::VFNMSUB231SSZr_Intkz:\n    case X86::VFIXUPIMMSSZrri:\n    case X86::VFIXUPIMMSSZrrik:\n    case X86::VFIXUPIMMSSZrrikz:\n    case X86::VFPCLASSSSZrr:\n    case X86::VFPCLASSSSZrrk:\n    case X86::VGETEXPSSZr:\n    case X86::VGETEXPSSZrk:\n    case X86::VGETEXPSSZrkz:\n    case X86::VGETMANTSSZrri:\n    case X86::VGETMANTSSZrrik:\n    case X86::VGETMANTSSZrrikz:\n    case X86::VRANGESSZrri:\n    case X86::VRANGESSZrrik:\n    case X86::VRANGESSZrrikz:\n    case X86::VRCP14SSZrr:\n    case X86::VRCP14SSZrrk:\n    case X86::VRCP14SSZrrkz:\n    case X86::VRCP28SSZr:\n    case X86::VRCP28SSZrk:\n    case X86::VRCP28SSZrkz:\n    case X86::VREDUCESSZrri:\n    case X86::VREDUCESSZrrik:\n    case X86::VREDUCESSZrrikz:\n    case X86::VRNDSCALESSZr_Int:\n    case X86::VRNDSCALESSZr_Intk:\n    case X86::VRNDSCALESSZr_Intkz:\n    case X86::VRSQRT14SSZrr:\n    case X86::VRSQRT14SSZrrk:\n    case X86::VRSQRT14SSZrrkz:\n    case X86::VRSQRT28SSZr:\n    case X86::VRSQRT28SSZrk:\n    case X86::VRSQRT28SSZrkz:\n    case X86::VSCALEFSSZrr:\n    case X86::VSCALEFSSZrrk:\n    case X86::VSCALEFSSZrrkz:\n      return false;\n    default:\n      return true;\n    }\n  }\n\n  if ((Opc == X86::MOVSDrm || Opc == X86::VMOVSDrm || Opc == X86::VMOVSDZrm ||\n       Opc == X86::MOVSDrm_alt || Opc == X86::VMOVSDrm_alt ||\n       Opc == X86::VMOVSDZrm_alt) &&\n      RegSize > 64) {\n    // These instructions only load 64 bits, we can't fold them if the\n    // destination register is wider than 64 bits (8 bytes), and its user\n    // instruction isn't scalar (SD).\n    switch (UserOpc) {\n    case X86::CVTSD2SSrr_Int:\n    case X86::VCVTSD2SSrr_Int:\n    case X86::VCVTSD2SSZrr_Int:\n    case X86::VCVTSD2SSZrr_Intk:\n    case X86::VCVTSD2SSZrr_Intkz:\n    case X86::CVTSD2SIrr_Int:     case X86::CVTSD2SI64rr_Int:\n    case X86::VCVTSD2SIrr_Int:    case X86::VCVTSD2SI64rr_Int:\n    case X86::VCVTSD2SIZrr_Int:   case X86::VCVTSD2SI64Zrr_Int:\n    case X86::CVTTSD2SIrr_Int:    case X86::CVTTSD2SI64rr_Int:\n    case X86::VCVTTSD2SIrr_Int:   case X86::VCVTTSD2SI64rr_Int:\n    case X86::VCVTTSD2SIZrr_Int:  case X86::VCVTTSD2SI64Zrr_Int:\n    case X86::VCVTSD2USIZrr_Int:  case X86::VCVTSD2USI64Zrr_Int:\n    case X86::VCVTTSD2USIZrr_Int: case X86::VCVTTSD2USI64Zrr_Int:\n    case X86::ROUNDSDr_Int: case X86::VROUNDSDr_Int:\n    case X86::COMISDrr_Int: case X86::VCOMISDrr_Int: case X86::VCOMISDZrr_Int:\n    case X86::UCOMISDrr_Int:case X86::VUCOMISDrr_Int:case X86::VUCOMISDZrr_Int:\n    case X86::ADDSDrr_Int: case X86::VADDSDrr_Int: case X86::VADDSDZrr_Int:\n    case X86::CMPSDrr_Int: case X86::VCMPSDrr_Int: case X86::VCMPSDZrr_Int:\n    case X86::DIVSDrr_Int: case X86::VDIVSDrr_Int: case X86::VDIVSDZrr_Int:\n    case X86::MAXSDrr_Int: case X86::VMAXSDrr_Int: case X86::VMAXSDZrr_Int:\n    case X86::MINSDrr_Int: case X86::VMINSDrr_Int: case X86::VMINSDZrr_Int:\n    case X86::MULSDrr_Int: case X86::VMULSDrr_Int: case X86::VMULSDZrr_Int:\n    case X86::SQRTSDr_Int: case X86::VSQRTSDr_Int: case X86::VSQRTSDZr_Int:\n    case X86::SUBSDrr_Int: case X86::VSUBSDrr_Int: case X86::VSUBSDZrr_Int:\n    case X86::VADDSDZrr_Intk: case X86::VADDSDZrr_Intkz:\n    case X86::VCMPSDZrr_Intk:\n    case X86::VDIVSDZrr_Intk: case X86::VDIVSDZrr_Intkz:\n    case X86::VMAXSDZrr_Intk: case X86::VMAXSDZrr_Intkz:\n    case X86::VMINSDZrr_Intk: case X86::VMINSDZrr_Intkz:\n    case X86::VMULSDZrr_Intk: case X86::VMULSDZrr_Intkz:\n    case X86::VSQRTSDZr_Intk: case X86::VSQRTSDZr_Intkz:\n    case X86::VSUBSDZrr_Intk: case X86::VSUBSDZrr_Intkz:\n    case X86::VFMADDSD4rr_Int:   case X86::VFNMADDSD4rr_Int:\n    case X86::VFMSUBSD4rr_Int:   case X86::VFNMSUBSD4rr_Int:\n    case X86::VFMADD132SDr_Int:  case X86::VFNMADD132SDr_Int:\n    case X86::VFMADD213SDr_Int:  case X86::VFNMADD213SDr_Int:\n    case X86::VFMADD231SDr_Int:  case X86::VFNMADD231SDr_Int:\n    case X86::VFMSUB132SDr_Int:  case X86::VFNMSUB132SDr_Int:\n    case X86::VFMSUB213SDr_Int:  case X86::VFNMSUB213SDr_Int:\n    case X86::VFMSUB231SDr_Int:  case X86::VFNMSUB231SDr_Int:\n    case X86::VFMADD132SDZr_Int: case X86::VFNMADD132SDZr_Int:\n    case X86::VFMADD213SDZr_Int: case X86::VFNMADD213SDZr_Int:\n    case X86::VFMADD231SDZr_Int: case X86::VFNMADD231SDZr_Int:\n    case X86::VFMSUB132SDZr_Int: case X86::VFNMSUB132SDZr_Int:\n    case X86::VFMSUB213SDZr_Int: case X86::VFNMSUB213SDZr_Int:\n    case X86::VFMSUB231SDZr_Int: case X86::VFNMSUB231SDZr_Int:\n    case X86::VFMADD132SDZr_Intk: case X86::VFNMADD132SDZr_Intk:\n    case X86::VFMADD213SDZr_Intk: case X86::VFNMADD213SDZr_Intk:\n    case X86::VFMADD231SDZr_Intk: case X86::VFNMADD231SDZr_Intk:\n    case X86::VFMSUB132SDZr_Intk: case X86::VFNMSUB132SDZr_Intk:\n    case X86::VFMSUB213SDZr_Intk: case X86::VFNMSUB213SDZr_Intk:\n    case X86::VFMSUB231SDZr_Intk: case X86::VFNMSUB231SDZr_Intk:\n    case X86::VFMADD132SDZr_Intkz: case X86::VFNMADD132SDZr_Intkz:\n    case X86::VFMADD213SDZr_Intkz: case X86::VFNMADD213SDZr_Intkz:\n    case X86::VFMADD231SDZr_Intkz: case X86::VFNMADD231SDZr_Intkz:\n    case X86::VFMSUB132SDZr_Intkz: case X86::VFNMSUB132SDZr_Intkz:\n    case X86::VFMSUB213SDZr_Intkz: case X86::VFNMSUB213SDZr_Intkz:\n    case X86::VFMSUB231SDZr_Intkz: case X86::VFNMSUB231SDZr_Intkz:\n    case X86::VFIXUPIMMSDZrri:\n    case X86::VFIXUPIMMSDZrrik:\n    case X86::VFIXUPIMMSDZrrikz:\n    case X86::VFPCLASSSDZrr:\n    case X86::VFPCLASSSDZrrk:\n    case X86::VGETEXPSDZr:\n    case X86::VGETEXPSDZrk:\n    case X86::VGETEXPSDZrkz:\n    case X86::VGETMANTSDZrri:\n    case X86::VGETMANTSDZrrik:\n    case X86::VGETMANTSDZrrikz:\n    case X86::VRANGESDZrri:\n    case X86::VRANGESDZrrik:\n    case X86::VRANGESDZrrikz:\n    case X86::VRCP14SDZrr:\n    case X86::VRCP14SDZrrk:\n    case X86::VRCP14SDZrrkz:\n    case X86::VRCP28SDZr:\n    case X86::VRCP28SDZrk:\n    case X86::VRCP28SDZrkz:\n    case X86::VREDUCESDZrri:\n    case X86::VREDUCESDZrrik:\n    case X86::VREDUCESDZrrikz:\n    case X86::VRNDSCALESDZr_Int:\n    case X86::VRNDSCALESDZr_Intk:\n    case X86::VRNDSCALESDZr_Intkz:\n    case X86::VRSQRT14SDZrr:\n    case X86::VRSQRT14SDZrrk:\n    case X86::VRSQRT14SDZrrkz:\n    case X86::VRSQRT28SDZr:\n    case X86::VRSQRT28SDZrk:\n    case X86::VRSQRT28SDZrkz:\n    case X86::VSCALEFSDZrr:\n    case X86::VSCALEFSDZrrk:\n    case X86::VSCALEFSDZrrkz:\n      return false;\n    default:\n      return true;\n    }\n  }\n\n  return false;\n}\n\nMachineInstr *X86InstrInfo::foldMemoryOperandImpl(\n    MachineFunction &MF, MachineInstr &MI, ArrayRef<unsigned> Ops,\n    MachineBasicBlock::iterator InsertPt, MachineInstr &LoadMI,\n    LiveIntervals *LIS) const {\n\n  // TODO: Support the case where LoadMI loads a wide register, but MI\n  // only uses a subreg.\n  for (auto Op : Ops) {\n    if (MI.getOperand(Op).getSubReg())\n      return nullptr;\n  }\n\n  // If loading from a FrameIndex, fold directly from the FrameIndex.\n  unsigned NumOps = LoadMI.getDesc().getNumOperands();\n  int FrameIndex;\n  if (isLoadFromStackSlot(LoadMI, FrameIndex)) {\n    if (isNonFoldablePartialRegisterLoad(LoadMI, MI, MF))\n      return nullptr;\n    return foldMemoryOperandImpl(MF, MI, Ops, InsertPt, FrameIndex, LIS);\n  }\n\n  // Check switch flag\n  if (NoFusing) return nullptr;\n\n  // Avoid partial and undef register update stalls unless optimizing for size.\n  if (!MF.getFunction().hasOptSize() &&\n      (hasPartialRegUpdate(MI.getOpcode(), Subtarget, /*ForLoadFold*/true) ||\n       shouldPreventUndefRegUpdateMemFold(MF, MI)))\n    return nullptr;\n\n  // Determine the alignment of the load.\n  Align Alignment;\n  if (LoadMI.hasOneMemOperand())\n    Alignment = (*LoadMI.memoperands_begin())->getAlign();\n  else\n    switch (LoadMI.getOpcode()) {\n    case X86::AVX512_512_SET0:\n    case X86::AVX512_512_SETALLONES:\n      Alignment = Align(64);\n      break;\n    case X86::AVX2_SETALLONES:\n    case X86::AVX1_SETALLONES:\n    case X86::AVX_SET0:\n    case X86::AVX512_256_SET0:\n      Alignment = Align(32);\n      break;\n    case X86::V_SET0:\n    case X86::V_SETALLONES:\n    case X86::AVX512_128_SET0:\n    case X86::FsFLD0F128:\n    case X86::AVX512_FsFLD0F128:\n      Alignment = Align(16);\n      break;\n    case X86::MMX_SET0:\n    case X86::FsFLD0SD:\n    case X86::AVX512_FsFLD0SD:\n      Alignment = Align(8);\n      break;\n    case X86::FsFLD0SS:\n    case X86::AVX512_FsFLD0SS:\n      Alignment = Align(4);\n      break;\n    default:\n      return nullptr;\n    }\n  if (Ops.size() == 2 && Ops[0] == 0 && Ops[1] == 1) {\n    unsigned NewOpc = 0;\n    switch (MI.getOpcode()) {\n    default: return nullptr;\n    case X86::TEST8rr:  NewOpc = X86::CMP8ri; break;\n    case X86::TEST16rr: NewOpc = X86::CMP16ri8; break;\n    case X86::TEST32rr: NewOpc = X86::CMP32ri8; break;\n    case X86::TEST64rr: NewOpc = X86::CMP64ri8; break;\n    }\n    // Change to CMPXXri r, 0 first.\n    MI.setDesc(get(NewOpc));\n    MI.getOperand(1).ChangeToImmediate(0);\n  } else if (Ops.size() != 1)\n    return nullptr;\n\n  // Make sure the subregisters match.\n  // Otherwise we risk changing the size of the load.\n  if (LoadMI.getOperand(0).getSubReg() != MI.getOperand(Ops[0]).getSubReg())\n    return nullptr;\n\n  SmallVector<MachineOperand,X86::AddrNumOperands> MOs;\n  switch (LoadMI.getOpcode()) {\n  case X86::MMX_SET0:\n  case X86::V_SET0:\n  case X86::V_SETALLONES:\n  case X86::AVX2_SETALLONES:\n  case X86::AVX1_SETALLONES:\n  case X86::AVX_SET0:\n  case X86::AVX512_128_SET0:\n  case X86::AVX512_256_SET0:\n  case X86::AVX512_512_SET0:\n  case X86::AVX512_512_SETALLONES:\n  case X86::FsFLD0SD:\n  case X86::AVX512_FsFLD0SD:\n  case X86::FsFLD0SS:\n  case X86::AVX512_FsFLD0SS:\n  case X86::FsFLD0F128:\n  case X86::AVX512_FsFLD0F128: {\n    // Folding a V_SET0 or V_SETALLONES as a load, to ease register pressure.\n    // Create a constant-pool entry and operands to load from it.\n\n    // Medium and large mode can't fold loads this way.\n    if (MF.getTarget().getCodeModel() != CodeModel::Small &&\n        MF.getTarget().getCodeModel() != CodeModel::Kernel)\n      return nullptr;\n\n    // x86-32 PIC requires a PIC base register for constant pools.\n    unsigned PICBase = 0;\n    if (MF.getTarget().isPositionIndependent()) {\n      if (Subtarget.is64Bit())\n        PICBase = X86::RIP;\n      else\n        // FIXME: PICBase = getGlobalBaseReg(&MF);\n        // This doesn't work for several reasons.\n        // 1. GlobalBaseReg may have been spilled.\n        // 2. It may not be live at MI.\n        return nullptr;\n    }\n\n    // Create a constant-pool entry.\n    MachineConstantPool &MCP = *MF.getConstantPool();\n    Type *Ty;\n    unsigned Opc = LoadMI.getOpcode();\n    if (Opc == X86::FsFLD0SS || Opc == X86::AVX512_FsFLD0SS)\n      Ty = Type::getFloatTy(MF.getFunction().getContext());\n    else if (Opc == X86::FsFLD0SD || Opc == X86::AVX512_FsFLD0SD)\n      Ty = Type::getDoubleTy(MF.getFunction().getContext());\n    else if (Opc == X86::FsFLD0F128 || Opc == X86::AVX512_FsFLD0F128)\n      Ty = Type::getFP128Ty(MF.getFunction().getContext());\n    else if (Opc == X86::AVX512_512_SET0 || Opc == X86::AVX512_512_SETALLONES)\n      Ty = FixedVectorType::get(Type::getInt32Ty(MF.getFunction().getContext()),\n                                16);\n    else if (Opc == X86::AVX2_SETALLONES || Opc == X86::AVX_SET0 ||\n             Opc == X86::AVX512_256_SET0 || Opc == X86::AVX1_SETALLONES)\n      Ty = FixedVectorType::get(Type::getInt32Ty(MF.getFunction().getContext()),\n                                8);\n    else if (Opc == X86::MMX_SET0)\n      Ty = FixedVectorType::get(Type::getInt32Ty(MF.getFunction().getContext()),\n                                2);\n    else\n      Ty = FixedVectorType::get(Type::getInt32Ty(MF.getFunction().getContext()),\n                                4);\n\n    bool IsAllOnes = (Opc == X86::V_SETALLONES || Opc == X86::AVX2_SETALLONES ||\n                      Opc == X86::AVX512_512_SETALLONES ||\n                      Opc == X86::AVX1_SETALLONES);\n    const Constant *C = IsAllOnes ? Constant::getAllOnesValue(Ty) :\n                                    Constant::getNullValue(Ty);\n    unsigned CPI = MCP.getConstantPoolIndex(C, Alignment);\n\n    // Create operands to load from the constant pool entry.\n    MOs.push_back(MachineOperand::CreateReg(PICBase, false));\n    MOs.push_back(MachineOperand::CreateImm(1));\n    MOs.push_back(MachineOperand::CreateReg(0, false));\n    MOs.push_back(MachineOperand::CreateCPI(CPI, 0));\n    MOs.push_back(MachineOperand::CreateReg(0, false));\n    break;\n  }\n  default: {\n    if (isNonFoldablePartialRegisterLoad(LoadMI, MI, MF))\n      return nullptr;\n\n    // Folding a normal load. Just copy the load's address operands.\n    MOs.append(LoadMI.operands_begin() + NumOps - X86::AddrNumOperands,\n               LoadMI.operands_begin() + NumOps);\n    break;\n  }\n  }\n  return foldMemoryOperandImpl(MF, MI, Ops[0], MOs, InsertPt,\n                               /*Size=*/0, Alignment, /*AllowCommute=*/true);\n}\n\nstatic SmallVector<MachineMemOperand *, 2>\nextractLoadMMOs(ArrayRef<MachineMemOperand *> MMOs, MachineFunction &MF) {\n  SmallVector<MachineMemOperand *, 2> LoadMMOs;\n\n  for (MachineMemOperand *MMO : MMOs) {\n    if (!MMO->isLoad())\n      continue;\n\n    if (!MMO->isStore()) {\n      // Reuse the MMO.\n      LoadMMOs.push_back(MMO);\n    } else {\n      // Clone the MMO and unset the store flag.\n      LoadMMOs.push_back(MF.getMachineMemOperand(\n          MMO, MMO->getFlags() & ~MachineMemOperand::MOStore));\n    }\n  }\n\n  return LoadMMOs;\n}\n\nstatic SmallVector<MachineMemOperand *, 2>\nextractStoreMMOs(ArrayRef<MachineMemOperand *> MMOs, MachineFunction &MF) {\n  SmallVector<MachineMemOperand *, 2> StoreMMOs;\n\n  for (MachineMemOperand *MMO : MMOs) {\n    if (!MMO->isStore())\n      continue;\n\n    if (!MMO->isLoad()) {\n      // Reuse the MMO.\n      StoreMMOs.push_back(MMO);\n    } else {\n      // Clone the MMO and unset the load flag.\n      StoreMMOs.push_back(MF.getMachineMemOperand(\n          MMO, MMO->getFlags() & ~MachineMemOperand::MOLoad));\n    }\n  }\n\n  return StoreMMOs;\n}\n\nstatic unsigned getBroadcastOpcode(const X86MemoryFoldTableEntry *I,\n                                   const TargetRegisterClass *RC,\n                                   const X86Subtarget &STI) {\n  assert(STI.hasAVX512() && \"Expected at least AVX512!\");\n  unsigned SpillSize = STI.getRegisterInfo()->getSpillSize(*RC);\n  assert((SpillSize == 64 || STI.hasVLX()) &&\n         \"Can't broadcast less than 64 bytes without AVX512VL!\");\n\n  switch (I->Flags & TB_BCAST_MASK) {\n  default: llvm_unreachable(\"Unexpected broadcast type!\");\n  case TB_BCAST_D:\n    switch (SpillSize) {\n    default: llvm_unreachable(\"Unknown spill size\");\n    case 16: return X86::VPBROADCASTDZ128rm;\n    case 32: return X86::VPBROADCASTDZ256rm;\n    case 64: return X86::VPBROADCASTDZrm;\n    }\n    break;\n  case TB_BCAST_Q:\n    switch (SpillSize) {\n    default: llvm_unreachable(\"Unknown spill size\");\n    case 16: return X86::VPBROADCASTQZ128rm;\n    case 32: return X86::VPBROADCASTQZ256rm;\n    case 64: return X86::VPBROADCASTQZrm;\n    }\n    break;\n  case TB_BCAST_SS:\n    switch (SpillSize) {\n    default: llvm_unreachable(\"Unknown spill size\");\n    case 16: return X86::VBROADCASTSSZ128rm;\n    case 32: return X86::VBROADCASTSSZ256rm;\n    case 64: return X86::VBROADCASTSSZrm;\n    }\n    break;\n  case TB_BCAST_SD:\n    switch (SpillSize) {\n    default: llvm_unreachable(\"Unknown spill size\");\n    case 16: return X86::VMOVDDUPZ128rm;\n    case 32: return X86::VBROADCASTSDZ256rm;\n    case 64: return X86::VBROADCASTSDZrm;\n    }\n    break;\n  }\n}\n\nbool X86InstrInfo::unfoldMemoryOperand(\n    MachineFunction &MF, MachineInstr &MI, unsigned Reg, bool UnfoldLoad,\n    bool UnfoldStore, SmallVectorImpl<MachineInstr *> &NewMIs) const {\n  const X86MemoryFoldTableEntry *I = lookupUnfoldTable(MI.getOpcode());\n  if (I == nullptr)\n    return false;\n  unsigned Opc = I->DstOp;\n  unsigned Index = I->Flags & TB_INDEX_MASK;\n  bool FoldedLoad = I->Flags & TB_FOLDED_LOAD;\n  bool FoldedStore = I->Flags & TB_FOLDED_STORE;\n  bool FoldedBCast = I->Flags & TB_FOLDED_BCAST;\n  if (UnfoldLoad && !FoldedLoad)\n    return false;\n  UnfoldLoad &= FoldedLoad;\n  if (UnfoldStore && !FoldedStore)\n    return false;\n  UnfoldStore &= FoldedStore;\n\n  const MCInstrDesc &MCID = get(Opc);\n\n  const TargetRegisterClass *RC = getRegClass(MCID, Index, &RI, MF);\n  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n  // TODO: Check if 32-byte or greater accesses are slow too?\n  if (!MI.hasOneMemOperand() && RC == &X86::VR128RegClass &&\n      Subtarget.isUnalignedMem16Slow())\n    // Without memoperands, loadRegFromAddr and storeRegToStackSlot will\n    // conservatively assume the address is unaligned. That's bad for\n    // performance.\n    return false;\n  SmallVector<MachineOperand, X86::AddrNumOperands> AddrOps;\n  SmallVector<MachineOperand,2> BeforeOps;\n  SmallVector<MachineOperand,2> AfterOps;\n  SmallVector<MachineOperand,4> ImpOps;\n  for (unsigned i = 0, e = MI.getNumOperands(); i != e; ++i) {\n    MachineOperand &Op = MI.getOperand(i);\n    if (i >= Index && i < Index + X86::AddrNumOperands)\n      AddrOps.push_back(Op);\n    else if (Op.isReg() && Op.isImplicit())\n      ImpOps.push_back(Op);\n    else if (i < Index)\n      BeforeOps.push_back(Op);\n    else if (i > Index)\n      AfterOps.push_back(Op);\n  }\n\n  // Emit the load or broadcast instruction.\n  if (UnfoldLoad) {\n    auto MMOs = extractLoadMMOs(MI.memoperands(), MF);\n\n    unsigned Opc;\n    if (FoldedBCast) {\n      Opc = getBroadcastOpcode(I, RC, Subtarget);\n    } else {\n      unsigned Alignment = std::max<uint32_t>(TRI.getSpillSize(*RC), 16);\n      bool isAligned = !MMOs.empty() && MMOs.front()->getAlign() >= Alignment;\n      Opc = getLoadRegOpcode(Reg, RC, isAligned, Subtarget);\n    }\n\n    DebugLoc DL;\n    MachineInstrBuilder MIB = BuildMI(MF, DL, get(Opc), Reg);\n    for (unsigned i = 0, e = AddrOps.size(); i != e; ++i)\n      MIB.add(AddrOps[i]);\n    MIB.setMemRefs(MMOs);\n    NewMIs.push_back(MIB);\n\n    if (UnfoldStore) {\n      // Address operands cannot be marked isKill.\n      for (unsigned i = 1; i != 1 + X86::AddrNumOperands; ++i) {\n        MachineOperand &MO = NewMIs[0]->getOperand(i);\n        if (MO.isReg())\n          MO.setIsKill(false);\n      }\n    }\n  }\n\n  // Emit the data processing instruction.\n  MachineInstr *DataMI = MF.CreateMachineInstr(MCID, MI.getDebugLoc(), true);\n  MachineInstrBuilder MIB(MF, DataMI);\n\n  if (FoldedStore)\n    MIB.addReg(Reg, RegState::Define);\n  for (MachineOperand &BeforeOp : BeforeOps)\n    MIB.add(BeforeOp);\n  if (FoldedLoad)\n    MIB.addReg(Reg);\n  for (MachineOperand &AfterOp : AfterOps)\n    MIB.add(AfterOp);\n  for (MachineOperand &ImpOp : ImpOps) {\n    MIB.addReg(ImpOp.getReg(),\n               getDefRegState(ImpOp.isDef()) |\n               RegState::Implicit |\n               getKillRegState(ImpOp.isKill()) |\n               getDeadRegState(ImpOp.isDead()) |\n               getUndefRegState(ImpOp.isUndef()));\n  }\n  // Change CMP32ri r, 0 back to TEST32rr r, r, etc.\n  switch (DataMI->getOpcode()) {\n  default: break;\n  case X86::CMP64ri32:\n  case X86::CMP64ri8:\n  case X86::CMP32ri:\n  case X86::CMP32ri8:\n  case X86::CMP16ri:\n  case X86::CMP16ri8:\n  case X86::CMP8ri: {\n    MachineOperand &MO0 = DataMI->getOperand(0);\n    MachineOperand &MO1 = DataMI->getOperand(1);\n    if (MO1.getImm() == 0) {\n      unsigned NewOpc;\n      switch (DataMI->getOpcode()) {\n      default: llvm_unreachable(\"Unreachable!\");\n      case X86::CMP64ri8:\n      case X86::CMP64ri32: NewOpc = X86::TEST64rr; break;\n      case X86::CMP32ri8:\n      case X86::CMP32ri:   NewOpc = X86::TEST32rr; break;\n      case X86::CMP16ri8:\n      case X86::CMP16ri:   NewOpc = X86::TEST16rr; break;\n      case X86::CMP8ri:    NewOpc = X86::TEST8rr; break;\n      }\n      DataMI->setDesc(get(NewOpc));\n      MO1.ChangeToRegister(MO0.getReg(), false);\n    }\n  }\n  }\n  NewMIs.push_back(DataMI);\n\n  // Emit the store instruction.\n  if (UnfoldStore) {\n    const TargetRegisterClass *DstRC = getRegClass(MCID, 0, &RI, MF);\n    auto MMOs = extractStoreMMOs(MI.memoperands(), MF);\n    unsigned Alignment = std::max<uint32_t>(TRI.getSpillSize(*DstRC), 16);\n    bool isAligned = !MMOs.empty() && MMOs.front()->getAlign() >= Alignment;\n    unsigned Opc = getStoreRegOpcode(Reg, DstRC, isAligned, Subtarget);\n    DebugLoc DL;\n    MachineInstrBuilder MIB = BuildMI(MF, DL, get(Opc));\n    for (unsigned i = 0, e = AddrOps.size(); i != e; ++i)\n      MIB.add(AddrOps[i]);\n    MIB.addReg(Reg, RegState::Kill);\n    MIB.setMemRefs(MMOs);\n    NewMIs.push_back(MIB);\n  }\n\n  return true;\n}\n\nbool\nX86InstrInfo::unfoldMemoryOperand(SelectionDAG &DAG, SDNode *N,\n                                  SmallVectorImpl<SDNode*> &NewNodes) const {\n  if (!N->isMachineOpcode())\n    return false;\n\n  const X86MemoryFoldTableEntry *I = lookupUnfoldTable(N->getMachineOpcode());\n  if (I == nullptr)\n    return false;\n  unsigned Opc = I->DstOp;\n  unsigned Index = I->Flags & TB_INDEX_MASK;\n  bool FoldedLoad = I->Flags & TB_FOLDED_LOAD;\n  bool FoldedStore = I->Flags & TB_FOLDED_STORE;\n  bool FoldedBCast = I->Flags & TB_FOLDED_BCAST;\n  const MCInstrDesc &MCID = get(Opc);\n  MachineFunction &MF = DAG.getMachineFunction();\n  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();\n  const TargetRegisterClass *RC = getRegClass(MCID, Index, &RI, MF);\n  unsigned NumDefs = MCID.NumDefs;\n  std::vector<SDValue> AddrOps;\n  std::vector<SDValue> BeforeOps;\n  std::vector<SDValue> AfterOps;\n  SDLoc dl(N);\n  unsigned NumOps = N->getNumOperands();\n  for (unsigned i = 0; i != NumOps-1; ++i) {\n    SDValue Op = N->getOperand(i);\n    if (i >= Index-NumDefs && i < Index-NumDefs + X86::AddrNumOperands)\n      AddrOps.push_back(Op);\n    else if (i < Index-NumDefs)\n      BeforeOps.push_back(Op);\n    else if (i > Index-NumDefs)\n      AfterOps.push_back(Op);\n  }\n  SDValue Chain = N->getOperand(NumOps-1);\n  AddrOps.push_back(Chain);\n\n  // Emit the load instruction.\n  SDNode *Load = nullptr;\n  if (FoldedLoad) {\n    EVT VT = *TRI.legalclasstypes_begin(*RC);\n    auto MMOs = extractLoadMMOs(cast<MachineSDNode>(N)->memoperands(), MF);\n    if (MMOs.empty() && RC == &X86::VR128RegClass &&\n        Subtarget.isUnalignedMem16Slow())\n      // Do not introduce a slow unaligned load.\n      return false;\n    // FIXME: If a VR128 can have size 32, we should be checking if a 32-byte\n    // memory access is slow above.\n\n    unsigned Opc;\n    if (FoldedBCast) {\n      Opc = getBroadcastOpcode(I, RC, Subtarget);\n    } else {\n      unsigned Alignment = std::max<uint32_t>(TRI.getSpillSize(*RC), 16);\n      bool isAligned = !MMOs.empty() && MMOs.front()->getAlign() >= Alignment;\n      Opc = getLoadRegOpcode(0, RC, isAligned, Subtarget);\n    }\n\n    Load = DAG.getMachineNode(Opc, dl, VT, MVT::Other, AddrOps);\n    NewNodes.push_back(Load);\n\n    // Preserve memory reference information.\n    DAG.setNodeMemRefs(cast<MachineSDNode>(Load), MMOs);\n  }\n\n  // Emit the data processing instruction.\n  std::vector<EVT> VTs;\n  const TargetRegisterClass *DstRC = nullptr;\n  if (MCID.getNumDefs() > 0) {\n    DstRC = getRegClass(MCID, 0, &RI, MF);\n    VTs.push_back(*TRI.legalclasstypes_begin(*DstRC));\n  }\n  for (unsigned i = 0, e = N->getNumValues(); i != e; ++i) {\n    EVT VT = N->getValueType(i);\n    if (VT != MVT::Other && i >= (unsigned)MCID.getNumDefs())\n      VTs.push_back(VT);\n  }\n  if (Load)\n    BeforeOps.push_back(SDValue(Load, 0));\n  llvm::append_range(BeforeOps, AfterOps);\n  // Change CMP32ri r, 0 back to TEST32rr r, r, etc.\n  switch (Opc) {\n    default: break;\n    case X86::CMP64ri32:\n    case X86::CMP64ri8:\n    case X86::CMP32ri:\n    case X86::CMP32ri8:\n    case X86::CMP16ri:\n    case X86::CMP16ri8:\n    case X86::CMP8ri:\n      if (isNullConstant(BeforeOps[1])) {\n        switch (Opc) {\n          default: llvm_unreachable(\"Unreachable!\");\n          case X86::CMP64ri8:\n          case X86::CMP64ri32: Opc = X86::TEST64rr; break;\n          case X86::CMP32ri8:\n          case X86::CMP32ri:   Opc = X86::TEST32rr; break;\n          case X86::CMP16ri8:\n          case X86::CMP16ri:   Opc = X86::TEST16rr; break;\n          case X86::CMP8ri:    Opc = X86::TEST8rr; break;\n        }\n        BeforeOps[1] = BeforeOps[0];\n      }\n  }\n  SDNode *NewNode= DAG.getMachineNode(Opc, dl, VTs, BeforeOps);\n  NewNodes.push_back(NewNode);\n\n  // Emit the store instruction.\n  if (FoldedStore) {\n    AddrOps.pop_back();\n    AddrOps.push_back(SDValue(NewNode, 0));\n    AddrOps.push_back(Chain);\n    auto MMOs = extractStoreMMOs(cast<MachineSDNode>(N)->memoperands(), MF);\n    if (MMOs.empty() && RC == &X86::VR128RegClass &&\n        Subtarget.isUnalignedMem16Slow())\n      // Do not introduce a slow unaligned store.\n      return false;\n    // FIXME: If a VR128 can have size 32, we should be checking if a 32-byte\n    // memory access is slow above.\n    unsigned Alignment = std::max<uint32_t>(TRI.getSpillSize(*RC), 16);\n    bool isAligned = !MMOs.empty() && MMOs.front()->getAlign() >= Alignment;\n    SDNode *Store =\n        DAG.getMachineNode(getStoreRegOpcode(0, DstRC, isAligned, Subtarget),\n                           dl, MVT::Other, AddrOps);\n    NewNodes.push_back(Store);\n\n    // Preserve memory reference information.\n    DAG.setNodeMemRefs(cast<MachineSDNode>(Store), MMOs);\n  }\n\n  return true;\n}\n\nunsigned X86InstrInfo::getOpcodeAfterMemoryUnfold(unsigned Opc,\n                                      bool UnfoldLoad, bool UnfoldStore,\n                                      unsigned *LoadRegIndex) const {\n  const X86MemoryFoldTableEntry *I = lookupUnfoldTable(Opc);\n  if (I == nullptr)\n    return 0;\n  bool FoldedLoad = I->Flags & TB_FOLDED_LOAD;\n  bool FoldedStore = I->Flags & TB_FOLDED_STORE;\n  if (UnfoldLoad && !FoldedLoad)\n    return 0;\n  if (UnfoldStore && !FoldedStore)\n    return 0;\n  if (LoadRegIndex)\n    *LoadRegIndex = I->Flags & TB_INDEX_MASK;\n  return I->DstOp;\n}\n\nbool\nX86InstrInfo::areLoadsFromSameBasePtr(SDNode *Load1, SDNode *Load2,\n                                     int64_t &Offset1, int64_t &Offset2) const {\n  if (!Load1->isMachineOpcode() || !Load2->isMachineOpcode())\n    return false;\n  unsigned Opc1 = Load1->getMachineOpcode();\n  unsigned Opc2 = Load2->getMachineOpcode();\n  switch (Opc1) {\n  default: return false;\n  case X86::MOV8rm:\n  case X86::MOV16rm:\n  case X86::MOV32rm:\n  case X86::MOV64rm:\n  case X86::LD_Fp32m:\n  case X86::LD_Fp64m:\n  case X86::LD_Fp80m:\n  case X86::MOVSSrm:\n  case X86::MOVSSrm_alt:\n  case X86::MOVSDrm:\n  case X86::MOVSDrm_alt:\n  case X86::MMX_MOVD64rm:\n  case X86::MMX_MOVQ64rm:\n  case X86::MOVAPSrm:\n  case X86::MOVUPSrm:\n  case X86::MOVAPDrm:\n  case X86::MOVUPDrm:\n  case X86::MOVDQArm:\n  case X86::MOVDQUrm:\n  // AVX load instructions\n  case X86::VMOVSSrm:\n  case X86::VMOVSSrm_alt:\n  case X86::VMOVSDrm:\n  case X86::VMOVSDrm_alt:\n  case X86::VMOVAPSrm:\n  case X86::VMOVUPSrm:\n  case X86::VMOVAPDrm:\n  case X86::VMOVUPDrm:\n  case X86::VMOVDQArm:\n  case X86::VMOVDQUrm:\n  case X86::VMOVAPSYrm:\n  case X86::VMOVUPSYrm:\n  case X86::VMOVAPDYrm:\n  case X86::VMOVUPDYrm:\n  case X86::VMOVDQAYrm:\n  case X86::VMOVDQUYrm:\n  // AVX512 load instructions\n  case X86::VMOVSSZrm:\n  case X86::VMOVSSZrm_alt:\n  case X86::VMOVSDZrm:\n  case X86::VMOVSDZrm_alt:\n  case X86::VMOVAPSZ128rm:\n  case X86::VMOVUPSZ128rm:\n  case X86::VMOVAPSZ128rm_NOVLX:\n  case X86::VMOVUPSZ128rm_NOVLX:\n  case X86::VMOVAPDZ128rm:\n  case X86::VMOVUPDZ128rm:\n  case X86::VMOVDQU8Z128rm:\n  case X86::VMOVDQU16Z128rm:\n  case X86::VMOVDQA32Z128rm:\n  case X86::VMOVDQU32Z128rm:\n  case X86::VMOVDQA64Z128rm:\n  case X86::VMOVDQU64Z128rm:\n  case X86::VMOVAPSZ256rm:\n  case X86::VMOVUPSZ256rm:\n  case X86::VMOVAPSZ256rm_NOVLX:\n  case X86::VMOVUPSZ256rm_NOVLX:\n  case X86::VMOVAPDZ256rm:\n  case X86::VMOVUPDZ256rm:\n  case X86::VMOVDQU8Z256rm:\n  case X86::VMOVDQU16Z256rm:\n  case X86::VMOVDQA32Z256rm:\n  case X86::VMOVDQU32Z256rm:\n  case X86::VMOVDQA64Z256rm:\n  case X86::VMOVDQU64Z256rm:\n  case X86::VMOVAPSZrm:\n  case X86::VMOVUPSZrm:\n  case X86::VMOVAPDZrm:\n  case X86::VMOVUPDZrm:\n  case X86::VMOVDQU8Zrm:\n  case X86::VMOVDQU16Zrm:\n  case X86::VMOVDQA32Zrm:\n  case X86::VMOVDQU32Zrm:\n  case X86::VMOVDQA64Zrm:\n  case X86::VMOVDQU64Zrm:\n  case X86::KMOVBkm:\n  case X86::KMOVWkm:\n  case X86::KMOVDkm:\n  case X86::KMOVQkm:\n    break;\n  }\n  switch (Opc2) {\n  default: return false;\n  case X86::MOV8rm:\n  case X86::MOV16rm:\n  case X86::MOV32rm:\n  case X86::MOV64rm:\n  case X86::LD_Fp32m:\n  case X86::LD_Fp64m:\n  case X86::LD_Fp80m:\n  case X86::MOVSSrm:\n  case X86::MOVSSrm_alt:\n  case X86::MOVSDrm:\n  case X86::MOVSDrm_alt:\n  case X86::MMX_MOVD64rm:\n  case X86::MMX_MOVQ64rm:\n  case X86::MOVAPSrm:\n  case X86::MOVUPSrm:\n  case X86::MOVAPDrm:\n  case X86::MOVUPDrm:\n  case X86::MOVDQArm:\n  case X86::MOVDQUrm:\n  // AVX load instructions\n  case X86::VMOVSSrm:\n  case X86::VMOVSSrm_alt:\n  case X86::VMOVSDrm:\n  case X86::VMOVSDrm_alt:\n  case X86::VMOVAPSrm:\n  case X86::VMOVUPSrm:\n  case X86::VMOVAPDrm:\n  case X86::VMOVUPDrm:\n  case X86::VMOVDQArm:\n  case X86::VMOVDQUrm:\n  case X86::VMOVAPSYrm:\n  case X86::VMOVUPSYrm:\n  case X86::VMOVAPDYrm:\n  case X86::VMOVUPDYrm:\n  case X86::VMOVDQAYrm:\n  case X86::VMOVDQUYrm:\n  // AVX512 load instructions\n  case X86::VMOVSSZrm:\n  case X86::VMOVSSZrm_alt:\n  case X86::VMOVSDZrm:\n  case X86::VMOVSDZrm_alt:\n  case X86::VMOVAPSZ128rm:\n  case X86::VMOVUPSZ128rm:\n  case X86::VMOVAPSZ128rm_NOVLX:\n  case X86::VMOVUPSZ128rm_NOVLX:\n  case X86::VMOVAPDZ128rm:\n  case X86::VMOVUPDZ128rm:\n  case X86::VMOVDQU8Z128rm:\n  case X86::VMOVDQU16Z128rm:\n  case X86::VMOVDQA32Z128rm:\n  case X86::VMOVDQU32Z128rm:\n  case X86::VMOVDQA64Z128rm:\n  case X86::VMOVDQU64Z128rm:\n  case X86::VMOVAPSZ256rm:\n  case X86::VMOVUPSZ256rm:\n  case X86::VMOVAPSZ256rm_NOVLX:\n  case X86::VMOVUPSZ256rm_NOVLX:\n  case X86::VMOVAPDZ256rm:\n  case X86::VMOVUPDZ256rm:\n  case X86::VMOVDQU8Z256rm:\n  case X86::VMOVDQU16Z256rm:\n  case X86::VMOVDQA32Z256rm:\n  case X86::VMOVDQU32Z256rm:\n  case X86::VMOVDQA64Z256rm:\n  case X86::VMOVDQU64Z256rm:\n  case X86::VMOVAPSZrm:\n  case X86::VMOVUPSZrm:\n  case X86::VMOVAPDZrm:\n  case X86::VMOVUPDZrm:\n  case X86::VMOVDQU8Zrm:\n  case X86::VMOVDQU16Zrm:\n  case X86::VMOVDQA32Zrm:\n  case X86::VMOVDQU32Zrm:\n  case X86::VMOVDQA64Zrm:\n  case X86::VMOVDQU64Zrm:\n  case X86::KMOVBkm:\n  case X86::KMOVWkm:\n  case X86::KMOVDkm:\n  case X86::KMOVQkm:\n    break;\n  }\n\n  // Lambda to check if both the loads have the same value for an operand index.\n  auto HasSameOp = [&](int I) {\n    return Load1->getOperand(I) == Load2->getOperand(I);\n  };\n\n  // All operands except the displacement should match.\n  if (!HasSameOp(X86::AddrBaseReg) || !HasSameOp(X86::AddrScaleAmt) ||\n      !HasSameOp(X86::AddrIndexReg) || !HasSameOp(X86::AddrSegmentReg))\n    return false;\n\n  // Chain Operand must be the same.\n  if (!HasSameOp(5))\n    return false;\n\n  // Now let's examine if the displacements are constants.\n  auto Disp1 = dyn_cast<ConstantSDNode>(Load1->getOperand(X86::AddrDisp));\n  auto Disp2 = dyn_cast<ConstantSDNode>(Load2->getOperand(X86::AddrDisp));\n  if (!Disp1 || !Disp2)\n    return false;\n\n  Offset1 = Disp1->getSExtValue();\n  Offset2 = Disp2->getSExtValue();\n  return true;\n}\n\nbool X86InstrInfo::shouldScheduleLoadsNear(SDNode *Load1, SDNode *Load2,\n                                           int64_t Offset1, int64_t Offset2,\n                                           unsigned NumLoads) const {\n  assert(Offset2 > Offset1);\n  if ((Offset2 - Offset1) / 8 > 64)\n    return false;\n\n  unsigned Opc1 = Load1->getMachineOpcode();\n  unsigned Opc2 = Load2->getMachineOpcode();\n  if (Opc1 != Opc2)\n    return false;  // FIXME: overly conservative?\n\n  switch (Opc1) {\n  default: break;\n  case X86::LD_Fp32m:\n  case X86::LD_Fp64m:\n  case X86::LD_Fp80m:\n  case X86::MMX_MOVD64rm:\n  case X86::MMX_MOVQ64rm:\n    return false;\n  }\n\n  EVT VT = Load1->getValueType(0);\n  switch (VT.getSimpleVT().SimpleTy) {\n  default:\n    // XMM registers. In 64-bit mode we can be a bit more aggressive since we\n    // have 16 of them to play with.\n    if (Subtarget.is64Bit()) {\n      if (NumLoads >= 3)\n        return false;\n    } else if (NumLoads) {\n      return false;\n    }\n    break;\n  case MVT::i8:\n  case MVT::i16:\n  case MVT::i32:\n  case MVT::i64:\n  case MVT::f32:\n  case MVT::f64:\n    if (NumLoads)\n      return false;\n    break;\n  }\n\n  return true;\n}\n\nbool X86InstrInfo::isSchedulingBoundary(const MachineInstr &MI,\n                                        const MachineBasicBlock *MBB,\n                                        const MachineFunction &MF) const {\n\n  // ENDBR instructions should not be scheduled around.\n  unsigned Opcode = MI.getOpcode();\n  if (Opcode == X86::ENDBR64 || Opcode == X86::ENDBR32 ||\n      Opcode == X86::LDTILECFG)\n    return true;\n\n  return TargetInstrInfo::isSchedulingBoundary(MI, MBB, MF);\n}\n\nbool X86InstrInfo::\nreverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const {\n  assert(Cond.size() == 1 && \"Invalid X86 branch condition!\");\n  X86::CondCode CC = static_cast<X86::CondCode>(Cond[0].getImm());\n  Cond[0].setImm(GetOppositeBranchCondition(CC));\n  return false;\n}\n\nbool X86InstrInfo::\nisSafeToMoveRegClassDefs(const TargetRegisterClass *RC) const {\n  // FIXME: Return false for x87 stack register classes for now. We can't\n  // allow any loads of these registers before FpGet_ST0_80.\n  return !(RC == &X86::CCRRegClass || RC == &X86::DFCCRRegClass ||\n           RC == &X86::RFP32RegClass || RC == &X86::RFP64RegClass ||\n           RC == &X86::RFP80RegClass);\n}\n\n/// Return a virtual register initialized with the\n/// the global base register value. Output instructions required to\n/// initialize the register in the function entry block, if necessary.\n///\n/// TODO: Eliminate this and move the code to X86MachineFunctionInfo.\n///\nunsigned X86InstrInfo::getGlobalBaseReg(MachineFunction *MF) const {\n  assert((!Subtarget.is64Bit() ||\n          MF->getTarget().getCodeModel() == CodeModel::Medium ||\n          MF->getTarget().getCodeModel() == CodeModel::Large) &&\n         \"X86-64 PIC uses RIP relative addressing\");\n\n  X86MachineFunctionInfo *X86FI = MF->getInfo<X86MachineFunctionInfo>();\n  Register GlobalBaseReg = X86FI->getGlobalBaseReg();\n  if (GlobalBaseReg != 0)\n    return GlobalBaseReg;\n\n  // Create the register. The code to initialize it is inserted\n  // later, by the CGBR pass (below).\n  MachineRegisterInfo &RegInfo = MF->getRegInfo();\n  GlobalBaseReg = RegInfo.createVirtualRegister(\n      Subtarget.is64Bit() ? &X86::GR64_NOSPRegClass : &X86::GR32_NOSPRegClass);\n  X86FI->setGlobalBaseReg(GlobalBaseReg);\n  return GlobalBaseReg;\n}\n\n// These are the replaceable SSE instructions. Some of these have Int variants\n// that we don't include here. We don't want to replace instructions selected\n// by intrinsics.\nstatic const uint16_t ReplaceableInstrs[][3] = {\n  //PackedSingle     PackedDouble    PackedInt\n  { X86::MOVAPSmr,   X86::MOVAPDmr,  X86::MOVDQAmr  },\n  { X86::MOVAPSrm,   X86::MOVAPDrm,  X86::MOVDQArm  },\n  { X86::MOVAPSrr,   X86::MOVAPDrr,  X86::MOVDQArr  },\n  { X86::MOVUPSmr,   X86::MOVUPDmr,  X86::MOVDQUmr  },\n  { X86::MOVUPSrm,   X86::MOVUPDrm,  X86::MOVDQUrm  },\n  { X86::MOVLPSmr,   X86::MOVLPDmr,  X86::MOVPQI2QImr },\n  { X86::MOVSDmr,    X86::MOVSDmr,   X86::MOVPQI2QImr },\n  { X86::MOVSSmr,    X86::MOVSSmr,   X86::MOVPDI2DImr },\n  { X86::MOVSDrm,    X86::MOVSDrm,   X86::MOVQI2PQIrm },\n  { X86::MOVSDrm_alt,X86::MOVSDrm_alt,X86::MOVQI2PQIrm },\n  { X86::MOVSSrm,    X86::MOVSSrm,   X86::MOVDI2PDIrm },\n  { X86::MOVSSrm_alt,X86::MOVSSrm_alt,X86::MOVDI2PDIrm },\n  { X86::MOVNTPSmr,  X86::MOVNTPDmr, X86::MOVNTDQmr },\n  { X86::ANDNPSrm,   X86::ANDNPDrm,  X86::PANDNrm   },\n  { X86::ANDNPSrr,   X86::ANDNPDrr,  X86::PANDNrr   },\n  { X86::ANDPSrm,    X86::ANDPDrm,   X86::PANDrm    },\n  { X86::ANDPSrr,    X86::ANDPDrr,   X86::PANDrr    },\n  { X86::ORPSrm,     X86::ORPDrm,    X86::PORrm     },\n  { X86::ORPSrr,     X86::ORPDrr,    X86::PORrr     },\n  { X86::XORPSrm,    X86::XORPDrm,   X86::PXORrm    },\n  { X86::XORPSrr,    X86::XORPDrr,   X86::PXORrr    },\n  { X86::UNPCKLPDrm, X86::UNPCKLPDrm, X86::PUNPCKLQDQrm },\n  { X86::MOVLHPSrr,  X86::UNPCKLPDrr, X86::PUNPCKLQDQrr },\n  { X86::UNPCKHPDrm, X86::UNPCKHPDrm, X86::PUNPCKHQDQrm },\n  { X86::UNPCKHPDrr, X86::UNPCKHPDrr, X86::PUNPCKHQDQrr },\n  { X86::UNPCKLPSrm, X86::UNPCKLPSrm, X86::PUNPCKLDQrm },\n  { X86::UNPCKLPSrr, X86::UNPCKLPSrr, X86::PUNPCKLDQrr },\n  { X86::UNPCKHPSrm, X86::UNPCKHPSrm, X86::PUNPCKHDQrm },\n  { X86::UNPCKHPSrr, X86::UNPCKHPSrr, X86::PUNPCKHDQrr },\n  { X86::EXTRACTPSmr, X86::EXTRACTPSmr, X86::PEXTRDmr },\n  { X86::EXTRACTPSrr, X86::EXTRACTPSrr, X86::PEXTRDrr },\n  // AVX 128-bit support\n  { X86::VMOVAPSmr,  X86::VMOVAPDmr,  X86::VMOVDQAmr  },\n  { X86::VMOVAPSrm,  X86::VMOVAPDrm,  X86::VMOVDQArm  },\n  { X86::VMOVAPSrr,  X86::VMOVAPDrr,  X86::VMOVDQArr  },\n  { X86::VMOVUPSmr,  X86::VMOVUPDmr,  X86::VMOVDQUmr  },\n  { X86::VMOVUPSrm,  X86::VMOVUPDrm,  X86::VMOVDQUrm  },\n  { X86::VMOVLPSmr,  X86::VMOVLPDmr,  X86::VMOVPQI2QImr },\n  { X86::VMOVSDmr,   X86::VMOVSDmr,   X86::VMOVPQI2QImr },\n  { X86::VMOVSSmr,   X86::VMOVSSmr,   X86::VMOVPDI2DImr },\n  { X86::VMOVSDrm,   X86::VMOVSDrm,   X86::VMOVQI2PQIrm },\n  { X86::VMOVSDrm_alt,X86::VMOVSDrm_alt,X86::VMOVQI2PQIrm },\n  { X86::VMOVSSrm,   X86::VMOVSSrm,   X86::VMOVDI2PDIrm },\n  { X86::VMOVSSrm_alt,X86::VMOVSSrm_alt,X86::VMOVDI2PDIrm },\n  { X86::VMOVNTPSmr, X86::VMOVNTPDmr, X86::VMOVNTDQmr },\n  { X86::VANDNPSrm,  X86::VANDNPDrm,  X86::VPANDNrm   },\n  { X86::VANDNPSrr,  X86::VANDNPDrr,  X86::VPANDNrr   },\n  { X86::VANDPSrm,   X86::VANDPDrm,   X86::VPANDrm    },\n  { X86::VANDPSrr,   X86::VANDPDrr,   X86::VPANDrr    },\n  { X86::VORPSrm,    X86::VORPDrm,    X86::VPORrm     },\n  { X86::VORPSrr,    X86::VORPDrr,    X86::VPORrr     },\n  { X86::VXORPSrm,   X86::VXORPDrm,   X86::VPXORrm    },\n  { X86::VXORPSrr,   X86::VXORPDrr,   X86::VPXORrr    },\n  { X86::VUNPCKLPDrm, X86::VUNPCKLPDrm, X86::VPUNPCKLQDQrm },\n  { X86::VMOVLHPSrr,  X86::VUNPCKLPDrr, X86::VPUNPCKLQDQrr },\n  { X86::VUNPCKHPDrm, X86::VUNPCKHPDrm, X86::VPUNPCKHQDQrm },\n  { X86::VUNPCKHPDrr, X86::VUNPCKHPDrr, X86::VPUNPCKHQDQrr },\n  { X86::VUNPCKLPSrm, X86::VUNPCKLPSrm, X86::VPUNPCKLDQrm },\n  { X86::VUNPCKLPSrr, X86::VUNPCKLPSrr, X86::VPUNPCKLDQrr },\n  { X86::VUNPCKHPSrm, X86::VUNPCKHPSrm, X86::VPUNPCKHDQrm },\n  { X86::VUNPCKHPSrr, X86::VUNPCKHPSrr, X86::VPUNPCKHDQrr },\n  { X86::VEXTRACTPSmr, X86::VEXTRACTPSmr, X86::VPEXTRDmr },\n  { X86::VEXTRACTPSrr, X86::VEXTRACTPSrr, X86::VPEXTRDrr },\n  // AVX 256-bit support\n  { X86::VMOVAPSYmr,   X86::VMOVAPDYmr,   X86::VMOVDQAYmr  },\n  { X86::VMOVAPSYrm,   X86::VMOVAPDYrm,   X86::VMOVDQAYrm  },\n  { X86::VMOVAPSYrr,   X86::VMOVAPDYrr,   X86::VMOVDQAYrr  },\n  { X86::VMOVUPSYmr,   X86::VMOVUPDYmr,   X86::VMOVDQUYmr  },\n  { X86::VMOVUPSYrm,   X86::VMOVUPDYrm,   X86::VMOVDQUYrm  },\n  { X86::VMOVNTPSYmr,  X86::VMOVNTPDYmr,  X86::VMOVNTDQYmr },\n  { X86::VPERMPSYrm,   X86::VPERMPSYrm,   X86::VPERMDYrm },\n  { X86::VPERMPSYrr,   X86::VPERMPSYrr,   X86::VPERMDYrr },\n  { X86::VPERMPDYmi,   X86::VPERMPDYmi,   X86::VPERMQYmi },\n  { X86::VPERMPDYri,   X86::VPERMPDYri,   X86::VPERMQYri },\n  // AVX512 support\n  { X86::VMOVLPSZ128mr,  X86::VMOVLPDZ128mr,  X86::VMOVPQI2QIZmr  },\n  { X86::VMOVNTPSZ128mr, X86::VMOVNTPDZ128mr, X86::VMOVNTDQZ128mr },\n  { X86::VMOVNTPSZ256mr, X86::VMOVNTPDZ256mr, X86::VMOVNTDQZ256mr },\n  { X86::VMOVNTPSZmr,    X86::VMOVNTPDZmr,    X86::VMOVNTDQZmr    },\n  { X86::VMOVSDZmr,      X86::VMOVSDZmr,      X86::VMOVPQI2QIZmr  },\n  { X86::VMOVSSZmr,      X86::VMOVSSZmr,      X86::VMOVPDI2DIZmr  },\n  { X86::VMOVSDZrm,      X86::VMOVSDZrm,      X86::VMOVQI2PQIZrm  },\n  { X86::VMOVSDZrm_alt,  X86::VMOVSDZrm_alt,  X86::VMOVQI2PQIZrm  },\n  { X86::VMOVSSZrm,      X86::VMOVSSZrm,      X86::VMOVDI2PDIZrm  },\n  { X86::VMOVSSZrm_alt,  X86::VMOVSSZrm_alt,  X86::VMOVDI2PDIZrm  },\n  { X86::VBROADCASTSSZ128rr,X86::VBROADCASTSSZ128rr,X86::VPBROADCASTDZ128rr },\n  { X86::VBROADCASTSSZ128rm,X86::VBROADCASTSSZ128rm,X86::VPBROADCASTDZ128rm },\n  { X86::VBROADCASTSSZ256rr,X86::VBROADCASTSSZ256rr,X86::VPBROADCASTDZ256rr },\n  { X86::VBROADCASTSSZ256rm,X86::VBROADCASTSSZ256rm,X86::VPBROADCASTDZ256rm },\n  { X86::VBROADCASTSSZrr,   X86::VBROADCASTSSZrr,   X86::VPBROADCASTDZrr },\n  { X86::VBROADCASTSSZrm,   X86::VBROADCASTSSZrm,   X86::VPBROADCASTDZrm },\n  { X86::VMOVDDUPZ128rr,    X86::VMOVDDUPZ128rr,    X86::VPBROADCASTQZ128rr },\n  { X86::VMOVDDUPZ128rm,    X86::VMOVDDUPZ128rm,    X86::VPBROADCASTQZ128rm },\n  { X86::VBROADCASTSDZ256rr,X86::VBROADCASTSDZ256rr,X86::VPBROADCASTQZ256rr },\n  { X86::VBROADCASTSDZ256rm,X86::VBROADCASTSDZ256rm,X86::VPBROADCASTQZ256rm },\n  { X86::VBROADCASTSDZrr,   X86::VBROADCASTSDZrr,   X86::VPBROADCASTQZrr },\n  { X86::VBROADCASTSDZrm,   X86::VBROADCASTSDZrm,   X86::VPBROADCASTQZrm },\n  { X86::VINSERTF32x4Zrr,   X86::VINSERTF32x4Zrr,   X86::VINSERTI32x4Zrr },\n  { X86::VINSERTF32x4Zrm,   X86::VINSERTF32x4Zrm,   X86::VINSERTI32x4Zrm },\n  { X86::VINSERTF32x8Zrr,   X86::VINSERTF32x8Zrr,   X86::VINSERTI32x8Zrr },\n  { X86::VINSERTF32x8Zrm,   X86::VINSERTF32x8Zrm,   X86::VINSERTI32x8Zrm },\n  { X86::VINSERTF64x2Zrr,   X86::VINSERTF64x2Zrr,   X86::VINSERTI64x2Zrr },\n  { X86::VINSERTF64x2Zrm,   X86::VINSERTF64x2Zrm,   X86::VINSERTI64x2Zrm },\n  { X86::VINSERTF64x4Zrr,   X86::VINSERTF64x4Zrr,   X86::VINSERTI64x4Zrr },\n  { X86::VINSERTF64x4Zrm,   X86::VINSERTF64x4Zrm,   X86::VINSERTI64x4Zrm },\n  { X86::VINSERTF32x4Z256rr,X86::VINSERTF32x4Z256rr,X86::VINSERTI32x4Z256rr },\n  { X86::VINSERTF32x4Z256rm,X86::VINSERTF32x4Z256rm,X86::VINSERTI32x4Z256rm },\n  { X86::VINSERTF64x2Z256rr,X86::VINSERTF64x2Z256rr,X86::VINSERTI64x2Z256rr },\n  { X86::VINSERTF64x2Z256rm,X86::VINSERTF64x2Z256rm,X86::VINSERTI64x2Z256rm },\n  { X86::VEXTRACTF32x4Zrr,   X86::VEXTRACTF32x4Zrr,   X86::VEXTRACTI32x4Zrr },\n  { X86::VEXTRACTF32x4Zmr,   X86::VEXTRACTF32x4Zmr,   X86::VEXTRACTI32x4Zmr },\n  { X86::VEXTRACTF32x8Zrr,   X86::VEXTRACTF32x8Zrr,   X86::VEXTRACTI32x8Zrr },\n  { X86::VEXTRACTF32x8Zmr,   X86::VEXTRACTF32x8Zmr,   X86::VEXTRACTI32x8Zmr },\n  { X86::VEXTRACTF64x2Zrr,   X86::VEXTRACTF64x2Zrr,   X86::VEXTRACTI64x2Zrr },\n  { X86::VEXTRACTF64x2Zmr,   X86::VEXTRACTF64x2Zmr,   X86::VEXTRACTI64x2Zmr },\n  { X86::VEXTRACTF64x4Zrr,   X86::VEXTRACTF64x4Zrr,   X86::VEXTRACTI64x4Zrr },\n  { X86::VEXTRACTF64x4Zmr,   X86::VEXTRACTF64x4Zmr,   X86::VEXTRACTI64x4Zmr },\n  { X86::VEXTRACTF32x4Z256rr,X86::VEXTRACTF32x4Z256rr,X86::VEXTRACTI32x4Z256rr },\n  { X86::VEXTRACTF32x4Z256mr,X86::VEXTRACTF32x4Z256mr,X86::VEXTRACTI32x4Z256mr },\n  { X86::VEXTRACTF64x2Z256rr,X86::VEXTRACTF64x2Z256rr,X86::VEXTRACTI64x2Z256rr },\n  { X86::VEXTRACTF64x2Z256mr,X86::VEXTRACTF64x2Z256mr,X86::VEXTRACTI64x2Z256mr },\n  { X86::VPERMILPSmi,        X86::VPERMILPSmi,        X86::VPSHUFDmi },\n  { X86::VPERMILPSri,        X86::VPERMILPSri,        X86::VPSHUFDri },\n  { X86::VPERMILPSZ128mi,    X86::VPERMILPSZ128mi,    X86::VPSHUFDZ128mi },\n  { X86::VPERMILPSZ128ri,    X86::VPERMILPSZ128ri,    X86::VPSHUFDZ128ri },\n  { X86::VPERMILPSZ256mi,    X86::VPERMILPSZ256mi,    X86::VPSHUFDZ256mi },\n  { X86::VPERMILPSZ256ri,    X86::VPERMILPSZ256ri,    X86::VPSHUFDZ256ri },\n  { X86::VPERMILPSZmi,       X86::VPERMILPSZmi,       X86::VPSHUFDZmi },\n  { X86::VPERMILPSZri,       X86::VPERMILPSZri,       X86::VPSHUFDZri },\n  { X86::VPERMPSZ256rm,      X86::VPERMPSZ256rm,      X86::VPERMDZ256rm },\n  { X86::VPERMPSZ256rr,      X86::VPERMPSZ256rr,      X86::VPERMDZ256rr },\n  { X86::VPERMPDZ256mi,      X86::VPERMPDZ256mi,      X86::VPERMQZ256mi },\n  { X86::VPERMPDZ256ri,      X86::VPERMPDZ256ri,      X86::VPERMQZ256ri },\n  { X86::VPERMPDZ256rm,      X86::VPERMPDZ256rm,      X86::VPERMQZ256rm },\n  { X86::VPERMPDZ256rr,      X86::VPERMPDZ256rr,      X86::VPERMQZ256rr },\n  { X86::VPERMPSZrm,         X86::VPERMPSZrm,         X86::VPERMDZrm },\n  { X86::VPERMPSZrr,         X86::VPERMPSZrr,         X86::VPERMDZrr },\n  { X86::VPERMPDZmi,         X86::VPERMPDZmi,         X86::VPERMQZmi },\n  { X86::VPERMPDZri,         X86::VPERMPDZri,         X86::VPERMQZri },\n  { X86::VPERMPDZrm,         X86::VPERMPDZrm,         X86::VPERMQZrm },\n  { X86::VPERMPDZrr,         X86::VPERMPDZrr,         X86::VPERMQZrr },\n  { X86::VUNPCKLPDZ256rm,    X86::VUNPCKLPDZ256rm,    X86::VPUNPCKLQDQZ256rm },\n  { X86::VUNPCKLPDZ256rr,    X86::VUNPCKLPDZ256rr,    X86::VPUNPCKLQDQZ256rr },\n  { X86::VUNPCKHPDZ256rm,    X86::VUNPCKHPDZ256rm,    X86::VPUNPCKHQDQZ256rm },\n  { X86::VUNPCKHPDZ256rr,    X86::VUNPCKHPDZ256rr,    X86::VPUNPCKHQDQZ256rr },\n  { X86::VUNPCKLPSZ256rm,    X86::VUNPCKLPSZ256rm,    X86::VPUNPCKLDQZ256rm },\n  { X86::VUNPCKLPSZ256rr,    X86::VUNPCKLPSZ256rr,    X86::VPUNPCKLDQZ256rr },\n  { X86::VUNPCKHPSZ256rm,    X86::VUNPCKHPSZ256rm,    X86::VPUNPCKHDQZ256rm },\n  { X86::VUNPCKHPSZ256rr,    X86::VUNPCKHPSZ256rr,    X86::VPUNPCKHDQZ256rr },\n  { X86::VUNPCKLPDZ128rm,    X86::VUNPCKLPDZ128rm,    X86::VPUNPCKLQDQZ128rm },\n  { X86::VMOVLHPSZrr,        X86::VUNPCKLPDZ128rr,    X86::VPUNPCKLQDQZ128rr },\n  { X86::VUNPCKHPDZ128rm,    X86::VUNPCKHPDZ128rm,    X86::VPUNPCKHQDQZ128rm },\n  { X86::VUNPCKHPDZ128rr,    X86::VUNPCKHPDZ128rr,    X86::VPUNPCKHQDQZ128rr },\n  { X86::VUNPCKLPSZ128rm,    X86::VUNPCKLPSZ128rm,    X86::VPUNPCKLDQZ128rm },\n  { X86::VUNPCKLPSZ128rr,    X86::VUNPCKLPSZ128rr,    X86::VPUNPCKLDQZ128rr },\n  { X86::VUNPCKHPSZ128rm,    X86::VUNPCKHPSZ128rm,    X86::VPUNPCKHDQZ128rm },\n  { X86::VUNPCKHPSZ128rr,    X86::VUNPCKHPSZ128rr,    X86::VPUNPCKHDQZ128rr },\n  { X86::VUNPCKLPDZrm,       X86::VUNPCKLPDZrm,       X86::VPUNPCKLQDQZrm },\n  { X86::VUNPCKLPDZrr,       X86::VUNPCKLPDZrr,       X86::VPUNPCKLQDQZrr },\n  { X86::VUNPCKHPDZrm,       X86::VUNPCKHPDZrm,       X86::VPUNPCKHQDQZrm },\n  { X86::VUNPCKHPDZrr,       X86::VUNPCKHPDZrr,       X86::VPUNPCKHQDQZrr },\n  { X86::VUNPCKLPSZrm,       X86::VUNPCKLPSZrm,       X86::VPUNPCKLDQZrm },\n  { X86::VUNPCKLPSZrr,       X86::VUNPCKLPSZrr,       X86::VPUNPCKLDQZrr },\n  { X86::VUNPCKHPSZrm,       X86::VUNPCKHPSZrm,       X86::VPUNPCKHDQZrm },\n  { X86::VUNPCKHPSZrr,       X86::VUNPCKHPSZrr,       X86::VPUNPCKHDQZrr },\n  { X86::VEXTRACTPSZmr,      X86::VEXTRACTPSZmr,      X86::VPEXTRDZmr },\n  { X86::VEXTRACTPSZrr,      X86::VEXTRACTPSZrr,      X86::VPEXTRDZrr },\n};\n\nstatic const uint16_t ReplaceableInstrsAVX2[][3] = {\n  //PackedSingle       PackedDouble       PackedInt\n  { X86::VANDNPSYrm,   X86::VANDNPDYrm,   X86::VPANDNYrm   },\n  { X86::VANDNPSYrr,   X86::VANDNPDYrr,   X86::VPANDNYrr   },\n  { X86::VANDPSYrm,    X86::VANDPDYrm,    X86::VPANDYrm    },\n  { X86::VANDPSYrr,    X86::VANDPDYrr,    X86::VPANDYrr    },\n  { X86::VORPSYrm,     X86::VORPDYrm,     X86::VPORYrm     },\n  { X86::VORPSYrr,     X86::VORPDYrr,     X86::VPORYrr     },\n  { X86::VXORPSYrm,    X86::VXORPDYrm,    X86::VPXORYrm    },\n  { X86::VXORPSYrr,    X86::VXORPDYrr,    X86::VPXORYrr    },\n  { X86::VPERM2F128rm,   X86::VPERM2F128rm,   X86::VPERM2I128rm },\n  { X86::VPERM2F128rr,   X86::VPERM2F128rr,   X86::VPERM2I128rr },\n  { X86::VBROADCASTSSrm, X86::VBROADCASTSSrm, X86::VPBROADCASTDrm},\n  { X86::VBROADCASTSSrr, X86::VBROADCASTSSrr, X86::VPBROADCASTDrr},\n  { X86::VMOVDDUPrm,     X86::VMOVDDUPrm,     X86::VPBROADCASTQrm},\n  { X86::VMOVDDUPrr,     X86::VMOVDDUPrr,     X86::VPBROADCASTQrr},\n  { X86::VBROADCASTSSYrr, X86::VBROADCASTSSYrr, X86::VPBROADCASTDYrr},\n  { X86::VBROADCASTSSYrm, X86::VBROADCASTSSYrm, X86::VPBROADCASTDYrm},\n  { X86::VBROADCASTSDYrr, X86::VBROADCASTSDYrr, X86::VPBROADCASTQYrr},\n  { X86::VBROADCASTSDYrm, X86::VBROADCASTSDYrm, X86::VPBROADCASTQYrm},\n  { X86::VBROADCASTF128,  X86::VBROADCASTF128,  X86::VBROADCASTI128 },\n  { X86::VBLENDPSYrri,    X86::VBLENDPSYrri,    X86::VPBLENDDYrri },\n  { X86::VBLENDPSYrmi,    X86::VBLENDPSYrmi,    X86::VPBLENDDYrmi },\n  { X86::VPERMILPSYmi,    X86::VPERMILPSYmi,    X86::VPSHUFDYmi },\n  { X86::VPERMILPSYri,    X86::VPERMILPSYri,    X86::VPSHUFDYri },\n  { X86::VUNPCKLPDYrm,    X86::VUNPCKLPDYrm,    X86::VPUNPCKLQDQYrm },\n  { X86::VUNPCKLPDYrr,    X86::VUNPCKLPDYrr,    X86::VPUNPCKLQDQYrr },\n  { X86::VUNPCKHPDYrm,    X86::VUNPCKHPDYrm,    X86::VPUNPCKHQDQYrm },\n  { X86::VUNPCKHPDYrr,    X86::VUNPCKHPDYrr,    X86::VPUNPCKHQDQYrr },\n  { X86::VUNPCKLPSYrm,    X86::VUNPCKLPSYrm,    X86::VPUNPCKLDQYrm },\n  { X86::VUNPCKLPSYrr,    X86::VUNPCKLPSYrr,    X86::VPUNPCKLDQYrr },\n  { X86::VUNPCKHPSYrm,    X86::VUNPCKHPSYrm,    X86::VPUNPCKHDQYrm },\n  { X86::VUNPCKHPSYrr,    X86::VUNPCKHPSYrr,    X86::VPUNPCKHDQYrr },\n};\n\nstatic const uint16_t ReplaceableInstrsFP[][3] = {\n  //PackedSingle         PackedDouble\n  { X86::MOVLPSrm,       X86::MOVLPDrm,      X86::INSTRUCTION_LIST_END },\n  { X86::MOVHPSrm,       X86::MOVHPDrm,      X86::INSTRUCTION_LIST_END },\n  { X86::MOVHPSmr,       X86::MOVHPDmr,      X86::INSTRUCTION_LIST_END },\n  { X86::VMOVLPSrm,      X86::VMOVLPDrm,     X86::INSTRUCTION_LIST_END },\n  { X86::VMOVHPSrm,      X86::VMOVHPDrm,     X86::INSTRUCTION_LIST_END },\n  { X86::VMOVHPSmr,      X86::VMOVHPDmr,     X86::INSTRUCTION_LIST_END },\n  { X86::VMOVLPSZ128rm,  X86::VMOVLPDZ128rm, X86::INSTRUCTION_LIST_END },\n  { X86::VMOVHPSZ128rm,  X86::VMOVHPDZ128rm, X86::INSTRUCTION_LIST_END },\n  { X86::VMOVHPSZ128mr,  X86::VMOVHPDZ128mr, X86::INSTRUCTION_LIST_END },\n};\n\nstatic const uint16_t ReplaceableInstrsAVX2InsertExtract[][3] = {\n  //PackedSingle       PackedDouble       PackedInt\n  { X86::VEXTRACTF128mr, X86::VEXTRACTF128mr, X86::VEXTRACTI128mr },\n  { X86::VEXTRACTF128rr, X86::VEXTRACTF128rr, X86::VEXTRACTI128rr },\n  { X86::VINSERTF128rm,  X86::VINSERTF128rm,  X86::VINSERTI128rm },\n  { X86::VINSERTF128rr,  X86::VINSERTF128rr,  X86::VINSERTI128rr },\n};\n\nstatic const uint16_t ReplaceableInstrsAVX512[][4] = {\n  // Two integer columns for 64-bit and 32-bit elements.\n  //PackedSingle        PackedDouble        PackedInt             PackedInt\n  { X86::VMOVAPSZ128mr, X86::VMOVAPDZ128mr, X86::VMOVDQA64Z128mr, X86::VMOVDQA32Z128mr  },\n  { X86::VMOVAPSZ128rm, X86::VMOVAPDZ128rm, X86::VMOVDQA64Z128rm, X86::VMOVDQA32Z128rm  },\n  { X86::VMOVAPSZ128rr, X86::VMOVAPDZ128rr, X86::VMOVDQA64Z128rr, X86::VMOVDQA32Z128rr  },\n  { X86::VMOVUPSZ128mr, X86::VMOVUPDZ128mr, X86::VMOVDQU64Z128mr, X86::VMOVDQU32Z128mr  },\n  { X86::VMOVUPSZ128rm, X86::VMOVUPDZ128rm, X86::VMOVDQU64Z128rm, X86::VMOVDQU32Z128rm  },\n  { X86::VMOVAPSZ256mr, X86::VMOVAPDZ256mr, X86::VMOVDQA64Z256mr, X86::VMOVDQA32Z256mr  },\n  { X86::VMOVAPSZ256rm, X86::VMOVAPDZ256rm, X86::VMOVDQA64Z256rm, X86::VMOVDQA32Z256rm  },\n  { X86::VMOVAPSZ256rr, X86::VMOVAPDZ256rr, X86::VMOVDQA64Z256rr, X86::VMOVDQA32Z256rr  },\n  { X86::VMOVUPSZ256mr, X86::VMOVUPDZ256mr, X86::VMOVDQU64Z256mr, X86::VMOVDQU32Z256mr  },\n  { X86::VMOVUPSZ256rm, X86::VMOVUPDZ256rm, X86::VMOVDQU64Z256rm, X86::VMOVDQU32Z256rm  },\n  { X86::VMOVAPSZmr,    X86::VMOVAPDZmr,    X86::VMOVDQA64Zmr,    X86::VMOVDQA32Zmr     },\n  { X86::VMOVAPSZrm,    X86::VMOVAPDZrm,    X86::VMOVDQA64Zrm,    X86::VMOVDQA32Zrm     },\n  { X86::VMOVAPSZrr,    X86::VMOVAPDZrr,    X86::VMOVDQA64Zrr,    X86::VMOVDQA32Zrr     },\n  { X86::VMOVUPSZmr,    X86::VMOVUPDZmr,    X86::VMOVDQU64Zmr,    X86::VMOVDQU32Zmr     },\n  { X86::VMOVUPSZrm,    X86::VMOVUPDZrm,    X86::VMOVDQU64Zrm,    X86::VMOVDQU32Zrm     },\n};\n\nstatic const uint16_t ReplaceableInstrsAVX512DQ[][4] = {\n  // Two integer columns for 64-bit and 32-bit elements.\n  //PackedSingle        PackedDouble        PackedInt           PackedInt\n  { X86::VANDNPSZ128rm, X86::VANDNPDZ128rm, X86::VPANDNQZ128rm, X86::VPANDNDZ128rm },\n  { X86::VANDNPSZ128rr, X86::VANDNPDZ128rr, X86::VPANDNQZ128rr, X86::VPANDNDZ128rr },\n  { X86::VANDPSZ128rm,  X86::VANDPDZ128rm,  X86::VPANDQZ128rm,  X86::VPANDDZ128rm  },\n  { X86::VANDPSZ128rr,  X86::VANDPDZ128rr,  X86::VPANDQZ128rr,  X86::VPANDDZ128rr  },\n  { X86::VORPSZ128rm,   X86::VORPDZ128rm,   X86::VPORQZ128rm,   X86::VPORDZ128rm   },\n  { X86::VORPSZ128rr,   X86::VORPDZ128rr,   X86::VPORQZ128rr,   X86::VPORDZ128rr   },\n  { X86::VXORPSZ128rm,  X86::VXORPDZ128rm,  X86::VPXORQZ128rm,  X86::VPXORDZ128rm  },\n  { X86::VXORPSZ128rr,  X86::VXORPDZ128rr,  X86::VPXORQZ128rr,  X86::VPXORDZ128rr  },\n  { X86::VANDNPSZ256rm, X86::VANDNPDZ256rm, X86::VPANDNQZ256rm, X86::VPANDNDZ256rm },\n  { X86::VANDNPSZ256rr, X86::VANDNPDZ256rr, X86::VPANDNQZ256rr, X86::VPANDNDZ256rr },\n  { X86::VANDPSZ256rm,  X86::VANDPDZ256rm,  X86::VPANDQZ256rm,  X86::VPANDDZ256rm  },\n  { X86::VANDPSZ256rr,  X86::VANDPDZ256rr,  X86::VPANDQZ256rr,  X86::VPANDDZ256rr  },\n  { X86::VORPSZ256rm,   X86::VORPDZ256rm,   X86::VPORQZ256rm,   X86::VPORDZ256rm   },\n  { X86::VORPSZ256rr,   X86::VORPDZ256rr,   X86::VPORQZ256rr,   X86::VPORDZ256rr   },\n  { X86::VXORPSZ256rm,  X86::VXORPDZ256rm,  X86::VPXORQZ256rm,  X86::VPXORDZ256rm  },\n  { X86::VXORPSZ256rr,  X86::VXORPDZ256rr,  X86::VPXORQZ256rr,  X86::VPXORDZ256rr  },\n  { X86::VANDNPSZrm,    X86::VANDNPDZrm,    X86::VPANDNQZrm,    X86::VPANDNDZrm    },\n  { X86::VANDNPSZrr,    X86::VANDNPDZrr,    X86::VPANDNQZrr,    X86::VPANDNDZrr    },\n  { X86::VANDPSZrm,     X86::VANDPDZrm,     X86::VPANDQZrm,     X86::VPANDDZrm     },\n  { X86::VANDPSZrr,     X86::VANDPDZrr,     X86::VPANDQZrr,     X86::VPANDDZrr     },\n  { X86::VORPSZrm,      X86::VORPDZrm,      X86::VPORQZrm,      X86::VPORDZrm      },\n  { X86::VORPSZrr,      X86::VORPDZrr,      X86::VPORQZrr,      X86::VPORDZrr      },\n  { X86::VXORPSZrm,     X86::VXORPDZrm,     X86::VPXORQZrm,     X86::VPXORDZrm     },\n  { X86::VXORPSZrr,     X86::VXORPDZrr,     X86::VPXORQZrr,     X86::VPXORDZrr     },\n};\n\nstatic const uint16_t ReplaceableInstrsAVX512DQMasked[][4] = {\n  // Two integer columns for 64-bit and 32-bit elements.\n  //PackedSingle          PackedDouble\n  //PackedInt             PackedInt\n  { X86::VANDNPSZ128rmk,  X86::VANDNPDZ128rmk,\n    X86::VPANDNQZ128rmk,  X86::VPANDNDZ128rmk  },\n  { X86::VANDNPSZ128rmkz, X86::VANDNPDZ128rmkz,\n    X86::VPANDNQZ128rmkz, X86::VPANDNDZ128rmkz },\n  { X86::VANDNPSZ128rrk,  X86::VANDNPDZ128rrk,\n    X86::VPANDNQZ128rrk,  X86::VPANDNDZ128rrk  },\n  { X86::VANDNPSZ128rrkz, X86::VANDNPDZ128rrkz,\n    X86::VPANDNQZ128rrkz, X86::VPANDNDZ128rrkz },\n  { X86::VANDPSZ128rmk,   X86::VANDPDZ128rmk,\n    X86::VPANDQZ128rmk,   X86::VPANDDZ128rmk   },\n  { X86::VANDPSZ128rmkz,  X86::VANDPDZ128rmkz,\n    X86::VPANDQZ128rmkz,  X86::VPANDDZ128rmkz  },\n  { X86::VANDPSZ128rrk,   X86::VANDPDZ128rrk,\n    X86::VPANDQZ128rrk,   X86::VPANDDZ128rrk   },\n  { X86::VANDPSZ128rrkz,  X86::VANDPDZ128rrkz,\n    X86::VPANDQZ128rrkz,  X86::VPANDDZ128rrkz  },\n  { X86::VORPSZ128rmk,    X86::VORPDZ128rmk,\n    X86::VPORQZ128rmk,    X86::VPORDZ128rmk    },\n  { X86::VORPSZ128rmkz,   X86::VORPDZ128rmkz,\n    X86::VPORQZ128rmkz,   X86::VPORDZ128rmkz   },\n  { X86::VORPSZ128rrk,    X86::VORPDZ128rrk,\n    X86::VPORQZ128rrk,    X86::VPORDZ128rrk    },\n  { X86::VORPSZ128rrkz,   X86::VORPDZ128rrkz,\n    X86::VPORQZ128rrkz,   X86::VPORDZ128rrkz   },\n  { X86::VXORPSZ128rmk,   X86::VXORPDZ128rmk,\n    X86::VPXORQZ128rmk,   X86::VPXORDZ128rmk   },\n  { X86::VXORPSZ128rmkz,  X86::VXORPDZ128rmkz,\n    X86::VPXORQZ128rmkz,  X86::VPXORDZ128rmkz  },\n  { X86::VXORPSZ128rrk,   X86::VXORPDZ128rrk,\n    X86::VPXORQZ128rrk,   X86::VPXORDZ128rrk   },\n  { X86::VXORPSZ128rrkz,  X86::VXORPDZ128rrkz,\n    X86::VPXORQZ128rrkz,  X86::VPXORDZ128rrkz  },\n  { X86::VANDNPSZ256rmk,  X86::VANDNPDZ256rmk,\n    X86::VPANDNQZ256rmk,  X86::VPANDNDZ256rmk  },\n  { X86::VANDNPSZ256rmkz, X86::VANDNPDZ256rmkz,\n    X86::VPANDNQZ256rmkz, X86::VPANDNDZ256rmkz },\n  { X86::VANDNPSZ256rrk,  X86::VANDNPDZ256rrk,\n    X86::VPANDNQZ256rrk,  X86::VPANDNDZ256rrk  },\n  { X86::VANDNPSZ256rrkz, X86::VANDNPDZ256rrkz,\n    X86::VPANDNQZ256rrkz, X86::VPANDNDZ256rrkz },\n  { X86::VANDPSZ256rmk,   X86::VANDPDZ256rmk,\n    X86::VPANDQZ256rmk,   X86::VPANDDZ256rmk   },\n  { X86::VANDPSZ256rmkz,  X86::VANDPDZ256rmkz,\n    X86::VPANDQZ256rmkz,  X86::VPANDDZ256rmkz  },\n  { X86::VANDPSZ256rrk,   X86::VANDPDZ256rrk,\n    X86::VPANDQZ256rrk,   X86::VPANDDZ256rrk   },\n  { X86::VANDPSZ256rrkz,  X86::VANDPDZ256rrkz,\n    X86::VPANDQZ256rrkz,  X86::VPANDDZ256rrkz  },\n  { X86::VORPSZ256rmk,    X86::VORPDZ256rmk,\n    X86::VPORQZ256rmk,    X86::VPORDZ256rmk    },\n  { X86::VORPSZ256rmkz,   X86::VORPDZ256rmkz,\n    X86::VPORQZ256rmkz,   X86::VPORDZ256rmkz   },\n  { X86::VORPSZ256rrk,    X86::VORPDZ256rrk,\n    X86::VPORQZ256rrk,    X86::VPORDZ256rrk    },\n  { X86::VORPSZ256rrkz,   X86::VORPDZ256rrkz,\n    X86::VPORQZ256rrkz,   X86::VPORDZ256rrkz   },\n  { X86::VXORPSZ256rmk,   X86::VXORPDZ256rmk,\n    X86::VPXORQZ256rmk,   X86::VPXORDZ256rmk   },\n  { X86::VXORPSZ256rmkz,  X86::VXORPDZ256rmkz,\n    X86::VPXORQZ256rmkz,  X86::VPXORDZ256rmkz  },\n  { X86::VXORPSZ256rrk,   X86::VXORPDZ256rrk,\n    X86::VPXORQZ256rrk,   X86::VPXORDZ256rrk   },\n  { X86::VXORPSZ256rrkz,  X86::VXORPDZ256rrkz,\n    X86::VPXORQZ256rrkz,  X86::VPXORDZ256rrkz  },\n  { X86::VANDNPSZrmk,     X86::VANDNPDZrmk,\n    X86::VPANDNQZrmk,     X86::VPANDNDZrmk     },\n  { X86::VANDNPSZrmkz,    X86::VANDNPDZrmkz,\n    X86::VPANDNQZrmkz,    X86::VPANDNDZrmkz    },\n  { X86::VANDNPSZrrk,     X86::VANDNPDZrrk,\n    X86::VPANDNQZrrk,     X86::VPANDNDZrrk     },\n  { X86::VANDNPSZrrkz,    X86::VANDNPDZrrkz,\n    X86::VPANDNQZrrkz,    X86::VPANDNDZrrkz    },\n  { X86::VANDPSZrmk,      X86::VANDPDZrmk,\n    X86::VPANDQZrmk,      X86::VPANDDZrmk      },\n  { X86::VANDPSZrmkz,     X86::VANDPDZrmkz,\n    X86::VPANDQZrmkz,     X86::VPANDDZrmkz     },\n  { X86::VANDPSZrrk,      X86::VANDPDZrrk,\n    X86::VPANDQZrrk,      X86::VPANDDZrrk      },\n  { X86::VANDPSZrrkz,     X86::VANDPDZrrkz,\n    X86::VPANDQZrrkz,     X86::VPANDDZrrkz     },\n  { X86::VORPSZrmk,       X86::VORPDZrmk,\n    X86::VPORQZrmk,       X86::VPORDZrmk       },\n  { X86::VORPSZrmkz,      X86::VORPDZrmkz,\n    X86::VPORQZrmkz,      X86::VPORDZrmkz      },\n  { X86::VORPSZrrk,       X86::VORPDZrrk,\n    X86::VPORQZrrk,       X86::VPORDZrrk       },\n  { X86::VORPSZrrkz,      X86::VORPDZrrkz,\n    X86::VPORQZrrkz,      X86::VPORDZrrkz      },\n  { X86::VXORPSZrmk,      X86::VXORPDZrmk,\n    X86::VPXORQZrmk,      X86::VPXORDZrmk      },\n  { X86::VXORPSZrmkz,     X86::VXORPDZrmkz,\n    X86::VPXORQZrmkz,     X86::VPXORDZrmkz     },\n  { X86::VXORPSZrrk,      X86::VXORPDZrrk,\n    X86::VPXORQZrrk,      X86::VPXORDZrrk      },\n  { X86::VXORPSZrrkz,     X86::VXORPDZrrkz,\n    X86::VPXORQZrrkz,     X86::VPXORDZrrkz     },\n  // Broadcast loads can be handled the same as masked operations to avoid\n  // changing element size.\n  { X86::VANDNPSZ128rmb,  X86::VANDNPDZ128rmb,\n    X86::VPANDNQZ128rmb,  X86::VPANDNDZ128rmb  },\n  { X86::VANDPSZ128rmb,   X86::VANDPDZ128rmb,\n    X86::VPANDQZ128rmb,   X86::VPANDDZ128rmb   },\n  { X86::VORPSZ128rmb,    X86::VORPDZ128rmb,\n    X86::VPORQZ128rmb,    X86::VPORDZ128rmb    },\n  { X86::VXORPSZ128rmb,   X86::VXORPDZ128rmb,\n    X86::VPXORQZ128rmb,   X86::VPXORDZ128rmb   },\n  { X86::VANDNPSZ256rmb,  X86::VANDNPDZ256rmb,\n    X86::VPANDNQZ256rmb,  X86::VPANDNDZ256rmb  },\n  { X86::VANDPSZ256rmb,   X86::VANDPDZ256rmb,\n    X86::VPANDQZ256rmb,   X86::VPANDDZ256rmb   },\n  { X86::VORPSZ256rmb,    X86::VORPDZ256rmb,\n    X86::VPORQZ256rmb,    X86::VPORDZ256rmb    },\n  { X86::VXORPSZ256rmb,   X86::VXORPDZ256rmb,\n    X86::VPXORQZ256rmb,   X86::VPXORDZ256rmb   },\n  { X86::VANDNPSZrmb,     X86::VANDNPDZrmb,\n    X86::VPANDNQZrmb,     X86::VPANDNDZrmb     },\n  { X86::VANDPSZrmb,      X86::VANDPDZrmb,\n    X86::VPANDQZrmb,      X86::VPANDDZrmb      },\n  { X86::VANDPSZrmb,      X86::VANDPDZrmb,\n    X86::VPANDQZrmb,      X86::VPANDDZrmb      },\n  { X86::VORPSZrmb,       X86::VORPDZrmb,\n    X86::VPORQZrmb,       X86::VPORDZrmb       },\n  { X86::VXORPSZrmb,      X86::VXORPDZrmb,\n    X86::VPXORQZrmb,      X86::VPXORDZrmb      },\n  { X86::VANDNPSZ128rmbk, X86::VANDNPDZ128rmbk,\n    X86::VPANDNQZ128rmbk, X86::VPANDNDZ128rmbk },\n  { X86::VANDPSZ128rmbk,  X86::VANDPDZ128rmbk,\n    X86::VPANDQZ128rmbk,  X86::VPANDDZ128rmbk  },\n  { X86::VORPSZ128rmbk,   X86::VORPDZ128rmbk,\n    X86::VPORQZ128rmbk,   X86::VPORDZ128rmbk   },\n  { X86::VXORPSZ128rmbk,  X86::VXORPDZ128rmbk,\n    X86::VPXORQZ128rmbk,  X86::VPXORDZ128rmbk  },\n  { X86::VANDNPSZ256rmbk, X86::VANDNPDZ256rmbk,\n    X86::VPANDNQZ256rmbk, X86::VPANDNDZ256rmbk },\n  { X86::VANDPSZ256rmbk,  X86::VANDPDZ256rmbk,\n    X86::VPANDQZ256rmbk,  X86::VPANDDZ256rmbk  },\n  { X86::VORPSZ256rmbk,   X86::VORPDZ256rmbk,\n    X86::VPORQZ256rmbk,   X86::VPORDZ256rmbk   },\n  { X86::VXORPSZ256rmbk,  X86::VXORPDZ256rmbk,\n    X86::VPXORQZ256rmbk,  X86::VPXORDZ256rmbk  },\n  { X86::VANDNPSZrmbk,    X86::VANDNPDZrmbk,\n    X86::VPANDNQZrmbk,    X86::VPANDNDZrmbk    },\n  { X86::VANDPSZrmbk,     X86::VANDPDZrmbk,\n    X86::VPANDQZrmbk,     X86::VPANDDZrmbk     },\n  { X86::VANDPSZrmbk,     X86::VANDPDZrmbk,\n    X86::VPANDQZrmbk,     X86::VPANDDZrmbk     },\n  { X86::VORPSZrmbk,      X86::VORPDZrmbk,\n    X86::VPORQZrmbk,      X86::VPORDZrmbk      },\n  { X86::VXORPSZrmbk,     X86::VXORPDZrmbk,\n    X86::VPXORQZrmbk,     X86::VPXORDZrmbk     },\n  { X86::VANDNPSZ128rmbkz,X86::VANDNPDZ128rmbkz,\n    X86::VPANDNQZ128rmbkz,X86::VPANDNDZ128rmbkz},\n  { X86::VANDPSZ128rmbkz, X86::VANDPDZ128rmbkz,\n    X86::VPANDQZ128rmbkz, X86::VPANDDZ128rmbkz },\n  { X86::VORPSZ128rmbkz,  X86::VORPDZ128rmbkz,\n    X86::VPORQZ128rmbkz,  X86::VPORDZ128rmbkz  },\n  { X86::VXORPSZ128rmbkz, X86::VXORPDZ128rmbkz,\n    X86::VPXORQZ128rmbkz, X86::VPXORDZ128rmbkz },\n  { X86::VANDNPSZ256rmbkz,X86::VANDNPDZ256rmbkz,\n    X86::VPANDNQZ256rmbkz,X86::VPANDNDZ256rmbkz},\n  { X86::VANDPSZ256rmbkz, X86::VANDPDZ256rmbkz,\n    X86::VPANDQZ256rmbkz, X86::VPANDDZ256rmbkz },\n  { X86::VORPSZ256rmbkz,  X86::VORPDZ256rmbkz,\n    X86::VPORQZ256rmbkz,  X86::VPORDZ256rmbkz  },\n  { X86::VXORPSZ256rmbkz, X86::VXORPDZ256rmbkz,\n    X86::VPXORQZ256rmbkz, X86::VPXORDZ256rmbkz },\n  { X86::VANDNPSZrmbkz,   X86::VANDNPDZrmbkz,\n    X86::VPANDNQZrmbkz,   X86::VPANDNDZrmbkz   },\n  { X86::VANDPSZrmbkz,    X86::VANDPDZrmbkz,\n    X86::VPANDQZrmbkz,    X86::VPANDDZrmbkz    },\n  { X86::VANDPSZrmbkz,    X86::VANDPDZrmbkz,\n    X86::VPANDQZrmbkz,    X86::VPANDDZrmbkz    },\n  { X86::VORPSZrmbkz,     X86::VORPDZrmbkz,\n    X86::VPORQZrmbkz,     X86::VPORDZrmbkz     },\n  { X86::VXORPSZrmbkz,    X86::VXORPDZrmbkz,\n    X86::VPXORQZrmbkz,    X86::VPXORDZrmbkz    },\n};\n\n// NOTE: These should only be used by the custom domain methods.\nstatic const uint16_t ReplaceableBlendInstrs[][3] = {\n  //PackedSingle             PackedDouble             PackedInt\n  { X86::BLENDPSrmi,         X86::BLENDPDrmi,         X86::PBLENDWrmi   },\n  { X86::BLENDPSrri,         X86::BLENDPDrri,         X86::PBLENDWrri   },\n  { X86::VBLENDPSrmi,        X86::VBLENDPDrmi,        X86::VPBLENDWrmi  },\n  { X86::VBLENDPSrri,        X86::VBLENDPDrri,        X86::VPBLENDWrri  },\n  { X86::VBLENDPSYrmi,       X86::VBLENDPDYrmi,       X86::VPBLENDWYrmi },\n  { X86::VBLENDPSYrri,       X86::VBLENDPDYrri,       X86::VPBLENDWYrri },\n};\nstatic const uint16_t ReplaceableBlendAVX2Instrs[][3] = {\n  //PackedSingle             PackedDouble             PackedInt\n  { X86::VBLENDPSrmi,        X86::VBLENDPDrmi,        X86::VPBLENDDrmi  },\n  { X86::VBLENDPSrri,        X86::VBLENDPDrri,        X86::VPBLENDDrri  },\n  { X86::VBLENDPSYrmi,       X86::VBLENDPDYrmi,       X86::VPBLENDDYrmi },\n  { X86::VBLENDPSYrri,       X86::VBLENDPDYrri,       X86::VPBLENDDYrri },\n};\n\n// Special table for changing EVEX logic instructions to VEX.\n// TODO: Should we run EVEX->VEX earlier?\nstatic const uint16_t ReplaceableCustomAVX512LogicInstrs[][4] = {\n  // Two integer columns for 64-bit and 32-bit elements.\n  //PackedSingle     PackedDouble     PackedInt           PackedInt\n  { X86::VANDNPSrm,  X86::VANDNPDrm,  X86::VPANDNQZ128rm, X86::VPANDNDZ128rm },\n  { X86::VANDNPSrr,  X86::VANDNPDrr,  X86::VPANDNQZ128rr, X86::VPANDNDZ128rr },\n  { X86::VANDPSrm,   X86::VANDPDrm,   X86::VPANDQZ128rm,  X86::VPANDDZ128rm  },\n  { X86::VANDPSrr,   X86::VANDPDrr,   X86::VPANDQZ128rr,  X86::VPANDDZ128rr  },\n  { X86::VORPSrm,    X86::VORPDrm,    X86::VPORQZ128rm,   X86::VPORDZ128rm   },\n  { X86::VORPSrr,    X86::VORPDrr,    X86::VPORQZ128rr,   X86::VPORDZ128rr   },\n  { X86::VXORPSrm,   X86::VXORPDrm,   X86::VPXORQZ128rm,  X86::VPXORDZ128rm  },\n  { X86::VXORPSrr,   X86::VXORPDrr,   X86::VPXORQZ128rr,  X86::VPXORDZ128rr  },\n  { X86::VANDNPSYrm, X86::VANDNPDYrm, X86::VPANDNQZ256rm, X86::VPANDNDZ256rm },\n  { X86::VANDNPSYrr, X86::VANDNPDYrr, X86::VPANDNQZ256rr, X86::VPANDNDZ256rr },\n  { X86::VANDPSYrm,  X86::VANDPDYrm,  X86::VPANDQZ256rm,  X86::VPANDDZ256rm  },\n  { X86::VANDPSYrr,  X86::VANDPDYrr,  X86::VPANDQZ256rr,  X86::VPANDDZ256rr  },\n  { X86::VORPSYrm,   X86::VORPDYrm,   X86::VPORQZ256rm,   X86::VPORDZ256rm   },\n  { X86::VORPSYrr,   X86::VORPDYrr,   X86::VPORQZ256rr,   X86::VPORDZ256rr   },\n  { X86::VXORPSYrm,  X86::VXORPDYrm,  X86::VPXORQZ256rm,  X86::VPXORDZ256rm  },\n  { X86::VXORPSYrr,  X86::VXORPDYrr,  X86::VPXORQZ256rr,  X86::VPXORDZ256rr  },\n};\n\n// FIXME: Some shuffle and unpack instructions have equivalents in different\n// domains, but they require a bit more work than just switching opcodes.\n\nstatic const uint16_t *lookup(unsigned opcode, unsigned domain,\n                              ArrayRef<uint16_t[3]> Table) {\n  for (const uint16_t (&Row)[3] : Table)\n    if (Row[domain-1] == opcode)\n      return Row;\n  return nullptr;\n}\n\nstatic const uint16_t *lookupAVX512(unsigned opcode, unsigned domain,\n                                    ArrayRef<uint16_t[4]> Table) {\n  // If this is the integer domain make sure to check both integer columns.\n  for (const uint16_t (&Row)[4] : Table)\n    if (Row[domain-1] == opcode || (domain == 3 && Row[3] == opcode))\n      return Row;\n  return nullptr;\n}\n\n// Helper to attempt to widen/narrow blend masks.\nstatic bool AdjustBlendMask(unsigned OldMask, unsigned OldWidth,\n                            unsigned NewWidth, unsigned *pNewMask = nullptr) {\n  assert(((OldWidth % NewWidth) == 0 || (NewWidth % OldWidth) == 0) &&\n         \"Illegal blend mask scale\");\n  unsigned NewMask = 0;\n\n  if ((OldWidth % NewWidth) == 0) {\n    unsigned Scale = OldWidth / NewWidth;\n    unsigned SubMask = (1u << Scale) - 1;\n    for (unsigned i = 0; i != NewWidth; ++i) {\n      unsigned Sub = (OldMask >> (i * Scale)) & SubMask;\n      if (Sub == SubMask)\n        NewMask |= (1u << i);\n      else if (Sub != 0x0)\n        return false;\n    }\n  } else {\n    unsigned Scale = NewWidth / OldWidth;\n    unsigned SubMask = (1u << Scale) - 1;\n    for (unsigned i = 0; i != OldWidth; ++i) {\n      if (OldMask & (1 << i)) {\n        NewMask |= (SubMask << (i * Scale));\n      }\n    }\n  }\n\n  if (pNewMask)\n    *pNewMask = NewMask;\n  return true;\n}\n\nuint16_t X86InstrInfo::getExecutionDomainCustom(const MachineInstr &MI) const {\n  unsigned Opcode = MI.getOpcode();\n  unsigned NumOperands = MI.getDesc().getNumOperands();\n\n  auto GetBlendDomains = [&](unsigned ImmWidth, bool Is256) {\n    uint16_t validDomains = 0;\n    if (MI.getOperand(NumOperands - 1).isImm()) {\n      unsigned Imm = MI.getOperand(NumOperands - 1).getImm();\n      if (AdjustBlendMask(Imm, ImmWidth, Is256 ? 8 : 4))\n        validDomains |= 0x2; // PackedSingle\n      if (AdjustBlendMask(Imm, ImmWidth, Is256 ? 4 : 2))\n        validDomains |= 0x4; // PackedDouble\n      if (!Is256 || Subtarget.hasAVX2())\n        validDomains |= 0x8; // PackedInt\n    }\n    return validDomains;\n  };\n\n  switch (Opcode) {\n  case X86::BLENDPDrmi:\n  case X86::BLENDPDrri:\n  case X86::VBLENDPDrmi:\n  case X86::VBLENDPDrri:\n    return GetBlendDomains(2, false);\n  case X86::VBLENDPDYrmi:\n  case X86::VBLENDPDYrri:\n    return GetBlendDomains(4, true);\n  case X86::BLENDPSrmi:\n  case X86::BLENDPSrri:\n  case X86::VBLENDPSrmi:\n  case X86::VBLENDPSrri:\n  case X86::VPBLENDDrmi:\n  case X86::VPBLENDDrri:\n    return GetBlendDomains(4, false);\n  case X86::VBLENDPSYrmi:\n  case X86::VBLENDPSYrri:\n  case X86::VPBLENDDYrmi:\n  case X86::VPBLENDDYrri:\n    return GetBlendDomains(8, true);\n  case X86::PBLENDWrmi:\n  case X86::PBLENDWrri:\n  case X86::VPBLENDWrmi:\n  case X86::VPBLENDWrri:\n  // Treat VPBLENDWY as a 128-bit vector as it repeats the lo/hi masks.\n  case X86::VPBLENDWYrmi:\n  case X86::VPBLENDWYrri:\n    return GetBlendDomains(8, false);\n  case X86::VPANDDZ128rr:  case X86::VPANDDZ128rm:\n  case X86::VPANDDZ256rr:  case X86::VPANDDZ256rm:\n  case X86::VPANDQZ128rr:  case X86::VPANDQZ128rm:\n  case X86::VPANDQZ256rr:  case X86::VPANDQZ256rm:\n  case X86::VPANDNDZ128rr: case X86::VPANDNDZ128rm:\n  case X86::VPANDNDZ256rr: case X86::VPANDNDZ256rm:\n  case X86::VPANDNQZ128rr: case X86::VPANDNQZ128rm:\n  case X86::VPANDNQZ256rr: case X86::VPANDNQZ256rm:\n  case X86::VPORDZ128rr:   case X86::VPORDZ128rm:\n  case X86::VPORDZ256rr:   case X86::VPORDZ256rm:\n  case X86::VPORQZ128rr:   case X86::VPORQZ128rm:\n  case X86::VPORQZ256rr:   case X86::VPORQZ256rm:\n  case X86::VPXORDZ128rr:  case X86::VPXORDZ128rm:\n  case X86::VPXORDZ256rr:  case X86::VPXORDZ256rm:\n  case X86::VPXORQZ128rr:  case X86::VPXORQZ128rm:\n  case X86::VPXORQZ256rr:  case X86::VPXORQZ256rm:\n    // If we don't have DQI see if we can still switch from an EVEX integer\n    // instruction to a VEX floating point instruction.\n    if (Subtarget.hasDQI())\n      return 0;\n\n    if (RI.getEncodingValue(MI.getOperand(0).getReg()) >= 16)\n      return 0;\n    if (RI.getEncodingValue(MI.getOperand(1).getReg()) >= 16)\n      return 0;\n    // Register forms will have 3 operands. Memory form will have more.\n    if (NumOperands == 3 &&\n        RI.getEncodingValue(MI.getOperand(2).getReg()) >= 16)\n      return 0;\n\n    // All domains are valid.\n    return 0xe;\n  case X86::MOVHLPSrr:\n    // We can swap domains when both inputs are the same register.\n    // FIXME: This doesn't catch all the cases we would like. If the input\n    // register isn't KILLed by the instruction, the two address instruction\n    // pass puts a COPY on one input. The other input uses the original\n    // register. This prevents the same physical register from being used by\n    // both inputs.\n    if (MI.getOperand(1).getReg() == MI.getOperand(2).getReg() &&\n        MI.getOperand(0).getSubReg() == 0 &&\n        MI.getOperand(1).getSubReg() == 0 &&\n        MI.getOperand(2).getSubReg() == 0)\n      return 0x6;\n    return 0;\n  case X86::SHUFPDrri:\n    return 0x6;\n  }\n  return 0;\n}\n\nbool X86InstrInfo::setExecutionDomainCustom(MachineInstr &MI,\n                                            unsigned Domain) const {\n  assert(Domain > 0 && Domain < 4 && \"Invalid execution domain\");\n  uint16_t dom = (MI.getDesc().TSFlags >> X86II::SSEDomainShift) & 3;\n  assert(dom && \"Not an SSE instruction\");\n\n  unsigned Opcode = MI.getOpcode();\n  unsigned NumOperands = MI.getDesc().getNumOperands();\n\n  auto SetBlendDomain = [&](unsigned ImmWidth, bool Is256) {\n    if (MI.getOperand(NumOperands - 1).isImm()) {\n      unsigned Imm = MI.getOperand(NumOperands - 1).getImm() & 255;\n      Imm = (ImmWidth == 16 ? ((Imm << 8) | Imm) : Imm);\n      unsigned NewImm = Imm;\n\n      const uint16_t *table = lookup(Opcode, dom, ReplaceableBlendInstrs);\n      if (!table)\n        table = lookup(Opcode, dom, ReplaceableBlendAVX2Instrs);\n\n      if (Domain == 1) { // PackedSingle\n        AdjustBlendMask(Imm, ImmWidth, Is256 ? 8 : 4, &NewImm);\n      } else if (Domain == 2) { // PackedDouble\n        AdjustBlendMask(Imm, ImmWidth, Is256 ? 4 : 2, &NewImm);\n      } else if (Domain == 3) { // PackedInt\n        if (Subtarget.hasAVX2()) {\n          // If we are already VPBLENDW use that, else use VPBLENDD.\n          if ((ImmWidth / (Is256 ? 2 : 1)) != 8) {\n            table = lookup(Opcode, dom, ReplaceableBlendAVX2Instrs);\n            AdjustBlendMask(Imm, ImmWidth, Is256 ? 8 : 4, &NewImm);\n          }\n        } else {\n          assert(!Is256 && \"128-bit vector expected\");\n          AdjustBlendMask(Imm, ImmWidth, 8, &NewImm);\n        }\n      }\n\n      assert(table && table[Domain - 1] && \"Unknown domain op\");\n      MI.setDesc(get(table[Domain - 1]));\n      MI.getOperand(NumOperands - 1).setImm(NewImm & 255);\n    }\n    return true;\n  };\n\n  switch (Opcode) {\n  case X86::BLENDPDrmi:\n  case X86::BLENDPDrri:\n  case X86::VBLENDPDrmi:\n  case X86::VBLENDPDrri:\n    return SetBlendDomain(2, false);\n  case X86::VBLENDPDYrmi:\n  case X86::VBLENDPDYrri:\n    return SetBlendDomain(4, true);\n  case X86::BLENDPSrmi:\n  case X86::BLENDPSrri:\n  case X86::VBLENDPSrmi:\n  case X86::VBLENDPSrri:\n  case X86::VPBLENDDrmi:\n  case X86::VPBLENDDrri:\n    return SetBlendDomain(4, false);\n  case X86::VBLENDPSYrmi:\n  case X86::VBLENDPSYrri:\n  case X86::VPBLENDDYrmi:\n  case X86::VPBLENDDYrri:\n    return SetBlendDomain(8, true);\n  case X86::PBLENDWrmi:\n  case X86::PBLENDWrri:\n  case X86::VPBLENDWrmi:\n  case X86::VPBLENDWrri:\n    return SetBlendDomain(8, false);\n  case X86::VPBLENDWYrmi:\n  case X86::VPBLENDWYrri:\n    return SetBlendDomain(16, true);\n  case X86::VPANDDZ128rr:  case X86::VPANDDZ128rm:\n  case X86::VPANDDZ256rr:  case X86::VPANDDZ256rm:\n  case X86::VPANDQZ128rr:  case X86::VPANDQZ128rm:\n  case X86::VPANDQZ256rr:  case X86::VPANDQZ256rm:\n  case X86::VPANDNDZ128rr: case X86::VPANDNDZ128rm:\n  case X86::VPANDNDZ256rr: case X86::VPANDNDZ256rm:\n  case X86::VPANDNQZ128rr: case X86::VPANDNQZ128rm:\n  case X86::VPANDNQZ256rr: case X86::VPANDNQZ256rm:\n  case X86::VPORDZ128rr:   case X86::VPORDZ128rm:\n  case X86::VPORDZ256rr:   case X86::VPORDZ256rm:\n  case X86::VPORQZ128rr:   case X86::VPORQZ128rm:\n  case X86::VPORQZ256rr:   case X86::VPORQZ256rm:\n  case X86::VPXORDZ128rr:  case X86::VPXORDZ128rm:\n  case X86::VPXORDZ256rr:  case X86::VPXORDZ256rm:\n  case X86::VPXORQZ128rr:  case X86::VPXORQZ128rm:\n  case X86::VPXORQZ256rr:  case X86::VPXORQZ256rm: {\n    // Without DQI, convert EVEX instructions to VEX instructions.\n    if (Subtarget.hasDQI())\n      return false;\n\n    const uint16_t *table = lookupAVX512(MI.getOpcode(), dom,\n                                         ReplaceableCustomAVX512LogicInstrs);\n    assert(table && \"Instruction not found in table?\");\n    // Don't change integer Q instructions to D instructions and\n    // use D intructions if we started with a PS instruction.\n    if (Domain == 3 && (dom == 1 || table[3] == MI.getOpcode()))\n      Domain = 4;\n    MI.setDesc(get(table[Domain - 1]));\n    return true;\n  }\n  case X86::UNPCKHPDrr:\n  case X86::MOVHLPSrr:\n    // We just need to commute the instruction which will switch the domains.\n    if (Domain != dom && Domain != 3 &&\n        MI.getOperand(1).getReg() == MI.getOperand(2).getReg() &&\n        MI.getOperand(0).getSubReg() == 0 &&\n        MI.getOperand(1).getSubReg() == 0 &&\n        MI.getOperand(2).getSubReg() == 0) {\n      commuteInstruction(MI, false);\n      return true;\n    }\n    // We must always return true for MOVHLPSrr.\n    if (Opcode == X86::MOVHLPSrr)\n      return true;\n    break;\n  case X86::SHUFPDrri: {\n    if (Domain == 1) {\n      unsigned Imm = MI.getOperand(3).getImm();\n      unsigned NewImm = 0x44;\n      if (Imm & 1) NewImm |= 0x0a;\n      if (Imm & 2) NewImm |= 0xa0;\n      MI.getOperand(3).setImm(NewImm);\n      MI.setDesc(get(X86::SHUFPSrri));\n    }\n    return true;\n  }\n  }\n  return false;\n}\n\nstd::pair<uint16_t, uint16_t>\nX86InstrInfo::getExecutionDomain(const MachineInstr &MI) const {\n  uint16_t domain = (MI.getDesc().TSFlags >> X86II::SSEDomainShift) & 3;\n  unsigned opcode = MI.getOpcode();\n  uint16_t validDomains = 0;\n  if (domain) {\n    // Attempt to match for custom instructions.\n    validDomains = getExecutionDomainCustom(MI);\n    if (validDomains)\n      return std::make_pair(domain, validDomains);\n\n    if (lookup(opcode, domain, ReplaceableInstrs)) {\n      validDomains = 0xe;\n    } else if (lookup(opcode, domain, ReplaceableInstrsAVX2)) {\n      validDomains = Subtarget.hasAVX2() ? 0xe : 0x6;\n    } else if (lookup(opcode, domain, ReplaceableInstrsFP)) {\n      validDomains = 0x6;\n    } else if (lookup(opcode, domain, ReplaceableInstrsAVX2InsertExtract)) {\n      // Insert/extract instructions should only effect domain if AVX2\n      // is enabled.\n      if (!Subtarget.hasAVX2())\n        return std::make_pair(0, 0);\n      validDomains = 0xe;\n    } else if (lookupAVX512(opcode, domain, ReplaceableInstrsAVX512)) {\n      validDomains = 0xe;\n    } else if (Subtarget.hasDQI() && lookupAVX512(opcode, domain,\n                                                  ReplaceableInstrsAVX512DQ)) {\n      validDomains = 0xe;\n    } else if (Subtarget.hasDQI()) {\n      if (const uint16_t *table = lookupAVX512(opcode, domain,\n                                             ReplaceableInstrsAVX512DQMasked)) {\n        if (domain == 1 || (domain == 3 && table[3] == opcode))\n          validDomains = 0xa;\n        else\n          validDomains = 0xc;\n      }\n    }\n  }\n  return std::make_pair(domain, validDomains);\n}\n\nvoid X86InstrInfo::setExecutionDomain(MachineInstr &MI, unsigned Domain) const {\n  assert(Domain>0 && Domain<4 && \"Invalid execution domain\");\n  uint16_t dom = (MI.getDesc().TSFlags >> X86II::SSEDomainShift) & 3;\n  assert(dom && \"Not an SSE instruction\");\n\n  // Attempt to match for custom instructions.\n  if (setExecutionDomainCustom(MI, Domain))\n    return;\n\n  const uint16_t *table = lookup(MI.getOpcode(), dom, ReplaceableInstrs);\n  if (!table) { // try the other table\n    assert((Subtarget.hasAVX2() || Domain < 3) &&\n           \"256-bit vector operations only available in AVX2\");\n    table = lookup(MI.getOpcode(), dom, ReplaceableInstrsAVX2);\n  }\n  if (!table) { // try the FP table\n    table = lookup(MI.getOpcode(), dom, ReplaceableInstrsFP);\n    assert((!table || Domain < 3) &&\n           \"Can only select PackedSingle or PackedDouble\");\n  }\n  if (!table) { // try the other table\n    assert(Subtarget.hasAVX2() &&\n           \"256-bit insert/extract only available in AVX2\");\n    table = lookup(MI.getOpcode(), dom, ReplaceableInstrsAVX2InsertExtract);\n  }\n  if (!table) { // try the AVX512 table\n    assert(Subtarget.hasAVX512() && \"Requires AVX-512\");\n    table = lookupAVX512(MI.getOpcode(), dom, ReplaceableInstrsAVX512);\n    // Don't change integer Q instructions to D instructions.\n    if (table && Domain == 3 && table[3] == MI.getOpcode())\n      Domain = 4;\n  }\n  if (!table) { // try the AVX512DQ table\n    assert((Subtarget.hasDQI() || Domain >= 3) && \"Requires AVX-512DQ\");\n    table = lookupAVX512(MI.getOpcode(), dom, ReplaceableInstrsAVX512DQ);\n    // Don't change integer Q instructions to D instructions and\n    // use D instructions if we started with a PS instruction.\n    if (table && Domain == 3 && (dom == 1 || table[3] == MI.getOpcode()))\n      Domain = 4;\n  }\n  if (!table) { // try the AVX512DQMasked table\n    assert((Subtarget.hasDQI() || Domain >= 3) && \"Requires AVX-512DQ\");\n    table = lookupAVX512(MI.getOpcode(), dom, ReplaceableInstrsAVX512DQMasked);\n    if (table && Domain == 3 && (dom == 1 || table[3] == MI.getOpcode()))\n      Domain = 4;\n  }\n  assert(table && \"Cannot change domain\");\n  MI.setDesc(get(table[Domain - 1]));\n}\n\n/// Return the noop instruction to use for a noop.\nvoid X86InstrInfo::getNoop(MCInst &NopInst) const {\n  NopInst.setOpcode(X86::NOOP);\n}\n\nbool X86InstrInfo::isHighLatencyDef(int opc) const {\n  switch (opc) {\n  default: return false;\n  case X86::DIVPDrm:\n  case X86::DIVPDrr:\n  case X86::DIVPSrm:\n  case X86::DIVPSrr:\n  case X86::DIVSDrm:\n  case X86::DIVSDrm_Int:\n  case X86::DIVSDrr:\n  case X86::DIVSDrr_Int:\n  case X86::DIVSSrm:\n  case X86::DIVSSrm_Int:\n  case X86::DIVSSrr:\n  case X86::DIVSSrr_Int:\n  case X86::SQRTPDm:\n  case X86::SQRTPDr:\n  case X86::SQRTPSm:\n  case X86::SQRTPSr:\n  case X86::SQRTSDm:\n  case X86::SQRTSDm_Int:\n  case X86::SQRTSDr:\n  case X86::SQRTSDr_Int:\n  case X86::SQRTSSm:\n  case X86::SQRTSSm_Int:\n  case X86::SQRTSSr:\n  case X86::SQRTSSr_Int:\n  // AVX instructions with high latency\n  case X86::VDIVPDrm:\n  case X86::VDIVPDrr:\n  case X86::VDIVPDYrm:\n  case X86::VDIVPDYrr:\n  case X86::VDIVPSrm:\n  case X86::VDIVPSrr:\n  case X86::VDIVPSYrm:\n  case X86::VDIVPSYrr:\n  case X86::VDIVSDrm:\n  case X86::VDIVSDrm_Int:\n  case X86::VDIVSDrr:\n  case X86::VDIVSDrr_Int:\n  case X86::VDIVSSrm:\n  case X86::VDIVSSrm_Int:\n  case X86::VDIVSSrr:\n  case X86::VDIVSSrr_Int:\n  case X86::VSQRTPDm:\n  case X86::VSQRTPDr:\n  case X86::VSQRTPDYm:\n  case X86::VSQRTPDYr:\n  case X86::VSQRTPSm:\n  case X86::VSQRTPSr:\n  case X86::VSQRTPSYm:\n  case X86::VSQRTPSYr:\n  case X86::VSQRTSDm:\n  case X86::VSQRTSDm_Int:\n  case X86::VSQRTSDr:\n  case X86::VSQRTSDr_Int:\n  case X86::VSQRTSSm:\n  case X86::VSQRTSSm_Int:\n  case X86::VSQRTSSr:\n  case X86::VSQRTSSr_Int:\n  // AVX512 instructions with high latency\n  case X86::VDIVPDZ128rm:\n  case X86::VDIVPDZ128rmb:\n  case X86::VDIVPDZ128rmbk:\n  case X86::VDIVPDZ128rmbkz:\n  case X86::VDIVPDZ128rmk:\n  case X86::VDIVPDZ128rmkz:\n  case X86::VDIVPDZ128rr:\n  case X86::VDIVPDZ128rrk:\n  case X86::VDIVPDZ128rrkz:\n  case X86::VDIVPDZ256rm:\n  case X86::VDIVPDZ256rmb:\n  case X86::VDIVPDZ256rmbk:\n  case X86::VDIVPDZ256rmbkz:\n  case X86::VDIVPDZ256rmk:\n  case X86::VDIVPDZ256rmkz:\n  case X86::VDIVPDZ256rr:\n  case X86::VDIVPDZ256rrk:\n  case X86::VDIVPDZ256rrkz:\n  case X86::VDIVPDZrrb:\n  case X86::VDIVPDZrrbk:\n  case X86::VDIVPDZrrbkz:\n  case X86::VDIVPDZrm:\n  case X86::VDIVPDZrmb:\n  case X86::VDIVPDZrmbk:\n  case X86::VDIVPDZrmbkz:\n  case X86::VDIVPDZrmk:\n  case X86::VDIVPDZrmkz:\n  case X86::VDIVPDZrr:\n  case X86::VDIVPDZrrk:\n  case X86::VDIVPDZrrkz:\n  case X86::VDIVPSZ128rm:\n  case X86::VDIVPSZ128rmb:\n  case X86::VDIVPSZ128rmbk:\n  case X86::VDIVPSZ128rmbkz:\n  case X86::VDIVPSZ128rmk:\n  case X86::VDIVPSZ128rmkz:\n  case X86::VDIVPSZ128rr:\n  case X86::VDIVPSZ128rrk:\n  case X86::VDIVPSZ128rrkz:\n  case X86::VDIVPSZ256rm:\n  case X86::VDIVPSZ256rmb:\n  case X86::VDIVPSZ256rmbk:\n  case X86::VDIVPSZ256rmbkz:\n  case X86::VDIVPSZ256rmk:\n  case X86::VDIVPSZ256rmkz:\n  case X86::VDIVPSZ256rr:\n  case X86::VDIVPSZ256rrk:\n  case X86::VDIVPSZ256rrkz:\n  case X86::VDIVPSZrrb:\n  case X86::VDIVPSZrrbk:\n  case X86::VDIVPSZrrbkz:\n  case X86::VDIVPSZrm:\n  case X86::VDIVPSZrmb:\n  case X86::VDIVPSZrmbk:\n  case X86::VDIVPSZrmbkz:\n  case X86::VDIVPSZrmk:\n  case X86::VDIVPSZrmkz:\n  case X86::VDIVPSZrr:\n  case X86::VDIVPSZrrk:\n  case X86::VDIVPSZrrkz:\n  case X86::VDIVSDZrm:\n  case X86::VDIVSDZrr:\n  case X86::VDIVSDZrm_Int:\n  case X86::VDIVSDZrm_Intk:\n  case X86::VDIVSDZrm_Intkz:\n  case X86::VDIVSDZrr_Int:\n  case X86::VDIVSDZrr_Intk:\n  case X86::VDIVSDZrr_Intkz:\n  case X86::VDIVSDZrrb_Int:\n  case X86::VDIVSDZrrb_Intk:\n  case X86::VDIVSDZrrb_Intkz:\n  case X86::VDIVSSZrm:\n  case X86::VDIVSSZrr:\n  case X86::VDIVSSZrm_Int:\n  case X86::VDIVSSZrm_Intk:\n  case X86::VDIVSSZrm_Intkz:\n  case X86::VDIVSSZrr_Int:\n  case X86::VDIVSSZrr_Intk:\n  case X86::VDIVSSZrr_Intkz:\n  case X86::VDIVSSZrrb_Int:\n  case X86::VDIVSSZrrb_Intk:\n  case X86::VDIVSSZrrb_Intkz:\n  case X86::VSQRTPDZ128m:\n  case X86::VSQRTPDZ128mb:\n  case X86::VSQRTPDZ128mbk:\n  case X86::VSQRTPDZ128mbkz:\n  case X86::VSQRTPDZ128mk:\n  case X86::VSQRTPDZ128mkz:\n  case X86::VSQRTPDZ128r:\n  case X86::VSQRTPDZ128rk:\n  case X86::VSQRTPDZ128rkz:\n  case X86::VSQRTPDZ256m:\n  case X86::VSQRTPDZ256mb:\n  case X86::VSQRTPDZ256mbk:\n  case X86::VSQRTPDZ256mbkz:\n  case X86::VSQRTPDZ256mk:\n  case X86::VSQRTPDZ256mkz:\n  case X86::VSQRTPDZ256r:\n  case X86::VSQRTPDZ256rk:\n  case X86::VSQRTPDZ256rkz:\n  case X86::VSQRTPDZm:\n  case X86::VSQRTPDZmb:\n  case X86::VSQRTPDZmbk:\n  case X86::VSQRTPDZmbkz:\n  case X86::VSQRTPDZmk:\n  case X86::VSQRTPDZmkz:\n  case X86::VSQRTPDZr:\n  case X86::VSQRTPDZrb:\n  case X86::VSQRTPDZrbk:\n  case X86::VSQRTPDZrbkz:\n  case X86::VSQRTPDZrk:\n  case X86::VSQRTPDZrkz:\n  case X86::VSQRTPSZ128m:\n  case X86::VSQRTPSZ128mb:\n  case X86::VSQRTPSZ128mbk:\n  case X86::VSQRTPSZ128mbkz:\n  case X86::VSQRTPSZ128mk:\n  case X86::VSQRTPSZ128mkz:\n  case X86::VSQRTPSZ128r:\n  case X86::VSQRTPSZ128rk:\n  case X86::VSQRTPSZ128rkz:\n  case X86::VSQRTPSZ256m:\n  case X86::VSQRTPSZ256mb:\n  case X86::VSQRTPSZ256mbk:\n  case X86::VSQRTPSZ256mbkz:\n  case X86::VSQRTPSZ256mk:\n  case X86::VSQRTPSZ256mkz:\n  case X86::VSQRTPSZ256r:\n  case X86::VSQRTPSZ256rk:\n  case X86::VSQRTPSZ256rkz:\n  case X86::VSQRTPSZm:\n  case X86::VSQRTPSZmb:\n  case X86::VSQRTPSZmbk:\n  case X86::VSQRTPSZmbkz:\n  case X86::VSQRTPSZmk:\n  case X86::VSQRTPSZmkz:\n  case X86::VSQRTPSZr:\n  case X86::VSQRTPSZrb:\n  case X86::VSQRTPSZrbk:\n  case X86::VSQRTPSZrbkz:\n  case X86::VSQRTPSZrk:\n  case X86::VSQRTPSZrkz:\n  case X86::VSQRTSDZm:\n  case X86::VSQRTSDZm_Int:\n  case X86::VSQRTSDZm_Intk:\n  case X86::VSQRTSDZm_Intkz:\n  case X86::VSQRTSDZr:\n  case X86::VSQRTSDZr_Int:\n  case X86::VSQRTSDZr_Intk:\n  case X86::VSQRTSDZr_Intkz:\n  case X86::VSQRTSDZrb_Int:\n  case X86::VSQRTSDZrb_Intk:\n  case X86::VSQRTSDZrb_Intkz:\n  case X86::VSQRTSSZm:\n  case X86::VSQRTSSZm_Int:\n  case X86::VSQRTSSZm_Intk:\n  case X86::VSQRTSSZm_Intkz:\n  case X86::VSQRTSSZr:\n  case X86::VSQRTSSZr_Int:\n  case X86::VSQRTSSZr_Intk:\n  case X86::VSQRTSSZr_Intkz:\n  case X86::VSQRTSSZrb_Int:\n  case X86::VSQRTSSZrb_Intk:\n  case X86::VSQRTSSZrb_Intkz:\n\n  case X86::VGATHERDPDYrm:\n  case X86::VGATHERDPDZ128rm:\n  case X86::VGATHERDPDZ256rm:\n  case X86::VGATHERDPDZrm:\n  case X86::VGATHERDPDrm:\n  case X86::VGATHERDPSYrm:\n  case X86::VGATHERDPSZ128rm:\n  case X86::VGATHERDPSZ256rm:\n  case X86::VGATHERDPSZrm:\n  case X86::VGATHERDPSrm:\n  case X86::VGATHERPF0DPDm:\n  case X86::VGATHERPF0DPSm:\n  case X86::VGATHERPF0QPDm:\n  case X86::VGATHERPF0QPSm:\n  case X86::VGATHERPF1DPDm:\n  case X86::VGATHERPF1DPSm:\n  case X86::VGATHERPF1QPDm:\n  case X86::VGATHERPF1QPSm:\n  case X86::VGATHERQPDYrm:\n  case X86::VGATHERQPDZ128rm:\n  case X86::VGATHERQPDZ256rm:\n  case X86::VGATHERQPDZrm:\n  case X86::VGATHERQPDrm:\n  case X86::VGATHERQPSYrm:\n  case X86::VGATHERQPSZ128rm:\n  case X86::VGATHERQPSZ256rm:\n  case X86::VGATHERQPSZrm:\n  case X86::VGATHERQPSrm:\n  case X86::VPGATHERDDYrm:\n  case X86::VPGATHERDDZ128rm:\n  case X86::VPGATHERDDZ256rm:\n  case X86::VPGATHERDDZrm:\n  case X86::VPGATHERDDrm:\n  case X86::VPGATHERDQYrm:\n  case X86::VPGATHERDQZ128rm:\n  case X86::VPGATHERDQZ256rm:\n  case X86::VPGATHERDQZrm:\n  case X86::VPGATHERDQrm:\n  case X86::VPGATHERQDYrm:\n  case X86::VPGATHERQDZ128rm:\n  case X86::VPGATHERQDZ256rm:\n  case X86::VPGATHERQDZrm:\n  case X86::VPGATHERQDrm:\n  case X86::VPGATHERQQYrm:\n  case X86::VPGATHERQQZ128rm:\n  case X86::VPGATHERQQZ256rm:\n  case X86::VPGATHERQQZrm:\n  case X86::VPGATHERQQrm:\n  case X86::VSCATTERDPDZ128mr:\n  case X86::VSCATTERDPDZ256mr:\n  case X86::VSCATTERDPDZmr:\n  case X86::VSCATTERDPSZ128mr:\n  case X86::VSCATTERDPSZ256mr:\n  case X86::VSCATTERDPSZmr:\n  case X86::VSCATTERPF0DPDm:\n  case X86::VSCATTERPF0DPSm:\n  case X86::VSCATTERPF0QPDm:\n  case X86::VSCATTERPF0QPSm:\n  case X86::VSCATTERPF1DPDm:\n  case X86::VSCATTERPF1DPSm:\n  case X86::VSCATTERPF1QPDm:\n  case X86::VSCATTERPF1QPSm:\n  case X86::VSCATTERQPDZ128mr:\n  case X86::VSCATTERQPDZ256mr:\n  case X86::VSCATTERQPDZmr:\n  case X86::VSCATTERQPSZ128mr:\n  case X86::VSCATTERQPSZ256mr:\n  case X86::VSCATTERQPSZmr:\n  case X86::VPSCATTERDDZ128mr:\n  case X86::VPSCATTERDDZ256mr:\n  case X86::VPSCATTERDDZmr:\n  case X86::VPSCATTERDQZ128mr:\n  case X86::VPSCATTERDQZ256mr:\n  case X86::VPSCATTERDQZmr:\n  case X86::VPSCATTERQDZ128mr:\n  case X86::VPSCATTERQDZ256mr:\n  case X86::VPSCATTERQDZmr:\n  case X86::VPSCATTERQQZ128mr:\n  case X86::VPSCATTERQQZ256mr:\n  case X86::VPSCATTERQQZmr:\n    return true;\n  }\n}\n\nbool X86InstrInfo::hasHighOperandLatency(const TargetSchedModel &SchedModel,\n                                         const MachineRegisterInfo *MRI,\n                                         const MachineInstr &DefMI,\n                                         unsigned DefIdx,\n                                         const MachineInstr &UseMI,\n                                         unsigned UseIdx) const {\n  return isHighLatencyDef(DefMI.getOpcode());\n}\n\nbool X86InstrInfo::hasReassociableOperands(const MachineInstr &Inst,\n                                           const MachineBasicBlock *MBB) const {\n  assert(Inst.getNumExplicitOperands() == 3 && Inst.getNumExplicitDefs() == 1 &&\n         Inst.getNumDefs() <= 2 && \"Reassociation needs binary operators\");\n\n  // Integer binary math/logic instructions have a third source operand:\n  // the EFLAGS register. That operand must be both defined here and never\n  // used; ie, it must be dead. If the EFLAGS operand is live, then we can\n  // not change anything because rearranging the operands could affect other\n  // instructions that depend on the exact status flags (zero, sign, etc.)\n  // that are set by using these particular operands with this operation.\n  const MachineOperand *FlagDef = Inst.findRegisterDefOperand(X86::EFLAGS);\n  assert((Inst.getNumDefs() == 1 || FlagDef) &&\n         \"Implicit def isn't flags?\");\n  if (FlagDef && !FlagDef->isDead())\n    return false;\n\n  return TargetInstrInfo::hasReassociableOperands(Inst, MBB);\n}\n\n// TODO: There are many more machine instruction opcodes to match:\n//       1. Other data types (integer, vectors)\n//       2. Other math / logic operations (xor, or)\n//       3. Other forms of the same operation (intrinsics and other variants)\nbool X86InstrInfo::isAssociativeAndCommutative(const MachineInstr &Inst) const {\n  switch (Inst.getOpcode()) {\n  case X86::AND8rr:\n  case X86::AND16rr:\n  case X86::AND32rr:\n  case X86::AND64rr:\n  case X86::OR8rr:\n  case X86::OR16rr:\n  case X86::OR32rr:\n  case X86::OR64rr:\n  case X86::XOR8rr:\n  case X86::XOR16rr:\n  case X86::XOR32rr:\n  case X86::XOR64rr:\n  case X86::IMUL16rr:\n  case X86::IMUL32rr:\n  case X86::IMUL64rr:\n  case X86::PANDrr:\n  case X86::PORrr:\n  case X86::PXORrr:\n  case X86::ANDPDrr:\n  case X86::ANDPSrr:\n  case X86::ORPDrr:\n  case X86::ORPSrr:\n  case X86::XORPDrr:\n  case X86::XORPSrr:\n  case X86::PADDBrr:\n  case X86::PADDWrr:\n  case X86::PADDDrr:\n  case X86::PADDQrr:\n  case X86::PMULLWrr:\n  case X86::PMULLDrr:\n  case X86::PMAXSBrr:\n  case X86::PMAXSDrr:\n  case X86::PMAXSWrr:\n  case X86::PMAXUBrr:\n  case X86::PMAXUDrr:\n  case X86::PMAXUWrr:\n  case X86::PMINSBrr:\n  case X86::PMINSDrr:\n  case X86::PMINSWrr:\n  case X86::PMINUBrr:\n  case X86::PMINUDrr:\n  case X86::PMINUWrr:\n  case X86::VPANDrr:\n  case X86::VPANDYrr:\n  case X86::VPANDDZ128rr:\n  case X86::VPANDDZ256rr:\n  case X86::VPANDDZrr:\n  case X86::VPANDQZ128rr:\n  case X86::VPANDQZ256rr:\n  case X86::VPANDQZrr:\n  case X86::VPORrr:\n  case X86::VPORYrr:\n  case X86::VPORDZ128rr:\n  case X86::VPORDZ256rr:\n  case X86::VPORDZrr:\n  case X86::VPORQZ128rr:\n  case X86::VPORQZ256rr:\n  case X86::VPORQZrr:\n  case X86::VPXORrr:\n  case X86::VPXORYrr:\n  case X86::VPXORDZ128rr:\n  case X86::VPXORDZ256rr:\n  case X86::VPXORDZrr:\n  case X86::VPXORQZ128rr:\n  case X86::VPXORQZ256rr:\n  case X86::VPXORQZrr:\n  case X86::VANDPDrr:\n  case X86::VANDPSrr:\n  case X86::VANDPDYrr:\n  case X86::VANDPSYrr:\n  case X86::VANDPDZ128rr:\n  case X86::VANDPSZ128rr:\n  case X86::VANDPDZ256rr:\n  case X86::VANDPSZ256rr:\n  case X86::VANDPDZrr:\n  case X86::VANDPSZrr:\n  case X86::VORPDrr:\n  case X86::VORPSrr:\n  case X86::VORPDYrr:\n  case X86::VORPSYrr:\n  case X86::VORPDZ128rr:\n  case X86::VORPSZ128rr:\n  case X86::VORPDZ256rr:\n  case X86::VORPSZ256rr:\n  case X86::VORPDZrr:\n  case X86::VORPSZrr:\n  case X86::VXORPDrr:\n  case X86::VXORPSrr:\n  case X86::VXORPDYrr:\n  case X86::VXORPSYrr:\n  case X86::VXORPDZ128rr:\n  case X86::VXORPSZ128rr:\n  case X86::VXORPDZ256rr:\n  case X86::VXORPSZ256rr:\n  case X86::VXORPDZrr:\n  case X86::VXORPSZrr:\n  case X86::KADDBrr:\n  case X86::KADDWrr:\n  case X86::KADDDrr:\n  case X86::KADDQrr:\n  case X86::KANDBrr:\n  case X86::KANDWrr:\n  case X86::KANDDrr:\n  case X86::KANDQrr:\n  case X86::KORBrr:\n  case X86::KORWrr:\n  case X86::KORDrr:\n  case X86::KORQrr:\n  case X86::KXORBrr:\n  case X86::KXORWrr:\n  case X86::KXORDrr:\n  case X86::KXORQrr:\n  case X86::VPADDBrr:\n  case X86::VPADDWrr:\n  case X86::VPADDDrr:\n  case X86::VPADDQrr:\n  case X86::VPADDBYrr:\n  case X86::VPADDWYrr:\n  case X86::VPADDDYrr:\n  case X86::VPADDQYrr:\n  case X86::VPADDBZ128rr:\n  case X86::VPADDWZ128rr:\n  case X86::VPADDDZ128rr:\n  case X86::VPADDQZ128rr:\n  case X86::VPADDBZ256rr:\n  case X86::VPADDWZ256rr:\n  case X86::VPADDDZ256rr:\n  case X86::VPADDQZ256rr:\n  case X86::VPADDBZrr:\n  case X86::VPADDWZrr:\n  case X86::VPADDDZrr:\n  case X86::VPADDQZrr:\n  case X86::VPMULLWrr:\n  case X86::VPMULLWYrr:\n  case X86::VPMULLWZ128rr:\n  case X86::VPMULLWZ256rr:\n  case X86::VPMULLWZrr:\n  case X86::VPMULLDrr:\n  case X86::VPMULLDYrr:\n  case X86::VPMULLDZ128rr:\n  case X86::VPMULLDZ256rr:\n  case X86::VPMULLDZrr:\n  case X86::VPMULLQZ128rr:\n  case X86::VPMULLQZ256rr:\n  case X86::VPMULLQZrr:\n  case X86::VPMAXSBrr:\n  case X86::VPMAXSBYrr:\n  case X86::VPMAXSBZ128rr:\n  case X86::VPMAXSBZ256rr:\n  case X86::VPMAXSBZrr:\n  case X86::VPMAXSDrr:\n  case X86::VPMAXSDYrr:\n  case X86::VPMAXSDZ128rr:\n  case X86::VPMAXSDZ256rr:\n  case X86::VPMAXSDZrr:\n  case X86::VPMAXSQZ128rr:\n  case X86::VPMAXSQZ256rr:\n  case X86::VPMAXSQZrr:\n  case X86::VPMAXSWrr:\n  case X86::VPMAXSWYrr:\n  case X86::VPMAXSWZ128rr:\n  case X86::VPMAXSWZ256rr:\n  case X86::VPMAXSWZrr:\n  case X86::VPMAXUBrr:\n  case X86::VPMAXUBYrr:\n  case X86::VPMAXUBZ128rr:\n  case X86::VPMAXUBZ256rr:\n  case X86::VPMAXUBZrr:\n  case X86::VPMAXUDrr:\n  case X86::VPMAXUDYrr:\n  case X86::VPMAXUDZ128rr:\n  case X86::VPMAXUDZ256rr:\n  case X86::VPMAXUDZrr:\n  case X86::VPMAXUQZ128rr:\n  case X86::VPMAXUQZ256rr:\n  case X86::VPMAXUQZrr:\n  case X86::VPMAXUWrr:\n  case X86::VPMAXUWYrr:\n  case X86::VPMAXUWZ128rr:\n  case X86::VPMAXUWZ256rr:\n  case X86::VPMAXUWZrr:\n  case X86::VPMINSBrr:\n  case X86::VPMINSBYrr:\n  case X86::VPMINSBZ128rr:\n  case X86::VPMINSBZ256rr:\n  case X86::VPMINSBZrr:\n  case X86::VPMINSDrr:\n  case X86::VPMINSDYrr:\n  case X86::VPMINSDZ128rr:\n  case X86::VPMINSDZ256rr:\n  case X86::VPMINSDZrr:\n  case X86::VPMINSQZ128rr:\n  case X86::VPMINSQZ256rr:\n  case X86::VPMINSQZrr:\n  case X86::VPMINSWrr:\n  case X86::VPMINSWYrr:\n  case X86::VPMINSWZ128rr:\n  case X86::VPMINSWZ256rr:\n  case X86::VPMINSWZrr:\n  case X86::VPMINUBrr:\n  case X86::VPMINUBYrr:\n  case X86::VPMINUBZ128rr:\n  case X86::VPMINUBZ256rr:\n  case X86::VPMINUBZrr:\n  case X86::VPMINUDrr:\n  case X86::VPMINUDYrr:\n  case X86::VPMINUDZ128rr:\n  case X86::VPMINUDZ256rr:\n  case X86::VPMINUDZrr:\n  case X86::VPMINUQZ128rr:\n  case X86::VPMINUQZ256rr:\n  case X86::VPMINUQZrr:\n  case X86::VPMINUWrr:\n  case X86::VPMINUWYrr:\n  case X86::VPMINUWZ128rr:\n  case X86::VPMINUWZ256rr:\n  case X86::VPMINUWZrr:\n  // Normal min/max instructions are not commutative because of NaN and signed\n  // zero semantics, but these are. Thus, there's no need to check for global\n  // relaxed math; the instructions themselves have the properties we need.\n  case X86::MAXCPDrr:\n  case X86::MAXCPSrr:\n  case X86::MAXCSDrr:\n  case X86::MAXCSSrr:\n  case X86::MINCPDrr:\n  case X86::MINCPSrr:\n  case X86::MINCSDrr:\n  case X86::MINCSSrr:\n  case X86::VMAXCPDrr:\n  case X86::VMAXCPSrr:\n  case X86::VMAXCPDYrr:\n  case X86::VMAXCPSYrr:\n  case X86::VMAXCPDZ128rr:\n  case X86::VMAXCPSZ128rr:\n  case X86::VMAXCPDZ256rr:\n  case X86::VMAXCPSZ256rr:\n  case X86::VMAXCPDZrr:\n  case X86::VMAXCPSZrr:\n  case X86::VMAXCSDrr:\n  case X86::VMAXCSSrr:\n  case X86::VMAXCSDZrr:\n  case X86::VMAXCSSZrr:\n  case X86::VMINCPDrr:\n  case X86::VMINCPSrr:\n  case X86::VMINCPDYrr:\n  case X86::VMINCPSYrr:\n  case X86::VMINCPDZ128rr:\n  case X86::VMINCPSZ128rr:\n  case X86::VMINCPDZ256rr:\n  case X86::VMINCPSZ256rr:\n  case X86::VMINCPDZrr:\n  case X86::VMINCPSZrr:\n  case X86::VMINCSDrr:\n  case X86::VMINCSSrr:\n  case X86::VMINCSDZrr:\n  case X86::VMINCSSZrr:\n    return true;\n  case X86::ADDPDrr:\n  case X86::ADDPSrr:\n  case X86::ADDSDrr:\n  case X86::ADDSSrr:\n  case X86::MULPDrr:\n  case X86::MULPSrr:\n  case X86::MULSDrr:\n  case X86::MULSSrr:\n  case X86::VADDPDrr:\n  case X86::VADDPSrr:\n  case X86::VADDPDYrr:\n  case X86::VADDPSYrr:\n  case X86::VADDPDZ128rr:\n  case X86::VADDPSZ128rr:\n  case X86::VADDPDZ256rr:\n  case X86::VADDPSZ256rr:\n  case X86::VADDPDZrr:\n  case X86::VADDPSZrr:\n  case X86::VADDSDrr:\n  case X86::VADDSSrr:\n  case X86::VADDSDZrr:\n  case X86::VADDSSZrr:\n  case X86::VMULPDrr:\n  case X86::VMULPSrr:\n  case X86::VMULPDYrr:\n  case X86::VMULPSYrr:\n  case X86::VMULPDZ128rr:\n  case X86::VMULPSZ128rr:\n  case X86::VMULPDZ256rr:\n  case X86::VMULPSZ256rr:\n  case X86::VMULPDZrr:\n  case X86::VMULPSZrr:\n  case X86::VMULSDrr:\n  case X86::VMULSSrr:\n  case X86::VMULSDZrr:\n  case X86::VMULSSZrr:\n    return Inst.getFlag(MachineInstr::MIFlag::FmReassoc) &&\n           Inst.getFlag(MachineInstr::MIFlag::FmNsz);\n  default:\n    return false;\n  }\n}\n\n/// If \\p DescribedReg overlaps with the MOVrr instruction's destination\n/// register then, if possible, describe the value in terms of the source\n/// register.\nstatic Optional<ParamLoadedValue>\ndescribeMOVrrLoadedValue(const MachineInstr &MI, Register DescribedReg,\n                         const TargetRegisterInfo *TRI) {\n  Register DestReg = MI.getOperand(0).getReg();\n  Register SrcReg = MI.getOperand(1).getReg();\n\n  auto Expr = DIExpression::get(MI.getMF()->getFunction().getContext(), {});\n\n  // If the described register is the destination, just return the source.\n  if (DestReg == DescribedReg)\n    return ParamLoadedValue(MachineOperand::CreateReg(SrcReg, false), Expr);\n\n  // If the described register is a sub-register of the destination register,\n  // then pick out the source register's corresponding sub-register.\n  if (unsigned SubRegIdx = TRI->getSubRegIndex(DestReg, DescribedReg)) {\n    Register SrcSubReg = TRI->getSubReg(SrcReg, SubRegIdx);\n    return ParamLoadedValue(MachineOperand::CreateReg(SrcSubReg, false), Expr);\n  }\n\n  // The remaining case to consider is when the described register is a\n  // super-register of the destination register. MOV8rr and MOV16rr does not\n  // write to any of the other bytes in the register, meaning that we'd have to\n  // describe the value using a combination of the source register and the\n  // non-overlapping bits in the described register, which is not currently\n  // possible.\n  if (MI.getOpcode() == X86::MOV8rr || MI.getOpcode() == X86::MOV16rr ||\n      !TRI->isSuperRegister(DestReg, DescribedReg))\n    return None;\n\n  assert(MI.getOpcode() == X86::MOV32rr && \"Unexpected super-register case\");\n  return ParamLoadedValue(MachineOperand::CreateReg(SrcReg, false), Expr);\n}\n\nOptional<ParamLoadedValue>\nX86InstrInfo::describeLoadedValue(const MachineInstr &MI, Register Reg) const {\n  const MachineOperand *Op = nullptr;\n  DIExpression *Expr = nullptr;\n\n  const TargetRegisterInfo *TRI = &getRegisterInfo();\n\n  switch (MI.getOpcode()) {\n  case X86::LEA32r:\n  case X86::LEA64r:\n  case X86::LEA64_32r: {\n    // We may need to describe a 64-bit parameter with a 32-bit LEA.\n    if (!TRI->isSuperRegisterEq(MI.getOperand(0).getReg(), Reg))\n      return None;\n\n    // Operand 4 could be global address. For now we do not support\n    // such situation.\n    if (!MI.getOperand(4).isImm() || !MI.getOperand(2).isImm())\n      return None;\n\n    const MachineOperand &Op1 = MI.getOperand(1);\n    const MachineOperand &Op2 = MI.getOperand(3);\n    assert(Op2.isReg() && (Op2.getReg() == X86::NoRegister ||\n                           Register::isPhysicalRegister(Op2.getReg())));\n\n    // Omit situations like:\n    // %rsi = lea %rsi, 4, ...\n    if ((Op1.isReg() && Op1.getReg() == MI.getOperand(0).getReg()) ||\n        Op2.getReg() == MI.getOperand(0).getReg())\n      return None;\n    else if ((Op1.isReg() && Op1.getReg() != X86::NoRegister &&\n              TRI->regsOverlap(Op1.getReg(), MI.getOperand(0).getReg())) ||\n             (Op2.getReg() != X86::NoRegister &&\n              TRI->regsOverlap(Op2.getReg(), MI.getOperand(0).getReg())))\n      return None;\n\n    int64_t Coef = MI.getOperand(2).getImm();\n    int64_t Offset = MI.getOperand(4).getImm();\n    SmallVector<uint64_t, 8> Ops;\n\n    if ((Op1.isReg() && Op1.getReg() != X86::NoRegister)) {\n      Op = &Op1;\n    } else if (Op1.isFI())\n      Op = &Op1;\n\n    if (Op && Op->isReg() && Op->getReg() == Op2.getReg() && Coef > 0) {\n      Ops.push_back(dwarf::DW_OP_constu);\n      Ops.push_back(Coef + 1);\n      Ops.push_back(dwarf::DW_OP_mul);\n    } else {\n      if (Op && Op2.getReg() != X86::NoRegister) {\n        int dwarfReg = TRI->getDwarfRegNum(Op2.getReg(), false);\n        if (dwarfReg < 0)\n          return None;\n        else if (dwarfReg < 32) {\n          Ops.push_back(dwarf::DW_OP_breg0 + dwarfReg);\n          Ops.push_back(0);\n        } else {\n          Ops.push_back(dwarf::DW_OP_bregx);\n          Ops.push_back(dwarfReg);\n          Ops.push_back(0);\n        }\n      } else if (!Op) {\n        assert(Op2.getReg() != X86::NoRegister);\n        Op = &Op2;\n      }\n\n      if (Coef > 1) {\n        assert(Op2.getReg() != X86::NoRegister);\n        Ops.push_back(dwarf::DW_OP_constu);\n        Ops.push_back(Coef);\n        Ops.push_back(dwarf::DW_OP_mul);\n      }\n\n      if (((Op1.isReg() && Op1.getReg() != X86::NoRegister) || Op1.isFI()) &&\n          Op2.getReg() != X86::NoRegister) {\n        Ops.push_back(dwarf::DW_OP_plus);\n      }\n    }\n\n    DIExpression::appendOffset(Ops, Offset);\n    Expr = DIExpression::get(MI.getMF()->getFunction().getContext(), Ops);\n\n    return ParamLoadedValue(*Op, Expr);;\n  }\n  case X86::MOV8ri:\n  case X86::MOV16ri:\n    // TODO: Handle MOV8ri and MOV16ri.\n    return None;\n  case X86::MOV32ri:\n  case X86::MOV64ri:\n  case X86::MOV64ri32:\n    // MOV32ri may be used for producing zero-extended 32-bit immediates in\n    // 64-bit parameters, so we need to consider super-registers.\n    if (!TRI->isSuperRegisterEq(MI.getOperand(0).getReg(), Reg))\n      return None;\n    return ParamLoadedValue(MI.getOperand(1), Expr);\n  case X86::MOV8rr:\n  case X86::MOV16rr:\n  case X86::MOV32rr:\n  case X86::MOV64rr:\n    return describeMOVrrLoadedValue(MI, Reg, TRI);\n  case X86::XOR32rr: {\n    // 64-bit parameters are zero-materialized using XOR32rr, so also consider\n    // super-registers.\n    if (!TRI->isSuperRegisterEq(MI.getOperand(0).getReg(), Reg))\n      return None;\n    if (MI.getOperand(1).getReg() == MI.getOperand(2).getReg())\n      return ParamLoadedValue(MachineOperand::CreateImm(0), Expr);\n    return None;\n  }\n  case X86::MOVSX64rr32: {\n    // We may need to describe the lower 32 bits of the MOVSX; for example, in\n    // cases like this:\n    //\n    //  $ebx = [...]\n    //  $rdi = MOVSX64rr32 $ebx\n    //  $esi = MOV32rr $edi\n    if (!TRI->isSubRegisterEq(MI.getOperand(0).getReg(), Reg))\n      return None;\n\n    Expr = DIExpression::get(MI.getMF()->getFunction().getContext(), {});\n\n    // If the described register is the destination register we need to\n    // sign-extend the source register from 32 bits. The other case we handle\n    // is when the described register is the 32-bit sub-register of the\n    // destination register, in case we just need to return the source\n    // register.\n    if (Reg == MI.getOperand(0).getReg())\n      Expr = DIExpression::appendExt(Expr, 32, 64, true);\n    else\n      assert(X86MCRegisterClasses[X86::GR32RegClassID].contains(Reg) &&\n             \"Unhandled sub-register case for MOVSX64rr32\");\n\n    return ParamLoadedValue(MI.getOperand(1), Expr);\n  }\n  default:\n    assert(!MI.isMoveImmediate() && \"Unexpected MoveImm instruction\");\n    return TargetInstrInfo::describeLoadedValue(MI, Reg);\n  }\n}\n\n/// This is an architecture-specific helper function of reassociateOps.\n/// Set special operand attributes for new instructions after reassociation.\nvoid X86InstrInfo::setSpecialOperandAttr(MachineInstr &OldMI1,\n                                         MachineInstr &OldMI2,\n                                         MachineInstr &NewMI1,\n                                         MachineInstr &NewMI2) const {\n  // Propagate FP flags from the original instructions.\n  // But clear poison-generating flags because those may not be valid now.\n  // TODO: There should be a helper function for copying only fast-math-flags.\n  uint16_t IntersectedFlags = OldMI1.getFlags() & OldMI2.getFlags();\n  NewMI1.setFlags(IntersectedFlags);\n  NewMI1.clearFlag(MachineInstr::MIFlag::NoSWrap);\n  NewMI1.clearFlag(MachineInstr::MIFlag::NoUWrap);\n  NewMI1.clearFlag(MachineInstr::MIFlag::IsExact);\n\n  NewMI2.setFlags(IntersectedFlags);\n  NewMI2.clearFlag(MachineInstr::MIFlag::NoSWrap);\n  NewMI2.clearFlag(MachineInstr::MIFlag::NoUWrap);\n  NewMI2.clearFlag(MachineInstr::MIFlag::IsExact);\n\n  // Integer instructions may define an implicit EFLAGS dest register operand.\n  MachineOperand *OldFlagDef1 = OldMI1.findRegisterDefOperand(X86::EFLAGS);\n  MachineOperand *OldFlagDef2 = OldMI2.findRegisterDefOperand(X86::EFLAGS);\n\n  assert(!OldFlagDef1 == !OldFlagDef2 &&\n         \"Unexpected instruction type for reassociation\");\n\n  if (!OldFlagDef1 || !OldFlagDef2)\n    return;\n\n  assert(OldFlagDef1->isDead() && OldFlagDef2->isDead() &&\n         \"Must have dead EFLAGS operand in reassociable instruction\");\n\n  MachineOperand *NewFlagDef1 = NewMI1.findRegisterDefOperand(X86::EFLAGS);\n  MachineOperand *NewFlagDef2 = NewMI2.findRegisterDefOperand(X86::EFLAGS);\n\n  assert(NewFlagDef1 && NewFlagDef2 &&\n         \"Unexpected operand in reassociable instruction\");\n\n  // Mark the new EFLAGS operands as dead to be helpful to subsequent iterations\n  // of this pass or other passes. The EFLAGS operands must be dead in these new\n  // instructions because the EFLAGS operands in the original instructions must\n  // be dead in order for reassociation to occur.\n  NewFlagDef1->setIsDead();\n  NewFlagDef2->setIsDead();\n}\n\nstd::pair<unsigned, unsigned>\nX86InstrInfo::decomposeMachineOperandsTargetFlags(unsigned TF) const {\n  return std::make_pair(TF, 0u);\n}\n\nArrayRef<std::pair<unsigned, const char *>>\nX86InstrInfo::getSerializableDirectMachineOperandTargetFlags() const {\n  using namespace X86II;\n  static const std::pair<unsigned, const char *> TargetFlags[] = {\n      {MO_GOT_ABSOLUTE_ADDRESS, \"x86-got-absolute-address\"},\n      {MO_PIC_BASE_OFFSET, \"x86-pic-base-offset\"},\n      {MO_GOT, \"x86-got\"},\n      {MO_GOTOFF, \"x86-gotoff\"},\n      {MO_GOTPCREL, \"x86-gotpcrel\"},\n      {MO_PLT, \"x86-plt\"},\n      {MO_TLSGD, \"x86-tlsgd\"},\n      {MO_TLSLD, \"x86-tlsld\"},\n      {MO_TLSLDM, \"x86-tlsldm\"},\n      {MO_GOTTPOFF, \"x86-gottpoff\"},\n      {MO_INDNTPOFF, \"x86-indntpoff\"},\n      {MO_TPOFF, \"x86-tpoff\"},\n      {MO_DTPOFF, \"x86-dtpoff\"},\n      {MO_NTPOFF, \"x86-ntpoff\"},\n      {MO_GOTNTPOFF, \"x86-gotntpoff\"},\n      {MO_DLLIMPORT, \"x86-dllimport\"},\n      {MO_DARWIN_NONLAZY, \"x86-darwin-nonlazy\"},\n      {MO_DARWIN_NONLAZY_PIC_BASE, \"x86-darwin-nonlazy-pic-base\"},\n      {MO_TLVP, \"x86-tlvp\"},\n      {MO_TLVP_PIC_BASE, \"x86-tlvp-pic-base\"},\n      {MO_SECREL, \"x86-secrel\"},\n      {MO_COFFSTUB, \"x86-coffstub\"}};\n  return makeArrayRef(TargetFlags);\n}\n\nnamespace {\n  /// Create Global Base Reg pass. This initializes the PIC\n  /// global base register for x86-32.\n  struct CGBR : public MachineFunctionPass {\n    static char ID;\n    CGBR() : MachineFunctionPass(ID) {}\n\n    bool runOnMachineFunction(MachineFunction &MF) override {\n      const X86TargetMachine *TM =\n        static_cast<const X86TargetMachine *>(&MF.getTarget());\n      const X86Subtarget &STI = MF.getSubtarget<X86Subtarget>();\n\n      // Don't do anything in the 64-bit small and kernel code models. They use\n      // RIP-relative addressing for everything.\n      if (STI.is64Bit() && (TM->getCodeModel() == CodeModel::Small ||\n                            TM->getCodeModel() == CodeModel::Kernel))\n        return false;\n\n      // Only emit a global base reg in PIC mode.\n      if (!TM->isPositionIndependent())\n        return false;\n\n      X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();\n      Register GlobalBaseReg = X86FI->getGlobalBaseReg();\n\n      // If we didn't need a GlobalBaseReg, don't insert code.\n      if (GlobalBaseReg == 0)\n        return false;\n\n      // Insert the set of GlobalBaseReg into the first MBB of the function\n      MachineBasicBlock &FirstMBB = MF.front();\n      MachineBasicBlock::iterator MBBI = FirstMBB.begin();\n      DebugLoc DL = FirstMBB.findDebugLoc(MBBI);\n      MachineRegisterInfo &RegInfo = MF.getRegInfo();\n      const X86InstrInfo *TII = STI.getInstrInfo();\n\n      Register PC;\n      if (STI.isPICStyleGOT())\n        PC = RegInfo.createVirtualRegister(&X86::GR32RegClass);\n      else\n        PC = GlobalBaseReg;\n\n      if (STI.is64Bit()) {\n        if (TM->getCodeModel() == CodeModel::Medium) {\n          // In the medium code model, use a RIP-relative LEA to materialize the\n          // GOT.\n          BuildMI(FirstMBB, MBBI, DL, TII->get(X86::LEA64r), PC)\n              .addReg(X86::RIP)\n              .addImm(0)\n              .addReg(0)\n              .addExternalSymbol(\"_GLOBAL_OFFSET_TABLE_\")\n              .addReg(0);\n        } else if (TM->getCodeModel() == CodeModel::Large) {\n          // In the large code model, we are aiming for this code, though the\n          // register allocation may vary:\n          //   leaq .LN$pb(%rip), %rax\n          //   movq $_GLOBAL_OFFSET_TABLE_ - .LN$pb, %rcx\n          //   addq %rcx, %rax\n          // RAX now holds address of _GLOBAL_OFFSET_TABLE_.\n          Register PBReg = RegInfo.createVirtualRegister(&X86::GR64RegClass);\n          Register GOTReg = RegInfo.createVirtualRegister(&X86::GR64RegClass);\n          BuildMI(FirstMBB, MBBI, DL, TII->get(X86::LEA64r), PBReg)\n              .addReg(X86::RIP)\n              .addImm(0)\n              .addReg(0)\n              .addSym(MF.getPICBaseSymbol())\n              .addReg(0);\n          std::prev(MBBI)->setPreInstrSymbol(MF, MF.getPICBaseSymbol());\n          BuildMI(FirstMBB, MBBI, DL, TII->get(X86::MOV64ri), GOTReg)\n              .addExternalSymbol(\"_GLOBAL_OFFSET_TABLE_\",\n                                 X86II::MO_PIC_BASE_OFFSET);\n          BuildMI(FirstMBB, MBBI, DL, TII->get(X86::ADD64rr), PC)\n              .addReg(PBReg, RegState::Kill)\n              .addReg(GOTReg, RegState::Kill);\n        } else {\n          llvm_unreachable(\"unexpected code model\");\n        }\n      } else {\n        // Operand of MovePCtoStack is completely ignored by asm printer. It's\n        // only used in JIT code emission as displacement to pc.\n        BuildMI(FirstMBB, MBBI, DL, TII->get(X86::MOVPC32r), PC).addImm(0);\n\n        // If we're using vanilla 'GOT' PIC style, we should use relative\n        // addressing not to pc, but to _GLOBAL_OFFSET_TABLE_ external.\n        if (STI.isPICStyleGOT()) {\n          // Generate addl $__GLOBAL_OFFSET_TABLE_ + [.-piclabel],\n          // %some_register\n          BuildMI(FirstMBB, MBBI, DL, TII->get(X86::ADD32ri), GlobalBaseReg)\n              .addReg(PC)\n              .addExternalSymbol(\"_GLOBAL_OFFSET_TABLE_\",\n                                 X86II::MO_GOT_ABSOLUTE_ADDRESS);\n        }\n      }\n\n      return true;\n    }\n\n    StringRef getPassName() const override {\n      return \"X86 PIC Global Base Reg Initialization\";\n    }\n\n    void getAnalysisUsage(AnalysisUsage &AU) const override {\n      AU.setPreservesCFG();\n      MachineFunctionPass::getAnalysisUsage(AU);\n    }\n  };\n} // namespace\n\nchar CGBR::ID = 0;\nFunctionPass*\nllvm::createX86GlobalBaseRegPass() { return new CGBR(); }\n\nnamespace {\n  struct LDTLSCleanup : public MachineFunctionPass {\n    static char ID;\n    LDTLSCleanup() : MachineFunctionPass(ID) {}\n\n    bool runOnMachineFunction(MachineFunction &MF) override {\n      if (skipFunction(MF.getFunction()))\n        return false;\n\n      X86MachineFunctionInfo *MFI = MF.getInfo<X86MachineFunctionInfo>();\n      if (MFI->getNumLocalDynamicTLSAccesses() < 2) {\n        // No point folding accesses if there isn't at least two.\n        return false;\n      }\n\n      MachineDominatorTree *DT = &getAnalysis<MachineDominatorTree>();\n      return VisitNode(DT->getRootNode(), 0);\n    }\n\n    // Visit the dominator subtree rooted at Node in pre-order.\n    // If TLSBaseAddrReg is non-null, then use that to replace any\n    // TLS_base_addr instructions. Otherwise, create the register\n    // when the first such instruction is seen, and then use it\n    // as we encounter more instructions.\n    bool VisitNode(MachineDomTreeNode *Node, unsigned TLSBaseAddrReg) {\n      MachineBasicBlock *BB = Node->getBlock();\n      bool Changed = false;\n\n      // Traverse the current block.\n      for (MachineBasicBlock::iterator I = BB->begin(), E = BB->end(); I != E;\n           ++I) {\n        switch (I->getOpcode()) {\n          case X86::TLS_base_addr32:\n          case X86::TLS_base_addr64:\n            if (TLSBaseAddrReg)\n              I = ReplaceTLSBaseAddrCall(*I, TLSBaseAddrReg);\n            else\n              I = SetRegister(*I, &TLSBaseAddrReg);\n            Changed = true;\n            break;\n          default:\n            break;\n        }\n      }\n\n      // Visit the children of this block in the dominator tree.\n      for (auto I = Node->begin(), E = Node->end(); I != E; ++I) {\n        Changed |= VisitNode(*I, TLSBaseAddrReg);\n      }\n\n      return Changed;\n    }\n\n    // Replace the TLS_base_addr instruction I with a copy from\n    // TLSBaseAddrReg, returning the new instruction.\n    MachineInstr *ReplaceTLSBaseAddrCall(MachineInstr &I,\n                                         unsigned TLSBaseAddrReg) {\n      MachineFunction *MF = I.getParent()->getParent();\n      const X86Subtarget &STI = MF->getSubtarget<X86Subtarget>();\n      const bool is64Bit = STI.is64Bit();\n      const X86InstrInfo *TII = STI.getInstrInfo();\n\n      // Insert a Copy from TLSBaseAddrReg to RAX/EAX.\n      MachineInstr *Copy =\n          BuildMI(*I.getParent(), I, I.getDebugLoc(),\n                  TII->get(TargetOpcode::COPY), is64Bit ? X86::RAX : X86::EAX)\n              .addReg(TLSBaseAddrReg);\n\n      // Erase the TLS_base_addr instruction.\n      I.eraseFromParent();\n\n      return Copy;\n    }\n\n    // Create a virtual register in *TLSBaseAddrReg, and populate it by\n    // inserting a copy instruction after I. Returns the new instruction.\n    MachineInstr *SetRegister(MachineInstr &I, unsigned *TLSBaseAddrReg) {\n      MachineFunction *MF = I.getParent()->getParent();\n      const X86Subtarget &STI = MF->getSubtarget<X86Subtarget>();\n      const bool is64Bit = STI.is64Bit();\n      const X86InstrInfo *TII = STI.getInstrInfo();\n\n      // Create a virtual register for the TLS base address.\n      MachineRegisterInfo &RegInfo = MF->getRegInfo();\n      *TLSBaseAddrReg = RegInfo.createVirtualRegister(is64Bit\n                                                      ? &X86::GR64RegClass\n                                                      : &X86::GR32RegClass);\n\n      // Insert a copy from RAX/EAX to TLSBaseAddrReg.\n      MachineInstr *Next = I.getNextNode();\n      MachineInstr *Copy =\n          BuildMI(*I.getParent(), Next, I.getDebugLoc(),\n                  TII->get(TargetOpcode::COPY), *TLSBaseAddrReg)\n              .addReg(is64Bit ? X86::RAX : X86::EAX);\n\n      return Copy;\n    }\n\n    StringRef getPassName() const override {\n      return \"Local Dynamic TLS Access Clean-up\";\n    }\n\n    void getAnalysisUsage(AnalysisUsage &AU) const override {\n      AU.setPreservesCFG();\n      AU.addRequired<MachineDominatorTree>();\n      MachineFunctionPass::getAnalysisUsage(AU);\n    }\n  };\n}\n\nchar LDTLSCleanup::ID = 0;\nFunctionPass*\nllvm::createCleanupLocalDynamicTLSPass() { return new LDTLSCleanup(); }\n\n/// Constants defining how certain sequences should be outlined.\n///\n/// \\p MachineOutlinerDefault implies that the function is called with a call\n/// instruction, and a return must be emitted for the outlined function frame.\n///\n/// That is,\n///\n/// I1                                 OUTLINED_FUNCTION:\n/// I2 --> call OUTLINED_FUNCTION       I1\n/// I3                                  I2\n///                                     I3\n///                                     ret\n///\n/// * Call construction overhead: 1 (call instruction)\n/// * Frame construction overhead: 1 (return instruction)\n///\n/// \\p MachineOutlinerTailCall implies that the function is being tail called.\n/// A jump is emitted instead of a call, and the return is already present in\n/// the outlined sequence. That is,\n///\n/// I1                                 OUTLINED_FUNCTION:\n/// I2 --> jmp OUTLINED_FUNCTION       I1\n/// ret                                I2\n///                                    ret\n///\n/// * Call construction overhead: 1 (jump instruction)\n/// * Frame construction overhead: 0 (don't need to return)\n///\nenum MachineOutlinerClass {\n  MachineOutlinerDefault,\n  MachineOutlinerTailCall\n};\n\noutliner::OutlinedFunction X86InstrInfo::getOutliningCandidateInfo(\n    std::vector<outliner::Candidate> &RepeatedSequenceLocs) const {\n  unsigned SequenceSize =\n      std::accumulate(RepeatedSequenceLocs[0].front(),\n                      std::next(RepeatedSequenceLocs[0].back()), 0,\n                      [](unsigned Sum, const MachineInstr &MI) {\n                        // FIXME: x86 doesn't implement getInstSizeInBytes, so\n                        // we can't tell the cost.  Just assume each instruction\n                        // is one byte.\n                        if (MI.isDebugInstr() || MI.isKill())\n                          return Sum;\n                        return Sum + 1;\n                      });\n\n  // We check to see if CFI Instructions are present, and if they are\n  // we find the number of CFI Instructions in the candidates.\n  unsigned CFICount = 0;\n  MachineBasicBlock::iterator MBBI = RepeatedSequenceLocs[0].front();\n  for (unsigned Loc = RepeatedSequenceLocs[0].getStartIdx();\n       Loc < RepeatedSequenceLocs[0].getEndIdx() + 1; Loc++) {\n    const std::vector<MCCFIInstruction> &CFIInstructions =\n        RepeatedSequenceLocs[0].getMF()->getFrameInstructions();\n    if (MBBI->isCFIInstruction()) {\n      unsigned CFIIndex = MBBI->getOperand(0).getCFIIndex();\n      MCCFIInstruction CFI = CFIInstructions[CFIIndex];\n      CFICount++;\n    }\n    MBBI++;\n  }\n\n  // We compare the number of found CFI Instructions to  the number of CFI\n  // instructions in the parent function for each candidate.  We must check this\n  // since if we outline one of the CFI instructions in a function, we have to\n  // outline them all for correctness. If we do not, the address offsets will be\n  // incorrect between the two sections of the program.\n  for (outliner::Candidate &C : RepeatedSequenceLocs) {\n    std::vector<MCCFIInstruction> CFIInstructions =\n        C.getMF()->getFrameInstructions();\n\n    if (CFICount > 0 && CFICount != CFIInstructions.size())\n      return outliner::OutlinedFunction();\n  }\n\n  // FIXME: Use real size in bytes for call and ret instructions.\n  if (RepeatedSequenceLocs[0].back()->isTerminator()) {\n    for (outliner::Candidate &C : RepeatedSequenceLocs)\n      C.setCallInfo(MachineOutlinerTailCall, 1);\n\n    return outliner::OutlinedFunction(RepeatedSequenceLocs, SequenceSize,\n                                      0, // Number of bytes to emit frame.\n                                      MachineOutlinerTailCall // Type of frame.\n    );\n  }\n\n  if (CFICount > 0)\n    return outliner::OutlinedFunction();\n\n  for (outliner::Candidate &C : RepeatedSequenceLocs)\n    C.setCallInfo(MachineOutlinerDefault, 1);\n\n  return outliner::OutlinedFunction(RepeatedSequenceLocs, SequenceSize, 1,\n                                    MachineOutlinerDefault);\n}\n\nbool X86InstrInfo::isFunctionSafeToOutlineFrom(MachineFunction &MF,\n                                           bool OutlineFromLinkOnceODRs) const {\n  const Function &F = MF.getFunction();\n\n  // Does the function use a red zone? If it does, then we can't risk messing\n  // with the stack.\n  if (Subtarget.getFrameLowering()->has128ByteRedZone(MF)) {\n    // It could have a red zone. If it does, then we don't want to touch it.\n    const X86MachineFunctionInfo *X86FI = MF.getInfo<X86MachineFunctionInfo>();\n    if (!X86FI || X86FI->getUsesRedZone())\n      return false;\n  }\n\n  // If we *don't* want to outline from things that could potentially be deduped\n  // then return false.\n  if (!OutlineFromLinkOnceODRs && F.hasLinkOnceODRLinkage())\n      return false;\n\n  // This function is viable for outlining, so return true.\n  return true;\n}\n\noutliner::InstrType\nX86InstrInfo::getOutliningType(MachineBasicBlock::iterator &MIT,  unsigned Flags) const {\n  MachineInstr &MI = *MIT;\n  // Don't allow debug values to impact outlining type.\n  if (MI.isDebugInstr() || MI.isIndirectDebugValue())\n    return outliner::InstrType::Invisible;\n\n  // At this point, KILL instructions don't really tell us much so we can go\n  // ahead and skip over them.\n  if (MI.isKill())\n    return outliner::InstrType::Invisible;\n\n  // Is this a tail call? If yes, we can outline as a tail call.\n  if (isTailCall(MI))\n    return outliner::InstrType::Legal;\n\n  // Is this the terminator of a basic block?\n  if (MI.isTerminator() || MI.isReturn()) {\n\n    // Does its parent have any successors in its MachineFunction?\n    if (MI.getParent()->succ_empty())\n      return outliner::InstrType::Legal;\n\n    // It does, so we can't tail call it.\n    return outliner::InstrType::Illegal;\n  }\n\n  // Don't outline anything that modifies or reads from the stack pointer.\n  //\n  // FIXME: There are instructions which are being manually built without\n  // explicit uses/defs so we also have to check the MCInstrDesc. We should be\n  // able to remove the extra checks once those are fixed up. For example,\n  // sometimes we might get something like %rax = POP64r 1. This won't be\n  // caught by modifiesRegister or readsRegister even though the instruction\n  // really ought to be formed so that modifiesRegister/readsRegister would\n  // catch it.\n  if (MI.modifiesRegister(X86::RSP, &RI) || MI.readsRegister(X86::RSP, &RI) ||\n      MI.getDesc().hasImplicitUseOfPhysReg(X86::RSP) ||\n      MI.getDesc().hasImplicitDefOfPhysReg(X86::RSP))\n    return outliner::InstrType::Illegal;\n\n  // Outlined calls change the instruction pointer, so don't read from it.\n  if (MI.readsRegister(X86::RIP, &RI) ||\n      MI.getDesc().hasImplicitUseOfPhysReg(X86::RIP) ||\n      MI.getDesc().hasImplicitDefOfPhysReg(X86::RIP))\n    return outliner::InstrType::Illegal;\n\n  // Positions can't safely be outlined.\n  if (MI.isPosition())\n    return outliner::InstrType::Illegal;\n\n  // Make sure none of the operands of this instruction do anything tricky.\n  for (const MachineOperand &MOP : MI.operands())\n    if (MOP.isCPI() || MOP.isJTI() || MOP.isCFIIndex() || MOP.isFI() ||\n        MOP.isTargetIndex())\n      return outliner::InstrType::Illegal;\n\n  return outliner::InstrType::Legal;\n}\n\nvoid X86InstrInfo::buildOutlinedFrame(MachineBasicBlock &MBB,\n                                          MachineFunction &MF,\n                                          const outliner::OutlinedFunction &OF)\n                                          const {\n  // If we're a tail call, we already have a return, so don't do anything.\n  if (OF.FrameConstructionID == MachineOutlinerTailCall)\n    return;\n\n  // We're a normal call, so our sequence doesn't have a return instruction.\n  // Add it in.\n  MachineInstr *retq = BuildMI(MF, DebugLoc(), get(X86::RETQ));\n  MBB.insert(MBB.end(), retq);\n}\n\nMachineBasicBlock::iterator\nX86InstrInfo::insertOutlinedCall(Module &M, MachineBasicBlock &MBB,\n                                 MachineBasicBlock::iterator &It,\n                                 MachineFunction &MF,\n                                 const outliner::Candidate &C) const {\n  // Is it a tail call?\n  if (C.CallConstructionID == MachineOutlinerTailCall) {\n    // Yes, just insert a JMP.\n    It = MBB.insert(It,\n                  BuildMI(MF, DebugLoc(), get(X86::TAILJMPd64))\n                      .addGlobalAddress(M.getNamedValue(MF.getName())));\n  } else {\n    // No, insert a call.\n    It = MBB.insert(It,\n                  BuildMI(MF, DebugLoc(), get(X86::CALL64pcrel32))\n                      .addGlobalAddress(M.getNamedValue(MF.getName())));\n  }\n\n  return It;\n}\n\n#define GET_INSTRINFO_HELPERS\n#include \"X86GenInstrInfo.inc\"\n"}, "3": {"id": 3, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86InstrInfo.h", "content": "//===-- X86InstrInfo.h - X86 Instruction Information ------------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the X86 implementation of the TargetInstrInfo class.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_LIB_TARGET_X86_X86INSTRINFO_H\n#define LLVM_LIB_TARGET_X86_X86INSTRINFO_H\n\n#include \"MCTargetDesc/X86BaseInfo.h\"\n#include \"X86InstrFMA3Info.h\"\n#include \"X86RegisterInfo.h\"\n#include \"llvm/CodeGen/ISDOpcodes.h\"\n#include \"llvm/CodeGen/TargetInstrInfo.h\"\n#include <vector>\n\n#define GET_INSTRINFO_HEADER\n#include \"X86GenInstrInfo.inc\"\n\nnamespace llvm {\nclass X86Subtarget;\n\nnamespace X86 {\n\nenum AsmComments {\n  // For instr that was compressed from EVEX to VEX.\n  AC_EVEX_2_VEX = MachineInstr::TAsmComments\n};\n\n/// Return a pair of condition code for the given predicate and whether\n/// the instruction operands should be swaped to match the condition code.\nstd::pair<CondCode, bool> getX86ConditionCode(CmpInst::Predicate Predicate);\n\n/// Return a setcc opcode based on whether it has a memory operand.\nunsigned getSETOpc(bool HasMemoryOperand = false);\n\n/// Return a cmov opcode for the given register size in bytes, and operand type.\nunsigned getCMovOpcode(unsigned RegBytes, bool HasMemoryOperand = false);\n\n// Turn jCC instruction into condition code.\nCondCode getCondFromBranch(const MachineInstr &MI);\n\n// Turn setCC instruction into condition code.\nCondCode getCondFromSETCC(const MachineInstr &MI);\n\n// Turn CMov instruction into condition code.\nCondCode getCondFromCMov(const MachineInstr &MI);\n\n/// GetOppositeBranchCondition - Return the inverse of the specified cond,\n/// e.g. turning COND_E to COND_NE.\nCondCode GetOppositeBranchCondition(CondCode CC);\n\n/// Get the VPCMP immediate for the given condition.\nunsigned getVPCMPImmForCond(ISD::CondCode CC);\n\n/// Get the VPCMP immediate if the opcodes are swapped.\nunsigned getSwappedVPCMPImm(unsigned Imm);\n\n/// Get the VPCOM immediate if the opcodes are swapped.\nunsigned getSwappedVPCOMImm(unsigned Imm);\n\n/// Get the VCMP immediate if the opcodes are swapped.\nunsigned getSwappedVCMPImm(unsigned Imm);\n\n} // namespace X86\n\n/// isGlobalStubReference - Return true if the specified TargetFlag operand is\n/// a reference to a stub for a global, not the global itself.\ninline static bool isGlobalStubReference(unsigned char TargetFlag) {\n  switch (TargetFlag) {\n  case X86II::MO_DLLIMPORT:               // dllimport stub.\n  case X86II::MO_GOTPCREL:                // rip-relative GOT reference.\n  case X86II::MO_GOT:                     // normal GOT reference.\n  case X86II::MO_DARWIN_NONLAZY_PIC_BASE: // Normal $non_lazy_ptr ref.\n  case X86II::MO_DARWIN_NONLAZY:          // Normal $non_lazy_ptr ref.\n  case X86II::MO_COFFSTUB:                // COFF .refptr stub.\n    return true;\n  default:\n    return false;\n  }\n}\n\n/// isGlobalRelativeToPICBase - Return true if the specified global value\n/// reference is relative to a 32-bit PIC base (X86ISD::GlobalBaseReg).  If this\n/// is true, the addressing mode has the PIC base register added in (e.g. EBX).\ninline static bool isGlobalRelativeToPICBase(unsigned char TargetFlag) {\n  switch (TargetFlag) {\n  case X86II::MO_GOTOFF:                  // isPICStyleGOT: local global.\n  case X86II::MO_GOT:                     // isPICStyleGOT: other global.\n  case X86II::MO_PIC_BASE_OFFSET:         // Darwin local global.\n  case X86II::MO_DARWIN_NONLAZY_PIC_BASE: // Darwin/32 external global.\n  case X86II::MO_TLVP:                    // ??? Pretty sure..\n    return true;\n  default:\n    return false;\n  }\n}\n\ninline static bool isScale(const MachineOperand &MO) {\n  return MO.isImm() && (MO.getImm() == 1 || MO.getImm() == 2 ||\n                        MO.getImm() == 4 || MO.getImm() == 8);\n}\n\ninline static bool isLeaMem(const MachineInstr &MI, unsigned Op) {\n  if (MI.getOperand(Op).isFI())\n    return true;\n  return Op + X86::AddrSegmentReg <= MI.getNumOperands() &&\n         MI.getOperand(Op + X86::AddrBaseReg).isReg() &&\n         isScale(MI.getOperand(Op + X86::AddrScaleAmt)) &&\n         MI.getOperand(Op + X86::AddrIndexReg).isReg() &&\n         (MI.getOperand(Op + X86::AddrDisp).isImm() ||\n          MI.getOperand(Op + X86::AddrDisp).isGlobal() ||\n          MI.getOperand(Op + X86::AddrDisp).isCPI() ||\n          MI.getOperand(Op + X86::AddrDisp).isJTI());\n}\n\ninline static bool isMem(const MachineInstr &MI, unsigned Op) {\n  if (MI.getOperand(Op).isFI())\n    return true;\n  return Op + X86::AddrNumOperands <= MI.getNumOperands() &&\n         MI.getOperand(Op + X86::AddrSegmentReg).isReg() && isLeaMem(MI, Op);\n}\n\nclass X86InstrInfo final : public X86GenInstrInfo {\n  X86Subtarget &Subtarget;\n  const X86RegisterInfo RI;\n\n  virtual void anchor();\n\n  bool AnalyzeBranchImpl(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,\n                         MachineBasicBlock *&FBB,\n                         SmallVectorImpl<MachineOperand> &Cond,\n                         SmallVectorImpl<MachineInstr *> &CondBranches,\n                         bool AllowModify) const;\n\npublic:\n  explicit X86InstrInfo(X86Subtarget &STI);\n\n  /// getRegisterInfo - TargetInstrInfo is a superset of MRegister info.  As\n  /// such, whenever a client has an instance of instruction info, it should\n  /// always be able to get register info as well (through this method).\n  ///\n  const X86RegisterInfo &getRegisterInfo() const { return RI; }\n\n  /// Returns the stack pointer adjustment that happens inside the frame\n  /// setup..destroy sequence (e.g. by pushes, or inside the callee).\n  int64_t getFrameAdjustment(const MachineInstr &I) const {\n    assert(isFrameInstr(I));\n    if (isFrameSetup(I))\n      return I.getOperand(2).getImm();\n    return I.getOperand(1).getImm();\n  }\n\n  /// Sets the stack pointer adjustment made inside the frame made up by this\n  /// instruction.\n  void setFrameAdjustment(MachineInstr &I, int64_t V) const {\n    assert(isFrameInstr(I));\n    if (isFrameSetup(I))\n      I.getOperand(2).setImm(V);\n    else\n      I.getOperand(1).setImm(V);\n  }\n\n  /// getSPAdjust - This returns the stack pointer adjustment made by\n  /// this instruction. For x86, we need to handle more complex call\n  /// sequences involving PUSHes.\n  int getSPAdjust(const MachineInstr &MI) const override;\n\n  /// isCoalescableExtInstr - Return true if the instruction is a \"coalescable\"\n  /// extension instruction. That is, it's like a copy where it's legal for the\n  /// source to overlap the destination. e.g. X86::MOVSX64rr32. If this returns\n  /// true, then it's expected the pre-extension value is available as a subreg\n  /// of the result register. This also returns the sub-register index in\n  /// SubIdx.\n  bool isCoalescableExtInstr(const MachineInstr &MI, Register &SrcReg,\n                             Register &DstReg, unsigned &SubIdx) const override;\n\n  /// Returns true if the instruction has no behavior (specified or otherwise)\n  /// that is based on the value of any of its register operands\n  ///\n  /// Instructions are considered data invariant even if they set EFLAGS.\n  ///\n  /// A classical example of something that is inherently not data invariant is\n  /// an indirect jump -- the destination is loaded into icache based on the\n  /// bits set in the jump destination register.\n  ///\n  /// FIXME: This should become part of our instruction tables.\n  static bool isDataInvariant(MachineInstr &MI);\n\n  /// Returns true if the instruction has no behavior (specified or otherwise)\n  /// that is based on the value loaded from memory or the value of any\n  /// non-address register operands.\n  ///\n  /// For example, if the latency of the instruction is dependent on the\n  /// particular bits set in any of the registers *or* any of the bits loaded\n  /// from memory.\n  ///\n  /// Instructions are considered data invariant even if they set EFLAGS.\n  ///\n  /// A classical example of something that is inherently not data invariant is\n  /// an indirect jump -- the destination is loaded into icache based on the\n  /// bits set in the jump destination register.\n  ///\n  /// FIXME: This should become part of our instruction tables.\n  static bool isDataInvariantLoad(MachineInstr &MI);\n\n  unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                               int &FrameIndex) const override;\n  unsigned isLoadFromStackSlot(const MachineInstr &MI,\n                               int &FrameIndex,\n                               unsigned &MemBytes) const override;\n  /// isLoadFromStackSlotPostFE - Check for post-frame ptr elimination\n  /// stack locations as well.  This uses a heuristic so it isn't\n  /// reliable for correctness.\n  unsigned isLoadFromStackSlotPostFE(const MachineInstr &MI,\n                                     int &FrameIndex) const override;\n\n  unsigned isStoreToStackSlot(const MachineInstr &MI,\n                              int &FrameIndex) const override;\n  unsigned isStoreToStackSlot(const MachineInstr &MI,\n                              int &FrameIndex,\n                              unsigned &MemBytes) const override;\n  /// isStoreToStackSlotPostFE - Check for post-frame ptr elimination\n  /// stack locations as well.  This uses a heuristic so it isn't\n  /// reliable for correctness.\n  unsigned isStoreToStackSlotPostFE(const MachineInstr &MI,\n                                    int &FrameIndex) const override;\n\n  bool isReallyTriviallyReMaterializable(const MachineInstr &MI,\n                                         AAResults *AA) const override;\n  void reMaterialize(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,\n                     Register DestReg, unsigned SubIdx,\n                     const MachineInstr &Orig,\n                     const TargetRegisterInfo &TRI) const override;\n\n  /// Given an operand within a MachineInstr, insert preceding code to put it\n  /// into the right format for a particular kind of LEA instruction. This may\n  /// involve using an appropriate super-register instead (with an implicit use\n  /// of the original) or creating a new virtual register and inserting COPY\n  /// instructions to get the data into the right class.\n  ///\n  /// Reference parameters are set to indicate how caller should add this\n  /// operand to the LEA instruction.\n  bool classifyLEAReg(MachineInstr &MI, const MachineOperand &Src,\n                      unsigned LEAOpcode, bool AllowSP, Register &NewSrc,\n                      bool &isKill, MachineOperand &ImplicitOp,\n                      LiveVariables *LV) const;\n\n  /// convertToThreeAddress - This method must be implemented by targets that\n  /// set the M_CONVERTIBLE_TO_3_ADDR flag.  When this flag is set, the target\n  /// may be able to convert a two-address instruction into a true\n  /// three-address instruction on demand.  This allows the X86 target (for\n  /// example) to convert ADD and SHL instructions into LEA instructions if they\n  /// would require register copies due to two-addressness.\n  ///\n  /// This method returns a null pointer if the transformation cannot be\n  /// performed, otherwise it returns the new instruction.\n  ///\n  MachineInstr *convertToThreeAddress(MachineFunction::iterator &MFI,\n                                      MachineInstr &MI,\n                                      LiveVariables *LV) const override;\n\n  /// Returns true iff the routine could find two commutable operands in the\n  /// given machine instruction.\n  /// The 'SrcOpIdx1' and 'SrcOpIdx2' are INPUT and OUTPUT arguments. Their\n  /// input values can be re-defined in this method only if the input values\n  /// are not pre-defined, which is designated by the special value\n  /// 'CommuteAnyOperandIndex' assigned to it.\n  /// If both of indices are pre-defined and refer to some operands, then the\n  /// method simply returns true if the corresponding operands are commutable\n  /// and returns false otherwise.\n  ///\n  /// For example, calling this method this way:\n  ///     unsigned Op1 = 1, Op2 = CommuteAnyOperandIndex;\n  ///     findCommutedOpIndices(MI, Op1, Op2);\n  /// can be interpreted as a query asking to find an operand that would be\n  /// commutable with the operand#1.\n  bool findCommutedOpIndices(const MachineInstr &MI, unsigned &SrcOpIdx1,\n                             unsigned &SrcOpIdx2) const override;\n\n  /// Returns an adjusted FMA opcode that must be used in FMA instruction that\n  /// performs the same computations as the given \\p MI but which has the\n  /// operands \\p SrcOpIdx1 and \\p SrcOpIdx2 commuted.\n  /// It may return 0 if it is unsafe to commute the operands.\n  /// Note that a machine instruction (instead of its opcode) is passed as the\n  /// first parameter to make it possible to analyze the instruction's uses and\n  /// commute the first operand of FMA even when it seems unsafe when you look\n  /// at the opcode. For example, it is Ok to commute the first operand of\n  /// VFMADD*SD_Int, if ONLY the lowest 64-bit element of the result is used.\n  ///\n  /// The returned FMA opcode may differ from the opcode in the given \\p MI.\n  /// For example, commuting the operands #1 and #3 in the following FMA\n  ///     FMA213 #1, #2, #3\n  /// results into instruction with adjusted opcode:\n  ///     FMA231 #3, #2, #1\n  unsigned\n  getFMA3OpcodeToCommuteOperands(const MachineInstr &MI, unsigned SrcOpIdx1,\n                                 unsigned SrcOpIdx2,\n                                 const X86InstrFMA3Group &FMA3Group) const;\n\n  // Branch analysis.\n  bool isUnconditionalTailCall(const MachineInstr &MI) const override;\n  bool canMakeTailCallConditional(SmallVectorImpl<MachineOperand> &Cond,\n                                  const MachineInstr &TailCall) const override;\n  void replaceBranchWithTailCall(MachineBasicBlock &MBB,\n                                 SmallVectorImpl<MachineOperand> &Cond,\n                                 const MachineInstr &TailCall) const override;\n\n  bool analyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,\n                     MachineBasicBlock *&FBB,\n                     SmallVectorImpl<MachineOperand> &Cond,\n                     bool AllowModify) const override;\n\n  Optional<ExtAddrMode>\n  getAddrModeFromMemoryOp(const MachineInstr &MemI,\n                          const TargetRegisterInfo *TRI) const override;\n\n  bool getConstValDefinedInReg(const MachineInstr &MI, const Register Reg,\n                               int64_t &ImmVal) const override;\n\n  bool preservesZeroValueInReg(const MachineInstr *MI,\n                               const Register NullValueReg,\n                               const TargetRegisterInfo *TRI) const override;\n\n  bool getMemOperandsWithOffsetWidth(\n      const MachineInstr &LdSt,\n      SmallVectorImpl<const MachineOperand *> &BaseOps, int64_t &Offset,\n      bool &OffsetIsScalable, unsigned &Width,\n      const TargetRegisterInfo *TRI) const override;\n  bool analyzeBranchPredicate(MachineBasicBlock &MBB,\n                              TargetInstrInfo::MachineBranchPredicate &MBP,\n                              bool AllowModify = false) const override;\n\n  unsigned removeBranch(MachineBasicBlock &MBB,\n                        int *BytesRemoved = nullptr) const override;\n  unsigned insertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,\n                        MachineBasicBlock *FBB, ArrayRef<MachineOperand> Cond,\n                        const DebugLoc &DL,\n                        int *BytesAdded = nullptr) const override;\n  bool canInsertSelect(const MachineBasicBlock &, ArrayRef<MachineOperand> Cond,\n                       Register, Register, Register, int &, int &,\n                       int &) const override;\n  void insertSelect(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,\n                    const DebugLoc &DL, Register DstReg,\n                    ArrayRef<MachineOperand> Cond, Register TrueReg,\n                    Register FalseReg) const override;\n  void copyPhysReg(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,\n                   const DebugLoc &DL, MCRegister DestReg, MCRegister SrcReg,\n                   bool KillSrc) const override;\n  void storeRegToStackSlot(MachineBasicBlock &MBB,\n                           MachineBasicBlock::iterator MI, Register SrcReg,\n                           bool isKill, int FrameIndex,\n                           const TargetRegisterClass *RC,\n                           const TargetRegisterInfo *TRI) const override;\n\n  void loadRegFromStackSlot(MachineBasicBlock &MBB,\n                            MachineBasicBlock::iterator MI, Register DestReg,\n                            int FrameIndex, const TargetRegisterClass *RC,\n                            const TargetRegisterInfo *TRI) const override;\n\n  bool expandPostRAPseudo(MachineInstr &MI) const override;\n\n  /// Check whether the target can fold a load that feeds a subreg operand\n  /// (or a subreg operand that feeds a store).\n  bool isSubregFoldable() const override { return true; }\n\n  /// foldMemoryOperand - If this target supports it, fold a load or store of\n  /// the specified stack slot into the specified machine instruction for the\n  /// specified operand(s).  If this is possible, the target should perform the\n  /// folding and return true, otherwise it should return false.  If it folds\n  /// the instruction, it is likely that the MachineInstruction the iterator\n  /// references has been changed.\n  MachineInstr *\n  foldMemoryOperandImpl(MachineFunction &MF, MachineInstr &MI,\n                        ArrayRef<unsigned> Ops,\n                        MachineBasicBlock::iterator InsertPt, int FrameIndex,\n                        LiveIntervals *LIS = nullptr,\n                        VirtRegMap *VRM = nullptr) const override;\n\n  /// foldMemoryOperand - Same as the previous version except it allows folding\n  /// of any load and store from / to any address, not just from a specific\n  /// stack slot.\n  MachineInstr *foldMemoryOperandImpl(\n      MachineFunction &MF, MachineInstr &MI, ArrayRef<unsigned> Ops,\n      MachineBasicBlock::iterator InsertPt, MachineInstr &LoadMI,\n      LiveIntervals *LIS = nullptr) const override;\n\n  /// unfoldMemoryOperand - Separate a single instruction which folded a load or\n  /// a store or a load and a store into two or more instruction. If this is\n  /// possible, returns true as well as the new instructions by reference.\n  bool\n  unfoldMemoryOperand(MachineFunction &MF, MachineInstr &MI, unsigned Reg,\n                      bool UnfoldLoad, bool UnfoldStore,\n                      SmallVectorImpl<MachineInstr *> &NewMIs) const override;\n\n  bool unfoldMemoryOperand(SelectionDAG &DAG, SDNode *N,\n                           SmallVectorImpl<SDNode *> &NewNodes) const override;\n\n  /// getOpcodeAfterMemoryUnfold - Returns the opcode of the would be new\n  /// instruction after load / store are unfolded from an instruction of the\n  /// specified opcode. It returns zero if the specified unfolding is not\n  /// possible. If LoadRegIndex is non-null, it is filled in with the operand\n  /// index of the operand which will hold the register holding the loaded\n  /// value.\n  unsigned\n  getOpcodeAfterMemoryUnfold(unsigned Opc, bool UnfoldLoad, bool UnfoldStore,\n                             unsigned *LoadRegIndex = nullptr) const override;\n\n  /// areLoadsFromSameBasePtr - This is used by the pre-regalloc scheduler\n  /// to determine if two loads are loading from the same base address. It\n  /// should only return true if the base pointers are the same and the\n  /// only differences between the two addresses are the offset. It also returns\n  /// the offsets by reference.\n  bool areLoadsFromSameBasePtr(SDNode *Load1, SDNode *Load2, int64_t &Offset1,\n                               int64_t &Offset2) const override;\n\n  /// isSchedulingBoundary - Overrides the isSchedulingBoundary from\n  ///\tCodegen/TargetInstrInfo.cpp to make it capable of identifying ENDBR\n  /// intructions and prevent it from being re-scheduled.\n  bool isSchedulingBoundary(const MachineInstr &MI,\n                            const MachineBasicBlock *MBB,\n                            const MachineFunction &MF) const override;\n\n  /// shouldScheduleLoadsNear - This is a used by the pre-regalloc scheduler to\n  /// determine (in conjunction with areLoadsFromSameBasePtr) if two loads\n  /// should be scheduled togther. On some targets if two loads are loading from\n  /// addresses in the same cache line, it's better if they are scheduled\n  /// together. This function takes two integers that represent the load offsets\n  /// from the common base address. It returns true if it decides it's desirable\n  /// to schedule the two loads together. \"NumLoads\" is the number of loads that\n  /// have already been scheduled after Load1.\n  bool shouldScheduleLoadsNear(SDNode *Load1, SDNode *Load2, int64_t Offset1,\n                               int64_t Offset2,\n                               unsigned NumLoads) const override;\n\n  void getNoop(MCInst &NopInst) const override;\n\n  bool\n  reverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const override;\n\n  /// isSafeToMoveRegClassDefs - Return true if it's safe to move a machine\n  /// instruction that defines the specified register class.\n  bool isSafeToMoveRegClassDefs(const TargetRegisterClass *RC) const override;\n\n  /// True if MI has a condition code def, e.g. EFLAGS, that is\n  /// not marked dead.\n  bool hasLiveCondCodeDef(MachineInstr &MI) const;\n\n  /// getGlobalBaseReg - Return a virtual register initialized with the\n  /// the global base register value. Output instructions required to\n  /// initialize the register in the function entry block, if necessary.\n  ///\n  unsigned getGlobalBaseReg(MachineFunction *MF) const;\n\n  std::pair<uint16_t, uint16_t>\n  getExecutionDomain(const MachineInstr &MI) const override;\n\n  uint16_t getExecutionDomainCustom(const MachineInstr &MI) const;\n\n  void setExecutionDomain(MachineInstr &MI, unsigned Domain) const override;\n\n  bool setExecutionDomainCustom(MachineInstr &MI, unsigned Domain) const;\n\n  unsigned\n  getPartialRegUpdateClearance(const MachineInstr &MI, unsigned OpNum,\n                               const TargetRegisterInfo *TRI) const override;\n  unsigned getUndefRegClearance(const MachineInstr &MI, unsigned OpNum,\n                                const TargetRegisterInfo *TRI) const override;\n  void breakPartialRegDependency(MachineInstr &MI, unsigned OpNum,\n                                 const TargetRegisterInfo *TRI) const override;\n\n  MachineInstr *foldMemoryOperandImpl(MachineFunction &MF, MachineInstr &MI,\n                                      unsigned OpNum,\n                                      ArrayRef<MachineOperand> MOs,\n                                      MachineBasicBlock::iterator InsertPt,\n                                      unsigned Size, Align Alignment,\n                                      bool AllowCommute) const;\n\n  bool isHighLatencyDef(int opc) const override;\n\n  bool hasHighOperandLatency(const TargetSchedModel &SchedModel,\n                             const MachineRegisterInfo *MRI,\n                             const MachineInstr &DefMI, unsigned DefIdx,\n                             const MachineInstr &UseMI,\n                             unsigned UseIdx) const override;\n\n  bool useMachineCombiner() const override { return true; }\n\n  bool isAssociativeAndCommutative(const MachineInstr &Inst) const override;\n\n  bool hasReassociableOperands(const MachineInstr &Inst,\n                               const MachineBasicBlock *MBB) const override;\n\n  void setSpecialOperandAttr(MachineInstr &OldMI1, MachineInstr &OldMI2,\n                             MachineInstr &NewMI1,\n                             MachineInstr &NewMI2) const override;\n\n  /// analyzeCompare - For a comparison instruction, return the source registers\n  /// in SrcReg and SrcReg2 if having two register operands, and the value it\n  /// compares against in CmpValue. Return true if the comparison instruction\n  /// can be analyzed.\n  bool analyzeCompare(const MachineInstr &MI, Register &SrcReg,\n                      Register &SrcReg2, int &CmpMask,\n                      int &CmpValue) const override;\n\n  /// optimizeCompareInstr - Check if there exists an earlier instruction that\n  /// operates on the same source operands and sets flags in the same way as\n  /// Compare; remove Compare if possible.\n  bool optimizeCompareInstr(MachineInstr &CmpInstr, Register SrcReg,\n                            Register SrcReg2, int CmpMask, int CmpValue,\n                            const MachineRegisterInfo *MRI) const override;\n\n  /// optimizeLoadInstr - Try to remove the load by folding it to a register\n  /// operand at the use. We fold the load instructions if and only if the\n  /// def and use are in the same BB. We only look at one load and see\n  /// whether it can be folded into MI. FoldAsLoadDefReg is the virtual register\n  /// defined by the load we are trying to fold. DefMI returns the machine\n  /// instruction that defines FoldAsLoadDefReg, and the function returns\n  /// the machine instruction generated due to folding.\n  MachineInstr *optimizeLoadInstr(MachineInstr &MI,\n                                  const MachineRegisterInfo *MRI,\n                                  Register &FoldAsLoadDefReg,\n                                  MachineInstr *&DefMI) const override;\n\n  std::pair<unsigned, unsigned>\n  decomposeMachineOperandsTargetFlags(unsigned TF) const override;\n\n  ArrayRef<std::pair<unsigned, const char *>>\n  getSerializableDirectMachineOperandTargetFlags() const override;\n\n  virtual outliner::OutlinedFunction getOutliningCandidateInfo(\n      std::vector<outliner::Candidate> &RepeatedSequenceLocs) const override;\n\n  bool isFunctionSafeToOutlineFrom(MachineFunction &MF,\n                                   bool OutlineFromLinkOnceODRs) const override;\n\n  outliner::InstrType\n  getOutliningType(MachineBasicBlock::iterator &MIT, unsigned Flags) const override;\n\n  void buildOutlinedFrame(MachineBasicBlock &MBB, MachineFunction &MF,\n                          const outliner::OutlinedFunction &OF) const override;\n\n  MachineBasicBlock::iterator\n  insertOutlinedCall(Module &M, MachineBasicBlock &MBB,\n                     MachineBasicBlock::iterator &It, MachineFunction &MF,\n                     const outliner::Candidate &C) const override;\n\n#define GET_INSTRINFO_HELPER_DECLS\n#include \"X86GenInstrInfo.inc\"\n\n  static bool hasLockPrefix(const MachineInstr &MI) {\n    return MI.getDesc().TSFlags & X86II::LOCK;\n  }\n\n  Optional<ParamLoadedValue> describeLoadedValue(const MachineInstr &MI,\n                                                 Register Reg) const override;\n\nprotected:\n  /// Commutes the operands in the given instruction by changing the operands\n  /// order and/or changing the instruction's opcode and/or the immediate value\n  /// operand.\n  ///\n  /// The arguments 'CommuteOpIdx1' and 'CommuteOpIdx2' specify the operands\n  /// to be commuted.\n  ///\n  /// Do not call this method for a non-commutable instruction or\n  /// non-commutable operands.\n  /// Even though the instruction is commutable, the method may still\n  /// fail to commute the operands, null pointer is returned in such cases.\n  MachineInstr *commuteInstructionImpl(MachineInstr &MI, bool NewMI,\n                                       unsigned CommuteOpIdx1,\n                                       unsigned CommuteOpIdx2) const override;\n\n  /// If the specific machine instruction is a instruction that moves/copies\n  /// value from one register to another register return destination and source\n  /// registers as machine operands.\n  Optional<DestSourcePair>\n  isCopyInstrImpl(const MachineInstr &MI) const override;\n\nprivate:\n  /// This is a helper for convertToThreeAddress for 8 and 16-bit instructions.\n  /// We use 32-bit LEA to form 3-address code by promoting to a 32-bit\n  /// super-register and then truncating back down to a 8/16-bit sub-register.\n  MachineInstr *convertToThreeAddressWithLEA(unsigned MIOpc,\n                                             MachineFunction::iterator &MFI,\n                                             MachineInstr &MI,\n                                             LiveVariables *LV,\n                                             bool Is8BitOp) const;\n\n  /// Handles memory folding for special case instructions, for instance those\n  /// requiring custom manipulation of the address.\n  MachineInstr *foldMemoryOperandCustom(MachineFunction &MF, MachineInstr &MI,\n                                        unsigned OpNum,\n                                        ArrayRef<MachineOperand> MOs,\n                                        MachineBasicBlock::iterator InsertPt,\n                                        unsigned Size, Align Alignment) const;\n\n  /// isFrameOperand - Return true and the FrameIndex if the specified\n  /// operand and follow operands form a reference to the stack frame.\n  bool isFrameOperand(const MachineInstr &MI, unsigned int Op,\n                      int &FrameIndex) const;\n\n  /// Returns true iff the routine could find two commutable operands in the\n  /// given machine instruction with 3 vector inputs.\n  /// The 'SrcOpIdx1' and 'SrcOpIdx2' are INPUT and OUTPUT arguments. Their\n  /// input values can be re-defined in this method only if the input values\n  /// are not pre-defined, which is designated by the special value\n  /// 'CommuteAnyOperandIndex' assigned to it.\n  /// If both of indices are pre-defined and refer to some operands, then the\n  /// method simply returns true if the corresponding operands are commutable\n  /// and returns false otherwise.\n  ///\n  /// For example, calling this method this way:\n  ///     unsigned Op1 = 1, Op2 = CommuteAnyOperandIndex;\n  ///     findThreeSrcCommutedOpIndices(MI, Op1, Op2);\n  /// can be interpreted as a query asking to find an operand that would be\n  /// commutable with the operand#1.\n  ///\n  /// If IsIntrinsic is set, operand 1 will be ignored for commuting.\n  bool findThreeSrcCommutedOpIndices(const MachineInstr &MI,\n                                     unsigned &SrcOpIdx1,\n                                     unsigned &SrcOpIdx2,\n                                     bool IsIntrinsic = false) const;\n};\n\n} // namespace llvm\n\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 20, "file": 4, "line": 1188}, "message": "the definition seen here"}, {"location": {"col": 8, "file": 3, "line": 250}, "message": "differing parameters are named here: ('LEAOpcode'), in definition: ('Opc')"}, {"location": {"col": 8, "file": 3, "line": 250}, "message": "function 'llvm::X86InstrInfo::classifyLEAReg' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86InstrInfo.h", "reportHash": "8710ebe5b0797d4963f89a9794129dd8", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 20, "file": 4, "line": 3734}, "message": "the definition seen here"}, {"location": {"col": 8, "file": 3, "line": 331}, "message": "differing parameters are named here: ('LdSt'), in definition: ('MemOp')"}, {"location": {"col": 8, "file": 3, "line": 331}, "message": "function 'llvm::X86InstrInfo::getMemOperandsWithOffsetWidth' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/lib/Target/X86/X86InstrInfo.h", "reportHash": "fe43edd713b0fca641d1b2e70eb89b13", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
