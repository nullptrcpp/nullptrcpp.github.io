<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"2": {"id": 2, "path": "/home/vsts/work/1/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp", "content": "//===- ScalarEvolution.cpp - Scalar Evolution Analysis --------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file contains the implementation of the scalar evolution analysis\n// engine, which is used primarily to analyze expressions involving induction\n// variables in loops.\n//\n// There are several aspects to this library.  First is the representation of\n// scalar expressions, which are represented as subclasses of the SCEV class.\n// These classes are used to represent certain types of subexpressions that we\n// can handle. We only create one SCEV of a particular shape, so\n// pointer-comparisons for equality are legal.\n//\n// One important aspect of the SCEV objects is that they are never cyclic, even\n// if there is a cycle in the dataflow for an expression (ie, a PHI node).  If\n// the PHI node is one of the idioms that we can represent (e.g., a polynomial\n// recurrence) then we represent it directly as a recurrence node, otherwise we\n// represent it as a SCEVUnknown node.\n//\n// In addition to being able to represent expressions of various types, we also\n// have folders that are used to build the *canonical* representation for a\n// particular expression.  These folders are capable of using a variety of\n// rewrite rules to simplify the expressions.\n//\n// Once the folders are defined, we can implement the more interesting\n// higher-level code, such as the code that recognizes PHI nodes of various\n// types, computes the execution count of a loop, etc.\n//\n// TODO: We should use these routines and value representations to implement\n// dependence analysis!\n//\n//===----------------------------------------------------------------------===//\n//\n// There are several good references for the techniques used in this analysis.\n//\n//  Chains of recurrences -- a method to expedite the evaluation\n//  of closed-form functions\n//  Olaf Bachmann, Paul S. Wang, Eugene V. Zima\n//\n//  On computational properties of chains of recurrences\n//  Eugene V. Zima\n//\n//  Symbolic Evaluation of Chains of Recurrences for Loop Optimization\n//  Robert A. van Engelen\n//\n//  Efficient Symbolic Analysis for Optimizing Compilers\n//  Robert A. van Engelen\n//\n//  Using the chains of recurrences algebra for data dependence testing and\n//  induction variable substitution\n//  MS Thesis, Johnie Birch\n//\n//===----------------------------------------------------------------------===//\n\n#include \"llvm/Analysis/ScalarEvolution.h\"\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DepthFirstIterator.h\"\n#include \"llvm/ADT/EquivalenceClasses.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/None.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/ScopeExit.h\"\n#include \"llvm/ADT/Sequence.h\"\n#include \"llvm/ADT/SetVector.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/Statistic.h\"\n#include \"llvm/ADT/StringRef.h\"\n#include \"llvm/Analysis/AssumptionCache.h\"\n#include \"llvm/Analysis/ConstantFolding.h\"\n#include \"llvm/Analysis/InstructionSimplify.h\"\n#include \"llvm/Analysis/LoopInfo.h\"\n#include \"llvm/Analysis/ScalarEvolutionDivision.h\"\n#include \"llvm/Analysis/ScalarEvolutionExpressions.h\"\n#include \"llvm/Analysis/TargetLibraryInfo.h\"\n#include \"llvm/Analysis/ValueTracking.h\"\n#include \"llvm/Config/llvm-config.h\"\n#include \"llvm/IR/Argument.h\"\n#include \"llvm/IR/BasicBlock.h\"\n#include \"llvm/IR/CFG.h\"\n#include \"llvm/IR/Constant.h\"\n#include \"llvm/IR/ConstantRange.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/DataLayout.h\"\n#include \"llvm/IR/DerivedTypes.h\"\n#include \"llvm/IR/Dominators.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/GlobalAlias.h\"\n#include \"llvm/IR/GlobalValue.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/InstIterator.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instruction.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/IntrinsicInst.h\"\n#include \"llvm/IR/Intrinsics.h\"\n#include \"llvm/IR/LLVMContext.h\"\n#include \"llvm/IR/Metadata.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/IR/PatternMatch.h\"\n#include \"llvm/IR/Type.h\"\n#include \"llvm/IR/Use.h\"\n#include \"llvm/IR/User.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/IR/Verifier.h\"\n#include \"llvm/InitializePasses.h\"\n#include \"llvm/Pass.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/CommandLine.h\"\n#include \"llvm/Support/Compiler.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include \"llvm/Support/KnownBits.h\"\n#include \"llvm/Support/SaveAndRestore.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstddef>\n#include <cstdint>\n#include <cstdlib>\n#include <map>\n#include <memory>\n#include <tuple>\n#include <utility>\n#include <vector>\n\nusing namespace llvm;\nusing namespace PatternMatch;\n\n#define DEBUG_TYPE \"scalar-evolution\"\n\nSTATISTIC(NumArrayLenItCounts,\n          \"Number of trip counts computed with array length\");\nSTATISTIC(NumTripCountsComputed,\n          \"Number of loops with predictable loop counts\");\nSTATISTIC(NumTripCountsNotComputed,\n          \"Number of loops without predictable loop counts\");\nSTATISTIC(NumBruteForceTripCountsComputed,\n          \"Number of loops with trip counts computed by force\");\n\nstatic cl::opt<unsigned>\nMaxBruteForceIterations(\"scalar-evolution-max-iterations\", cl::ReallyHidden,\n                        cl::ZeroOrMore,\n                        cl::desc(\"Maximum number of iterations SCEV will \"\n                                 \"symbolically execute a constant \"\n                                 \"derived loop\"),\n                        cl::init(100));\n\n// FIXME: Enable this with EXPENSIVE_CHECKS when the test suite is clean.\nstatic cl::opt<bool> VerifySCEV(\n    \"verify-scev\", cl::Hidden,\n    cl::desc(\"Verify ScalarEvolution's backedge taken counts (slow)\"));\nstatic cl::opt<bool> VerifySCEVStrict(\n    \"verify-scev-strict\", cl::Hidden,\n    cl::desc(\"Enable stricter verification with -verify-scev is passed\"));\nstatic cl::opt<bool>\n    VerifySCEVMap(\"verify-scev-maps\", cl::Hidden,\n                  cl::desc(\"Verify no dangling value in ScalarEvolution's \"\n                           \"ExprValueMap (slow)\"));\n\nstatic cl::opt<bool> VerifyIR(\n    \"scev-verify-ir\", cl::Hidden,\n    cl::desc(\"Verify IR correctness when making sensitive SCEV queries (slow)\"),\n    cl::init(false));\n\nstatic cl::opt<unsigned> MulOpsInlineThreshold(\n    \"scev-mulops-inline-threshold\", cl::Hidden,\n    cl::desc(\"Threshold for inlining multiplication operands into a SCEV\"),\n    cl::init(32));\n\nstatic cl::opt<unsigned> AddOpsInlineThreshold(\n    \"scev-addops-inline-threshold\", cl::Hidden,\n    cl::desc(\"Threshold for inlining addition operands into a SCEV\"),\n    cl::init(500));\n\nstatic cl::opt<unsigned> MaxSCEVCompareDepth(\n    \"scalar-evolution-max-scev-compare-depth\", cl::Hidden,\n    cl::desc(\"Maximum depth of recursive SCEV complexity comparisons\"),\n    cl::init(32));\n\nstatic cl::opt<unsigned> MaxSCEVOperationsImplicationDepth(\n    \"scalar-evolution-max-scev-operations-implication-depth\", cl::Hidden,\n    cl::desc(\"Maximum depth of recursive SCEV operations implication analysis\"),\n    cl::init(2));\n\nstatic cl::opt<unsigned> MaxValueCompareDepth(\n    \"scalar-evolution-max-value-compare-depth\", cl::Hidden,\n    cl::desc(\"Maximum depth of recursive value complexity comparisons\"),\n    cl::init(2));\n\nstatic cl::opt<unsigned>\n    MaxArithDepth(\"scalar-evolution-max-arith-depth\", cl::Hidden,\n                  cl::desc(\"Maximum depth of recursive arithmetics\"),\n                  cl::init(32));\n\nstatic cl::opt<unsigned> MaxConstantEvolvingDepth(\n    \"scalar-evolution-max-constant-evolving-depth\", cl::Hidden,\n    cl::desc(\"Maximum depth of recursive constant evolving\"), cl::init(32));\n\nstatic cl::opt<unsigned>\n    MaxCastDepth(\"scalar-evolution-max-cast-depth\", cl::Hidden,\n                 cl::desc(\"Maximum depth of recursive SExt/ZExt/Trunc\"),\n                 cl::init(8));\n\nstatic cl::opt<unsigned>\n    MaxAddRecSize(\"scalar-evolution-max-add-rec-size\", cl::Hidden,\n                  cl::desc(\"Max coefficients in AddRec during evolving\"),\n                  cl::init(8));\n\nstatic cl::opt<unsigned>\n    HugeExprThreshold(\"scalar-evolution-huge-expr-threshold\", cl::Hidden,\n                  cl::desc(\"Size of the expression which is considered huge\"),\n                  cl::init(4096));\n\nstatic cl::opt<bool>\nClassifyExpressions(\"scalar-evolution-classify-expressions\",\n    cl::Hidden, cl::init(true),\n    cl::desc(\"When printing analysis, include information on every instruction\"));\n\nstatic cl::opt<bool> UseExpensiveRangeSharpening(\n    \"scalar-evolution-use-expensive-range-sharpening\", cl::Hidden,\n    cl::init(false),\n    cl::desc(\"Use more powerful methods of sharpening expression ranges. May \"\n             \"be costly in terms of compile time\"));\n\n//===----------------------------------------------------------------------===//\n//                           SCEV class definitions\n//===----------------------------------------------------------------------===//\n\n//===----------------------------------------------------------------------===//\n// Implementation of the SCEV class.\n//\n\n#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)\nLLVM_DUMP_METHOD void SCEV::dump() const {\n  print(dbgs());\n  dbgs() << '\\n';\n}\n#endif\n\nvoid SCEV::print(raw_ostream &OS) const {\n  switch (getSCEVType()) {\n  case scConstant:\n    cast<SCEVConstant>(this)->getValue()->printAsOperand(OS, false);\n    return;\n  case scPtrToInt: {\n    const SCEVPtrToIntExpr *PtrToInt = cast<SCEVPtrToIntExpr>(this);\n    const SCEV *Op = PtrToInt->getOperand();\n    OS << \"(ptrtoint \" << *Op->getType() << \" \" << *Op << \" to \"\n       << *PtrToInt->getType() << \")\";\n    return;\n  }\n  case scTruncate: {\n    const SCEVTruncateExpr *Trunc = cast<SCEVTruncateExpr>(this);\n    const SCEV *Op = Trunc->getOperand();\n    OS << \"(trunc \" << *Op->getType() << \" \" << *Op << \" to \"\n       << *Trunc->getType() << \")\";\n    return;\n  }\n  case scZeroExtend: {\n    const SCEVZeroExtendExpr *ZExt = cast<SCEVZeroExtendExpr>(this);\n    const SCEV *Op = ZExt->getOperand();\n    OS << \"(zext \" << *Op->getType() << \" \" << *Op << \" to \"\n       << *ZExt->getType() << \")\";\n    return;\n  }\n  case scSignExtend: {\n    const SCEVSignExtendExpr *SExt = cast<SCEVSignExtendExpr>(this);\n    const SCEV *Op = SExt->getOperand();\n    OS << \"(sext \" << *Op->getType() << \" \" << *Op << \" to \"\n       << *SExt->getType() << \")\";\n    return;\n  }\n  case scAddRecExpr: {\n    const SCEVAddRecExpr *AR = cast<SCEVAddRecExpr>(this);\n    OS << \"{\" << *AR->getOperand(0);\n    for (unsigned i = 1, e = AR->getNumOperands(); i != e; ++i)\n      OS << \",+,\" << *AR->getOperand(i);\n    OS << \"}<\";\n    if (AR->hasNoUnsignedWrap())\n      OS << \"nuw><\";\n    if (AR->hasNoSignedWrap())\n      OS << \"nsw><\";\n    if (AR->hasNoSelfWrap() &&\n        !AR->getNoWrapFlags((NoWrapFlags)(FlagNUW | FlagNSW)))\n      OS << \"nw><\";\n    AR->getLoop()->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n    OS << \">\";\n    return;\n  }\n  case scAddExpr:\n  case scMulExpr:\n  case scUMaxExpr:\n  case scSMaxExpr:\n  case scUMinExpr:\n  case scSMinExpr: {\n    const SCEVNAryExpr *NAry = cast<SCEVNAryExpr>(this);\n    const char *OpStr = nullptr;\n    switch (NAry->getSCEVType()) {\n    case scAddExpr: OpStr = \" + \"; break;\n    case scMulExpr: OpStr = \" * \"; break;\n    case scUMaxExpr: OpStr = \" umax \"; break;\n    case scSMaxExpr: OpStr = \" smax \"; break;\n    case scUMinExpr:\n      OpStr = \" umin \";\n      break;\n    case scSMinExpr:\n      OpStr = \" smin \";\n      break;\n    default:\n      llvm_unreachable(\"There are no other nary expression types.\");\n    }\n    OS << \"(\";\n    for (SCEVNAryExpr::op_iterator I = NAry->op_begin(), E = NAry->op_end();\n         I != E; ++I) {\n      OS << **I;\n      if (std::next(I) != E)\n        OS << OpStr;\n    }\n    OS << \")\";\n    switch (NAry->getSCEVType()) {\n    case scAddExpr:\n    case scMulExpr:\n      if (NAry->hasNoUnsignedWrap())\n        OS << \"<nuw>\";\n      if (NAry->hasNoSignedWrap())\n        OS << \"<nsw>\";\n      break;\n    default:\n      // Nothing to print for other nary expressions.\n      break;\n    }\n    return;\n  }\n  case scUDivExpr: {\n    const SCEVUDivExpr *UDiv = cast<SCEVUDivExpr>(this);\n    OS << \"(\" << *UDiv->getLHS() << \" /u \" << *UDiv->getRHS() << \")\";\n    return;\n  }\n  case scUnknown: {\n    const SCEVUnknown *U = cast<SCEVUnknown>(this);\n    Type *AllocTy;\n    if (U->isSizeOf(AllocTy)) {\n      OS << \"sizeof(\" << *AllocTy << \")\";\n      return;\n    }\n    if (U->isAlignOf(AllocTy)) {\n      OS << \"alignof(\" << *AllocTy << \")\";\n      return;\n    }\n\n    Type *CTy;\n    Constant *FieldNo;\n    if (U->isOffsetOf(CTy, FieldNo)) {\n      OS << \"offsetof(\" << *CTy << \", \";\n      FieldNo->printAsOperand(OS, false);\n      OS << \")\";\n      return;\n    }\n\n    // Otherwise just print it normally.\n    U->getValue()->printAsOperand(OS, false);\n    return;\n  }\n  case scCouldNotCompute:\n    OS << \"***COULDNOTCOMPUTE***\";\n    return;\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\nType *SCEV::getType() const {\n  switch (getSCEVType()) {\n  case scConstant:\n    return cast<SCEVConstant>(this)->getType();\n  case scPtrToInt:\n  case scTruncate:\n  case scZeroExtend:\n  case scSignExtend:\n    return cast<SCEVCastExpr>(this)->getType();\n  case scAddRecExpr:\n  case scMulExpr:\n  case scUMaxExpr:\n  case scSMaxExpr:\n  case scUMinExpr:\n  case scSMinExpr:\n    return cast<SCEVNAryExpr>(this)->getType();\n  case scAddExpr:\n    return cast<SCEVAddExpr>(this)->getType();\n  case scUDivExpr:\n    return cast<SCEVUDivExpr>(this)->getType();\n  case scUnknown:\n    return cast<SCEVUnknown>(this)->getType();\n  case scCouldNotCompute:\n    llvm_unreachable(\"Attempt to use a SCEVCouldNotCompute object!\");\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\nbool SCEV::isZero() const {\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(this))\n    return SC->getValue()->isZero();\n  return false;\n}\n\nbool SCEV::isOne() const {\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(this))\n    return SC->getValue()->isOne();\n  return false;\n}\n\nbool SCEV::isAllOnesValue() const {\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(this))\n    return SC->getValue()->isMinusOne();\n  return false;\n}\n\nbool SCEV::isNonConstantNegative() const {\n  const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(this);\n  if (!Mul) return false;\n\n  // If there is a constant factor, it will be first.\n  const SCEVConstant *SC = dyn_cast<SCEVConstant>(Mul->getOperand(0));\n  if (!SC) return false;\n\n  // Return true if the value is negative, this matches things like (-42 * V).\n  return SC->getAPInt().isNegative();\n}\n\nSCEVCouldNotCompute::SCEVCouldNotCompute() :\n  SCEV(FoldingSetNodeIDRef(), scCouldNotCompute, 0) {}\n\nbool SCEVCouldNotCompute::classof(const SCEV *S) {\n  return S->getSCEVType() == scCouldNotCompute;\n}\n\nconst SCEV *ScalarEvolution::getConstant(ConstantInt *V) {\n  FoldingSetNodeID ID;\n  ID.AddInteger(scConstant);\n  ID.AddPointer(V);\n  void *IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  SCEV *S = new (SCEVAllocator) SCEVConstant(ID.Intern(SCEVAllocator), V);\n  UniqueSCEVs.InsertNode(S, IP);\n  return S;\n}\n\nconst SCEV *ScalarEvolution::getConstant(const APInt &Val) {\n  return getConstant(ConstantInt::get(getContext(), Val));\n}\n\nconst SCEV *\nScalarEvolution::getConstant(Type *Ty, uint64_t V, bool isSigned) {\n  IntegerType *ITy = cast<IntegerType>(getEffectiveSCEVType(Ty));\n  return getConstant(ConstantInt::get(ITy, V, isSigned));\n}\n\nSCEVCastExpr::SCEVCastExpr(const FoldingSetNodeIDRef ID, SCEVTypes SCEVTy,\n                           const SCEV *op, Type *ty)\n    : SCEV(ID, SCEVTy, computeExpressionSize(op)), Ty(ty) {\n  Operands[0] = op;\n}\n\nSCEVPtrToIntExpr::SCEVPtrToIntExpr(const FoldingSetNodeIDRef ID, const SCEV *Op,\n                                   Type *ITy)\n    : SCEVCastExpr(ID, scPtrToInt, Op, ITy) {\n  assert(getOperand()->getType()->isPointerTy() && Ty->isIntegerTy() &&\n         \"Must be a non-bit-width-changing pointer-to-integer cast!\");\n}\n\nSCEVIntegralCastExpr::SCEVIntegralCastExpr(const FoldingSetNodeIDRef ID,\n                                           SCEVTypes SCEVTy, const SCEV *op,\n                                           Type *ty)\n    : SCEVCastExpr(ID, SCEVTy, op, ty) {}\n\nSCEVTruncateExpr::SCEVTruncateExpr(const FoldingSetNodeIDRef ID, const SCEV *op,\n                                   Type *ty)\n    : SCEVIntegralCastExpr(ID, scTruncate, op, ty) {\n  assert(getOperand()->getType()->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot truncate non-integer value!\");\n}\n\nSCEVZeroExtendExpr::SCEVZeroExtendExpr(const FoldingSetNodeIDRef ID,\n                                       const SCEV *op, Type *ty)\n    : SCEVIntegralCastExpr(ID, scZeroExtend, op, ty) {\n  assert(getOperand()->getType()->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot zero extend non-integer value!\");\n}\n\nSCEVSignExtendExpr::SCEVSignExtendExpr(const FoldingSetNodeIDRef ID,\n                                       const SCEV *op, Type *ty)\n    : SCEVIntegralCastExpr(ID, scSignExtend, op, ty) {\n  assert(getOperand()->getType()->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot sign extend non-integer value!\");\n}\n\nvoid SCEVUnknown::deleted() {\n  // Clear this SCEVUnknown from various maps.\n  SE->forgetMemoizedResults(this);\n\n  // Remove this SCEVUnknown from the uniquing map.\n  SE->UniqueSCEVs.RemoveNode(this);\n\n  // Release the value.\n  setValPtr(nullptr);\n}\n\nvoid SCEVUnknown::allUsesReplacedWith(Value *New) {\n  // Remove this SCEVUnknown from the uniquing map.\n  SE->UniqueSCEVs.RemoveNode(this);\n\n  // Update this SCEVUnknown to point to the new value. This is needed\n  // because there may still be outstanding SCEVs which still point to\n  // this SCEVUnknown.\n  setValPtr(New);\n}\n\nbool SCEVUnknown::isSizeOf(Type *&AllocTy) const {\n  if (ConstantExpr *VCE = dyn_cast<ConstantExpr>(getValue()))\n    if (VCE->getOpcode() == Instruction::PtrToInt)\n      if (ConstantExpr *CE = dyn_cast<ConstantExpr>(VCE->getOperand(0)))\n        if (CE->getOpcode() == Instruction::GetElementPtr &&\n            CE->getOperand(0)->isNullValue() &&\n            CE->getNumOperands() == 2)\n          if (ConstantInt *CI = dyn_cast<ConstantInt>(CE->getOperand(1)))\n            if (CI->isOne()) {\n              AllocTy = cast<PointerType>(CE->getOperand(0)->getType())\n                                 ->getElementType();\n              return true;\n            }\n\n  return false;\n}\n\nbool SCEVUnknown::isAlignOf(Type *&AllocTy) const {\n  if (ConstantExpr *VCE = dyn_cast<ConstantExpr>(getValue()))\n    if (VCE->getOpcode() == Instruction::PtrToInt)\n      if (ConstantExpr *CE = dyn_cast<ConstantExpr>(VCE->getOperand(0)))\n        if (CE->getOpcode() == Instruction::GetElementPtr &&\n            CE->getOperand(0)->isNullValue()) {\n          Type *Ty =\n            cast<PointerType>(CE->getOperand(0)->getType())->getElementType();\n          if (StructType *STy = dyn_cast<StructType>(Ty))\n            if (!STy->isPacked() &&\n                CE->getNumOperands() == 3 &&\n                CE->getOperand(1)->isNullValue()) {\n              if (ConstantInt *CI = dyn_cast<ConstantInt>(CE->getOperand(2)))\n                if (CI->isOne() &&\n                    STy->getNumElements() == 2 &&\n                    STy->getElementType(0)->isIntegerTy(1)) {\n                  AllocTy = STy->getElementType(1);\n                  return true;\n                }\n            }\n        }\n\n  return false;\n}\n\nbool SCEVUnknown::isOffsetOf(Type *&CTy, Constant *&FieldNo) const {\n  if (ConstantExpr *VCE = dyn_cast<ConstantExpr>(getValue()))\n    if (VCE->getOpcode() == Instruction::PtrToInt)\n      if (ConstantExpr *CE = dyn_cast<ConstantExpr>(VCE->getOperand(0)))\n        if (CE->getOpcode() == Instruction::GetElementPtr &&\n            CE->getNumOperands() == 3 &&\n            CE->getOperand(0)->isNullValue() &&\n            CE->getOperand(1)->isNullValue()) {\n          Type *Ty =\n            cast<PointerType>(CE->getOperand(0)->getType())->getElementType();\n          // Ignore vector types here so that ScalarEvolutionExpander doesn't\n          // emit getelementptrs that index into vectors.\n          if (Ty->isStructTy() || Ty->isArrayTy()) {\n            CTy = Ty;\n            FieldNo = CE->getOperand(2);\n            return true;\n          }\n        }\n\n  return false;\n}\n\n//===----------------------------------------------------------------------===//\n//                               SCEV Utilities\n//===----------------------------------------------------------------------===//\n\n/// Compare the two values \\p LV and \\p RV in terms of their \"complexity\" where\n/// \"complexity\" is a partial (and somewhat ad-hoc) relation used to order\n/// operands in SCEV expressions.  \\p EqCache is a set of pairs of values that\n/// have been previously deemed to be \"equally complex\" by this routine.  It is\n/// intended to avoid exponential time complexity in cases like:\n///\n///   %a = f(%x, %y)\n///   %b = f(%a, %a)\n///   %c = f(%b, %b)\n///\n///   %d = f(%x, %y)\n///   %e = f(%d, %d)\n///   %f = f(%e, %e)\n///\n///   CompareValueComplexity(%f, %c)\n///\n/// Since we do not continue running this routine on expression trees once we\n/// have seen unequal values, there is no need to track them in the cache.\nstatic int\nCompareValueComplexity(EquivalenceClasses<const Value *> &EqCacheValue,\n                       const LoopInfo *const LI, Value *LV, Value *RV,\n                       unsigned Depth) {\n  if (Depth > MaxValueCompareDepth || EqCacheValue.isEquivalent(LV, RV))\n    return 0;\n\n  // Order pointer values after integer values. This helps SCEVExpander form\n  // GEPs.\n  bool LIsPointer = LV->getType()->isPointerTy(),\n       RIsPointer = RV->getType()->isPointerTy();\n  if (LIsPointer != RIsPointer)\n    return (int)LIsPointer - (int)RIsPointer;\n\n  // Compare getValueID values.\n  unsigned LID = LV->getValueID(), RID = RV->getValueID();\n  if (LID != RID)\n    return (int)LID - (int)RID;\n\n  // Sort arguments by their position.\n  if (const auto *LA = dyn_cast<Argument>(LV)) {\n    const auto *RA = cast<Argument>(RV);\n    unsigned LArgNo = LA->getArgNo(), RArgNo = RA->getArgNo();\n    return (int)LArgNo - (int)RArgNo;\n  }\n\n  if (const auto *LGV = dyn_cast<GlobalValue>(LV)) {\n    const auto *RGV = cast<GlobalValue>(RV);\n\n    const auto IsGVNameSemantic = [&](const GlobalValue *GV) {\n      auto LT = GV->getLinkage();\n      return !(GlobalValue::isPrivateLinkage(LT) ||\n               GlobalValue::isInternalLinkage(LT));\n    };\n\n    // Use the names to distinguish the two values, but only if the\n    // names are semantically important.\n    if (IsGVNameSemantic(LGV) && IsGVNameSemantic(RGV))\n      return LGV->getName().compare(RGV->getName());\n  }\n\n  // For instructions, compare their loop depth, and their operand count.  This\n  // is pretty loose.\n  if (const auto *LInst = dyn_cast<Instruction>(LV)) {\n    const auto *RInst = cast<Instruction>(RV);\n\n    // Compare loop depths.\n    const BasicBlock *LParent = LInst->getParent(),\n                     *RParent = RInst->getParent();\n    if (LParent != RParent) {\n      unsigned LDepth = LI->getLoopDepth(LParent),\n               RDepth = LI->getLoopDepth(RParent);\n      if (LDepth != RDepth)\n        return (int)LDepth - (int)RDepth;\n    }\n\n    // Compare the number of operands.\n    unsigned LNumOps = LInst->getNumOperands(),\n             RNumOps = RInst->getNumOperands();\n    if (LNumOps != RNumOps)\n      return (int)LNumOps - (int)RNumOps;\n\n    for (unsigned Idx : seq(0u, LNumOps)) {\n      int Result =\n          CompareValueComplexity(EqCacheValue, LI, LInst->getOperand(Idx),\n                                 RInst->getOperand(Idx), Depth + 1);\n      if (Result != 0)\n        return Result;\n    }\n  }\n\n  EqCacheValue.unionSets(LV, RV);\n  return 0;\n}\n\n// Return negative, zero, or positive, if LHS is less than, equal to, or greater\n// than RHS, respectively. A three-way result allows recursive comparisons to be\n// more efficient.\n// If the max analysis depth was reached, return None, assuming we do not know\n// if they are equivalent for sure.\nstatic Optional<int>\nCompareSCEVComplexity(EquivalenceClasses<const SCEV *> &EqCacheSCEV,\n                      EquivalenceClasses<const Value *> &EqCacheValue,\n                      const LoopInfo *const LI, const SCEV *LHS,\n                      const SCEV *RHS, DominatorTree &DT, unsigned Depth = 0) {\n  // Fast-path: SCEVs are uniqued so we can do a quick equality check.\n  if (LHS == RHS)\n    return 0;\n\n  // Primarily, sort the SCEVs by their getSCEVType().\n  SCEVTypes LType = LHS->getSCEVType(), RType = RHS->getSCEVType();\n  if (LType != RType)\n    return (int)LType - (int)RType;\n\n  if (EqCacheSCEV.isEquivalent(LHS, RHS))\n    return 0;\n\n  if (Depth > MaxSCEVCompareDepth)\n    return None;\n\n  // Aside from the getSCEVType() ordering, the particular ordering\n  // isn't very important except that it's beneficial to be consistent,\n  // so that (a + b) and (b + a) don't end up as different expressions.\n  switch (LType) {\n  case scUnknown: {\n    const SCEVUnknown *LU = cast<SCEVUnknown>(LHS);\n    const SCEVUnknown *RU = cast<SCEVUnknown>(RHS);\n\n    int X = CompareValueComplexity(EqCacheValue, LI, LU->getValue(),\n                                   RU->getValue(), Depth + 1);\n    if (X == 0)\n      EqCacheSCEV.unionSets(LHS, RHS);\n    return X;\n  }\n\n  case scConstant: {\n    const SCEVConstant *LC = cast<SCEVConstant>(LHS);\n    const SCEVConstant *RC = cast<SCEVConstant>(RHS);\n\n    // Compare constant values.\n    const APInt &LA = LC->getAPInt();\n    const APInt &RA = RC->getAPInt();\n    unsigned LBitWidth = LA.getBitWidth(), RBitWidth = RA.getBitWidth();\n    if (LBitWidth != RBitWidth)\n      return (int)LBitWidth - (int)RBitWidth;\n    return LA.ult(RA) ? -1 : 1;\n  }\n\n  case scAddRecExpr: {\n    const SCEVAddRecExpr *LA = cast<SCEVAddRecExpr>(LHS);\n    const SCEVAddRecExpr *RA = cast<SCEVAddRecExpr>(RHS);\n\n    // There is always a dominance between two recs that are used by one SCEV,\n    // so we can safely sort recs by loop header dominance. We require such\n    // order in getAddExpr.\n    const Loop *LLoop = LA->getLoop(), *RLoop = RA->getLoop();\n    if (LLoop != RLoop) {\n      const BasicBlock *LHead = LLoop->getHeader(), *RHead = RLoop->getHeader();\n      assert(LHead != RHead && \"Two loops share the same header?\");\n      if (DT.dominates(LHead, RHead))\n        return 1;\n      else\n        assert(DT.dominates(RHead, LHead) &&\n               \"No dominance between recurrences used by one SCEV?\");\n      return -1;\n    }\n\n    // Addrec complexity grows with operand count.\n    unsigned LNumOps = LA->getNumOperands(), RNumOps = RA->getNumOperands();\n    if (LNumOps != RNumOps)\n      return (int)LNumOps - (int)RNumOps;\n\n    // Lexicographically compare.\n    for (unsigned i = 0; i != LNumOps; ++i) {\n      auto X = CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI,\n                                     LA->getOperand(i), RA->getOperand(i), DT,\n                                     Depth + 1);\n      if (X != 0)\n        return X;\n    }\n    EqCacheSCEV.unionSets(LHS, RHS);\n    return 0;\n  }\n\n  case scAddExpr:\n  case scMulExpr:\n  case scSMaxExpr:\n  case scUMaxExpr:\n  case scSMinExpr:\n  case scUMinExpr: {\n    const SCEVNAryExpr *LC = cast<SCEVNAryExpr>(LHS);\n    const SCEVNAryExpr *RC = cast<SCEVNAryExpr>(RHS);\n\n    // Lexicographically compare n-ary expressions.\n    unsigned LNumOps = LC->getNumOperands(), RNumOps = RC->getNumOperands();\n    if (LNumOps != RNumOps)\n      return (int)LNumOps - (int)RNumOps;\n\n    for (unsigned i = 0; i != LNumOps; ++i) {\n      auto X = CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI,\n                                     LC->getOperand(i), RC->getOperand(i), DT,\n                                     Depth + 1);\n      if (X != 0)\n        return X;\n    }\n    EqCacheSCEV.unionSets(LHS, RHS);\n    return 0;\n  }\n\n  case scUDivExpr: {\n    const SCEVUDivExpr *LC = cast<SCEVUDivExpr>(LHS);\n    const SCEVUDivExpr *RC = cast<SCEVUDivExpr>(RHS);\n\n    // Lexicographically compare udiv expressions.\n    auto X = CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI, LC->getLHS(),\n                                   RC->getLHS(), DT, Depth + 1);\n    if (X != 0)\n      return X;\n    X = CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI, LC->getRHS(),\n                              RC->getRHS(), DT, Depth + 1);\n    if (X == 0)\n      EqCacheSCEV.unionSets(LHS, RHS);\n    return X;\n  }\n\n  case scPtrToInt:\n  case scTruncate:\n  case scZeroExtend:\n  case scSignExtend: {\n    const SCEVCastExpr *LC = cast<SCEVCastExpr>(LHS);\n    const SCEVCastExpr *RC = cast<SCEVCastExpr>(RHS);\n\n    // Compare cast expressions by operand.\n    auto X =\n        CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI, LC->getOperand(),\n                              RC->getOperand(), DT, Depth + 1);\n    if (X == 0)\n      EqCacheSCEV.unionSets(LHS, RHS);\n    return X;\n  }\n\n  case scCouldNotCompute:\n    llvm_unreachable(\"Attempt to use a SCEVCouldNotCompute object!\");\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\n/// Given a list of SCEV objects, order them by their complexity, and group\n/// objects of the same complexity together by value.  When this routine is\n/// finished, we know that any duplicates in the vector are consecutive and that\n/// complexity is monotonically increasing.\n///\n/// Note that we go take special precautions to ensure that we get deterministic\n/// results from this routine.  In other words, we don't want the results of\n/// this to depend on where the addresses of various SCEV objects happened to\n/// land in memory.\nstatic void GroupByComplexity(SmallVectorImpl<const SCEV *> &Ops,\n                              LoopInfo *LI, DominatorTree &DT) {\n  if (Ops.size() < 2) return;  // Noop\n\n  EquivalenceClasses<const SCEV *> EqCacheSCEV;\n  EquivalenceClasses<const Value *> EqCacheValue;\n\n  // Whether LHS has provably less complexity than RHS.\n  auto IsLessComplex = [&](const SCEV *LHS, const SCEV *RHS) {\n    auto Complexity =\n        CompareSCEVComplexity(EqCacheSCEV, EqCacheValue, LI, LHS, RHS, DT);\n    return Complexity && *Complexity < 0;\n  };\n  if (Ops.size() == 2) {\n    // This is the common case, which also happens to be trivially simple.\n    // Special case it.\n    const SCEV *&LHS = Ops[0], *&RHS = Ops[1];\n    if (IsLessComplex(RHS, LHS))\n      std::swap(LHS, RHS);\n    return;\n  }\n\n  // Do the rough sort by complexity.\n  llvm::stable_sort(Ops, [&](const SCEV *LHS, const SCEV *RHS) {\n    return IsLessComplex(LHS, RHS);\n  });\n\n  // Now that we are sorted by complexity, group elements of the same\n  // complexity.  Note that this is, at worst, N^2, but the vector is likely to\n  // be extremely short in practice.  Note that we take this approach because we\n  // do not want to depend on the addresses of the objects we are grouping.\n  for (unsigned i = 0, e = Ops.size(); i != e-2; ++i) {\n    const SCEV *S = Ops[i];\n    unsigned Complexity = S->getSCEVType();\n\n    // If there are any objects of the same complexity and same value as this\n    // one, group them.\n    for (unsigned j = i+1; j != e && Ops[j]->getSCEVType() == Complexity; ++j) {\n      if (Ops[j] == S) { // Found a duplicate.\n        // Move it to immediately after i'th element.\n        std::swap(Ops[i+1], Ops[j]);\n        ++i;   // no need to rescan it.\n        if (i == e-2) return;  // Done!\n      }\n    }\n  }\n}\n\n/// Returns true if \\p Ops contains a huge SCEV (the subtree of S contains at\n/// least HugeExprThreshold nodes).\nstatic bool hasHugeExpression(ArrayRef<const SCEV *> Ops) {\n  return any_of(Ops, [](const SCEV *S) {\n    return S->getExpressionSize() >= HugeExprThreshold;\n  });\n}\n\n//===----------------------------------------------------------------------===//\n//                      Simple SCEV method implementations\n//===----------------------------------------------------------------------===//\n\n/// Compute BC(It, K).  The result has width W.  Assume, K > 0.\nstatic const SCEV *BinomialCoefficient(const SCEV *It, unsigned K,\n                                       ScalarEvolution &SE,\n                                       Type *ResultTy) {\n  // Handle the simplest case efficiently.\n  if (K == 1)\n    return SE.getTruncateOrZeroExtend(It, ResultTy);\n\n  // We are using the following formula for BC(It, K):\n  //\n  //   BC(It, K) = (It * (It - 1) * ... * (It - K + 1)) / K!\n  //\n  // Suppose, W is the bitwidth of the return value.  We must be prepared for\n  // overflow.  Hence, we must assure that the result of our computation is\n  // equal to the accurate one modulo 2^W.  Unfortunately, division isn't\n  // safe in modular arithmetic.\n  //\n  // However, this code doesn't use exactly that formula; the formula it uses\n  // is something like the following, where T is the number of factors of 2 in\n  // K! (i.e. trailing zeros in the binary representation of K!), and ^ is\n  // exponentiation:\n  //\n  //   BC(It, K) = (It * (It - 1) * ... * (It - K + 1)) / 2^T / (K! / 2^T)\n  //\n  // This formula is trivially equivalent to the previous formula.  However,\n  // this formula can be implemented much more efficiently.  The trick is that\n  // K! / 2^T is odd, and exact division by an odd number *is* safe in modular\n  // arithmetic.  To do exact division in modular arithmetic, all we have\n  // to do is multiply by the inverse.  Therefore, this step can be done at\n  // width W.\n  //\n  // The next issue is how to safely do the division by 2^T.  The way this\n  // is done is by doing the multiplication step at a width of at least W + T\n  // bits.  This way, the bottom W+T bits of the product are accurate. Then,\n  // when we perform the division by 2^T (which is equivalent to a right shift\n  // by T), the bottom W bits are accurate.  Extra bits are okay; they'll get\n  // truncated out after the division by 2^T.\n  //\n  // In comparison to just directly using the first formula, this technique\n  // is much more efficient; using the first formula requires W * K bits,\n  // but this formula less than W + K bits. Also, the first formula requires\n  // a division step, whereas this formula only requires multiplies and shifts.\n  //\n  // It doesn't matter whether the subtraction step is done in the calculation\n  // width or the input iteration count's width; if the subtraction overflows,\n  // the result must be zero anyway.  We prefer here to do it in the width of\n  // the induction variable because it helps a lot for certain cases; CodeGen\n  // isn't smart enough to ignore the overflow, which leads to much less\n  // efficient code if the width of the subtraction is wider than the native\n  // register width.\n  //\n  // (It's possible to not widen at all by pulling out factors of 2 before\n  // the multiplication; for example, K=2 can be calculated as\n  // It/2*(It+(It*INT_MIN/INT_MIN)+-1). However, it requires\n  // extra arithmetic, so it's not an obvious win, and it gets\n  // much more complicated for K > 3.)\n\n  // Protection from insane SCEVs; this bound is conservative,\n  // but it probably doesn't matter.\n  if (K > 1000)\n    return SE.getCouldNotCompute();\n\n  unsigned W = SE.getTypeSizeInBits(ResultTy);\n\n  // Calculate K! / 2^T and T; we divide out the factors of two before\n  // multiplying for calculating K! / 2^T to avoid overflow.\n  // Other overflow doesn't matter because we only care about the bottom\n  // W bits of the result.\n  APInt OddFactorial(W, 1);\n  unsigned T = 1;\n  for (unsigned i = 3; i <= K; ++i) {\n    APInt Mult(W, i);\n    unsigned TwoFactors = Mult.countTrailingZeros();\n    T += TwoFactors;\n    Mult.lshrInPlace(TwoFactors);\n    OddFactorial *= Mult;\n  }\n\n  // We need at least W + T bits for the multiplication step\n  unsigned CalculationBits = W + T;\n\n  // Calculate 2^T, at width T+W.\n  APInt DivFactor = APInt::getOneBitSet(CalculationBits, T);\n\n  // Calculate the multiplicative inverse of K! / 2^T;\n  // this multiplication factor will perform the exact division by\n  // K! / 2^T.\n  APInt Mod = APInt::getSignedMinValue(W+1);\n  APInt MultiplyFactor = OddFactorial.zext(W+1);\n  MultiplyFactor = MultiplyFactor.multiplicativeInverse(Mod);\n  MultiplyFactor = MultiplyFactor.trunc(W);\n\n  // Calculate the product, at width T+W\n  IntegerType *CalculationTy = IntegerType::get(SE.getContext(),\n                                                      CalculationBits);\n  const SCEV *Dividend = SE.getTruncateOrZeroExtend(It, CalculationTy);\n  for (unsigned i = 1; i != K; ++i) {\n    const SCEV *S = SE.getMinusSCEV(It, SE.getConstant(It->getType(), i));\n    Dividend = SE.getMulExpr(Dividend,\n                             SE.getTruncateOrZeroExtend(S, CalculationTy));\n  }\n\n  // Divide by 2^T\n  const SCEV *DivResult = SE.getUDivExpr(Dividend, SE.getConstant(DivFactor));\n\n  // Truncate the result, and divide by K! / 2^T.\n\n  return SE.getMulExpr(SE.getConstant(MultiplyFactor),\n                       SE.getTruncateOrZeroExtend(DivResult, ResultTy));\n}\n\n/// Return the value of this chain of recurrences at the specified iteration\n/// number.  We can evaluate this recurrence by multiplying each element in the\n/// chain by the binomial coefficient corresponding to it.  In other words, we\n/// can evaluate {A,+,B,+,C,+,D} as:\n///\n///   A*BC(It, 0) + B*BC(It, 1) + C*BC(It, 2) + D*BC(It, 3)\n///\n/// where BC(It, k) stands for binomial coefficient.\nconst SCEV *SCEVAddRecExpr::evaluateAtIteration(const SCEV *It,\n                                                ScalarEvolution &SE) const {\n  const SCEV *Result = getStart();\n  for (unsigned i = 1, e = getNumOperands(); i != e; ++i) {\n    // The computation is correct in the face of overflow provided that the\n    // multiplication is performed _after_ the evaluation of the binomial\n    // coefficient.\n    const SCEV *Coeff = BinomialCoefficient(It, i, SE, getType());\n    if (isa<SCEVCouldNotCompute>(Coeff))\n      return Coeff;\n\n    Result = SE.getAddExpr(Result, SE.getMulExpr(getOperand(i), Coeff));\n  }\n  return Result;\n}\n\n//===----------------------------------------------------------------------===//\n//                    SCEV Expression folder implementations\n//===----------------------------------------------------------------------===//\n\nconst SCEV *ScalarEvolution::getPtrToIntExpr(const SCEV *Op, Type *Ty,\n                                             unsigned Depth) {\n  assert(Ty->isIntegerTy() && \"Target type must be an integer type!\");\n  assert(Depth <= 1 && \"getPtrToIntExpr() should self-recurse at most once.\");\n\n  // We could be called with an integer-typed operands during SCEV rewrites.\n  // Since the operand is an integer already, just perform zext/trunc/self cast.\n  if (!Op->getType()->isPointerTy())\n    return getTruncateOrZeroExtend(Op, Ty);\n\n  // What would be an ID for such a SCEV cast expression?\n  FoldingSetNodeID ID;\n  ID.AddInteger(scPtrToInt);\n  ID.AddPointer(Op);\n\n  void *IP = nullptr;\n\n  // Is there already an expression for such a cast?\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))\n    return getTruncateOrZeroExtend(S, Ty);\n\n  // If not, is this expression something we can't reduce any further?\n  if (isa<SCEVUnknown>(Op)) {\n    // Create an explicit cast node.\n    // We can reuse the existing insert position since if we get here,\n    // we won't have made any changes which would invalidate it.\n    Type *IntPtrTy = getDataLayout().getIntPtrType(Op->getType());\n    assert(getDataLayout().getTypeSizeInBits(getEffectiveSCEVType(\n               Op->getType())) == getDataLayout().getTypeSizeInBits(IntPtrTy) &&\n           \"We can only model ptrtoint if SCEV's effective (integer) type is \"\n           \"sufficiently wide to represent all possible pointer values.\");\n    SCEV *S = new (SCEVAllocator)\n        SCEVPtrToIntExpr(ID.Intern(SCEVAllocator), Op, IntPtrTy);\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n    return getTruncateOrZeroExtend(S, Ty);\n  }\n\n  assert(Depth == 0 &&\n         \"getPtrToIntExpr() should not self-recurse for non-SCEVUnknown's.\");\n\n  // Otherwise, we've got some expression that is more complex than just a\n  // single SCEVUnknown. But we don't want to have a SCEVPtrToIntExpr of an\n  // arbitrary expression, we want to have SCEVPtrToIntExpr of an SCEVUnknown\n  // only, and the expressions must otherwise be integer-typed.\n  // So sink the cast down to the SCEVUnknown's.\n\n  /// The SCEVPtrToIntSinkingRewriter takes a scalar evolution expression,\n  /// which computes a pointer-typed value, and rewrites the whole expression\n  /// tree so that *all* the computations are done on integers, and the only\n  /// pointer-typed operands in the expression are SCEVUnknown.\n  class SCEVPtrToIntSinkingRewriter\n      : public SCEVRewriteVisitor<SCEVPtrToIntSinkingRewriter> {\n    using Base = SCEVRewriteVisitor<SCEVPtrToIntSinkingRewriter>;\n\n  public:\n    SCEVPtrToIntSinkingRewriter(ScalarEvolution &SE) : SCEVRewriteVisitor(SE) {}\n\n    static const SCEV *rewrite(const SCEV *Scev, ScalarEvolution &SE) {\n      SCEVPtrToIntSinkingRewriter Rewriter(SE);\n      return Rewriter.visit(Scev);\n    }\n\n    const SCEV *visit(const SCEV *S) {\n      Type *STy = S->getType();\n      // If the expression is not pointer-typed, just keep it as-is.\n      if (!STy->isPointerTy())\n        return S;\n      // Else, recursively sink the cast down into it.\n      return Base::visit(S);\n    }\n\n    const SCEV *visitAddExpr(const SCEVAddExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getAddExpr(Operands, Expr->getNoWrapFlags());\n    }\n\n    const SCEV *visitMulExpr(const SCEVMulExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getMulExpr(Operands, Expr->getNoWrapFlags());\n    }\n\n    const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n      Type *ExprPtrTy = Expr->getType();\n      assert(ExprPtrTy->isPointerTy() &&\n             \"Should only reach pointer-typed SCEVUnknown's.\");\n      Type *ExprIntPtrTy = SE.getDataLayout().getIntPtrType(ExprPtrTy);\n      return SE.getPtrToIntExpr(Expr, ExprIntPtrTy, /*Depth=*/1);\n    }\n  };\n\n  // And actually perform the cast sinking.\n  const SCEV *IntOp = SCEVPtrToIntSinkingRewriter::rewrite(Op, *this);\n  assert(IntOp->getType()->isIntegerTy() &&\n         \"We must have succeeded in sinking the cast, \"\n         \"and ending up with an integer-typed expression!\");\n  return getTruncateOrZeroExtend(IntOp, Ty);\n}\n\nconst SCEV *ScalarEvolution::getTruncateExpr(const SCEV *Op, Type *Ty,\n                                             unsigned Depth) {\n  assert(getTypeSizeInBits(Op->getType()) > getTypeSizeInBits(Ty) &&\n         \"This is not a truncating conversion!\");\n  assert(isSCEVable(Ty) &&\n         \"This is not a conversion to a SCEVable type!\");\n  Ty = getEffectiveSCEVType(Ty);\n\n  FoldingSetNodeID ID;\n  ID.AddInteger(scTruncate);\n  ID.AddPointer(Op);\n  ID.AddPointer(Ty);\n  void *IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n\n  // Fold if the operand is constant.\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))\n    return getConstant(\n      cast<ConstantInt>(ConstantExpr::getTrunc(SC->getValue(), Ty)));\n\n  // trunc(trunc(x)) --> trunc(x)\n  if (const SCEVTruncateExpr *ST = dyn_cast<SCEVTruncateExpr>(Op))\n    return getTruncateExpr(ST->getOperand(), Ty, Depth + 1);\n\n  // trunc(sext(x)) --> sext(x) if widening or trunc(x) if narrowing\n  if (const SCEVSignExtendExpr *SS = dyn_cast<SCEVSignExtendExpr>(Op))\n    return getTruncateOrSignExtend(SS->getOperand(), Ty, Depth + 1);\n\n  // trunc(zext(x)) --> zext(x) if widening or trunc(x) if narrowing\n  if (const SCEVZeroExtendExpr *SZ = dyn_cast<SCEVZeroExtendExpr>(Op))\n    return getTruncateOrZeroExtend(SZ->getOperand(), Ty, Depth + 1);\n\n  if (Depth > MaxCastDepth) {\n    SCEV *S =\n        new (SCEVAllocator) SCEVTruncateExpr(ID.Intern(SCEVAllocator), Op, Ty);\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n    return S;\n  }\n\n  // trunc(x1 + ... + xN) --> trunc(x1) + ... + trunc(xN) and\n  // trunc(x1 * ... * xN) --> trunc(x1) * ... * trunc(xN),\n  // if after transforming we have at most one truncate, not counting truncates\n  // that replace other casts.\n  if (isa<SCEVAddExpr>(Op) || isa<SCEVMulExpr>(Op)) {\n    auto *CommOp = cast<SCEVCommutativeExpr>(Op);\n    SmallVector<const SCEV *, 4> Operands;\n    unsigned numTruncs = 0;\n    for (unsigned i = 0, e = CommOp->getNumOperands(); i != e && numTruncs < 2;\n         ++i) {\n      const SCEV *S = getTruncateExpr(CommOp->getOperand(i), Ty, Depth + 1);\n      if (!isa<SCEVIntegralCastExpr>(CommOp->getOperand(i)) &&\n          isa<SCEVTruncateExpr>(S))\n        numTruncs++;\n      Operands.push_back(S);\n    }\n    if (numTruncs < 2) {\n      if (isa<SCEVAddExpr>(Op))\n        return getAddExpr(Operands);\n      else if (isa<SCEVMulExpr>(Op))\n        return getMulExpr(Operands);\n      else\n        llvm_unreachable(\"Unexpected SCEV type for Op.\");\n    }\n    // Although we checked in the beginning that ID is not in the cache, it is\n    // possible that during recursion and different modification ID was inserted\n    // into the cache. So if we find it, just return it.\n    if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))\n      return S;\n  }\n\n  // If the input value is a chrec scev, truncate the chrec's operands.\n  if (const SCEVAddRecExpr *AddRec = dyn_cast<SCEVAddRecExpr>(Op)) {\n    SmallVector<const SCEV *, 4> Operands;\n    for (const SCEV *Op : AddRec->operands())\n      Operands.push_back(getTruncateExpr(Op, Ty, Depth + 1));\n    return getAddRecExpr(Operands, AddRec->getLoop(), SCEV::FlagAnyWrap);\n  }\n\n  // Return zero if truncating to known zeros.\n  uint32_t MinTrailingZeros = GetMinTrailingZeros(Op);\n  if (MinTrailingZeros >= getTypeSizeInBits(Ty))\n    return getZero(Ty);\n\n  // The cast wasn't folded; create an explicit cast node. We can reuse\n  // the existing insert position since if we get here, we won't have\n  // made any changes which would invalidate it.\n  SCEV *S = new (SCEVAllocator) SCEVTruncateExpr(ID.Intern(SCEVAllocator),\n                                                 Op, Ty);\n  UniqueSCEVs.InsertNode(S, IP);\n  addToLoopUseLists(S);\n  return S;\n}\n\n// Get the limit of a recurrence such that incrementing by Step cannot cause\n// signed overflow as long as the value of the recurrence within the\n// loop does not exceed this limit before incrementing.\nstatic const SCEV *getSignedOverflowLimitForStep(const SCEV *Step,\n                                                 ICmpInst::Predicate *Pred,\n                                                 ScalarEvolution *SE) {\n  unsigned BitWidth = SE->getTypeSizeInBits(Step->getType());\n  if (SE->isKnownPositive(Step)) {\n    *Pred = ICmpInst::ICMP_SLT;\n    return SE->getConstant(APInt::getSignedMinValue(BitWidth) -\n                           SE->getSignedRangeMax(Step));\n  }\n  if (SE->isKnownNegative(Step)) {\n    *Pred = ICmpInst::ICMP_SGT;\n    return SE->getConstant(APInt::getSignedMaxValue(BitWidth) -\n                           SE->getSignedRangeMin(Step));\n  }\n  return nullptr;\n}\n\n// Get the limit of a recurrence such that incrementing by Step cannot cause\n// unsigned overflow as long as the value of the recurrence within the loop does\n// not exceed this limit before incrementing.\nstatic const SCEV *getUnsignedOverflowLimitForStep(const SCEV *Step,\n                                                   ICmpInst::Predicate *Pred,\n                                                   ScalarEvolution *SE) {\n  unsigned BitWidth = SE->getTypeSizeInBits(Step->getType());\n  *Pred = ICmpInst::ICMP_ULT;\n\n  return SE->getConstant(APInt::getMinValue(BitWidth) -\n                         SE->getUnsignedRangeMax(Step));\n}\n\nnamespace {\n\nstruct ExtendOpTraitsBase {\n  typedef const SCEV *(ScalarEvolution::*GetExtendExprTy)(const SCEV *, Type *,\n                                                          unsigned);\n};\n\n// Used to make code generic over signed and unsigned overflow.\ntemplate <typename ExtendOp> struct ExtendOpTraits {\n  // Members present:\n  //\n  // static const SCEV::NoWrapFlags WrapType;\n  //\n  // static const ExtendOpTraitsBase::GetExtendExprTy GetExtendExpr;\n  //\n  // static const SCEV *getOverflowLimitForStep(const SCEV *Step,\n  //                                           ICmpInst::Predicate *Pred,\n  //                                           ScalarEvolution *SE);\n};\n\ntemplate <>\nstruct ExtendOpTraits<SCEVSignExtendExpr> : public ExtendOpTraitsBase {\n  static const SCEV::NoWrapFlags WrapType = SCEV::FlagNSW;\n\n  static const GetExtendExprTy GetExtendExpr;\n\n  static const SCEV *getOverflowLimitForStep(const SCEV *Step,\n                                             ICmpInst::Predicate *Pred,\n                                             ScalarEvolution *SE) {\n    return getSignedOverflowLimitForStep(Step, Pred, SE);\n  }\n};\n\nconst ExtendOpTraitsBase::GetExtendExprTy ExtendOpTraits<\n    SCEVSignExtendExpr>::GetExtendExpr = &ScalarEvolution::getSignExtendExpr;\n\ntemplate <>\nstruct ExtendOpTraits<SCEVZeroExtendExpr> : public ExtendOpTraitsBase {\n  static const SCEV::NoWrapFlags WrapType = SCEV::FlagNUW;\n\n  static const GetExtendExprTy GetExtendExpr;\n\n  static const SCEV *getOverflowLimitForStep(const SCEV *Step,\n                                             ICmpInst::Predicate *Pred,\n                                             ScalarEvolution *SE) {\n    return getUnsignedOverflowLimitForStep(Step, Pred, SE);\n  }\n};\n\nconst ExtendOpTraitsBase::GetExtendExprTy ExtendOpTraits<\n    SCEVZeroExtendExpr>::GetExtendExpr = &ScalarEvolution::getZeroExtendExpr;\n\n} // end anonymous namespace\n\n// The recurrence AR has been shown to have no signed/unsigned wrap or something\n// close to it. Typically, if we can prove NSW/NUW for AR, then we can just as\n// easily prove NSW/NUW for its preincrement or postincrement sibling. This\n// allows normalizing a sign/zero extended AddRec as such: {sext/zext(Step +\n// Start),+,Step} => {(Step + sext/zext(Start),+,Step} As a result, the\n// expression \"Step + sext/zext(PreIncAR)\" is congruent with\n// \"sext/zext(PostIncAR)\"\ntemplate <typename ExtendOpTy>\nstatic const SCEV *getPreStartForExtend(const SCEVAddRecExpr *AR, Type *Ty,\n                                        ScalarEvolution *SE, unsigned Depth) {\n  auto WrapType = ExtendOpTraits<ExtendOpTy>::WrapType;\n  auto GetExtendExpr = ExtendOpTraits<ExtendOpTy>::GetExtendExpr;\n\n  const Loop *L = AR->getLoop();\n  const SCEV *Start = AR->getStart();\n  const SCEV *Step = AR->getStepRecurrence(*SE);\n\n  // Check for a simple looking step prior to loop entry.\n  const SCEVAddExpr *SA = dyn_cast<SCEVAddExpr>(Start);\n  if (!SA)\n    return nullptr;\n\n  // Create an AddExpr for \"PreStart\" after subtracting Step. Full SCEV\n  // subtraction is expensive. For this purpose, perform a quick and dirty\n  // difference, by checking for Step in the operand list.\n  SmallVector<const SCEV *, 4> DiffOps;\n  for (const SCEV *Op : SA->operands())\n    if (Op != Step)\n      DiffOps.push_back(Op);\n\n  if (DiffOps.size() == SA->getNumOperands())\n    return nullptr;\n\n  // Try to prove `WrapType` (SCEV::FlagNSW or SCEV::FlagNUW) on `PreStart` +\n  // `Step`:\n\n  // 1. NSW/NUW flags on the step increment.\n  auto PreStartFlags =\n    ScalarEvolution::maskFlags(SA->getNoWrapFlags(), SCEV::FlagNUW);\n  const SCEV *PreStart = SE->getAddExpr(DiffOps, PreStartFlags);\n  const SCEVAddRecExpr *PreAR = dyn_cast<SCEVAddRecExpr>(\n      SE->getAddRecExpr(PreStart, Step, L, SCEV::FlagAnyWrap));\n\n  // \"{S,+,X} is <nsw>/<nuw>\" and \"the backedge is taken at least once\" implies\n  // \"S+X does not sign/unsign-overflow\".\n  //\n\n  const SCEV *BECount = SE->getBackedgeTakenCount(L);\n  if (PreAR && PreAR->getNoWrapFlags(WrapType) &&\n      !isa<SCEVCouldNotCompute>(BECount) && SE->isKnownPositive(BECount))\n    return PreStart;\n\n  // 2. Direct overflow check on the step operation's expression.\n  unsigned BitWidth = SE->getTypeSizeInBits(AR->getType());\n  Type *WideTy = IntegerType::get(SE->getContext(), BitWidth * 2);\n  const SCEV *OperandExtendedStart =\n      SE->getAddExpr((SE->*GetExtendExpr)(PreStart, WideTy, Depth),\n                     (SE->*GetExtendExpr)(Step, WideTy, Depth));\n  if ((SE->*GetExtendExpr)(Start, WideTy, Depth) == OperandExtendedStart) {\n    if (PreAR && AR->getNoWrapFlags(WrapType)) {\n      // If we know `AR` == {`PreStart`+`Step`,+,`Step`} is `WrapType` (FlagNSW\n      // or FlagNUW) and that `PreStart` + `Step` is `WrapType` too, then\n      // `PreAR` == {`PreStart`,+,`Step`} is also `WrapType`.  Cache this fact.\n      SE->setNoWrapFlags(const_cast<SCEVAddRecExpr *>(PreAR), WrapType);\n    }\n    return PreStart;\n  }\n\n  // 3. Loop precondition.\n  ICmpInst::Predicate Pred;\n  const SCEV *OverflowLimit =\n      ExtendOpTraits<ExtendOpTy>::getOverflowLimitForStep(Step, &Pred, SE);\n\n  if (OverflowLimit &&\n      SE->isLoopEntryGuardedByCond(L, Pred, PreStart, OverflowLimit))\n    return PreStart;\n\n  return nullptr;\n}\n\n// Get the normalized zero or sign extended expression for this AddRec's Start.\ntemplate <typename ExtendOpTy>\nstatic const SCEV *getExtendAddRecStart(const SCEVAddRecExpr *AR, Type *Ty,\n                                        ScalarEvolution *SE,\n                                        unsigned Depth) {\n  auto GetExtendExpr = ExtendOpTraits<ExtendOpTy>::GetExtendExpr;\n\n  const SCEV *PreStart = getPreStartForExtend<ExtendOpTy>(AR, Ty, SE, Depth);\n  if (!PreStart)\n    return (SE->*GetExtendExpr)(AR->getStart(), Ty, Depth);\n\n  return SE->getAddExpr((SE->*GetExtendExpr)(AR->getStepRecurrence(*SE), Ty,\n                                             Depth),\n                        (SE->*GetExtendExpr)(PreStart, Ty, Depth));\n}\n\n// Try to prove away overflow by looking at \"nearby\" add recurrences.  A\n// motivating example for this rule: if we know `{0,+,4}` is `ult` `-1` and it\n// does not itself wrap then we can conclude that `{1,+,4}` is `nuw`.\n//\n// Formally:\n//\n//     {S,+,X} == {S-T,+,X} + T\n//  => Ext({S,+,X}) == Ext({S-T,+,X} + T)\n//\n// If ({S-T,+,X} + T) does not overflow  ... (1)\n//\n//  RHS == Ext({S-T,+,X} + T) == Ext({S-T,+,X}) + Ext(T)\n//\n// If {S-T,+,X} does not overflow  ... (2)\n//\n//  RHS == Ext({S-T,+,X}) + Ext(T) == {Ext(S-T),+,Ext(X)} + Ext(T)\n//      == {Ext(S-T)+Ext(T),+,Ext(X)}\n//\n// If (S-T)+T does not overflow  ... (3)\n//\n//  RHS == {Ext(S-T)+Ext(T),+,Ext(X)} == {Ext(S-T+T),+,Ext(X)}\n//      == {Ext(S),+,Ext(X)} == LHS\n//\n// Thus, if (1), (2) and (3) are true for some T, then\n//   Ext({S,+,X}) == {Ext(S),+,Ext(X)}\n//\n// (3) is implied by (1) -- \"(S-T)+T does not overflow\" is simply \"({S-T,+,X}+T)\n// does not overflow\" restricted to the 0th iteration.  Therefore we only need\n// to check for (1) and (2).\n//\n// In the current context, S is `Start`, X is `Step`, Ext is `ExtendOpTy` and T\n// is `Delta` (defined below).\ntemplate <typename ExtendOpTy>\nbool ScalarEvolution::proveNoWrapByVaryingStart(const SCEV *Start,\n                                                const SCEV *Step,\n                                                const Loop *L) {\n  auto WrapType = ExtendOpTraits<ExtendOpTy>::WrapType;\n\n  // We restrict `Start` to a constant to prevent SCEV from spending too much\n  // time here.  It is correct (but more expensive) to continue with a\n  // non-constant `Start` and do a general SCEV subtraction to compute\n  // `PreStart` below.\n  const SCEVConstant *StartC = dyn_cast<SCEVConstant>(Start);\n  if (!StartC)\n    return false;\n\n  APInt StartAI = StartC->getAPInt();\n\n  for (unsigned Delta : {-2, -1, 1, 2}) {\n    const SCEV *PreStart = getConstant(StartAI - Delta);\n\n    FoldingSetNodeID ID;\n    ID.AddInteger(scAddRecExpr);\n    ID.AddPointer(PreStart);\n    ID.AddPointer(Step);\n    ID.AddPointer(L);\n    void *IP = nullptr;\n    const auto *PreAR =\n      static_cast<SCEVAddRecExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));\n\n    // Give up if we don't already have the add recurrence we need because\n    // actually constructing an add recurrence is relatively expensive.\n    if (PreAR && PreAR->getNoWrapFlags(WrapType)) {  // proves (2)\n      const SCEV *DeltaS = getConstant(StartC->getType(), Delta);\n      ICmpInst::Predicate Pred = ICmpInst::BAD_ICMP_PREDICATE;\n      const SCEV *Limit = ExtendOpTraits<ExtendOpTy>::getOverflowLimitForStep(\n          DeltaS, &Pred, this);\n      if (Limit && isKnownPredicate(Pred, PreAR, Limit))  // proves (1)\n        return true;\n    }\n  }\n\n  return false;\n}\n\n// Finds an integer D for an expression (C + x + y + ...) such that the top\n// level addition in (D + (C - D + x + y + ...)) would not wrap (signed or\n// unsigned) and the number of trailing zeros of (C - D + x + y + ...) is\n// maximized, where C is the \\p ConstantTerm, x, y, ... are arbitrary SCEVs, and\n// the (C + x + y + ...) expression is \\p WholeAddExpr.\nstatic APInt extractConstantWithoutWrapping(ScalarEvolution &SE,\n                                            const SCEVConstant *ConstantTerm,\n                                            const SCEVAddExpr *WholeAddExpr) {\n  const APInt &C = ConstantTerm->getAPInt();\n  const unsigned BitWidth = C.getBitWidth();\n  // Find number of trailing zeros of (x + y + ...) w/o the C first:\n  uint32_t TZ = BitWidth;\n  for (unsigned I = 1, E = WholeAddExpr->getNumOperands(); I < E && TZ; ++I)\n    TZ = std::min(TZ, SE.GetMinTrailingZeros(WholeAddExpr->getOperand(I)));\n  if (TZ) {\n    // Set D to be as many least significant bits of C as possible while still\n    // guaranteeing that adding D to (C - D + x + y + ...) won't cause a wrap:\n    return TZ < BitWidth ? C.trunc(TZ).zext(BitWidth) : C;\n  }\n  return APInt(BitWidth, 0);\n}\n\n// Finds an integer D for an affine AddRec expression {C,+,x} such that the top\n// level addition in (D + {C-D,+,x}) would not wrap (signed or unsigned) and the\n// number of trailing zeros of (C - D + x * n) is maximized, where C is the \\p\n// ConstantStart, x is an arbitrary \\p Step, and n is the loop trip count.\nstatic APInt extractConstantWithoutWrapping(ScalarEvolution &SE,\n                                            const APInt &ConstantStart,\n                                            const SCEV *Step) {\n  const unsigned BitWidth = ConstantStart.getBitWidth();\n  const uint32_t TZ = SE.GetMinTrailingZeros(Step);\n  if (TZ)\n    return TZ < BitWidth ? ConstantStart.trunc(TZ).zext(BitWidth)\n                         : ConstantStart;\n  return APInt(BitWidth, 0);\n}\n\nconst SCEV *\nScalarEvolution::getZeroExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth) {\n  assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&\n         \"This is not an extending conversion!\");\n  assert(isSCEVable(Ty) &&\n         \"This is not a conversion to a SCEVable type!\");\n  Ty = getEffectiveSCEVType(Ty);\n\n  // Fold if the operand is constant.\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))\n    return getConstant(\n      cast<ConstantInt>(ConstantExpr::getZExt(SC->getValue(), Ty)));\n\n  // zext(zext(x)) --> zext(x)\n  if (const SCEVZeroExtendExpr *SZ = dyn_cast<SCEVZeroExtendExpr>(Op))\n    return getZeroExtendExpr(SZ->getOperand(), Ty, Depth + 1);\n\n  // Before doing any expensive analysis, check to see if we've already\n  // computed a SCEV for this Op and Ty.\n  FoldingSetNodeID ID;\n  ID.AddInteger(scZeroExtend);\n  ID.AddPointer(Op);\n  ID.AddPointer(Ty);\n  void *IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  if (Depth > MaxCastDepth) {\n    SCEV *S = new (SCEVAllocator) SCEVZeroExtendExpr(ID.Intern(SCEVAllocator),\n                                                     Op, Ty);\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n    return S;\n  }\n\n  // zext(trunc(x)) --> zext(x) or x or trunc(x)\n  if (const SCEVTruncateExpr *ST = dyn_cast<SCEVTruncateExpr>(Op)) {\n    // It's possible the bits taken off by the truncate were all zero bits. If\n    // so, we should be able to simplify this further.\n    const SCEV *X = ST->getOperand();\n    ConstantRange CR = getUnsignedRange(X);\n    unsigned TruncBits = getTypeSizeInBits(ST->getType());\n    unsigned NewBits = getTypeSizeInBits(Ty);\n    if (CR.truncate(TruncBits).zeroExtend(NewBits).contains(\n            CR.zextOrTrunc(NewBits)))\n      return getTruncateOrZeroExtend(X, Ty, Depth);\n  }\n\n  // If the input value is a chrec scev, and we can prove that the value\n  // did not overflow the old, smaller, value, we can zero extend all of the\n  // operands (often constants).  This allows analysis of something like\n  // this:  for (unsigned char X = 0; X < 100; ++X) { int Y = X; }\n  if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(Op))\n    if (AR->isAffine()) {\n      const SCEV *Start = AR->getStart();\n      const SCEV *Step = AR->getStepRecurrence(*this);\n      unsigned BitWidth = getTypeSizeInBits(AR->getType());\n      const Loop *L = AR->getLoop();\n\n      if (!AR->hasNoUnsignedWrap()) {\n        auto NewFlags = proveNoWrapViaConstantRanges(AR);\n        setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), NewFlags);\n      }\n\n      // If we have special knowledge that this addrec won't overflow,\n      // we don't need to do any further analysis.\n      if (AR->hasNoUnsignedWrap())\n        return getAddRecExpr(\n            getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this, Depth + 1),\n            getZeroExtendExpr(Step, Ty, Depth + 1), L, AR->getNoWrapFlags());\n\n      // Check whether the backedge-taken count is SCEVCouldNotCompute.\n      // Note that this serves two purposes: It filters out loops that are\n      // simply not analyzable, and it covers the case where this code is\n      // being called from within backedge-taken count analysis, such that\n      // attempting to ask for the backedge-taken count would likely result\n      // in infinite recursion. In the later case, the analysis code will\n      // cope with a conservative value, and it will take care to purge\n      // that value once it has finished.\n      const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(L);\n      if (!isa<SCEVCouldNotCompute>(MaxBECount)) {\n        // Manually compute the final value for AR, checking for overflow.\n\n        // Check whether the backedge-taken count can be losslessly casted to\n        // the addrec's type. The count is always unsigned.\n        const SCEV *CastedMaxBECount =\n            getTruncateOrZeroExtend(MaxBECount, Start->getType(), Depth);\n        const SCEV *RecastedMaxBECount = getTruncateOrZeroExtend(\n            CastedMaxBECount, MaxBECount->getType(), Depth);\n        if (MaxBECount == RecastedMaxBECount) {\n          Type *WideTy = IntegerType::get(getContext(), BitWidth * 2);\n          // Check whether Start+Step*MaxBECount has no unsigned overflow.\n          const SCEV *ZMul = getMulExpr(CastedMaxBECount, Step,\n                                        SCEV::FlagAnyWrap, Depth + 1);\n          const SCEV *ZAdd = getZeroExtendExpr(getAddExpr(Start, ZMul,\n                                                          SCEV::FlagAnyWrap,\n                                                          Depth + 1),\n                                               WideTy, Depth + 1);\n          const SCEV *WideStart = getZeroExtendExpr(Start, WideTy, Depth + 1);\n          const SCEV *WideMaxBECount =\n            getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);\n          const SCEV *OperandExtendedAdd =\n            getAddExpr(WideStart,\n                       getMulExpr(WideMaxBECount,\n                                  getZeroExtendExpr(Step, WideTy, Depth + 1),\n                                  SCEV::FlagAnyWrap, Depth + 1),\n                       SCEV::FlagAnyWrap, Depth + 1);\n          if (ZAdd == OperandExtendedAdd) {\n            // Cache knowledge of AR NUW, which is propagated to this AddRec.\n            setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNUW);\n            // Return the expression with the addrec on the outside.\n            return getAddRecExpr(\n                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getZeroExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n          }\n          // Similar to above, only this time treat the step value as signed.\n          // This covers loops that count down.\n          OperandExtendedAdd =\n            getAddExpr(WideStart,\n                       getMulExpr(WideMaxBECount,\n                                  getSignExtendExpr(Step, WideTy, Depth + 1),\n                                  SCEV::FlagAnyWrap, Depth + 1),\n                       SCEV::FlagAnyWrap, Depth + 1);\n          if (ZAdd == OperandExtendedAdd) {\n            // Cache knowledge of AR NW, which is propagated to this AddRec.\n            // Negative step causes unsigned wrap, but it still can't self-wrap.\n            setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);\n            // Return the expression with the addrec on the outside.\n            return getAddRecExpr(\n                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getSignExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n          }\n        }\n      }\n\n      // Normally, in the cases we can prove no-overflow via a\n      // backedge guarding condition, we can also compute a backedge\n      // taken count for the loop.  The exceptions are assumptions and\n      // guards present in the loop -- SCEV is not great at exploiting\n      // these to compute max backedge taken counts, but can still use\n      // these to prove lack of overflow.  Use this fact to avoid\n      // doing extra work that may not pay off.\n      if (!isa<SCEVCouldNotCompute>(MaxBECount) || HasGuards ||\n          !AC.assumptions().empty()) {\n\n        auto NewFlags = proveNoUnsignedWrapViaInduction(AR);\n        setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), NewFlags);\n        if (AR->hasNoUnsignedWrap()) {\n          // Same as nuw case above - duplicated here to avoid a compile time\n          // issue.  It's not clear that the order of checks does matter, but\n          // it's one of two issue possible causes for a change which was\n          // reverted.  Be conservative for the moment.\n          return getAddRecExpr(\n                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getZeroExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n        }\n\n        // For a negative step, we can extend the operands iff doing so only\n        // traverses values in the range zext([0,UINT_MAX]).\n        if (isKnownNegative(Step)) {\n          const SCEV *N = getConstant(APInt::getMaxValue(BitWidth) -\n                                      getSignedRangeMin(Step));\n          if (isLoopBackedgeGuardedByCond(L, ICmpInst::ICMP_UGT, AR, N) ||\n              isKnownOnEveryIteration(ICmpInst::ICMP_UGT, AR, N)) {\n            // Cache knowledge of AR NW, which is propagated to this\n            // AddRec.  Negative step causes unsigned wrap, but it\n            // still can't self-wrap.\n            setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);\n            // Return the expression with the addrec on the outside.\n            return getAddRecExpr(\n                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getSignExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n          }\n        }\n      }\n\n      // zext({C,+,Step}) --> (zext(D) + zext({C-D,+,Step}))<nuw><nsw>\n      // if D + (C - D + Step * n) could be proven to not unsigned wrap\n      // where D maximizes the number of trailing zeros of (C - D + Step * n)\n      if (const auto *SC = dyn_cast<SCEVConstant>(Start)) {\n        const APInt &C = SC->getAPInt();\n        const APInt &D = extractConstantWithoutWrapping(*this, C, Step);\n        if (D != 0) {\n          const SCEV *SZExtD = getZeroExtendExpr(getConstant(D), Ty, Depth);\n          const SCEV *SResidual =\n              getAddRecExpr(getConstant(C - D), Step, L, AR->getNoWrapFlags());\n          const SCEV *SZExtR = getZeroExtendExpr(SResidual, Ty, Depth + 1);\n          return getAddExpr(SZExtD, SZExtR,\n                            (SCEV::NoWrapFlags)(SCEV::FlagNSW | SCEV::FlagNUW),\n                            Depth + 1);\n        }\n      }\n\n      if (proveNoWrapByVaryingStart<SCEVZeroExtendExpr>(Start, Step, L)) {\n        setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNUW);\n        return getAddRecExpr(\n            getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this, Depth + 1),\n            getZeroExtendExpr(Step, Ty, Depth + 1), L, AR->getNoWrapFlags());\n      }\n    }\n\n  // zext(A % B) --> zext(A) % zext(B)\n  {\n    const SCEV *LHS;\n    const SCEV *RHS;\n    if (matchURem(Op, LHS, RHS))\n      return getURemExpr(getZeroExtendExpr(LHS, Ty, Depth + 1),\n                         getZeroExtendExpr(RHS, Ty, Depth + 1));\n  }\n\n  // zext(A / B) --> zext(A) / zext(B).\n  if (auto *Div = dyn_cast<SCEVUDivExpr>(Op))\n    return getUDivExpr(getZeroExtendExpr(Div->getLHS(), Ty, Depth + 1),\n                       getZeroExtendExpr(Div->getRHS(), Ty, Depth + 1));\n\n  if (auto *SA = dyn_cast<SCEVAddExpr>(Op)) {\n    // zext((A + B + ...)<nuw>) --> (zext(A) + zext(B) + ...)<nuw>\n    if (SA->hasNoUnsignedWrap()) {\n      // If the addition does not unsign overflow then we can, by definition,\n      // commute the zero extension with the addition operation.\n      SmallVector<const SCEV *, 4> Ops;\n      for (const auto *Op : SA->operands())\n        Ops.push_back(getZeroExtendExpr(Op, Ty, Depth + 1));\n      return getAddExpr(Ops, SCEV::FlagNUW, Depth + 1);\n    }\n\n    // zext(C + x + y + ...) --> (zext(D) + zext((C - D) + x + y + ...))\n    // if D + (C - D + x + y + ...) could be proven to not unsigned wrap\n    // where D maximizes the number of trailing zeros of (C - D + x + y + ...)\n    //\n    // Often address arithmetics contain expressions like\n    // (zext (add (shl X, C1), C2)), for instance, (zext (5 + (4 * X))).\n    // This transformation is useful while proving that such expressions are\n    // equal or differ by a small constant amount, see LoadStoreVectorizer pass.\n    if (const auto *SC = dyn_cast<SCEVConstant>(SA->getOperand(0))) {\n      const APInt &D = extractConstantWithoutWrapping(*this, SC, SA);\n      if (D != 0) {\n        const SCEV *SZExtD = getZeroExtendExpr(getConstant(D), Ty, Depth);\n        const SCEV *SResidual =\n            getAddExpr(getConstant(-D), SA, SCEV::FlagAnyWrap, Depth);\n        const SCEV *SZExtR = getZeroExtendExpr(SResidual, Ty, Depth + 1);\n        return getAddExpr(SZExtD, SZExtR,\n                          (SCEV::NoWrapFlags)(SCEV::FlagNSW | SCEV::FlagNUW),\n                          Depth + 1);\n      }\n    }\n  }\n\n  if (auto *SM = dyn_cast<SCEVMulExpr>(Op)) {\n    // zext((A * B * ...)<nuw>) --> (zext(A) * zext(B) * ...)<nuw>\n    if (SM->hasNoUnsignedWrap()) {\n      // If the multiply does not unsign overflow then we can, by definition,\n      // commute the zero extension with the multiply operation.\n      SmallVector<const SCEV *, 4> Ops;\n      for (const auto *Op : SM->operands())\n        Ops.push_back(getZeroExtendExpr(Op, Ty, Depth + 1));\n      return getMulExpr(Ops, SCEV::FlagNUW, Depth + 1);\n    }\n\n    // zext(2^K * (trunc X to iN)) to iM ->\n    // 2^K * (zext(trunc X to i{N-K}) to iM)<nuw>\n    //\n    // Proof:\n    //\n    //     zext(2^K * (trunc X to iN)) to iM\n    //   = zext((trunc X to iN) << K) to iM\n    //   = zext((trunc X to i{N-K}) << K)<nuw> to iM\n    //     (because shl removes the top K bits)\n    //   = zext((2^K * (trunc X to i{N-K}))<nuw>) to iM\n    //   = (2^K * (zext(trunc X to i{N-K}) to iM))<nuw>.\n    //\n    if (SM->getNumOperands() == 2)\n      if (auto *MulLHS = dyn_cast<SCEVConstant>(SM->getOperand(0)))\n        if (MulLHS->getAPInt().isPowerOf2())\n          if (auto *TruncRHS = dyn_cast<SCEVTruncateExpr>(SM->getOperand(1))) {\n            int NewTruncBits = getTypeSizeInBits(TruncRHS->getType()) -\n                               MulLHS->getAPInt().logBase2();\n            Type *NewTruncTy = IntegerType::get(getContext(), NewTruncBits);\n            return getMulExpr(\n                getZeroExtendExpr(MulLHS, Ty),\n                getZeroExtendExpr(\n                    getTruncateExpr(TruncRHS->getOperand(), NewTruncTy), Ty),\n                SCEV::FlagNUW, Depth + 1);\n          }\n  }\n\n  // The cast wasn't folded; create an explicit cast node.\n  // Recompute the insert position, as it may have been invalidated.\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  SCEV *S = new (SCEVAllocator) SCEVZeroExtendExpr(ID.Intern(SCEVAllocator),\n                                                   Op, Ty);\n  UniqueSCEVs.InsertNode(S, IP);\n  addToLoopUseLists(S);\n  return S;\n}\n\nconst SCEV *\nScalarEvolution::getSignExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth) {\n  assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&\n         \"This is not an extending conversion!\");\n  assert(isSCEVable(Ty) &&\n         \"This is not a conversion to a SCEVable type!\");\n  Ty = getEffectiveSCEVType(Ty);\n\n  // Fold if the operand is constant.\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))\n    return getConstant(\n      cast<ConstantInt>(ConstantExpr::getSExt(SC->getValue(), Ty)));\n\n  // sext(sext(x)) --> sext(x)\n  if (const SCEVSignExtendExpr *SS = dyn_cast<SCEVSignExtendExpr>(Op))\n    return getSignExtendExpr(SS->getOperand(), Ty, Depth + 1);\n\n  // sext(zext(x)) --> zext(x)\n  if (const SCEVZeroExtendExpr *SZ = dyn_cast<SCEVZeroExtendExpr>(Op))\n    return getZeroExtendExpr(SZ->getOperand(), Ty, Depth + 1);\n\n  // Before doing any expensive analysis, check to see if we've already\n  // computed a SCEV for this Op and Ty.\n  FoldingSetNodeID ID;\n  ID.AddInteger(scSignExtend);\n  ID.AddPointer(Op);\n  ID.AddPointer(Ty);\n  void *IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  // Limit recursion depth.\n  if (Depth > MaxCastDepth) {\n    SCEV *S = new (SCEVAllocator) SCEVSignExtendExpr(ID.Intern(SCEVAllocator),\n                                                     Op, Ty);\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n    return S;\n  }\n\n  // sext(trunc(x)) --> sext(x) or x or trunc(x)\n  if (const SCEVTruncateExpr *ST = dyn_cast<SCEVTruncateExpr>(Op)) {\n    // It's possible the bits taken off by the truncate were all sign bits. If\n    // so, we should be able to simplify this further.\n    const SCEV *X = ST->getOperand();\n    ConstantRange CR = getSignedRange(X);\n    unsigned TruncBits = getTypeSizeInBits(ST->getType());\n    unsigned NewBits = getTypeSizeInBits(Ty);\n    if (CR.truncate(TruncBits).signExtend(NewBits).contains(\n            CR.sextOrTrunc(NewBits)))\n      return getTruncateOrSignExtend(X, Ty, Depth);\n  }\n\n  if (auto *SA = dyn_cast<SCEVAddExpr>(Op)) {\n    // sext((A + B + ...)<nsw>) --> (sext(A) + sext(B) + ...)<nsw>\n    if (SA->hasNoSignedWrap()) {\n      // If the addition does not sign overflow then we can, by definition,\n      // commute the sign extension with the addition operation.\n      SmallVector<const SCEV *, 4> Ops;\n      for (const auto *Op : SA->operands())\n        Ops.push_back(getSignExtendExpr(Op, Ty, Depth + 1));\n      return getAddExpr(Ops, SCEV::FlagNSW, Depth + 1);\n    }\n\n    // sext(C + x + y + ...) --> (sext(D) + sext((C - D) + x + y + ...))\n    // if D + (C - D + x + y + ...) could be proven to not signed wrap\n    // where D maximizes the number of trailing zeros of (C - D + x + y + ...)\n    //\n    // For instance, this will bring two seemingly different expressions:\n    //     1 + sext(5 + 20 * %x + 24 * %y)  and\n    //         sext(6 + 20 * %x + 24 * %y)\n    // to the same form:\n    //     2 + sext(4 + 20 * %x + 24 * %y)\n    if (const auto *SC = dyn_cast<SCEVConstant>(SA->getOperand(0))) {\n      const APInt &D = extractConstantWithoutWrapping(*this, SC, SA);\n      if (D != 0) {\n        const SCEV *SSExtD = getSignExtendExpr(getConstant(D), Ty, Depth);\n        const SCEV *SResidual =\n            getAddExpr(getConstant(-D), SA, SCEV::FlagAnyWrap, Depth);\n        const SCEV *SSExtR = getSignExtendExpr(SResidual, Ty, Depth + 1);\n        return getAddExpr(SSExtD, SSExtR,\n                          (SCEV::NoWrapFlags)(SCEV::FlagNSW | SCEV::FlagNUW),\n                          Depth + 1);\n      }\n    }\n  }\n  // If the input value is a chrec scev, and we can prove that the value\n  // did not overflow the old, smaller, value, we can sign extend all of the\n  // operands (often constants).  This allows analysis of something like\n  // this:  for (signed char X = 0; X < 100; ++X) { int Y = X; }\n  if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(Op))\n    if (AR->isAffine()) {\n      const SCEV *Start = AR->getStart();\n      const SCEV *Step = AR->getStepRecurrence(*this);\n      unsigned BitWidth = getTypeSizeInBits(AR->getType());\n      const Loop *L = AR->getLoop();\n\n      if (!AR->hasNoSignedWrap()) {\n        auto NewFlags = proveNoWrapViaConstantRanges(AR);\n        setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), NewFlags);\n      }\n\n      // If we have special knowledge that this addrec won't overflow,\n      // we don't need to do any further analysis.\n      if (AR->hasNoSignedWrap())\n        return getAddRecExpr(\n            getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this, Depth + 1),\n            getSignExtendExpr(Step, Ty, Depth + 1), L, SCEV::FlagNSW);\n\n      // Check whether the backedge-taken count is SCEVCouldNotCompute.\n      // Note that this serves two purposes: It filters out loops that are\n      // simply not analyzable, and it covers the case where this code is\n      // being called from within backedge-taken count analysis, such that\n      // attempting to ask for the backedge-taken count would likely result\n      // in infinite recursion. In the later case, the analysis code will\n      // cope with a conservative value, and it will take care to purge\n      // that value once it has finished.\n      const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(L);\n      if (!isa<SCEVCouldNotCompute>(MaxBECount)) {\n        // Manually compute the final value for AR, checking for\n        // overflow.\n\n        // Check whether the backedge-taken count can be losslessly casted to\n        // the addrec's type. The count is always unsigned.\n        const SCEV *CastedMaxBECount =\n            getTruncateOrZeroExtend(MaxBECount, Start->getType(), Depth);\n        const SCEV *RecastedMaxBECount = getTruncateOrZeroExtend(\n            CastedMaxBECount, MaxBECount->getType(), Depth);\n        if (MaxBECount == RecastedMaxBECount) {\n          Type *WideTy = IntegerType::get(getContext(), BitWidth * 2);\n          // Check whether Start+Step*MaxBECount has no signed overflow.\n          const SCEV *SMul = getMulExpr(CastedMaxBECount, Step,\n                                        SCEV::FlagAnyWrap, Depth + 1);\n          const SCEV *SAdd = getSignExtendExpr(getAddExpr(Start, SMul,\n                                                          SCEV::FlagAnyWrap,\n                                                          Depth + 1),\n                                               WideTy, Depth + 1);\n          const SCEV *WideStart = getSignExtendExpr(Start, WideTy, Depth + 1);\n          const SCEV *WideMaxBECount =\n            getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);\n          const SCEV *OperandExtendedAdd =\n            getAddExpr(WideStart,\n                       getMulExpr(WideMaxBECount,\n                                  getSignExtendExpr(Step, WideTy, Depth + 1),\n                                  SCEV::FlagAnyWrap, Depth + 1),\n                       SCEV::FlagAnyWrap, Depth + 1);\n          if (SAdd == OperandExtendedAdd) {\n            // Cache knowledge of AR NSW, which is propagated to this AddRec.\n            setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNSW);\n            // Return the expression with the addrec on the outside.\n            return getAddRecExpr(\n                getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getSignExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n          }\n          // Similar to above, only this time treat the step value as unsigned.\n          // This covers loops that count up with an unsigned step.\n          OperandExtendedAdd =\n            getAddExpr(WideStart,\n                       getMulExpr(WideMaxBECount,\n                                  getZeroExtendExpr(Step, WideTy, Depth + 1),\n                                  SCEV::FlagAnyWrap, Depth + 1),\n                       SCEV::FlagAnyWrap, Depth + 1);\n          if (SAdd == OperandExtendedAdd) {\n            // If AR wraps around then\n            //\n            //    abs(Step) * MaxBECount > unsigned-max(AR->getType())\n            // => SAdd != OperandExtendedAdd\n            //\n            // Thus (AR is not NW => SAdd != OperandExtendedAdd) <=>\n            // (SAdd == OperandExtendedAdd => AR is NW)\n\n            setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);\n\n            // Return the expression with the addrec on the outside.\n            return getAddRecExpr(\n                getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this,\n                                                         Depth + 1),\n                getZeroExtendExpr(Step, Ty, Depth + 1), L,\n                AR->getNoWrapFlags());\n          }\n        }\n      }\n\n      auto NewFlags = proveNoSignedWrapViaInduction(AR);\n      setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), NewFlags);\n      if (AR->hasNoSignedWrap()) {\n        // Same as nsw case above - duplicated here to avoid a compile time\n        // issue.  It's not clear that the order of checks does matter, but\n        // it's one of two issue possible causes for a change which was\n        // reverted.  Be conservative for the moment.\n        return getAddRecExpr(\n            getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this, Depth + 1),\n            getSignExtendExpr(Step, Ty, Depth + 1), L, AR->getNoWrapFlags());\n      }\n\n      // sext({C,+,Step}) --> (sext(D) + sext({C-D,+,Step}))<nuw><nsw>\n      // if D + (C - D + Step * n) could be proven to not signed wrap\n      // where D maximizes the number of trailing zeros of (C - D + Step * n)\n      if (const auto *SC = dyn_cast<SCEVConstant>(Start)) {\n        const APInt &C = SC->getAPInt();\n        const APInt &D = extractConstantWithoutWrapping(*this, C, Step);\n        if (D != 0) {\n          const SCEV *SSExtD = getSignExtendExpr(getConstant(D), Ty, Depth);\n          const SCEV *SResidual =\n              getAddRecExpr(getConstant(C - D), Step, L, AR->getNoWrapFlags());\n          const SCEV *SSExtR = getSignExtendExpr(SResidual, Ty, Depth + 1);\n          return getAddExpr(SSExtD, SSExtR,\n                            (SCEV::NoWrapFlags)(SCEV::FlagNSW | SCEV::FlagNUW),\n                            Depth + 1);\n        }\n      }\n\n      if (proveNoWrapByVaryingStart<SCEVSignExtendExpr>(Start, Step, L)) {\n        setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNSW);\n        return getAddRecExpr(\n            getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this, Depth + 1),\n            getSignExtendExpr(Step, Ty, Depth + 1), L, AR->getNoWrapFlags());\n      }\n    }\n\n  // If the input value is provably positive and we could not simplify\n  // away the sext build a zext instead.\n  if (isKnownNonNegative(Op))\n    return getZeroExtendExpr(Op, Ty, Depth + 1);\n\n  // The cast wasn't folded; create an explicit cast node.\n  // Recompute the insert position, as it may have been invalidated.\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  SCEV *S = new (SCEVAllocator) SCEVSignExtendExpr(ID.Intern(SCEVAllocator),\n                                                   Op, Ty);\n  UniqueSCEVs.InsertNode(S, IP);\n  addToLoopUseLists(S);\n  return S;\n}\n\n/// getAnyExtendExpr - Return a SCEV for the given operand extended with\n/// unspecified bits out to the given type.\nconst SCEV *ScalarEvolution::getAnyExtendExpr(const SCEV *Op,\n                                              Type *Ty) {\n  assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&\n         \"This is not an extending conversion!\");\n  assert(isSCEVable(Ty) &&\n         \"This is not a conversion to a SCEVable type!\");\n  Ty = getEffectiveSCEVType(Ty);\n\n  // Sign-extend negative constants.\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))\n    if (SC->getAPInt().isNegative())\n      return getSignExtendExpr(Op, Ty);\n\n  // Peel off a truncate cast.\n  if (const SCEVTruncateExpr *T = dyn_cast<SCEVTruncateExpr>(Op)) {\n    const SCEV *NewOp = T->getOperand();\n    if (getTypeSizeInBits(NewOp->getType()) < getTypeSizeInBits(Ty))\n      return getAnyExtendExpr(NewOp, Ty);\n    return getTruncateOrNoop(NewOp, Ty);\n  }\n\n  // Next try a zext cast. If the cast is folded, use it.\n  const SCEV *ZExt = getZeroExtendExpr(Op, Ty);\n  if (!isa<SCEVZeroExtendExpr>(ZExt))\n    return ZExt;\n\n  // Next try a sext cast. If the cast is folded, use it.\n  const SCEV *SExt = getSignExtendExpr(Op, Ty);\n  if (!isa<SCEVSignExtendExpr>(SExt))\n    return SExt;\n\n  // Force the cast to be folded into the operands of an addrec.\n  if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(Op)) {\n    SmallVector<const SCEV *, 4> Ops;\n    for (const SCEV *Op : AR->operands())\n      Ops.push_back(getAnyExtendExpr(Op, Ty));\n    return getAddRecExpr(Ops, AR->getLoop(), SCEV::FlagNW);\n  }\n\n  // If the expression is obviously signed, use the sext cast value.\n  if (isa<SCEVSMaxExpr>(Op))\n    return SExt;\n\n  // Absent any other information, use the zext cast value.\n  return ZExt;\n}\n\n/// Process the given Ops list, which is a list of operands to be added under\n/// the given scale, update the given map. This is a helper function for\n/// getAddRecExpr. As an example of what it does, given a sequence of operands\n/// that would form an add expression like this:\n///\n///    m + n + 13 + (A * (o + p + (B * (q + m + 29)))) + r + (-1 * r)\n///\n/// where A and B are constants, update the map with these values:\n///\n///    (m, 1+A*B), (n, 1), (o, A), (p, A), (q, A*B), (r, 0)\n///\n/// and add 13 + A*B*29 to AccumulatedConstant.\n/// This will allow getAddRecExpr to produce this:\n///\n///    13+A*B*29 + n + (m * (1+A*B)) + ((o + p) * A) + (q * A*B)\n///\n/// This form often exposes folding opportunities that are hidden in\n/// the original operand list.\n///\n/// Return true iff it appears that any interesting folding opportunities\n/// may be exposed. This helps getAddRecExpr short-circuit extra work in\n/// the common case where no interesting opportunities are present, and\n/// is also used as a check to avoid infinite recursion.\nstatic bool\nCollectAddOperandsWithScales(DenseMap<const SCEV *, APInt> &M,\n                             SmallVectorImpl<const SCEV *> &NewOps,\n                             APInt &AccumulatedConstant,\n                             const SCEV *const *Ops, size_t NumOperands,\n                             const APInt &Scale,\n                             ScalarEvolution &SE) {\n  bool Interesting = false;\n\n  // Iterate over the add operands. They are sorted, with constants first.\n  unsigned i = 0;\n  while (const SCEVConstant *C = dyn_cast<SCEVConstant>(Ops[i])) {\n    ++i;\n    // Pull a buried constant out to the outside.\n    if (Scale != 1 || AccumulatedConstant != 0 || C->getValue()->isZero())\n      Interesting = true;\n    AccumulatedConstant += Scale * C->getAPInt();\n  }\n\n  // Next comes everything else. We're especially interested in multiplies\n  // here, but they're in the middle, so just visit the rest with one loop.\n  for (; i != NumOperands; ++i) {\n    const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(Ops[i]);\n    if (Mul && isa<SCEVConstant>(Mul->getOperand(0))) {\n      APInt NewScale =\n          Scale * cast<SCEVConstant>(Mul->getOperand(0))->getAPInt();\n      if (Mul->getNumOperands() == 2 && isa<SCEVAddExpr>(Mul->getOperand(1))) {\n        // A multiplication of a constant with another add; recurse.\n        const SCEVAddExpr *Add = cast<SCEVAddExpr>(Mul->getOperand(1));\n        Interesting |=\n          CollectAddOperandsWithScales(M, NewOps, AccumulatedConstant,\n                                       Add->op_begin(), Add->getNumOperands(),\n                                       NewScale, SE);\n      } else {\n        // A multiplication of a constant with some other value. Update\n        // the map.\n        SmallVector<const SCEV *, 4> MulOps(drop_begin(Mul->operands()));\n        const SCEV *Key = SE.getMulExpr(MulOps);\n        auto Pair = M.insert({Key, NewScale});\n        if (Pair.second) {\n          NewOps.push_back(Pair.first->first);\n        } else {\n          Pair.first->second += NewScale;\n          // The map already had an entry for this value, which may indicate\n          // a folding opportunity.\n          Interesting = true;\n        }\n      }\n    } else {\n      // An ordinary operand. Update the map.\n      std::pair<DenseMap<const SCEV *, APInt>::iterator, bool> Pair =\n          M.insert({Ops[i], Scale});\n      if (Pair.second) {\n        NewOps.push_back(Pair.first->first);\n      } else {\n        Pair.first->second += Scale;\n        // The map already had an entry for this value, which may indicate\n        // a folding opportunity.\n        Interesting = true;\n      }\n    }\n  }\n\n  return Interesting;\n}\n\n// We're trying to construct a SCEV of type `Type' with `Ops' as operands and\n// `OldFlags' as can't-wrap behavior.  Infer a more aggressive set of\n// can't-overflow flags for the operation if possible.\nstatic SCEV::NoWrapFlags\nStrengthenNoWrapFlags(ScalarEvolution *SE, SCEVTypes Type,\n                      const ArrayRef<const SCEV *> Ops,\n                      SCEV::NoWrapFlags Flags) {\n  using namespace std::placeholders;\n\n  using OBO = OverflowingBinaryOperator;\n\n  bool CanAnalyze =\n      Type == scAddExpr || Type == scAddRecExpr || Type == scMulExpr;\n  (void)CanAnalyze;\n  assert(CanAnalyze && \"don't call from other places!\");\n\n  int SignOrUnsignMask = SCEV::FlagNUW | SCEV::FlagNSW;\n  SCEV::NoWrapFlags SignOrUnsignWrap =\n      ScalarEvolution::maskFlags(Flags, SignOrUnsignMask);\n\n  // If FlagNSW is true and all the operands are non-negative, infer FlagNUW.\n  auto IsKnownNonNegative = [&](const SCEV *S) {\n    return SE->isKnownNonNegative(S);\n  };\n\n  if (SignOrUnsignWrap == SCEV::FlagNSW && all_of(Ops, IsKnownNonNegative))\n    Flags =\n        ScalarEvolution::setFlags(Flags, (SCEV::NoWrapFlags)SignOrUnsignMask);\n\n  SignOrUnsignWrap = ScalarEvolution::maskFlags(Flags, SignOrUnsignMask);\n\n  if (SignOrUnsignWrap != SignOrUnsignMask &&\n      (Type == scAddExpr || Type == scMulExpr) && Ops.size() == 2 &&\n      isa<SCEVConstant>(Ops[0])) {\n\n    auto Opcode = [&] {\n      switch (Type) {\n      case scAddExpr:\n        return Instruction::Add;\n      case scMulExpr:\n        return Instruction::Mul;\n      default:\n        llvm_unreachable(\"Unexpected SCEV op.\");\n      }\n    }();\n\n    const APInt &C = cast<SCEVConstant>(Ops[0])->getAPInt();\n\n    // (A <opcode> C) --> (A <opcode> C)<nsw> if the op doesn't sign overflow.\n    if (!(SignOrUnsignWrap & SCEV::FlagNSW)) {\n      auto NSWRegion = ConstantRange::makeGuaranteedNoWrapRegion(\n          Opcode, C, OBO::NoSignedWrap);\n      if (NSWRegion.contains(SE->getSignedRange(Ops[1])))\n        Flags = ScalarEvolution::setFlags(Flags, SCEV::FlagNSW);\n    }\n\n    // (A <opcode> C) --> (A <opcode> C)<nuw> if the op doesn't unsign overflow.\n    if (!(SignOrUnsignWrap & SCEV::FlagNUW)) {\n      auto NUWRegion = ConstantRange::makeGuaranteedNoWrapRegion(\n          Opcode, C, OBO::NoUnsignedWrap);\n      if (NUWRegion.contains(SE->getUnsignedRange(Ops[1])))\n        Flags = ScalarEvolution::setFlags(Flags, SCEV::FlagNUW);\n    }\n  }\n\n  return Flags;\n}\n\nbool ScalarEvolution::isAvailableAtLoopEntry(const SCEV *S, const Loop *L) {\n  return isLoopInvariant(S, L) && properlyDominates(S, L->getHeader());\n}\n\n/// Get a canonical add expression, or something simpler if possible.\nconst SCEV *ScalarEvolution::getAddExpr(SmallVectorImpl<const SCEV *> &Ops,\n                                        SCEV::NoWrapFlags OrigFlags,\n                                        unsigned Depth) {\n  assert(!(OrigFlags & ~(SCEV::FlagNUW | SCEV::FlagNSW)) &&\n         \"only nuw or nsw allowed\");\n  assert(!Ops.empty() && \"Cannot get empty add!\");\n  if (Ops.size() == 1) return Ops[0];\n#ifndef NDEBUG\n  Type *ETy = getEffectiveSCEVType(Ops[0]->getType());\n  for (unsigned i = 1, e = Ops.size(); i != e; ++i)\n    assert(getEffectiveSCEVType(Ops[i]->getType()) == ETy &&\n           \"SCEVAddExpr operand types don't match!\");\n#endif\n\n  // Sort by complexity, this groups all similar expression types together.\n  GroupByComplexity(Ops, &LI, DT);\n\n  // If there are any constants, fold them together.\n  unsigned Idx = 0;\n  if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(Ops[0])) {\n    ++Idx;\n    assert(Idx < Ops.size());\n    while (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(Ops[Idx])) {\n      // We found two constants, fold them together!\n      Ops[0] = getConstant(LHSC->getAPInt() + RHSC->getAPInt());\n      if (Ops.size() == 2) return Ops[0];\n      Ops.erase(Ops.begin()+1);  // Erase the folded element\n      LHSC = cast<SCEVConstant>(Ops[0]);\n    }\n\n    // If we are left with a constant zero being added, strip it off.\n    if (LHSC->getValue()->isZero()) {\n      Ops.erase(Ops.begin());\n      --Idx;\n    }\n\n    if (Ops.size() == 1) return Ops[0];\n  }\n\n  // Delay expensive flag strengthening until necessary.\n  auto ComputeFlags = [this, OrigFlags](const ArrayRef<const SCEV *> Ops) {\n    return StrengthenNoWrapFlags(this, scAddExpr, Ops, OrigFlags);\n  };\n\n  // Limit recursion calls depth.\n  if (Depth > MaxArithDepth || hasHugeExpression(Ops))\n    return getOrCreateAddExpr(Ops, ComputeFlags(Ops));\n\n  if (SCEV *S = std::get<0>(findExistingSCEVInCache(scAddExpr, Ops))) {\n    // Don't strengthen flags if we have no new information.\n    SCEVAddExpr *Add = static_cast<SCEVAddExpr *>(S);\n    if (Add->getNoWrapFlags(OrigFlags) != OrigFlags)\n      Add->setNoWrapFlags(ComputeFlags(Ops));\n    return S;\n  }\n\n  // Okay, check to see if the same value occurs in the operand list more than\n  // once.  If so, merge them together into an multiply expression.  Since we\n  // sorted the list, these values are required to be adjacent.\n  Type *Ty = Ops[0]->getType();\n  bool FoundMatch = false;\n  for (unsigned i = 0, e = Ops.size(); i != e-1; ++i)\n    if (Ops[i] == Ops[i+1]) {      //  X + Y + Y  -->  X + Y*2\n      // Scan ahead to count how many equal operands there are.\n      unsigned Count = 2;\n      while (i+Count != e && Ops[i+Count] == Ops[i])\n        ++Count;\n      // Merge the values into a multiply.\n      const SCEV *Scale = getConstant(Ty, Count);\n      const SCEV *Mul = getMulExpr(Scale, Ops[i], SCEV::FlagAnyWrap, Depth + 1);\n      if (Ops.size() == Count)\n        return Mul;\n      Ops[i] = Mul;\n      Ops.erase(Ops.begin()+i+1, Ops.begin()+i+Count);\n      --i; e -= Count - 1;\n      FoundMatch = true;\n    }\n  if (FoundMatch)\n    return getAddExpr(Ops, OrigFlags, Depth + 1);\n\n  // Check for truncates. If all the operands are truncated from the same\n  // type, see if factoring out the truncate would permit the result to be\n  // folded. eg., n*trunc(x) + m*trunc(y) --> trunc(trunc(m)*x + trunc(n)*y)\n  // if the contents of the resulting outer trunc fold to something simple.\n  auto FindTruncSrcType = [&]() -> Type * {\n    // We're ultimately looking to fold an addrec of truncs and muls of only\n    // constants and truncs, so if we find any other types of SCEV\n    // as operands of the addrec then we bail and return nullptr here.\n    // Otherwise, we return the type of the operand of a trunc that we find.\n    if (auto *T = dyn_cast<SCEVTruncateExpr>(Ops[Idx]))\n      return T->getOperand()->getType();\n    if (const auto *Mul = dyn_cast<SCEVMulExpr>(Ops[Idx])) {\n      const auto *LastOp = Mul->getOperand(Mul->getNumOperands() - 1);\n      if (const auto *T = dyn_cast<SCEVTruncateExpr>(LastOp))\n        return T->getOperand()->getType();\n    }\n    return nullptr;\n  };\n  if (auto *SrcType = FindTruncSrcType()) {\n    SmallVector<const SCEV *, 8> LargeOps;\n    bool Ok = true;\n    // Check all the operands to see if they can be represented in the\n    // source type of the truncate.\n    for (unsigned i = 0, e = Ops.size(); i != e; ++i) {\n      if (const SCEVTruncateExpr *T = dyn_cast<SCEVTruncateExpr>(Ops[i])) {\n        if (T->getOperand()->getType() != SrcType) {\n          Ok = false;\n          break;\n        }\n        LargeOps.push_back(T->getOperand());\n      } else if (const SCEVConstant *C = dyn_cast<SCEVConstant>(Ops[i])) {\n        LargeOps.push_back(getAnyExtendExpr(C, SrcType));\n      } else if (const SCEVMulExpr *M = dyn_cast<SCEVMulExpr>(Ops[i])) {\n        SmallVector<const SCEV *, 8> LargeMulOps;\n        for (unsigned j = 0, f = M->getNumOperands(); j != f && Ok; ++j) {\n          if (const SCEVTruncateExpr *T =\n                dyn_cast<SCEVTruncateExpr>(M->getOperand(j))) {\n            if (T->getOperand()->getType() != SrcType) {\n              Ok = false;\n              break;\n            }\n            LargeMulOps.push_back(T->getOperand());\n          } else if (const auto *C = dyn_cast<SCEVConstant>(M->getOperand(j))) {\n            LargeMulOps.push_back(getAnyExtendExpr(C, SrcType));\n          } else {\n            Ok = false;\n            break;\n          }\n        }\n        if (Ok)\n          LargeOps.push_back(getMulExpr(LargeMulOps, SCEV::FlagAnyWrap, Depth + 1));\n      } else {\n        Ok = false;\n        break;\n      }\n    }\n    if (Ok) {\n      // Evaluate the expression in the larger type.\n      const SCEV *Fold = getAddExpr(LargeOps, SCEV::FlagAnyWrap, Depth + 1);\n      // If it folds to something simple, use it. Otherwise, don't.\n      if (isa<SCEVConstant>(Fold) || isa<SCEVUnknown>(Fold))\n        return getTruncateExpr(Fold, Ty);\n    }\n  }\n\n  // Skip past any other cast SCEVs.\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < scAddExpr)\n    ++Idx;\n\n  // If there are add operands they would be next.\n  if (Idx < Ops.size()) {\n    bool DeletedAdd = false;\n    while (const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(Ops[Idx])) {\n      if (Ops.size() > AddOpsInlineThreshold ||\n          Add->getNumOperands() > AddOpsInlineThreshold)\n        break;\n      // If we have an add, expand the add operands onto the end of the operands\n      // list.\n      Ops.erase(Ops.begin()+Idx);\n      Ops.append(Add->op_begin(), Add->op_end());\n      DeletedAdd = true;\n    }\n\n    // If we deleted at least one add, we added operands to the end of the list,\n    // and they are not necessarily sorted.  Recurse to resort and resimplify\n    // any operands we just acquired.\n    if (DeletedAdd)\n      return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n  }\n\n  // Skip over the add expression until we get to a multiply.\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < scMulExpr)\n    ++Idx;\n\n  // Check to see if there are any folding opportunities present with\n  // operands multiplied by constant values.\n  if (Idx < Ops.size() && isa<SCEVMulExpr>(Ops[Idx])) {\n    uint64_t BitWidth = getTypeSizeInBits(Ty);\n    DenseMap<const SCEV *, APInt> M;\n    SmallVector<const SCEV *, 8> NewOps;\n    APInt AccumulatedConstant(BitWidth, 0);\n    if (CollectAddOperandsWithScales(M, NewOps, AccumulatedConstant,\n                                     Ops.data(), Ops.size(),\n                                     APInt(BitWidth, 1), *this)) {\n      struct APIntCompare {\n        bool operator()(const APInt &LHS, const APInt &RHS) const {\n          return LHS.ult(RHS);\n        }\n      };\n\n      // Some interesting folding opportunity is present, so its worthwhile to\n      // re-generate the operands list. Group the operands by constant scale,\n      // to avoid multiplying by the same constant scale multiple times.\n      std::map<APInt, SmallVector<const SCEV *, 4>, APIntCompare> MulOpLists;\n      for (const SCEV *NewOp : NewOps)\n        MulOpLists[M.find(NewOp)->second].push_back(NewOp);\n      // Re-generate the operands list.\n      Ops.clear();\n      if (AccumulatedConstant != 0)\n        Ops.push_back(getConstant(AccumulatedConstant));\n      for (auto &MulOp : MulOpLists)\n        if (MulOp.first != 0)\n          Ops.push_back(getMulExpr(\n              getConstant(MulOp.first),\n              getAddExpr(MulOp.second, SCEV::FlagAnyWrap, Depth + 1),\n              SCEV::FlagAnyWrap, Depth + 1));\n      if (Ops.empty())\n        return getZero(Ty);\n      if (Ops.size() == 1)\n        return Ops[0];\n      return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n    }\n  }\n\n  // If we are adding something to a multiply expression, make sure the\n  // something is not already an operand of the multiply.  If so, merge it into\n  // the multiply.\n  for (; Idx < Ops.size() && isa<SCEVMulExpr>(Ops[Idx]); ++Idx) {\n    const SCEVMulExpr *Mul = cast<SCEVMulExpr>(Ops[Idx]);\n    for (unsigned MulOp = 0, e = Mul->getNumOperands(); MulOp != e; ++MulOp) {\n      const SCEV *MulOpSCEV = Mul->getOperand(MulOp);\n      if (isa<SCEVConstant>(MulOpSCEV))\n        continue;\n      for (unsigned AddOp = 0, e = Ops.size(); AddOp != e; ++AddOp)\n        if (MulOpSCEV == Ops[AddOp]) {\n          // Fold W + X + (X * Y * Z)  -->  W + (X * ((Y*Z)+1))\n          const SCEV *InnerMul = Mul->getOperand(MulOp == 0);\n          if (Mul->getNumOperands() != 2) {\n            // If the multiply has more than two operands, we must get the\n            // Y*Z term.\n            SmallVector<const SCEV *, 4> MulOps(Mul->op_begin(),\n                                                Mul->op_begin()+MulOp);\n            MulOps.append(Mul->op_begin()+MulOp+1, Mul->op_end());\n            InnerMul = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);\n          }\n          SmallVector<const SCEV *, 2> TwoOps = {getOne(Ty), InnerMul};\n          const SCEV *AddOne = getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);\n          const SCEV *OuterMul = getMulExpr(AddOne, MulOpSCEV,\n                                            SCEV::FlagAnyWrap, Depth + 1);\n          if (Ops.size() == 2) return OuterMul;\n          if (AddOp < Idx) {\n            Ops.erase(Ops.begin()+AddOp);\n            Ops.erase(Ops.begin()+Idx-1);\n          } else {\n            Ops.erase(Ops.begin()+Idx);\n            Ops.erase(Ops.begin()+AddOp-1);\n          }\n          Ops.push_back(OuterMul);\n          return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n        }\n\n      // Check this multiply against other multiplies being added together.\n      for (unsigned OtherMulIdx = Idx+1;\n           OtherMulIdx < Ops.size() && isa<SCEVMulExpr>(Ops[OtherMulIdx]);\n           ++OtherMulIdx) {\n        const SCEVMulExpr *OtherMul = cast<SCEVMulExpr>(Ops[OtherMulIdx]);\n        // If MulOp occurs in OtherMul, we can fold the two multiplies\n        // together.\n        for (unsigned OMulOp = 0, e = OtherMul->getNumOperands();\n             OMulOp != e; ++OMulOp)\n          if (OtherMul->getOperand(OMulOp) == MulOpSCEV) {\n            // Fold X + (A*B*C) + (A*D*E) --> X + (A*(B*C+D*E))\n            const SCEV *InnerMul1 = Mul->getOperand(MulOp == 0);\n            if (Mul->getNumOperands() != 2) {\n              SmallVector<const SCEV *, 4> MulOps(Mul->op_begin(),\n                                                  Mul->op_begin()+MulOp);\n              MulOps.append(Mul->op_begin()+MulOp+1, Mul->op_end());\n              InnerMul1 = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);\n            }\n            const SCEV *InnerMul2 = OtherMul->getOperand(OMulOp == 0);\n            if (OtherMul->getNumOperands() != 2) {\n              SmallVector<const SCEV *, 4> MulOps(OtherMul->op_begin(),\n                                                  OtherMul->op_begin()+OMulOp);\n              MulOps.append(OtherMul->op_begin()+OMulOp+1, OtherMul->op_end());\n              InnerMul2 = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);\n            }\n            SmallVector<const SCEV *, 2> TwoOps = {InnerMul1, InnerMul2};\n            const SCEV *InnerMulSum =\n                getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);\n            const SCEV *OuterMul = getMulExpr(MulOpSCEV, InnerMulSum,\n                                              SCEV::FlagAnyWrap, Depth + 1);\n            if (Ops.size() == 2) return OuterMul;\n            Ops.erase(Ops.begin()+Idx);\n            Ops.erase(Ops.begin()+OtherMulIdx-1);\n            Ops.push_back(OuterMul);\n            return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n          }\n      }\n    }\n  }\n\n  // If there are any add recurrences in the operands list, see if any other\n  // added values are loop invariant.  If so, we can fold them into the\n  // recurrence.\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < scAddRecExpr)\n    ++Idx;\n\n  // Scan over all recurrences, trying to fold loop invariants into them.\n  for (; Idx < Ops.size() && isa<SCEVAddRecExpr>(Ops[Idx]); ++Idx) {\n    // Scan all of the other operands to this add and add them to the vector if\n    // they are loop invariant w.r.t. the recurrence.\n    SmallVector<const SCEV *, 8> LIOps;\n    const SCEVAddRecExpr *AddRec = cast<SCEVAddRecExpr>(Ops[Idx]);\n    const Loop *AddRecLoop = AddRec->getLoop();\n    for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n      if (isAvailableAtLoopEntry(Ops[i], AddRecLoop)) {\n        LIOps.push_back(Ops[i]);\n        Ops.erase(Ops.begin()+i);\n        --i; --e;\n      }\n\n    // If we found some loop invariants, fold them into the recurrence.\n    if (!LIOps.empty()) {\n      // Compute nowrap flags for the addition of the loop-invariant ops and\n      // the addrec. Temporarily push it as an operand for that purpose.\n      LIOps.push_back(AddRec);\n      SCEV::NoWrapFlags Flags = ComputeFlags(LIOps);\n      LIOps.pop_back();\n\n      //  NLI + LI + {Start,+,Step}  -->  NLI + {LI+Start,+,Step}\n      LIOps.push_back(AddRec->getStart());\n\n      SmallVector<const SCEV *, 4> AddRecOps(AddRec->operands());\n      // This follows from the fact that the no-wrap flags on the outer add\n      // expression are applicable on the 0th iteration, when the add recurrence\n      // will be equal to its start value.\n      AddRecOps[0] = getAddExpr(LIOps, Flags, Depth + 1);\n\n      // Build the new addrec. Propagate the NUW and NSW flags if both the\n      // outer add and the inner addrec are guaranteed to have no overflow.\n      // Always propagate NW.\n      Flags = AddRec->getNoWrapFlags(setFlags(Flags, SCEV::FlagNW));\n      const SCEV *NewRec = getAddRecExpr(AddRecOps, AddRecLoop, Flags);\n\n      // If all of the other operands were loop invariant, we are done.\n      if (Ops.size() == 1) return NewRec;\n\n      // Otherwise, add the folded AddRec by the non-invariant parts.\n      for (unsigned i = 0;; ++i)\n        if (Ops[i] == AddRec) {\n          Ops[i] = NewRec;\n          break;\n        }\n      return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n    }\n\n    // Okay, if there weren't any loop invariants to be folded, check to see if\n    // there are multiple AddRec's with the same loop induction variable being\n    // added together.  If so, we can fold them.\n    for (unsigned OtherIdx = Idx+1;\n         OtherIdx < Ops.size() && isa<SCEVAddRecExpr>(Ops[OtherIdx]);\n         ++OtherIdx) {\n      // We expect the AddRecExpr's to be sorted in reverse dominance order,\n      // so that the 1st found AddRecExpr is dominated by all others.\n      assert(DT.dominates(\n           cast<SCEVAddRecExpr>(Ops[OtherIdx])->getLoop()->getHeader(),\n           AddRec->getLoop()->getHeader()) &&\n        \"AddRecExprs are not sorted in reverse dominance order?\");\n      if (AddRecLoop == cast<SCEVAddRecExpr>(Ops[OtherIdx])->getLoop()) {\n        // Other + {A,+,B}<L> + {C,+,D}<L>  -->  Other + {A+C,+,B+D}<L>\n        SmallVector<const SCEV *, 4> AddRecOps(AddRec->operands());\n        for (; OtherIdx != Ops.size() && isa<SCEVAddRecExpr>(Ops[OtherIdx]);\n             ++OtherIdx) {\n          const auto *OtherAddRec = cast<SCEVAddRecExpr>(Ops[OtherIdx]);\n          if (OtherAddRec->getLoop() == AddRecLoop) {\n            for (unsigned i = 0, e = OtherAddRec->getNumOperands();\n                 i != e; ++i) {\n              if (i >= AddRecOps.size()) {\n                AddRecOps.append(OtherAddRec->op_begin()+i,\n                                 OtherAddRec->op_end());\n                break;\n              }\n              SmallVector<const SCEV *, 2> TwoOps = {\n                  AddRecOps[i], OtherAddRec->getOperand(i)};\n              AddRecOps[i] = getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);\n            }\n            Ops.erase(Ops.begin() + OtherIdx); --OtherIdx;\n          }\n        }\n        // Step size has changed, so we cannot guarantee no self-wraparound.\n        Ops[Idx] = getAddRecExpr(AddRecOps, AddRecLoop, SCEV::FlagAnyWrap);\n        return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n      }\n    }\n\n    // Otherwise couldn't fold anything into this recurrence.  Move onto the\n    // next one.\n  }\n\n  // Okay, it looks like we really DO need an add expr.  Check to see if we\n  // already have one, otherwise create a new one.\n  return getOrCreateAddExpr(Ops, ComputeFlags(Ops));\n}\n\nconst SCEV *\nScalarEvolution::getOrCreateAddExpr(ArrayRef<const SCEV *> Ops,\n                                    SCEV::NoWrapFlags Flags) {\n  FoldingSetNodeID ID;\n  ID.AddInteger(scAddExpr);\n  for (const SCEV *Op : Ops)\n    ID.AddPointer(Op);\n  void *IP = nullptr;\n  SCEVAddExpr *S =\n      static_cast<SCEVAddExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));\n  if (!S) {\n    const SCEV **O = SCEVAllocator.Allocate<const SCEV *>(Ops.size());\n    std::uninitialized_copy(Ops.begin(), Ops.end(), O);\n    S = new (SCEVAllocator)\n        SCEVAddExpr(ID.Intern(SCEVAllocator), O, Ops.size());\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n  }\n  S->setNoWrapFlags(Flags);\n  return S;\n}\n\nconst SCEV *\nScalarEvolution::getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops,\n                                       const Loop *L, SCEV::NoWrapFlags Flags) {\n  FoldingSetNodeID ID;\n  ID.AddInteger(scAddRecExpr);\n  for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n    ID.AddPointer(Ops[i]);\n  ID.AddPointer(L);\n  void *IP = nullptr;\n  SCEVAddRecExpr *S =\n      static_cast<SCEVAddRecExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));\n  if (!S) {\n    const SCEV **O = SCEVAllocator.Allocate<const SCEV *>(Ops.size());\n    std::uninitialized_copy(Ops.begin(), Ops.end(), O);\n    S = new (SCEVAllocator)\n        SCEVAddRecExpr(ID.Intern(SCEVAllocator), O, Ops.size(), L);\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n  }\n  setNoWrapFlags(S, Flags);\n  return S;\n}\n\nconst SCEV *\nScalarEvolution::getOrCreateMulExpr(ArrayRef<const SCEV *> Ops,\n                                    SCEV::NoWrapFlags Flags) {\n  FoldingSetNodeID ID;\n  ID.AddInteger(scMulExpr);\n  for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n    ID.AddPointer(Ops[i]);\n  void *IP = nullptr;\n  SCEVMulExpr *S =\n    static_cast<SCEVMulExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));\n  if (!S) {\n    const SCEV **O = SCEVAllocator.Allocate<const SCEV *>(Ops.size());\n    std::uninitialized_copy(Ops.begin(), Ops.end(), O);\n    S = new (SCEVAllocator) SCEVMulExpr(ID.Intern(SCEVAllocator),\n                                        O, Ops.size());\n    UniqueSCEVs.InsertNode(S, IP);\n    addToLoopUseLists(S);\n  }\n  S->setNoWrapFlags(Flags);\n  return S;\n}\n\nstatic uint64_t umul_ov(uint64_t i, uint64_t j, bool &Overflow) {\n  uint64_t k = i*j;\n  if (j > 1 && k / j != i) Overflow = true;\n  return k;\n}\n\n/// Compute the result of \"n choose k\", the binomial coefficient.  If an\n/// intermediate computation overflows, Overflow will be set and the return will\n/// be garbage. Overflow is not cleared on absence of overflow.\nstatic uint64_t Choose(uint64_t n, uint64_t k, bool &Overflow) {\n  // We use the multiplicative formula:\n  //     n(n-1)(n-2)...(n-(k-1)) / k(k-1)(k-2)...1 .\n  // At each iteration, we take the n-th term of the numeral and divide by the\n  // (k-n)th term of the denominator.  This division will always produce an\n  // integral result, and helps reduce the chance of overflow in the\n  // intermediate computations. However, we can still overflow even when the\n  // final result would fit.\n\n  if (n == 0 || n == k) return 1;\n  if (k > n) return 0;\n\n  if (k > n/2)\n    k = n-k;\n\n  uint64_t r = 1;\n  for (uint64_t i = 1; i <= k; ++i) {\n    r = umul_ov(r, n-(i-1), Overflow);\n    r /= i;\n  }\n  return r;\n}\n\n/// Determine if any of the operands in this SCEV are a constant or if\n/// any of the add or multiply expressions in this SCEV contain a constant.\nstatic bool containsConstantInAddMulChain(const SCEV *StartExpr) {\n  struct FindConstantInAddMulChain {\n    bool FoundConstant = false;\n\n    bool follow(const SCEV *S) {\n      FoundConstant |= isa<SCEVConstant>(S);\n      return isa<SCEVAddExpr>(S) || isa<SCEVMulExpr>(S);\n    }\n\n    bool isDone() const {\n      return FoundConstant;\n    }\n  };\n\n  FindConstantInAddMulChain F;\n  SCEVTraversal<FindConstantInAddMulChain> ST(F);\n  ST.visitAll(StartExpr);\n  return F.FoundConstant;\n}\n\n/// Get a canonical multiply expression, or something simpler if possible.\nconst SCEV *ScalarEvolution::getMulExpr(SmallVectorImpl<const SCEV *> &Ops,\n                                        SCEV::NoWrapFlags OrigFlags,\n                                        unsigned Depth) {\n  assert(OrigFlags == maskFlags(OrigFlags, SCEV::FlagNUW | SCEV::FlagNSW) &&\n         \"only nuw or nsw allowed\");\n  assert(!Ops.empty() && \"Cannot get empty mul!\");\n  if (Ops.size() == 1) return Ops[0];\n#ifndef NDEBUG\n  Type *ETy = getEffectiveSCEVType(Ops[0]->getType());\n  for (unsigned i = 1, e = Ops.size(); i != e; ++i)\n    assert(getEffectiveSCEVType(Ops[i]->getType()) == ETy &&\n           \"SCEVMulExpr operand types don't match!\");\n#endif\n\n  // Sort by complexity, this groups all similar expression types together.\n  GroupByComplexity(Ops, &LI, DT);\n\n  // If there are any constants, fold them together.\n  unsigned Idx = 0;\n  if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(Ops[0])) {\n    ++Idx;\n    assert(Idx < Ops.size());\n    while (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(Ops[Idx])) {\n      // We found two constants, fold them together!\n      Ops[0] = getConstant(LHSC->getAPInt() * RHSC->getAPInt());\n      if (Ops.size() == 2) return Ops[0];\n      Ops.erase(Ops.begin()+1);  // Erase the folded element\n      LHSC = cast<SCEVConstant>(Ops[0]);\n    }\n\n    // If we have a multiply of zero, it will always be zero.\n    if (LHSC->getValue()->isZero())\n      return LHSC;\n\n    // If we are left with a constant one being multiplied, strip it off.\n    if (LHSC->getValue()->isOne()) {\n      Ops.erase(Ops.begin());\n      --Idx;\n    }\n\n    if (Ops.size() == 1)\n      return Ops[0];\n  }\n\n  // Delay expensive flag strengthening until necessary.\n  auto ComputeFlags = [this, OrigFlags](const ArrayRef<const SCEV *> Ops) {\n    return StrengthenNoWrapFlags(this, scMulExpr, Ops, OrigFlags);\n  };\n\n  // Limit recursion calls depth.\n  if (Depth > MaxArithDepth || hasHugeExpression(Ops))\n    return getOrCreateMulExpr(Ops, ComputeFlags(Ops));\n\n  if (SCEV *S = std::get<0>(findExistingSCEVInCache(scMulExpr, Ops))) {\n    // Don't strengthen flags if we have no new information.\n    SCEVMulExpr *Mul = static_cast<SCEVMulExpr *>(S);\n    if (Mul->getNoWrapFlags(OrigFlags) != OrigFlags)\n      Mul->setNoWrapFlags(ComputeFlags(Ops));\n    return S;\n  }\n\n  if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(Ops[0])) {\n    if (Ops.size() == 2) {\n      // C1*(C2+V) -> C1*C2 + C1*V\n      if (const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(Ops[1]))\n        // If any of Add's ops are Adds or Muls with a constant, apply this\n        // transformation as well.\n        //\n        // TODO: There are some cases where this transformation is not\n        // profitable; for example, Add = (C0 + X) * Y + Z.  Maybe the scope of\n        // this transformation should be narrowed down.\n        if (Add->getNumOperands() == 2 && containsConstantInAddMulChain(Add))\n          return getAddExpr(getMulExpr(LHSC, Add->getOperand(0),\n                                       SCEV::FlagAnyWrap, Depth + 1),\n                            getMulExpr(LHSC, Add->getOperand(1),\n                                       SCEV::FlagAnyWrap, Depth + 1),\n                            SCEV::FlagAnyWrap, Depth + 1);\n\n      if (Ops[0]->isAllOnesValue()) {\n        // If we have a mul by -1 of an add, try distributing the -1 among the\n        // add operands.\n        if (const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(Ops[1])) {\n          SmallVector<const SCEV *, 4> NewOps;\n          bool AnyFolded = false;\n          for (const SCEV *AddOp : Add->operands()) {\n            const SCEV *Mul = getMulExpr(Ops[0], AddOp, SCEV::FlagAnyWrap,\n                                         Depth + 1);\n            if (!isa<SCEVMulExpr>(Mul)) AnyFolded = true;\n            NewOps.push_back(Mul);\n          }\n          if (AnyFolded)\n            return getAddExpr(NewOps, SCEV::FlagAnyWrap, Depth + 1);\n        } else if (const auto *AddRec = dyn_cast<SCEVAddRecExpr>(Ops[1])) {\n          // Negation preserves a recurrence's no self-wrap property.\n          SmallVector<const SCEV *, 4> Operands;\n          for (const SCEV *AddRecOp : AddRec->operands())\n            Operands.push_back(getMulExpr(Ops[0], AddRecOp, SCEV::FlagAnyWrap,\n                                          Depth + 1));\n\n          return getAddRecExpr(Operands, AddRec->getLoop(),\n                               AddRec->getNoWrapFlags(SCEV::FlagNW));\n        }\n      }\n    }\n  }\n\n  // Skip over the add expression until we get to a multiply.\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < scMulExpr)\n    ++Idx;\n\n  // If there are mul operands inline them all into this expression.\n  if (Idx < Ops.size()) {\n    bool DeletedMul = false;\n    while (const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(Ops[Idx])) {\n      if (Ops.size() > MulOpsInlineThreshold)\n        break;\n      // If we have an mul, expand the mul operands onto the end of the\n      // operands list.\n      Ops.erase(Ops.begin()+Idx);\n      Ops.append(Mul->op_begin(), Mul->op_end());\n      DeletedMul = true;\n    }\n\n    // If we deleted at least one mul, we added operands to the end of the\n    // list, and they are not necessarily sorted.  Recurse to resort and\n    // resimplify any operands we just acquired.\n    if (DeletedMul)\n      return getMulExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n  }\n\n  // If there are any add recurrences in the operands list, see if any other\n  // added values are loop invariant.  If so, we can fold them into the\n  // recurrence.\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < scAddRecExpr)\n    ++Idx;\n\n  // Scan over all recurrences, trying to fold loop invariants into them.\n  for (; Idx < Ops.size() && isa<SCEVAddRecExpr>(Ops[Idx]); ++Idx) {\n    // Scan all of the other operands to this mul and add them to the vector\n    // if they are loop invariant w.r.t. the recurrence.\n    SmallVector<const SCEV *, 8> LIOps;\n    const SCEVAddRecExpr *AddRec = cast<SCEVAddRecExpr>(Ops[Idx]);\n    const Loop *AddRecLoop = AddRec->getLoop();\n    for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n      if (isAvailableAtLoopEntry(Ops[i], AddRecLoop)) {\n        LIOps.push_back(Ops[i]);\n        Ops.erase(Ops.begin()+i);\n        --i; --e;\n      }\n\n    // If we found some loop invariants, fold them into the recurrence.\n    if (!LIOps.empty()) {\n      //  NLI * LI * {Start,+,Step}  -->  NLI * {LI*Start,+,LI*Step}\n      SmallVector<const SCEV *, 4> NewOps;\n      NewOps.reserve(AddRec->getNumOperands());\n      const SCEV *Scale = getMulExpr(LIOps, SCEV::FlagAnyWrap, Depth + 1);\n      for (unsigned i = 0, e = AddRec->getNumOperands(); i != e; ++i)\n        NewOps.push_back(getMulExpr(Scale, AddRec->getOperand(i),\n                                    SCEV::FlagAnyWrap, Depth + 1));\n\n      // Build the new addrec. Propagate the NUW and NSW flags if both the\n      // outer mul and the inner addrec are guaranteed to have no overflow.\n      //\n      // No self-wrap cannot be guaranteed after changing the step size, but\n      // will be inferred if either NUW or NSW is true.\n      SCEV::NoWrapFlags Flags = ComputeFlags({Scale, AddRec});\n      const SCEV *NewRec = getAddRecExpr(\n          NewOps, AddRecLoop, AddRec->getNoWrapFlags(Flags));\n\n      // If all of the other operands were loop invariant, we are done.\n      if (Ops.size() == 1) return NewRec;\n\n      // Otherwise, multiply the folded AddRec by the non-invariant parts.\n      for (unsigned i = 0;; ++i)\n        if (Ops[i] == AddRec) {\n          Ops[i] = NewRec;\n          break;\n        }\n      return getMulExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n    }\n\n    // Okay, if there weren't any loop invariants to be folded, check to see\n    // if there are multiple AddRec's with the same loop induction variable\n    // being multiplied together.  If so, we can fold them.\n\n    // {A1,+,A2,+,...,+,An}<L> * {B1,+,B2,+,...,+,Bn}<L>\n    // = {x=1 in [ sum y=x..2x [ sum z=max(y-x, y-n)..min(x,n) [\n    //       choose(x, 2x)*choose(2x-y, x-z)*A_{y-z}*B_z\n    //   ]]],+,...up to x=2n}.\n    // Note that the arguments to choose() are always integers with values\n    // known at compile time, never SCEV objects.\n    //\n    // The implementation avoids pointless extra computations when the two\n    // addrec's are of different length (mathematically, it's equivalent to\n    // an infinite stream of zeros on the right).\n    bool OpsModified = false;\n    for (unsigned OtherIdx = Idx+1;\n         OtherIdx != Ops.size() && isa<SCEVAddRecExpr>(Ops[OtherIdx]);\n         ++OtherIdx) {\n      const SCEVAddRecExpr *OtherAddRec =\n        dyn_cast<SCEVAddRecExpr>(Ops[OtherIdx]);\n      if (!OtherAddRec || OtherAddRec->getLoop() != AddRecLoop)\n        continue;\n\n      // Limit max number of arguments to avoid creation of unreasonably big\n      // SCEVAddRecs with very complex operands.\n      if (AddRec->getNumOperands() + OtherAddRec->getNumOperands() - 1 >\n          MaxAddRecSize || hasHugeExpression({AddRec, OtherAddRec}))\n        continue;\n\n      bool Overflow = false;\n      Type *Ty = AddRec->getType();\n      bool LargerThan64Bits = getTypeSizeInBits(Ty) > 64;\n      SmallVector<const SCEV*, 7> AddRecOps;\n      for (int x = 0, xe = AddRec->getNumOperands() +\n             OtherAddRec->getNumOperands() - 1; x != xe && !Overflow; ++x) {\n        SmallVector <const SCEV *, 7> SumOps;\n        for (int y = x, ye = 2*x+1; y != ye && !Overflow; ++y) {\n          uint64_t Coeff1 = Choose(x, 2*x - y, Overflow);\n          for (int z = std::max(y-x, y-(int)AddRec->getNumOperands()+1),\n                 ze = std::min(x+1, (int)OtherAddRec->getNumOperands());\n               z < ze && !Overflow; ++z) {\n            uint64_t Coeff2 = Choose(2*x - y, x-z, Overflow);\n            uint64_t Coeff;\n            if (LargerThan64Bits)\n              Coeff = umul_ov(Coeff1, Coeff2, Overflow);\n            else\n              Coeff = Coeff1*Coeff2;\n            const SCEV *CoeffTerm = getConstant(Ty, Coeff);\n            const SCEV *Term1 = AddRec->getOperand(y-z);\n            const SCEV *Term2 = OtherAddRec->getOperand(z);\n            SumOps.push_back(getMulExpr(CoeffTerm, Term1, Term2,\n                                        SCEV::FlagAnyWrap, Depth + 1));\n          }\n        }\n        if (SumOps.empty())\n          SumOps.push_back(getZero(Ty));\n        AddRecOps.push_back(getAddExpr(SumOps, SCEV::FlagAnyWrap, Depth + 1));\n      }\n      if (!Overflow) {\n        const SCEV *NewAddRec = getAddRecExpr(AddRecOps, AddRecLoop,\n                                              SCEV::FlagAnyWrap);\n        if (Ops.size() == 2) return NewAddRec;\n        Ops[Idx] = NewAddRec;\n        Ops.erase(Ops.begin() + OtherIdx); --OtherIdx;\n        OpsModified = true;\n        AddRec = dyn_cast<SCEVAddRecExpr>(NewAddRec);\n        if (!AddRec)\n          break;\n      }\n    }\n    if (OpsModified)\n      return getMulExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);\n\n    // Otherwise couldn't fold anything into this recurrence.  Move onto the\n    // next one.\n  }\n\n  // Okay, it looks like we really DO need an mul expr.  Check to see if we\n  // already have one, otherwise create a new one.\n  return getOrCreateMulExpr(Ops, ComputeFlags(Ops));\n}\n\n/// Represents an unsigned remainder expression based on unsigned division.\nconst SCEV *ScalarEvolution::getURemExpr(const SCEV *LHS,\n                                         const SCEV *RHS) {\n  assert(getEffectiveSCEVType(LHS->getType()) ==\n         getEffectiveSCEVType(RHS->getType()) &&\n         \"SCEVURemExpr operand types don't match!\");\n\n  // Short-circuit easy cases\n  if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS)) {\n    // If constant is one, the result is trivial\n    if (RHSC->getValue()->isOne())\n      return getZero(LHS->getType()); // X urem 1 --> 0\n\n    // If constant is a power of two, fold into a zext(trunc(LHS)).\n    if (RHSC->getAPInt().isPowerOf2()) {\n      Type *FullTy = LHS->getType();\n      Type *TruncTy =\n          IntegerType::get(getContext(), RHSC->getAPInt().logBase2());\n      return getZeroExtendExpr(getTruncateExpr(LHS, TruncTy), FullTy);\n    }\n  }\n\n  // Fallback to %a == %x urem %y == %x -<nuw> ((%x udiv %y) *<nuw> %y)\n  const SCEV *UDiv = getUDivExpr(LHS, RHS);\n  const SCEV *Mult = getMulExpr(UDiv, RHS, SCEV::FlagNUW);\n  return getMinusSCEV(LHS, Mult, SCEV::FlagNUW);\n}\n\n/// Get a canonical unsigned division expression, or something simpler if\n/// possible.\nconst SCEV *ScalarEvolution::getUDivExpr(const SCEV *LHS,\n                                         const SCEV *RHS) {\n  assert(getEffectiveSCEVType(LHS->getType()) ==\n         getEffectiveSCEVType(RHS->getType()) &&\n         \"SCEVUDivExpr operand types don't match!\");\n\n  FoldingSetNodeID ID;\n  ID.AddInteger(scUDivExpr);\n  ID.AddPointer(LHS);\n  ID.AddPointer(RHS);\n  void *IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))\n    return S;\n\n  if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS)) {\n    if (RHSC->getValue()->isOne())\n      return LHS;                               // X udiv 1 --> x\n    // If the denominator is zero, the result of the udiv is undefined. Don't\n    // try to analyze it, because the resolution chosen here may differ from\n    // the resolution chosen in other parts of the compiler.\n    if (!RHSC->getValue()->isZero()) {\n      // Determine if the division can be folded into the operands of\n      // its operands.\n      // TODO: Generalize this to non-constants by using known-bits information.\n      Type *Ty = LHS->getType();\n      unsigned LZ = RHSC->getAPInt().countLeadingZeros();\n      unsigned MaxShiftAmt = getTypeSizeInBits(Ty) - LZ - 1;\n      // For non-power-of-two values, effectively round the value up to the\n      // nearest power of two.\n      if (!RHSC->getAPInt().isPowerOf2())\n        ++MaxShiftAmt;\n      IntegerType *ExtTy =\n        IntegerType::get(getContext(), getTypeSizeInBits(Ty) + MaxShiftAmt);\n      if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(LHS))\n        if (const SCEVConstant *Step =\n            dyn_cast<SCEVConstant>(AR->getStepRecurrence(*this))) {\n          // {X,+,N}/C --> {X/C,+,N/C} if safe and N/C can be folded.\n          const APInt &StepInt = Step->getAPInt();\n          const APInt &DivInt = RHSC->getAPInt();\n          if (!StepInt.urem(DivInt) &&\n              getZeroExtendExpr(AR, ExtTy) ==\n              getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),\n                            getZeroExtendExpr(Step, ExtTy),\n                            AR->getLoop(), SCEV::FlagAnyWrap)) {\n            SmallVector<const SCEV *, 4> Operands;\n            for (const SCEV *Op : AR->operands())\n              Operands.push_back(getUDivExpr(Op, RHS));\n            return getAddRecExpr(Operands, AR->getLoop(), SCEV::FlagNW);\n          }\n          /// Get a canonical UDivExpr for a recurrence.\n          /// {X,+,N}/C => {Y,+,N}/C where Y=X-(X%N). Safe when C%N=0.\n          // We can currently only fold X%N if X is constant.\n          const SCEVConstant *StartC = dyn_cast<SCEVConstant>(AR->getStart());\n          if (StartC && !DivInt.urem(StepInt) &&\n              getZeroExtendExpr(AR, ExtTy) ==\n              getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),\n                            getZeroExtendExpr(Step, ExtTy),\n                            AR->getLoop(), SCEV::FlagAnyWrap)) {\n            const APInt &StartInt = StartC->getAPInt();\n            const APInt &StartRem = StartInt.urem(StepInt);\n            if (StartRem != 0) {\n              const SCEV *NewLHS =\n                  getAddRecExpr(getConstant(StartInt - StartRem), Step,\n                                AR->getLoop(), SCEV::FlagNW);\n              if (LHS != NewLHS) {\n                LHS = NewLHS;\n\n                // Reset the ID to include the new LHS, and check if it is\n                // already cached.\n                ID.clear();\n                ID.AddInteger(scUDivExpr);\n                ID.AddPointer(LHS);\n                ID.AddPointer(RHS);\n                IP = nullptr;\n                if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))\n                  return S;\n              }\n            }\n          }\n        }\n      // (A*B)/C --> A*(B/C) if safe and B/C can be folded.\n      if (const SCEVMulExpr *M = dyn_cast<SCEVMulExpr>(LHS)) {\n        SmallVector<const SCEV *, 4> Operands;\n        for (const SCEV *Op : M->operands())\n          Operands.push_back(getZeroExtendExpr(Op, ExtTy));\n        if (getZeroExtendExpr(M, ExtTy) == getMulExpr(Operands))\n          // Find an operand that's safely divisible.\n          for (unsigned i = 0, e = M->getNumOperands(); i != e; ++i) {\n            const SCEV *Op = M->getOperand(i);\n            const SCEV *Div = getUDivExpr(Op, RHSC);\n            if (!isa<SCEVUDivExpr>(Div) && getMulExpr(Div, RHSC) == Op) {\n              Operands = SmallVector<const SCEV *, 4>(M->operands());\n              Operands[i] = Div;\n              return getMulExpr(Operands);\n            }\n          }\n      }\n\n      // (A/B)/C --> A/(B*C) if safe and B*C can be folded.\n      if (const SCEVUDivExpr *OtherDiv = dyn_cast<SCEVUDivExpr>(LHS)) {\n        if (auto *DivisorConstant =\n                dyn_cast<SCEVConstant>(OtherDiv->getRHS())) {\n          bool Overflow = false;\n          APInt NewRHS =\n              DivisorConstant->getAPInt().umul_ov(RHSC->getAPInt(), Overflow);\n          if (Overflow) {\n            return getConstant(RHSC->getType(), 0, false);\n          }\n          return getUDivExpr(OtherDiv->getLHS(), getConstant(NewRHS));\n        }\n      }\n\n      // (A+B)/C --> (A/C + B/C) if safe and A/C and B/C can be folded.\n      if (const SCEVAddExpr *A = dyn_cast<SCEVAddExpr>(LHS)) {\n        SmallVector<const SCEV *, 4> Operands;\n        for (const SCEV *Op : A->operands())\n          Operands.push_back(getZeroExtendExpr(Op, ExtTy));\n        if (getZeroExtendExpr(A, ExtTy) == getAddExpr(Operands)) {\n          Operands.clear();\n          for (unsigned i = 0, e = A->getNumOperands(); i != e; ++i) {\n            const SCEV *Op = getUDivExpr(A->getOperand(i), RHS);\n            if (isa<SCEVUDivExpr>(Op) ||\n                getMulExpr(Op, RHS) != A->getOperand(i))\n              break;\n            Operands.push_back(Op);\n          }\n          if (Operands.size() == A->getNumOperands())\n            return getAddExpr(Operands);\n        }\n      }\n\n      // Fold if both operands are constant.\n      if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(LHS)) {\n        Constant *LHSCV = LHSC->getValue();\n        Constant *RHSCV = RHSC->getValue();\n        return getConstant(cast<ConstantInt>(ConstantExpr::getUDiv(LHSCV,\n                                                                   RHSCV)));\n      }\n    }\n  }\n\n  // The Insertion Point (IP) might be invalid by now (due to UniqueSCEVs\n  // changes). Make sure we get a new one.\n  IP = nullptr;\n  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;\n  SCEV *S = new (SCEVAllocator) SCEVUDivExpr(ID.Intern(SCEVAllocator),\n                                             LHS, RHS);\n  UniqueSCEVs.InsertNode(S, IP);\n  addToLoopUseLists(S);\n  return S;\n}\n\nstatic const APInt gcd(const SCEVConstant *C1, const SCEVConstant *C2) {\n  APInt A = C1->getAPInt().abs();\n  APInt B = C2->getAPInt().abs();\n  uint32_t ABW = A.getBitWidth();\n  uint32_t BBW = B.getBitWidth();\n\n  if (ABW > BBW)\n    B = B.zext(ABW);\n  else if (ABW < BBW)\n    A = A.zext(BBW);\n\n  return APIntOps::GreatestCommonDivisor(std::move(A), std::move(B));\n}\n\n/// Get a canonical unsigned division expression, or something simpler if\n/// possible. There is no representation for an exact udiv in SCEV IR, but we\n/// can attempt to remove factors from the LHS and RHS.  We can't do this when\n/// it's not exact because the udiv may be clearing bits.\nconst SCEV *ScalarEvolution::getUDivExactExpr(const SCEV *LHS,\n                                              const SCEV *RHS) {\n  // TODO: we could try to find factors in all sorts of things, but for now we\n  // just deal with u/exact (multiply, constant). See SCEVDivision towards the\n  // end of this file for inspiration.\n\n  const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(LHS);\n  if (!Mul || !Mul->hasNoUnsignedWrap())\n    return getUDivExpr(LHS, RHS);\n\n  if (const SCEVConstant *RHSCst = dyn_cast<SCEVConstant>(RHS)) {\n    // If the mulexpr multiplies by a constant, then that constant must be the\n    // first element of the mulexpr.\n    if (const auto *LHSCst = dyn_cast<SCEVConstant>(Mul->getOperand(0))) {\n      if (LHSCst == RHSCst) {\n        SmallVector<const SCEV *, 2> Operands(drop_begin(Mul->operands()));\n        return getMulExpr(Operands);\n      }\n\n      // We can't just assume that LHSCst divides RHSCst cleanly, it could be\n      // that there's a factor provided by one of the other terms. We need to\n      // check.\n      APInt Factor = gcd(LHSCst, RHSCst);\n      if (!Factor.isIntN(1)) {\n        LHSCst =\n            cast<SCEVConstant>(getConstant(LHSCst->getAPInt().udiv(Factor)));\n        RHSCst =\n            cast<SCEVConstant>(getConstant(RHSCst->getAPInt().udiv(Factor)));\n        SmallVector<const SCEV *, 2> Operands;\n        Operands.push_back(LHSCst);\n        Operands.append(Mul->op_begin() + 1, Mul->op_end());\n        LHS = getMulExpr(Operands);\n        RHS = RHSCst;\n        Mul = dyn_cast<SCEVMulExpr>(LHS);\n        if (!Mul)\n          return getUDivExactExpr(LHS, RHS);\n      }\n    }\n  }\n\n  for (int i = 0, e = Mul->getNumOperands(); i != e; ++i) {\n    if (Mul->getOperand(i) == RHS) {\n      SmallVector<const SCEV *, 2> Operands;\n      Operands.append(Mul->op_begin(), Mul->op_begin() + i);\n      Operands.append(Mul->op_begin() + i + 1, Mul->op_end());\n      return getMulExpr(Operands);\n    }\n  }\n\n  return getUDivExpr(LHS, RHS);\n}\n\n/// Get an add recurrence expression for the specified loop.  Simplify the\n/// expression as much as possible.\nconst SCEV *ScalarEvolution::getAddRecExpr(const SCEV *Start, const SCEV *Step,\n                                           const Loop *L,\n                                           SCEV::NoWrapFlags Flags) {\n  SmallVector<const SCEV *, 4> Operands;\n  Operands.push_back(Start);\n  if (const SCEVAddRecExpr *StepChrec = dyn_cast<SCEVAddRecExpr>(Step))\n    if (StepChrec->getLoop() == L) {\n      Operands.append(StepChrec->op_begin(), StepChrec->op_end());\n      return getAddRecExpr(Operands, L, maskFlags(Flags, SCEV::FlagNW));\n    }\n\n  Operands.push_back(Step);\n  return getAddRecExpr(Operands, L, Flags);\n}\n\n/// Get an add recurrence expression for the specified loop.  Simplify the\n/// expression as much as possible.\nconst SCEV *\nScalarEvolution::getAddRecExpr(SmallVectorImpl<const SCEV *> &Operands,\n                               const Loop *L, SCEV::NoWrapFlags Flags) {\n  if (Operands.size() == 1) return Operands[0];\n#ifndef NDEBUG\n  Type *ETy = getEffectiveSCEVType(Operands[0]->getType());\n  for (unsigned i = 1, e = Operands.size(); i != e; ++i)\n    assert(getEffectiveSCEVType(Operands[i]->getType()) == ETy &&\n           \"SCEVAddRecExpr operand types don't match!\");\n  for (unsigned i = 0, e = Operands.size(); i != e; ++i)\n    assert(isLoopInvariant(Operands[i], L) &&\n           \"SCEVAddRecExpr operand is not loop-invariant!\");\n#endif\n\n  if (Operands.back()->isZero()) {\n    Operands.pop_back();\n    return getAddRecExpr(Operands, L, SCEV::FlagAnyWrap); // {X,+,0}  -->  X\n  }\n\n  // It's tempting to want to call getConstantMaxBackedgeTakenCount count here and\n  // use that information to infer NUW and NSW flags. However, computing a\n  // BE count requires calling getAddRecExpr, so we may not yet have a\n  // meaningful BE count at this point (and if we don't, we'd be stuck\n  // with a SCEVCouldNotCompute as the cached BE count).\n\n  Flags = StrengthenNoWrapFlags(this, scAddRecExpr, Operands, Flags);\n\n  // Canonicalize nested AddRecs in by nesting them in order of loop depth.\n  if (const SCEVAddRecExpr *NestedAR = dyn_cast<SCEVAddRecExpr>(Operands[0])) {\n    const Loop *NestedLoop = NestedAR->getLoop();\n    if (L->contains(NestedLoop)\n            ? (L->getLoopDepth() < NestedLoop->getLoopDepth())\n            : (!NestedLoop->contains(L) &&\n               DT.dominates(L->getHeader(), NestedLoop->getHeader()))) {\n      SmallVector<const SCEV *, 4> NestedOperands(NestedAR->operands());\n      Operands[0] = NestedAR->getStart();\n      // AddRecs require their operands be loop-invariant with respect to their\n      // loops. Don't perform this transformation if it would break this\n      // requirement.\n      bool AllInvariant = all_of(\n          Operands, [&](const SCEV *Op) { return isLoopInvariant(Op, L); });\n\n      if (AllInvariant) {\n        // Create a recurrence for the outer loop with the same step size.\n        //\n        // The outer recurrence keeps its NW flag but only keeps NUW/NSW if the\n        // inner recurrence has the same property.\n        SCEV::NoWrapFlags OuterFlags =\n          maskFlags(Flags, SCEV::FlagNW | NestedAR->getNoWrapFlags());\n\n        NestedOperands[0] = getAddRecExpr(Operands, L, OuterFlags);\n        AllInvariant = all_of(NestedOperands, [&](const SCEV *Op) {\n          return isLoopInvariant(Op, NestedLoop);\n        });\n\n        if (AllInvariant) {\n          // Ok, both add recurrences are valid after the transformation.\n          //\n          // The inner recurrence keeps its NW flag but only keeps NUW/NSW if\n          // the outer recurrence has the same property.\n          SCEV::NoWrapFlags InnerFlags =\n            maskFlags(NestedAR->getNoWrapFlags(), SCEV::FlagNW | Flags);\n          return getAddRecExpr(NestedOperands, NestedLoop, InnerFlags);\n        }\n      }\n      // Reset Operands to its original state.\n      Operands[0] = NestedAR;\n    }\n  }\n\n  // Okay, it looks like we really DO need an addrec expr.  Check to see if we\n  // already have one, otherwise create a new one.\n  return getOrCreateAddRecExpr(Operands, L, Flags);\n}\n\nconst SCEV *\nScalarEvolution::getGEPExpr(GEPOperator *GEP,\n                            const SmallVectorImpl<const SCEV *> &IndexExprs) {\n  const SCEV *BaseExpr = getSCEV(GEP->getPointerOperand());\n  // getSCEV(Base)->getType() has the same address space as Base->getType()\n  // because SCEV::getType() preserves the address space.\n  Type *IntIdxTy = getEffectiveSCEVType(BaseExpr->getType());\n  // FIXME(PR23527): Don't blindly transfer the inbounds flag from the GEP\n  // instruction to its SCEV, because the Instruction may be guarded by control\n  // flow and the no-overflow bits may not be valid for the expression in any\n  // context. This can be fixed similarly to how these flags are handled for\n  // adds.\n  SCEV::NoWrapFlags OffsetWrap =\n      GEP->isInBounds() ? SCEV::FlagNSW : SCEV::FlagAnyWrap;\n\n  Type *CurTy = GEP->getType();\n  bool FirstIter = true;\n  SmallVector<const SCEV *, 4> Offsets;\n  for (const SCEV *IndexExpr : IndexExprs) {\n    // Compute the (potentially symbolic) offset in bytes for this index.\n    if (StructType *STy = dyn_cast<StructType>(CurTy)) {\n      // For a struct, add the member offset.\n      ConstantInt *Index = cast<SCEVConstant>(IndexExpr)->getValue();\n      unsigned FieldNo = Index->getZExtValue();\n      const SCEV *FieldOffset = getOffsetOfExpr(IntIdxTy, STy, FieldNo);\n      Offsets.push_back(FieldOffset);\n\n      // Update CurTy to the type of the field at Index.\n      CurTy = STy->getTypeAtIndex(Index);\n    } else {\n      // Update CurTy to its element type.\n      if (FirstIter) {\n        assert(isa<PointerType>(CurTy) &&\n               \"The first index of a GEP indexes a pointer\");\n        CurTy = GEP->getSourceElementType();\n        FirstIter = false;\n      } else {\n        CurTy = GetElementPtrInst::getTypeAtIndex(CurTy, (uint64_t)0);\n      }\n      // For an array, add the element offset, explicitly scaled.\n      const SCEV *ElementSize = getSizeOfExpr(IntIdxTy, CurTy);\n      // Getelementptr indices are signed.\n      IndexExpr = getTruncateOrSignExtend(IndexExpr, IntIdxTy);\n\n      // Multiply the index by the element size to compute the element offset.\n      const SCEV *LocalOffset = getMulExpr(IndexExpr, ElementSize, OffsetWrap);\n      Offsets.push_back(LocalOffset);\n    }\n  }\n\n  // Handle degenerate case of GEP without offsets.\n  if (Offsets.empty())\n    return BaseExpr;\n\n  // Add the offsets together, assuming nsw if inbounds.\n  const SCEV *Offset = getAddExpr(Offsets, OffsetWrap);\n  // Add the base address and the offset. We cannot use the nsw flag, as the\n  // base address is unsigned. However, if we know that the offset is\n  // non-negative, we can use nuw.\n  SCEV::NoWrapFlags BaseWrap = GEP->isInBounds() && isKnownNonNegative(Offset)\n                                   ? SCEV::FlagNUW : SCEV::FlagAnyWrap;\n  return getAddExpr(BaseExpr, Offset, BaseWrap);\n}\n\nstd::tuple<SCEV *, FoldingSetNodeID, void *>\nScalarEvolution::findExistingSCEVInCache(SCEVTypes SCEVType,\n                                         ArrayRef<const SCEV *> Ops) {\n  FoldingSetNodeID ID;\n  void *IP = nullptr;\n  ID.AddInteger(SCEVType);\n  for (unsigned i = 0, e = Ops.size(); i != e; ++i)\n    ID.AddPointer(Ops[i]);\n  return std::tuple<SCEV *, FoldingSetNodeID, void *>(\n      UniqueSCEVs.FindNodeOrInsertPos(ID, IP), std::move(ID), IP);\n}\n\nconst SCEV *ScalarEvolution::getAbsExpr(const SCEV *Op, bool IsNSW) {\n  SCEV::NoWrapFlags Flags = IsNSW ? SCEV::FlagNSW : SCEV::FlagAnyWrap;\n  return getSMaxExpr(Op, getNegativeSCEV(Op, Flags));\n}\n\nconst SCEV *ScalarEvolution::getSignumExpr(const SCEV *Op) {\n  Type *Ty = Op->getType();\n  return getSMinExpr(getSMaxExpr(Op, getMinusOne(Ty)), getOne(Ty));\n}\n\nconst SCEV *ScalarEvolution::getMinMaxExpr(SCEVTypes Kind,\n                                           SmallVectorImpl<const SCEV *> &Ops) {\n  assert(!Ops.empty() && \"Cannot get empty (u|s)(min|max)!\");\n  if (Ops.size() == 1) return Ops[0];\n#ifndef NDEBUG\n  Type *ETy = getEffectiveSCEVType(Ops[0]->getType());\n  for (unsigned i = 1, e = Ops.size(); i != e; ++i)\n    assert(getEffectiveSCEVType(Ops[i]->getType()) == ETy &&\n           \"Operand types don't match!\");\n#endif\n\n  bool IsSigned = Kind == scSMaxExpr || Kind == scSMinExpr;\n  bool IsMax = Kind == scSMaxExpr || Kind == scUMaxExpr;\n\n  // Sort by complexity, this groups all similar expression types together.\n  GroupByComplexity(Ops, &LI, DT);\n\n  // Check if we have created the same expression before.\n  if (const SCEV *S = std::get<0>(findExistingSCEVInCache(Kind, Ops))) {\n    return S;\n  }\n\n  // If there are any constants, fold them together.\n  unsigned Idx = 0;\n  if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(Ops[0])) {\n    ++Idx;\n    assert(Idx < Ops.size());\n    auto FoldOp = [&](const APInt &LHS, const APInt &RHS) {\n      if (Kind == scSMaxExpr)\n        return APIntOps::smax(LHS, RHS);\n      else if (Kind == scSMinExpr)\n        return APIntOps::smin(LHS, RHS);\n      else if (Kind == scUMaxExpr)\n        return APIntOps::umax(LHS, RHS);\n      else if (Kind == scUMinExpr)\n        return APIntOps::umin(LHS, RHS);\n      llvm_unreachable(\"Unknown SCEV min/max opcode\");\n    };\n\n    while (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(Ops[Idx])) {\n      // We found two constants, fold them together!\n      ConstantInt *Fold = ConstantInt::get(\n          getContext(), FoldOp(LHSC->getAPInt(), RHSC->getAPInt()));\n      Ops[0] = getConstant(Fold);\n      Ops.erase(Ops.begin()+1);  // Erase the folded element\n      if (Ops.size() == 1) return Ops[0];\n      LHSC = cast<SCEVConstant>(Ops[0]);\n    }\n\n    bool IsMinV = LHSC->getValue()->isMinValue(IsSigned);\n    bool IsMaxV = LHSC->getValue()->isMaxValue(IsSigned);\n\n    if (IsMax ? IsMinV : IsMaxV) {\n      // If we are left with a constant minimum(/maximum)-int, strip it off.\n      Ops.erase(Ops.begin());\n      --Idx;\n    } else if (IsMax ? IsMaxV : IsMinV) {\n      // If we have a max(/min) with a constant maximum(/minimum)-int,\n      // it will always be the extremum.\n      return LHSC;\n    }\n\n    if (Ops.size() == 1) return Ops[0];\n  }\n\n  // Find the first operation of the same kind\n  while (Idx < Ops.size() && Ops[Idx]->getSCEVType() < Kind)\n    ++Idx;\n\n  // Check to see if one of the operands is of the same kind. If so, expand its\n  // operands onto our operand list, and recurse to simplify.\n  if (Idx < Ops.size()) {\n    bool DeletedAny = false;\n    while (Ops[Idx]->getSCEVType() == Kind) {\n      const SCEVMinMaxExpr *SMME = cast<SCEVMinMaxExpr>(Ops[Idx]);\n      Ops.erase(Ops.begin()+Idx);\n      Ops.append(SMME->op_begin(), SMME->op_end());\n      DeletedAny = true;\n    }\n\n    if (DeletedAny)\n      return getMinMaxExpr(Kind, Ops);\n  }\n\n  // Okay, check to see if the same value occurs in the operand list twice.  If\n  // so, delete one.  Since we sorted the list, these values are required to\n  // be adjacent.\n  llvm::CmpInst::Predicate GEPred =\n      IsSigned ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE;\n  llvm::CmpInst::Predicate LEPred =\n      IsSigned ? ICmpInst::ICMP_SLE : ICmpInst::ICMP_ULE;\n  llvm::CmpInst::Predicate FirstPred = IsMax ? GEPred : LEPred;\n  llvm::CmpInst::Predicate SecondPred = IsMax ? LEPred : GEPred;\n  for (unsigned i = 0, e = Ops.size() - 1; i != e; ++i) {\n    if (Ops[i] == Ops[i + 1] ||\n        isKnownViaNonRecursiveReasoning(FirstPred, Ops[i], Ops[i + 1])) {\n      //  X op Y op Y  -->  X op Y\n      //  X op Y       -->  X, if we know X, Y are ordered appropriately\n      Ops.erase(Ops.begin() + i + 1, Ops.begin() + i + 2);\n      --i;\n      --e;\n    } else if (isKnownViaNonRecursiveReasoning(SecondPred, Ops[i],\n                                               Ops[i + 1])) {\n      //  X op Y       -->  Y, if we know X, Y are ordered appropriately\n      Ops.erase(Ops.begin() + i, Ops.begin() + i + 1);\n      --i;\n      --e;\n    }\n  }\n\n  if (Ops.size() == 1) return Ops[0];\n\n  assert(!Ops.empty() && \"Reduced smax down to nothing!\");\n\n  // Okay, it looks like we really DO need an expr.  Check to see if we\n  // already have one, otherwise create a new one.\n  const SCEV *ExistingSCEV;\n  FoldingSetNodeID ID;\n  void *IP;\n  std::tie(ExistingSCEV, ID, IP) = findExistingSCEVInCache(Kind, Ops);\n  if (ExistingSCEV)\n    return ExistingSCEV;\n  const SCEV **O = SCEVAllocator.Allocate<const SCEV *>(Ops.size());\n  std::uninitialized_copy(Ops.begin(), Ops.end(), O);\n  SCEV *S = new (SCEVAllocator)\n      SCEVMinMaxExpr(ID.Intern(SCEVAllocator), Kind, O, Ops.size());\n\n  UniqueSCEVs.InsertNode(S, IP);\n  addToLoopUseLists(S);\n  return S;\n}\n\nconst SCEV *ScalarEvolution::getSMaxExpr(const SCEV *LHS, const SCEV *RHS) {\n  SmallVector<const SCEV *, 2> Ops = {LHS, RHS};\n  return getSMaxExpr(Ops);\n}\n\nconst SCEV *ScalarEvolution::getSMaxExpr(SmallVectorImpl<const SCEV *> &Ops) {\n  return getMinMaxExpr(scSMaxExpr, Ops);\n}\n\nconst SCEV *ScalarEvolution::getUMaxExpr(const SCEV *LHS, const SCEV *RHS) {\n  SmallVector<const SCEV *, 2> Ops = {LHS, RHS};\n  return getUMaxExpr(Ops);\n}\n\nconst SCEV *ScalarEvolution::getUMaxExpr(SmallVectorImpl<const SCEV *> &Ops) {\n  return getMinMaxExpr(scUMaxExpr, Ops);\n}\n\nconst SCEV *ScalarEvolution::getSMinExpr(const SCEV *LHS,\n                                         const SCEV *RHS) {\n  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };\n  return getSMinExpr(Ops);\n}\n\nconst SCEV *ScalarEvolution::getSMinExpr(SmallVectorImpl<const SCEV *> &Ops) {\n  return getMinMaxExpr(scSMinExpr, Ops);\n}\n\nconst SCEV *ScalarEvolution::getUMinExpr(const SCEV *LHS,\n                                         const SCEV *RHS) {\n  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };\n  return getUMinExpr(Ops);\n}\n\nconst SCEV *ScalarEvolution::getUMinExpr(SmallVectorImpl<const SCEV *> &Ops) {\n  return getMinMaxExpr(scUMinExpr, Ops);\n}\n\nconst SCEV *\nScalarEvolution::getSizeOfScalableVectorExpr(Type *IntTy,\n                                             ScalableVectorType *ScalableTy) {\n  Constant *NullPtr = Constant::getNullValue(ScalableTy->getPointerTo());\n  Constant *One = ConstantInt::get(IntTy, 1);\n  Constant *GEP = ConstantExpr::getGetElementPtr(ScalableTy, NullPtr, One);\n  // Note that the expression we created is the final expression, we don't\n  // want to simplify it any further Also, if we call a normal getSCEV(),\n  // we'll end up in an endless recursion. So just create an SCEVUnknown.\n  return getUnknown(ConstantExpr::getPtrToInt(GEP, IntTy));\n}\n\nconst SCEV *ScalarEvolution::getSizeOfExpr(Type *IntTy, Type *AllocTy) {\n  if (auto *ScalableAllocTy = dyn_cast<ScalableVectorType>(AllocTy))\n    return getSizeOfScalableVectorExpr(IntTy, ScalableAllocTy);\n  // We can bypass creating a target-independent constant expression and then\n  // folding it back into a ConstantInt. This is just a compile-time\n  // optimization.\n  return getConstant(IntTy, getDataLayout().getTypeAllocSize(AllocTy));\n}\n\nconst SCEV *ScalarEvolution::getStoreSizeOfExpr(Type *IntTy, Type *StoreTy) {\n  if (auto *ScalableStoreTy = dyn_cast<ScalableVectorType>(StoreTy))\n    return getSizeOfScalableVectorExpr(IntTy, ScalableStoreTy);\n  // We can bypass creating a target-independent constant expression and then\n  // folding it back into a ConstantInt. This is just a compile-time\n  // optimization.\n  return getConstant(IntTy, getDataLayout().getTypeStoreSize(StoreTy));\n}\n\nconst SCEV *ScalarEvolution::getOffsetOfExpr(Type *IntTy,\n                                             StructType *STy,\n                                             unsigned FieldNo) {\n  // We can bypass creating a target-independent constant expression and then\n  // folding it back into a ConstantInt. This is just a compile-time\n  // optimization.\n  return getConstant(\n      IntTy, getDataLayout().getStructLayout(STy)->getElementOffset(FieldNo));\n}\n\nconst SCEV *ScalarEvolution::getUnknown(Value *V) {\n  // Don't attempt to do anything other than create a SCEVUnknown object\n  // here.  createSCEV only calls getUnknown after checking for all other\n  // interesting possibilities, and any other code that calls getUnknown\n  // is doing so in order to hide a value from SCEV canonicalization.\n\n  FoldingSetNodeID ID;\n  ID.AddInteger(scUnknown);\n  ID.AddPointer(V);\n  void *IP = nullptr;\n  if (SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) {\n    assert(cast<SCEVUnknown>(S)->getValue() == V &&\n           \"Stale SCEVUnknown in uniquing map!\");\n    return S;\n  }\n  SCEV *S = new (SCEVAllocator) SCEVUnknown(ID.Intern(SCEVAllocator), V, this,\n                                            FirstUnknown);\n  FirstUnknown = cast<SCEVUnknown>(S);\n  UniqueSCEVs.InsertNode(S, IP);\n  return S;\n}\n\n//===----------------------------------------------------------------------===//\n//            Basic SCEV Analysis and PHI Idiom Recognition Code\n//\n\n/// Test if values of the given type are analyzable within the SCEV\n/// framework. This primarily includes integer types, and it can optionally\n/// include pointer types if the ScalarEvolution class has access to\n/// target-specific information.\nbool ScalarEvolution::isSCEVable(Type *Ty) const {\n  // Integers and pointers are always SCEVable.\n  return Ty->isIntOrPtrTy();\n}\n\n/// Return the size in bits of the specified type, for which isSCEVable must\n/// return true.\nuint64_t ScalarEvolution::getTypeSizeInBits(Type *Ty) const {\n  assert(isSCEVable(Ty) && \"Type is not SCEVable!\");\n  if (Ty->isPointerTy())\n    return getDataLayout().getIndexTypeSizeInBits(Ty);\n  return getDataLayout().getTypeSizeInBits(Ty);\n}\n\n/// Return a type with the same bitwidth as the given type and which represents\n/// how SCEV will treat the given type, for which isSCEVable must return\n/// true. For pointer types, this is the pointer index sized integer type.\nType *ScalarEvolution::getEffectiveSCEVType(Type *Ty) const {\n  assert(isSCEVable(Ty) && \"Type is not SCEVable!\");\n\n  if (Ty->isIntegerTy())\n    return Ty;\n\n  // The only other support type is pointer.\n  assert(Ty->isPointerTy() && \"Unexpected non-pointer non-integer type!\");\n  return getDataLayout().getIndexType(Ty);\n}\n\nType *ScalarEvolution::getWiderType(Type *T1, Type *T2) const {\n  return  getTypeSizeInBits(T1) >= getTypeSizeInBits(T2) ? T1 : T2;\n}\n\nconst SCEV *ScalarEvolution::getCouldNotCompute() {\n  return CouldNotCompute.get();\n}\n\nbool ScalarEvolution::checkValidity(const SCEV *S) const {\n  bool ContainsNulls = SCEVExprContains(S, [](const SCEV *S) {\n    auto *SU = dyn_cast<SCEVUnknown>(S);\n    return SU && SU->getValue() == nullptr;\n  });\n\n  return !ContainsNulls;\n}\n\nbool ScalarEvolution::containsAddRecurrence(const SCEV *S) {\n  HasRecMapType::iterator I = HasRecMap.find(S);\n  if (I != HasRecMap.end())\n    return I->second;\n\n  bool FoundAddRec =\n      SCEVExprContains(S, [](const SCEV *S) { return isa<SCEVAddRecExpr>(S); });\n  HasRecMap.insert({S, FoundAddRec});\n  return FoundAddRec;\n}\n\n/// Try to split a SCEVAddExpr into a pair of {SCEV, ConstantInt}.\n/// If \\p S is a SCEVAddExpr and is composed of a sub SCEV S' and an\n/// offset I, then return {S', I}, else return {\\p S, nullptr}.\nstatic std::pair<const SCEV *, ConstantInt *> splitAddExpr(const SCEV *S) {\n  const auto *Add = dyn_cast<SCEVAddExpr>(S);\n  if (!Add)\n    return {S, nullptr};\n\n  if (Add->getNumOperands() != 2)\n    return {S, nullptr};\n\n  auto *ConstOp = dyn_cast<SCEVConstant>(Add->getOperand(0));\n  if (!ConstOp)\n    return {S, nullptr};\n\n  return {Add->getOperand(1), ConstOp->getValue()};\n}\n\n/// Return the ValueOffsetPair set for \\p S. \\p S can be represented\n/// by the value and offset from any ValueOffsetPair in the set.\nSetVector<ScalarEvolution::ValueOffsetPair> *\nScalarEvolution::getSCEVValues(const SCEV *S) {\n  ExprValueMapType::iterator SI = ExprValueMap.find_as(S);\n  if (SI == ExprValueMap.end())\n    return nullptr;\n#ifndef NDEBUG\n  if (VerifySCEVMap) {\n    // Check there is no dangling Value in the set returned.\n    for (const auto &VE : SI->second)\n      assert(ValueExprMap.count(VE.first));\n  }\n#endif\n  return &SI->second;\n}\n\n/// Erase Value from ValueExprMap and ExprValueMap. ValueExprMap.erase(V)\n/// cannot be used separately. eraseValueFromMap should be used to remove\n/// V from ValueExprMap and ExprValueMap at the same time.\nvoid ScalarEvolution::eraseValueFromMap(Value *V) {\n  ValueExprMapType::iterator I = ValueExprMap.find_as(V);\n  if (I != ValueExprMap.end()) {\n    const SCEV *S = I->second;\n    // Remove {V, 0} from the set of ExprValueMap[S]\n    if (SetVector<ValueOffsetPair> *SV = getSCEVValues(S))\n      SV->remove({V, nullptr});\n\n    // Remove {V, Offset} from the set of ExprValueMap[Stripped]\n    const SCEV *Stripped;\n    ConstantInt *Offset;\n    std::tie(Stripped, Offset) = splitAddExpr(S);\n    if (Offset != nullptr) {\n      if (SetVector<ValueOffsetPair> *SV = getSCEVValues(Stripped))\n        SV->remove({V, Offset});\n    }\n    ValueExprMap.erase(V);\n  }\n}\n\n/// Check whether value has nuw/nsw/exact set but SCEV does not.\n/// TODO: In reality it is better to check the poison recursively\n/// but this is better than nothing.\nstatic bool SCEVLostPoisonFlags(const SCEV *S, const Value *V) {\n  if (auto *I = dyn_cast<Instruction>(V)) {\n    if (isa<OverflowingBinaryOperator>(I)) {\n      if (auto *NS = dyn_cast<SCEVNAryExpr>(S)) {\n        if (I->hasNoSignedWrap() && !NS->hasNoSignedWrap())\n          return true;\n        if (I->hasNoUnsignedWrap() && !NS->hasNoUnsignedWrap())\n          return true;\n      }\n    } else if (isa<PossiblyExactOperator>(I) && I->isExact())\n      return true;\n  }\n  return false;\n}\n\n/// Return an existing SCEV if it exists, otherwise analyze the expression and\n/// create a new one.\nconst SCEV *ScalarEvolution::getSCEV(Value *V) {\n  assert(isSCEVable(V->getType()) && \"Value is not SCEVable!\");\n\n  const SCEV *S = getExistingSCEV(V);\n  if (S == nullptr) {\n    S = createSCEV(V);\n    // During PHI resolution, it is possible to create two SCEVs for the same\n    // V, so it is needed to double check whether V->S is inserted into\n    // ValueExprMap before insert S->{V, 0} into ExprValueMap.\n    std::pair<ValueExprMapType::iterator, bool> Pair =\n        ValueExprMap.insert({SCEVCallbackVH(V, this), S});\n    if (Pair.second && !SCEVLostPoisonFlags(S, V)) {\n      ExprValueMap[S].insert({V, nullptr});\n\n      // If S == Stripped + Offset, add Stripped -> {V, Offset} into\n      // ExprValueMap.\n      const SCEV *Stripped = S;\n      ConstantInt *Offset = nullptr;\n      std::tie(Stripped, Offset) = splitAddExpr(S);\n      // If stripped is SCEVUnknown, don't bother to save\n      // Stripped -> {V, offset}. It doesn't simplify and sometimes even\n      // increase the complexity of the expansion code.\n      // If V is GetElementPtrInst, don't save Stripped -> {V, offset}\n      // because it may generate add/sub instead of GEP in SCEV expansion.\n      if (Offset != nullptr && !isa<SCEVUnknown>(Stripped) &&\n          !isa<GetElementPtrInst>(V))\n        ExprValueMap[Stripped].insert({V, Offset});\n    }\n  }\n  return S;\n}\n\nconst SCEV *ScalarEvolution::getExistingSCEV(Value *V) {\n  assert(isSCEVable(V->getType()) && \"Value is not SCEVable!\");\n\n  ValueExprMapType::iterator I = ValueExprMap.find_as(V);\n  if (I != ValueExprMap.end()) {\n    const SCEV *S = I->second;\n    if (checkValidity(S))\n      return S;\n    eraseValueFromMap(V);\n    forgetMemoizedResults(S);\n  }\n  return nullptr;\n}\n\n/// Return a SCEV corresponding to -V = -1*V\nconst SCEV *ScalarEvolution::getNegativeSCEV(const SCEV *V,\n                                             SCEV::NoWrapFlags Flags) {\n  if (const SCEVConstant *VC = dyn_cast<SCEVConstant>(V))\n    return getConstant(\n               cast<ConstantInt>(ConstantExpr::getNeg(VC->getValue())));\n\n  Type *Ty = V->getType();\n  Ty = getEffectiveSCEVType(Ty);\n  return getMulExpr(V, getMinusOne(Ty), Flags);\n}\n\n/// If Expr computes ~A, return A else return nullptr\nstatic const SCEV *MatchNotExpr(const SCEV *Expr) {\n  const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(Expr);\n  if (!Add || Add->getNumOperands() != 2 ||\n      !Add->getOperand(0)->isAllOnesValue())\n    return nullptr;\n\n  const SCEVMulExpr *AddRHS = dyn_cast<SCEVMulExpr>(Add->getOperand(1));\n  if (!AddRHS || AddRHS->getNumOperands() != 2 ||\n      !AddRHS->getOperand(0)->isAllOnesValue())\n    return nullptr;\n\n  return AddRHS->getOperand(1);\n}\n\n/// Return a SCEV corresponding to ~V = -1-V\nconst SCEV *ScalarEvolution::getNotSCEV(const SCEV *V) {\n  if (const SCEVConstant *VC = dyn_cast<SCEVConstant>(V))\n    return getConstant(\n                cast<ConstantInt>(ConstantExpr::getNot(VC->getValue())));\n\n  // Fold ~(u|s)(min|max)(~x, ~y) to (u|s)(max|min)(x, y)\n  if (const SCEVMinMaxExpr *MME = dyn_cast<SCEVMinMaxExpr>(V)) {\n    auto MatchMinMaxNegation = [&](const SCEVMinMaxExpr *MME) {\n      SmallVector<const SCEV *, 2> MatchedOperands;\n      for (const SCEV *Operand : MME->operands()) {\n        const SCEV *Matched = MatchNotExpr(Operand);\n        if (!Matched)\n          return (const SCEV *)nullptr;\n        MatchedOperands.push_back(Matched);\n      }\n      return getMinMaxExpr(SCEVMinMaxExpr::negate(MME->getSCEVType()),\n                           MatchedOperands);\n    };\n    if (const SCEV *Replaced = MatchMinMaxNegation(MME))\n      return Replaced;\n  }\n\n  Type *Ty = V->getType();\n  Ty = getEffectiveSCEVType(Ty);\n  return getMinusSCEV(getMinusOne(Ty), V);\n}\n\nconst SCEV *ScalarEvolution::getMinusSCEV(const SCEV *LHS, const SCEV *RHS,\n                                          SCEV::NoWrapFlags Flags,\n                                          unsigned Depth) {\n  // Fast path: X - X --> 0.\n  if (LHS == RHS)\n    return getZero(LHS->getType());\n\n  // We represent LHS - RHS as LHS + (-1)*RHS. This transformation\n  // makes it so that we cannot make much use of NUW.\n  auto AddFlags = SCEV::FlagAnyWrap;\n  const bool RHSIsNotMinSigned =\n      !getSignedRangeMin(RHS).isMinSignedValue();\n  if (maskFlags(Flags, SCEV::FlagNSW) == SCEV::FlagNSW) {\n    // Let M be the minimum representable signed value. Then (-1)*RHS\n    // signed-wraps if and only if RHS is M. That can happen even for\n    // a NSW subtraction because e.g. (-1)*M signed-wraps even though\n    // -1 - M does not. So to transfer NSW from LHS - RHS to LHS +\n    // (-1)*RHS, we need to prove that RHS != M.\n    //\n    // If LHS is non-negative and we know that LHS - RHS does not\n    // signed-wrap, then RHS cannot be M. So we can rule out signed-wrap\n    // either by proving that RHS > M or that LHS >= 0.\n    if (RHSIsNotMinSigned || isKnownNonNegative(LHS)) {\n      AddFlags = SCEV::FlagNSW;\n    }\n  }\n\n  // FIXME: Find a correct way to transfer NSW to (-1)*M when LHS -\n  // RHS is NSW and LHS >= 0.\n  //\n  // The difficulty here is that the NSW flag may have been proven\n  // relative to a loop that is to be found in a recurrence in LHS and\n  // not in RHS. Applying NSW to (-1)*M may then let the NSW have a\n  // larger scope than intended.\n  auto NegFlags = RHSIsNotMinSigned ? SCEV::FlagNSW : SCEV::FlagAnyWrap;\n\n  return getAddExpr(LHS, getNegativeSCEV(RHS, NegFlags), AddFlags, Depth);\n}\n\nconst SCEV *ScalarEvolution::getTruncateOrZeroExtend(const SCEV *V, Type *Ty,\n                                                     unsigned Depth) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot truncate or zero extend with non-integer arguments!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  if (getTypeSizeInBits(SrcTy) > getTypeSizeInBits(Ty))\n    return getTruncateExpr(V, Ty, Depth);\n  return getZeroExtendExpr(V, Ty, Depth);\n}\n\nconst SCEV *ScalarEvolution::getTruncateOrSignExtend(const SCEV *V, Type *Ty,\n                                                     unsigned Depth) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot truncate or zero extend with non-integer arguments!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  if (getTypeSizeInBits(SrcTy) > getTypeSizeInBits(Ty))\n    return getTruncateExpr(V, Ty, Depth);\n  return getSignExtendExpr(V, Ty, Depth);\n}\n\nconst SCEV *\nScalarEvolution::getNoopOrZeroExtend(const SCEV *V, Type *Ty) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot noop or zero extend with non-integer arguments!\");\n  assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&\n         \"getNoopOrZeroExtend cannot truncate!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  return getZeroExtendExpr(V, Ty);\n}\n\nconst SCEV *\nScalarEvolution::getNoopOrSignExtend(const SCEV *V, Type *Ty) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot noop or sign extend with non-integer arguments!\");\n  assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&\n         \"getNoopOrSignExtend cannot truncate!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  return getSignExtendExpr(V, Ty);\n}\n\nconst SCEV *\nScalarEvolution::getNoopOrAnyExtend(const SCEV *V, Type *Ty) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot noop or any extend with non-integer arguments!\");\n  assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&\n         \"getNoopOrAnyExtend cannot truncate!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  return getAnyExtendExpr(V, Ty);\n}\n\nconst SCEV *\nScalarEvolution::getTruncateOrNoop(const SCEV *V, Type *Ty) {\n  Type *SrcTy = V->getType();\n  assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&\n         \"Cannot truncate or noop with non-integer arguments!\");\n  assert(getTypeSizeInBits(SrcTy) >= getTypeSizeInBits(Ty) &&\n         \"getTruncateOrNoop cannot extend!\");\n  if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))\n    return V;  // No conversion\n  return getTruncateExpr(V, Ty);\n}\n\nconst SCEV *ScalarEvolution::getUMaxFromMismatchedTypes(const SCEV *LHS,\n                                                        const SCEV *RHS) {\n  const SCEV *PromotedLHS = LHS;\n  const SCEV *PromotedRHS = RHS;\n\n  if (getTypeSizeInBits(LHS->getType()) > getTypeSizeInBits(RHS->getType()))\n    PromotedRHS = getZeroExtendExpr(RHS, LHS->getType());\n  else\n    PromotedLHS = getNoopOrZeroExtend(LHS, RHS->getType());\n\n  return getUMaxExpr(PromotedLHS, PromotedRHS);\n}\n\nconst SCEV *ScalarEvolution::getUMinFromMismatchedTypes(const SCEV *LHS,\n                                                        const SCEV *RHS) {\n  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };\n  return getUMinFromMismatchedTypes(Ops);\n}\n\nconst SCEV *ScalarEvolution::getUMinFromMismatchedTypes(\n    SmallVectorImpl<const SCEV *> &Ops) {\n  assert(!Ops.empty() && \"At least one operand must be!\");\n  // Trivial case.\n  if (Ops.size() == 1)\n    return Ops[0];\n\n  // Find the max type first.\n  Type *MaxType = nullptr;\n  for (auto *S : Ops)\n    if (MaxType)\n      MaxType = getWiderType(MaxType, S->getType());\n    else\n      MaxType = S->getType();\n  assert(MaxType && \"Failed to find maximum type!\");\n\n  // Extend all ops to max type.\n  SmallVector<const SCEV *, 2> PromotedOps;\n  for (auto *S : Ops)\n    PromotedOps.push_back(getNoopOrZeroExtend(S, MaxType));\n\n  // Generate umin.\n  return getUMinExpr(PromotedOps);\n}\n\nconst SCEV *ScalarEvolution::getPointerBase(const SCEV *V) {\n  // A pointer operand may evaluate to a nonpointer expression, such as null.\n  if (!V->getType()->isPointerTy())\n    return V;\n\n  while (true) {\n    if (const SCEVIntegralCastExpr *Cast = dyn_cast<SCEVIntegralCastExpr>(V)) {\n      V = Cast->getOperand();\n    } else if (const SCEVNAryExpr *NAry = dyn_cast<SCEVNAryExpr>(V)) {\n      const SCEV *PtrOp = nullptr;\n      for (const SCEV *NAryOp : NAry->operands()) {\n        if (NAryOp->getType()->isPointerTy()) {\n          // Cannot find the base of an expression with multiple pointer ops.\n          if (PtrOp)\n            return V;\n          PtrOp = NAryOp;\n        }\n      }\n      if (!PtrOp) // All operands were non-pointer.\n        return V;\n      V = PtrOp;\n    } else // Not something we can look further into.\n      return V;\n  }\n}\n\n/// Push users of the given Instruction onto the given Worklist.\nstatic void\nPushDefUseChildren(Instruction *I,\n                   SmallVectorImpl<Instruction *> &Worklist) {\n  // Push the def-use children onto the Worklist stack.\n  for (User *U : I->users())\n    Worklist.push_back(cast<Instruction>(U));\n}\n\nvoid ScalarEvolution::forgetSymbolicName(Instruction *PN, const SCEV *SymName) {\n  SmallVector<Instruction *, 16> Worklist;\n  PushDefUseChildren(PN, Worklist);\n\n  SmallPtrSet<Instruction *, 8> Visited;\n  Visited.insert(PN);\n  while (!Worklist.empty()) {\n    Instruction *I = Worklist.pop_back_val();\n    if (!Visited.insert(I).second)\n      continue;\n\n    auto It = ValueExprMap.find_as(static_cast<Value *>(I));\n    if (It != ValueExprMap.end()) {\n      const SCEV *Old = It->second;\n\n      // Short-circuit the def-use traversal if the symbolic name\n      // ceases to appear in expressions.\n      if (Old != SymName && !hasOperand(Old, SymName))\n        continue;\n\n      // SCEVUnknown for a PHI either means that it has an unrecognized\n      // structure, it's a PHI that's in the progress of being computed\n      // by createNodeForPHI, or it's a single-value PHI. In the first case,\n      // additional loop trip count information isn't going to change anything.\n      // In the second case, createNodeForPHI will perform the necessary\n      // updates on its own when it gets to that point. In the third, we do\n      // want to forget the SCEVUnknown.\n      if (!isa<PHINode>(I) ||\n          !isa<SCEVUnknown>(Old) ||\n          (I != PN && Old == SymName)) {\n        eraseValueFromMap(It->first);\n        forgetMemoizedResults(Old);\n      }\n    }\n\n    PushDefUseChildren(I, Worklist);\n  }\n}\n\nnamespace {\n\n/// Takes SCEV S and Loop L. For each AddRec sub-expression, use its start\n/// expression in case its Loop is L. If it is not L then\n/// if IgnoreOtherLoops is true then use AddRec itself\n/// otherwise rewrite cannot be done.\n/// If SCEV contains non-invariant unknown SCEV rewrite cannot be done.\nclass SCEVInitRewriter : public SCEVRewriteVisitor<SCEVInitRewriter> {\npublic:\n  static const SCEV *rewrite(const SCEV *S, const Loop *L, ScalarEvolution &SE,\n                             bool IgnoreOtherLoops = true) {\n    SCEVInitRewriter Rewriter(L, SE);\n    const SCEV *Result = Rewriter.visit(S);\n    if (Rewriter.hasSeenLoopVariantSCEVUnknown())\n      return SE.getCouldNotCompute();\n    return Rewriter.hasSeenOtherLoops() && !IgnoreOtherLoops\n               ? SE.getCouldNotCompute()\n               : Result;\n  }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    if (!SE.isLoopInvariant(Expr, L))\n      SeenLoopVariantSCEVUnknown = true;\n    return Expr;\n  }\n\n  const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) {\n    // Only re-write AddRecExprs for this loop.\n    if (Expr->getLoop() == L)\n      return Expr->getStart();\n    SeenOtherLoops = true;\n    return Expr;\n  }\n\n  bool hasSeenLoopVariantSCEVUnknown() { return SeenLoopVariantSCEVUnknown; }\n\n  bool hasSeenOtherLoops() { return SeenOtherLoops; }\n\nprivate:\n  explicit SCEVInitRewriter(const Loop *L, ScalarEvolution &SE)\n      : SCEVRewriteVisitor(SE), L(L) {}\n\n  const Loop *L;\n  bool SeenLoopVariantSCEVUnknown = false;\n  bool SeenOtherLoops = false;\n};\n\n/// Takes SCEV S and Loop L. For each AddRec sub-expression, use its post\n/// increment expression in case its Loop is L. If it is not L then\n/// use AddRec itself.\n/// If SCEV contains non-invariant unknown SCEV rewrite cannot be done.\nclass SCEVPostIncRewriter : public SCEVRewriteVisitor<SCEVPostIncRewriter> {\npublic:\n  static const SCEV *rewrite(const SCEV *S, const Loop *L, ScalarEvolution &SE) {\n    SCEVPostIncRewriter Rewriter(L, SE);\n    const SCEV *Result = Rewriter.visit(S);\n    return Rewriter.hasSeenLoopVariantSCEVUnknown()\n        ? SE.getCouldNotCompute()\n        : Result;\n  }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    if (!SE.isLoopInvariant(Expr, L))\n      SeenLoopVariantSCEVUnknown = true;\n    return Expr;\n  }\n\n  const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) {\n    // Only re-write AddRecExprs for this loop.\n    if (Expr->getLoop() == L)\n      return Expr->getPostIncExpr(SE);\n    SeenOtherLoops = true;\n    return Expr;\n  }\n\n  bool hasSeenLoopVariantSCEVUnknown() { return SeenLoopVariantSCEVUnknown; }\n\n  bool hasSeenOtherLoops() { return SeenOtherLoops; }\n\nprivate:\n  explicit SCEVPostIncRewriter(const Loop *L, ScalarEvolution &SE)\n      : SCEVRewriteVisitor(SE), L(L) {}\n\n  const Loop *L;\n  bool SeenLoopVariantSCEVUnknown = false;\n  bool SeenOtherLoops = false;\n};\n\n/// This class evaluates the compare condition by matching it against the\n/// condition of loop latch. If there is a match we assume a true value\n/// for the condition while building SCEV nodes.\nclass SCEVBackedgeConditionFolder\n    : public SCEVRewriteVisitor<SCEVBackedgeConditionFolder> {\npublic:\n  static const SCEV *rewrite(const SCEV *S, const Loop *L,\n                             ScalarEvolution &SE) {\n    bool IsPosBECond = false;\n    Value *BECond = nullptr;\n    if (BasicBlock *Latch = L->getLoopLatch()) {\n      BranchInst *BI = dyn_cast<BranchInst>(Latch->getTerminator());\n      if (BI && BI->isConditional()) {\n        assert(BI->getSuccessor(0) != BI->getSuccessor(1) &&\n               \"Both outgoing branches should not target same header!\");\n        BECond = BI->getCondition();\n        IsPosBECond = BI->getSuccessor(0) == L->getHeader();\n      } else {\n        return S;\n      }\n    }\n    SCEVBackedgeConditionFolder Rewriter(L, BECond, IsPosBECond, SE);\n    return Rewriter.visit(S);\n  }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    const SCEV *Result = Expr;\n    bool InvariantF = SE.isLoopInvariant(Expr, L);\n\n    if (!InvariantF) {\n      Instruction *I = cast<Instruction>(Expr->getValue());\n      switch (I->getOpcode()) {\n      case Instruction::Select: {\n        SelectInst *SI = cast<SelectInst>(I);\n        Optional<const SCEV *> Res =\n            compareWithBackedgeCondition(SI->getCondition());\n        if (Res.hasValue()) {\n          bool IsOne = cast<SCEVConstant>(Res.getValue())->getValue()->isOne();\n          Result = SE.getSCEV(IsOne ? SI->getTrueValue() : SI->getFalseValue());\n        }\n        break;\n      }\n      default: {\n        Optional<const SCEV *> Res = compareWithBackedgeCondition(I);\n        if (Res.hasValue())\n          Result = Res.getValue();\n        break;\n      }\n      }\n    }\n    return Result;\n  }\n\nprivate:\n  explicit SCEVBackedgeConditionFolder(const Loop *L, Value *BECond,\n                                       bool IsPosBECond, ScalarEvolution &SE)\n      : SCEVRewriteVisitor(SE), L(L), BackedgeCond(BECond),\n        IsPositiveBECond(IsPosBECond) {}\n\n  Optional<const SCEV *> compareWithBackedgeCondition(Value *IC);\n\n  const Loop *L;\n  /// Loop back condition.\n  Value *BackedgeCond = nullptr;\n  /// Set to true if loop back is on positive branch condition.\n  bool IsPositiveBECond;\n};\n\nOptional<const SCEV *>\nSCEVBackedgeConditionFolder::compareWithBackedgeCondition(Value *IC) {\n\n  // If value matches the backedge condition for loop latch,\n  // then return a constant evolution node based on loopback\n  // branch taken.\n  if (BackedgeCond == IC)\n    return IsPositiveBECond ? SE.getOne(Type::getInt1Ty(SE.getContext()))\n                            : SE.getZero(Type::getInt1Ty(SE.getContext()));\n  return None;\n}\n\nclass SCEVShiftRewriter : public SCEVRewriteVisitor<SCEVShiftRewriter> {\npublic:\n  static const SCEV *rewrite(const SCEV *S, const Loop *L,\n                             ScalarEvolution &SE) {\n    SCEVShiftRewriter Rewriter(L, SE);\n    const SCEV *Result = Rewriter.visit(S);\n    return Rewriter.isValid() ? Result : SE.getCouldNotCompute();\n  }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    // Only allow AddRecExprs for this loop.\n    if (!SE.isLoopInvariant(Expr, L))\n      Valid = false;\n    return Expr;\n  }\n\n  const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) {\n    if (Expr->getLoop() == L && Expr->isAffine())\n      return SE.getMinusSCEV(Expr, Expr->getStepRecurrence(SE));\n    Valid = false;\n    return Expr;\n  }\n\n  bool isValid() { return Valid; }\n\nprivate:\n  explicit SCEVShiftRewriter(const Loop *L, ScalarEvolution &SE)\n      : SCEVRewriteVisitor(SE), L(L) {}\n\n  const Loop *L;\n  bool Valid = true;\n};\n\n} // end anonymous namespace\n\nSCEV::NoWrapFlags\nScalarEvolution::proveNoWrapViaConstantRanges(const SCEVAddRecExpr *AR) {\n  if (!AR->isAffine())\n    return SCEV::FlagAnyWrap;\n\n  using OBO = OverflowingBinaryOperator;\n\n  SCEV::NoWrapFlags Result = SCEV::FlagAnyWrap;\n\n  if (!AR->hasNoSignedWrap()) {\n    ConstantRange AddRecRange = getSignedRange(AR);\n    ConstantRange IncRange = getSignedRange(AR->getStepRecurrence(*this));\n\n    auto NSWRegion = ConstantRange::makeGuaranteedNoWrapRegion(\n        Instruction::Add, IncRange, OBO::NoSignedWrap);\n    if (NSWRegion.contains(AddRecRange))\n      Result = ScalarEvolution::setFlags(Result, SCEV::FlagNSW);\n  }\n\n  if (!AR->hasNoUnsignedWrap()) {\n    ConstantRange AddRecRange = getUnsignedRange(AR);\n    ConstantRange IncRange = getUnsignedRange(AR->getStepRecurrence(*this));\n\n    auto NUWRegion = ConstantRange::makeGuaranteedNoWrapRegion(\n        Instruction::Add, IncRange, OBO::NoUnsignedWrap);\n    if (NUWRegion.contains(AddRecRange))\n      Result = ScalarEvolution::setFlags(Result, SCEV::FlagNUW);\n  }\n\n  return Result;\n}\n\nSCEV::NoWrapFlags\nScalarEvolution::proveNoSignedWrapViaInduction(const SCEVAddRecExpr *AR) {\n  SCEV::NoWrapFlags Result = AR->getNoWrapFlags();\n\n  if (AR->hasNoSignedWrap())\n    return Result;\n\n  if (!AR->isAffine())\n    return Result;\n\n  const SCEV *Step = AR->getStepRecurrence(*this);\n  const Loop *L = AR->getLoop();\n\n  // Check whether the backedge-taken count is SCEVCouldNotCompute.\n  // Note that this serves two purposes: It filters out loops that are\n  // simply not analyzable, and it covers the case where this code is\n  // being called from within backedge-taken count analysis, such that\n  // attempting to ask for the backedge-taken count would likely result\n  // in infinite recursion. In the later case, the analysis code will\n  // cope with a conservative value, and it will take care to purge\n  // that value once it has finished.\n  const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(L);\n\n  // Normally, in the cases we can prove no-overflow via a\n  // backedge guarding condition, we can also compute a backedge\n  // taken count for the loop.  The exceptions are assumptions and\n  // guards present in the loop -- SCEV is not great at exploiting\n  // these to compute max backedge taken counts, but can still use\n  // these to prove lack of overflow.  Use this fact to avoid\n  // doing extra work that may not pay off.\n\n  if (isa<SCEVCouldNotCompute>(MaxBECount) && !HasGuards &&\n      AC.assumptions().empty())\n    return Result;\n\n  // If the backedge is guarded by a comparison with the pre-inc  value the\n  // addrec is safe. Also, if the entry is guarded by a comparison with the\n  // start value and the backedge is guarded by a comparison with the post-inc\n  // value, the addrec is safe.\n  ICmpInst::Predicate Pred;\n  const SCEV *OverflowLimit =\n    getSignedOverflowLimitForStep(Step, &Pred, this);\n  if (OverflowLimit &&\n      (isLoopBackedgeGuardedByCond(L, Pred, AR, OverflowLimit) ||\n       isKnownOnEveryIteration(Pred, AR, OverflowLimit))) {\n    Result = setFlags(Result, SCEV::FlagNSW);\n  }\n  return Result;\n}\nSCEV::NoWrapFlags\nScalarEvolution::proveNoUnsignedWrapViaInduction(const SCEVAddRecExpr *AR) {\n  SCEV::NoWrapFlags Result = AR->getNoWrapFlags();\n\n  if (AR->hasNoUnsignedWrap())\n    return Result;\n\n  if (!AR->isAffine())\n    return Result;\n\n  const SCEV *Step = AR->getStepRecurrence(*this);\n  unsigned BitWidth = getTypeSizeInBits(AR->getType());\n  const Loop *L = AR->getLoop();\n\n  // Check whether the backedge-taken count is SCEVCouldNotCompute.\n  // Note that this serves two purposes: It filters out loops that are\n  // simply not analyzable, and it covers the case where this code is\n  // being called from within backedge-taken count analysis, such that\n  // attempting to ask for the backedge-taken count would likely result\n  // in infinite recursion. In the later case, the analysis code will\n  // cope with a conservative value, and it will take care to purge\n  // that value once it has finished.\n  const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(L);\n\n  // Normally, in the cases we can prove no-overflow via a\n  // backedge guarding condition, we can also compute a backedge\n  // taken count for the loop.  The exceptions are assumptions and\n  // guards present in the loop -- SCEV is not great at exploiting\n  // these to compute max backedge taken counts, but can still use\n  // these to prove lack of overflow.  Use this fact to avoid\n  // doing extra work that may not pay off.\n\n  if (isa<SCEVCouldNotCompute>(MaxBECount) && !HasGuards &&\n      AC.assumptions().empty())\n    return Result;\n\n  // If the backedge is guarded by a comparison with the pre-inc  value the\n  // addrec is safe. Also, if the entry is guarded by a comparison with the\n  // start value and the backedge is guarded by a comparison with the post-inc\n  // value, the addrec is safe.\n  if (isKnownPositive(Step)) {\n    const SCEV *N = getConstant(APInt::getMinValue(BitWidth) -\n                                getUnsignedRangeMax(Step));\n    if (isLoopBackedgeGuardedByCond(L, ICmpInst::ICMP_ULT, AR, N) ||\n        isKnownOnEveryIteration(ICmpInst::ICMP_ULT, AR, N)) {\n      Result = setFlags(Result, SCEV::FlagNUW);\n    }\n  }\n\n  return Result;\n}\n\nnamespace {\n\n/// Represents an abstract binary operation.  This may exist as a\n/// normal instruction or constant expression, or may have been\n/// derived from an expression tree.\nstruct BinaryOp {\n  unsigned Opcode;\n  Value *LHS;\n  Value *RHS;\n  bool IsNSW = false;\n  bool IsNUW = false;\n  bool IsExact = false;\n\n  /// Op is set if this BinaryOp corresponds to a concrete LLVM instruction or\n  /// constant expression.\n  Operator *Op = nullptr;\n\n  explicit BinaryOp(Operator *Op)\n      : Opcode(Op->getOpcode()), LHS(Op->getOperand(0)), RHS(Op->getOperand(1)),\n        Op(Op) {\n    if (auto *OBO = dyn_cast<OverflowingBinaryOperator>(Op)) {\n      IsNSW = OBO->hasNoSignedWrap();\n      IsNUW = OBO->hasNoUnsignedWrap();\n    }\n    if (auto *PEO = dyn_cast<PossiblyExactOperator>(Op))\n      IsExact = PEO->isExact();\n  }\n\n  explicit BinaryOp(unsigned Opcode, Value *LHS, Value *RHS, bool IsNSW = false,\n                    bool IsNUW = false, bool IsExact = false)\n      : Opcode(Opcode), LHS(LHS), RHS(RHS), IsNSW(IsNSW), IsNUW(IsNUW),\n        IsExact(IsExact) {}\n};\n\n} // end anonymous namespace\n\n/// Try to map \\p V into a BinaryOp, and return \\c None on failure.\nstatic Optional<BinaryOp> MatchBinaryOp(Value *V, DominatorTree &DT) {\n  auto *Op = dyn_cast<Operator>(V);\n  if (!Op)\n    return None;\n\n  // Implementation detail: all the cleverness here should happen without\n  // creating new SCEV expressions -- our caller knowns tricks to avoid creating\n  // SCEV expressions when possible, and we should not break that.\n\n  switch (Op->getOpcode()) {\n  case Instruction::Add:\n  case Instruction::Sub:\n  case Instruction::Mul:\n  case Instruction::UDiv:\n  case Instruction::URem:\n  case Instruction::And:\n  case Instruction::Or:\n  case Instruction::AShr:\n  case Instruction::Shl:\n    return BinaryOp(Op);\n\n  case Instruction::Xor:\n    if (auto *RHSC = dyn_cast<ConstantInt>(Op->getOperand(1)))\n      // If the RHS of the xor is a signmask, then this is just an add.\n      // Instcombine turns add of signmask into xor as a strength reduction step.\n      if (RHSC->getValue().isSignMask())\n        return BinaryOp(Instruction::Add, Op->getOperand(0), Op->getOperand(1));\n    return BinaryOp(Op);\n\n  case Instruction::LShr:\n    // Turn logical shift right of a constant into a unsigned divide.\n    if (ConstantInt *SA = dyn_cast<ConstantInt>(Op->getOperand(1))) {\n      uint32_t BitWidth = cast<IntegerType>(Op->getType())->getBitWidth();\n\n      // If the shift count is not less than the bitwidth, the result of\n      // the shift is undefined. Don't try to analyze it, because the\n      // resolution chosen here may differ from the resolution chosen in\n      // other parts of the compiler.\n      if (SA->getValue().ult(BitWidth)) {\n        Constant *X =\n            ConstantInt::get(SA->getContext(),\n                             APInt::getOneBitSet(BitWidth, SA->getZExtValue()));\n        return BinaryOp(Instruction::UDiv, Op->getOperand(0), X);\n      }\n    }\n    return BinaryOp(Op);\n\n  case Instruction::ExtractValue: {\n    auto *EVI = cast<ExtractValueInst>(Op);\n    if (EVI->getNumIndices() != 1 || EVI->getIndices()[0] != 0)\n      break;\n\n    auto *WO = dyn_cast<WithOverflowInst>(EVI->getAggregateOperand());\n    if (!WO)\n      break;\n\n    Instruction::BinaryOps BinOp = WO->getBinaryOp();\n    bool Signed = WO->isSigned();\n    // TODO: Should add nuw/nsw flags for mul as well.\n    if (BinOp == Instruction::Mul || !isOverflowIntrinsicNoWrap(WO, DT))\n      return BinaryOp(BinOp, WO->getLHS(), WO->getRHS());\n\n    // Now that we know that all uses of the arithmetic-result component of\n    // CI are guarded by the overflow check, we can go ahead and pretend\n    // that the arithmetic is non-overflowing.\n    return BinaryOp(BinOp, WO->getLHS(), WO->getRHS(),\n                    /* IsNSW = */ Signed, /* IsNUW = */ !Signed);\n  }\n\n  default:\n    break;\n  }\n\n  // Recognise intrinsic loop.decrement.reg, and as this has exactly the same\n  // semantics as a Sub, return a binary sub expression.\n  if (auto *II = dyn_cast<IntrinsicInst>(V))\n    if (II->getIntrinsicID() == Intrinsic::loop_decrement_reg)\n      return BinaryOp(Instruction::Sub, II->getOperand(0), II->getOperand(1));\n\n  return None;\n}\n\n/// Helper function to createAddRecFromPHIWithCasts. We have a phi\n/// node whose symbolic (unknown) SCEV is \\p SymbolicPHI, which is updated via\n/// the loop backedge by a SCEVAddExpr, possibly also with a few casts on the\n/// way. This function checks if \\p Op, an operand of this SCEVAddExpr,\n/// follows one of the following patterns:\n/// Op == (SExt ix (Trunc iy (%SymbolicPHI) to ix) to iy)\n/// Op == (ZExt ix (Trunc iy (%SymbolicPHI) to ix) to iy)\n/// If the SCEV expression of \\p Op conforms with one of the expected patterns\n/// we return the type of the truncation operation, and indicate whether the\n/// truncated type should be treated as signed/unsigned by setting\n/// \\p Signed to true/false, respectively.\nstatic Type *isSimpleCastedPHI(const SCEV *Op, const SCEVUnknown *SymbolicPHI,\n                               bool &Signed, ScalarEvolution &SE) {\n  // The case where Op == SymbolicPHI (that is, with no type conversions on\n  // the way) is handled by the regular add recurrence creating logic and\n  // would have already been triggered in createAddRecForPHI. Reaching it here\n  // means that createAddRecFromPHI had failed for this PHI before (e.g.,\n  // because one of the other operands of the SCEVAddExpr updating this PHI is\n  // not invariant).\n  //\n  // Here we look for the case where Op = (ext(trunc(SymbolicPHI))), and in\n  // this case predicates that allow us to prove that Op == SymbolicPHI will\n  // be added.\n  if (Op == SymbolicPHI)\n    return nullptr;\n\n  unsigned SourceBits = SE.getTypeSizeInBits(SymbolicPHI->getType());\n  unsigned NewBits = SE.getTypeSizeInBits(Op->getType());\n  if (SourceBits != NewBits)\n    return nullptr;\n\n  const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(Op);\n  const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(Op);\n  if (!SExt && !ZExt)\n    return nullptr;\n  const SCEVTruncateExpr *Trunc =\n      SExt ? dyn_cast<SCEVTruncateExpr>(SExt->getOperand())\n           : dyn_cast<SCEVTruncateExpr>(ZExt->getOperand());\n  if (!Trunc)\n    return nullptr;\n  const SCEV *X = Trunc->getOperand();\n  if (X != SymbolicPHI)\n    return nullptr;\n  Signed = SExt != nullptr;\n  return Trunc->getType();\n}\n\nstatic const Loop *isIntegerLoopHeaderPHI(const PHINode *PN, LoopInfo &LI) {\n  if (!PN->getType()->isIntegerTy())\n    return nullptr;\n  const Loop *L = LI.getLoopFor(PN->getParent());\n  if (!L || L->getHeader() != PN->getParent())\n    return nullptr;\n  return L;\n}\n\n// Analyze \\p SymbolicPHI, a SCEV expression of a phi node, and check if the\n// computation that updates the phi follows the following pattern:\n//   (SExt/ZExt ix (Trunc iy (%SymbolicPHI) to ix) to iy) + InvariantAccum\n// which correspond to a phi->trunc->sext/zext->add->phi update chain.\n// If so, try to see if it can be rewritten as an AddRecExpr under some\n// Predicates. If successful, return them as a pair. Also cache the results\n// of the analysis.\n//\n// Example usage scenario:\n//    Say the Rewriter is called for the following SCEV:\n//         8 * ((sext i32 (trunc i64 %X to i32) to i64) + %Step)\n//    where:\n//         %X = phi i64 (%Start, %BEValue)\n//    It will visitMul->visitAdd->visitSExt->visitTrunc->visitUnknown(%X),\n//    and call this function with %SymbolicPHI = %X.\n//\n//    The analysis will find that the value coming around the backedge has\n//    the following SCEV:\n//         BEValue = ((sext i32 (trunc i64 %X to i32) to i64) + %Step)\n//    Upon concluding that this matches the desired pattern, the function\n//    will return the pair {NewAddRec, SmallPredsVec} where:\n//         NewAddRec = {%Start,+,%Step}\n//         SmallPredsVec = {P1, P2, P3} as follows:\n//           P1(WrapPred): AR: {trunc(%Start),+,(trunc %Step)}<nsw> Flags: <nssw>\n//           P2(EqualPred): %Start == (sext i32 (trunc i64 %Start to i32) to i64)\n//           P3(EqualPred): %Step == (sext i32 (trunc i64 %Step to i32) to i64)\n//    The returned pair means that SymbolicPHI can be rewritten into NewAddRec\n//    under the predicates {P1,P2,P3}.\n//    This predicated rewrite will be cached in PredicatedSCEVRewrites:\n//         PredicatedSCEVRewrites[{%X,L}] = {NewAddRec, {P1,P2,P3)}\n//\n// TODO's:\n//\n// 1) Extend the Induction descriptor to also support inductions that involve\n//    casts: When needed (namely, when we are called in the context of the\n//    vectorizer induction analysis), a Set of cast instructions will be\n//    populated by this method, and provided back to isInductionPHI. This is\n//    needed to allow the vectorizer to properly record them to be ignored by\n//    the cost model and to avoid vectorizing them (otherwise these casts,\n//    which are redundant under the runtime overflow checks, will be\n//    vectorized, which can be costly).\n//\n// 2) Support additional induction/PHISCEV patterns: We also want to support\n//    inductions where the sext-trunc / zext-trunc operations (partly) occur\n//    after the induction update operation (the induction increment):\n//\n//      (Trunc iy (SExt/ZExt ix (%SymbolicPHI + InvariantAccum) to iy) to ix)\n//    which correspond to a phi->add->trunc->sext/zext->phi update chain.\n//\n//      (Trunc iy ((SExt/ZExt ix (%SymbolicPhi) to iy) + InvariantAccum) to ix)\n//    which correspond to a phi->trunc->add->sext/zext->phi update chain.\n//\n// 3) Outline common code with createAddRecFromPHI to avoid duplication.\nOptional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\nScalarEvolution::createAddRecFromPHIWithCastsImpl(const SCEVUnknown *SymbolicPHI) {\n  SmallVector<const SCEVPredicate *, 3> Predicates;\n\n  // *** Part1: Analyze if we have a phi-with-cast pattern for which we can\n  // return an AddRec expression under some predicate.\n\n  auto *PN = cast<PHINode>(SymbolicPHI->getValue());\n  const Loop *L = isIntegerLoopHeaderPHI(PN, LI);\n  assert(L && \"Expecting an integer loop header phi\");\n\n  // The loop may have multiple entrances or multiple exits; we can analyze\n  // this phi as an addrec if it has a unique entry value and a unique\n  // backedge value.\n  Value *BEValueV = nullptr, *StartValueV = nullptr;\n  for (unsigned i = 0, e = PN->getNumIncomingValues(); i != e; ++i) {\n    Value *V = PN->getIncomingValue(i);\n    if (L->contains(PN->getIncomingBlock(i))) {\n      if (!BEValueV) {\n        BEValueV = V;\n      } else if (BEValueV != V) {\n        BEValueV = nullptr;\n        break;\n      }\n    } else if (!StartValueV) {\n      StartValueV = V;\n    } else if (StartValueV != V) {\n      StartValueV = nullptr;\n      break;\n    }\n  }\n  if (!BEValueV || !StartValueV)\n    return None;\n\n  const SCEV *BEValue = getSCEV(BEValueV);\n\n  // If the value coming around the backedge is an add with the symbolic\n  // value we just inserted, possibly with casts that we can ignore under\n  // an appropriate runtime guard, then we found a simple induction variable!\n  const auto *Add = dyn_cast<SCEVAddExpr>(BEValue);\n  if (!Add)\n    return None;\n\n  // If there is a single occurrence of the symbolic value, possibly\n  // casted, replace it with a recurrence.\n  unsigned FoundIndex = Add->getNumOperands();\n  Type *TruncTy = nullptr;\n  bool Signed;\n  for (unsigned i = 0, e = Add->getNumOperands(); i != e; ++i)\n    if ((TruncTy =\n             isSimpleCastedPHI(Add->getOperand(i), SymbolicPHI, Signed, *this)))\n      if (FoundIndex == e) {\n        FoundIndex = i;\n        break;\n      }\n\n  if (FoundIndex == Add->getNumOperands())\n    return None;\n\n  // Create an add with everything but the specified operand.\n  SmallVector<const SCEV *, 8> Ops;\n  for (unsigned i = 0, e = Add->getNumOperands(); i != e; ++i)\n    if (i != FoundIndex)\n      Ops.push_back(Add->getOperand(i));\n  const SCEV *Accum = getAddExpr(Ops);\n\n  // The runtime checks will not be valid if the step amount is\n  // varying inside the loop.\n  if (!isLoopInvariant(Accum, L))\n    return None;\n\n  // *** Part2: Create the predicates\n\n  // Analysis was successful: we have a phi-with-cast pattern for which we\n  // can return an AddRec expression under the following predicates:\n  //\n  // P1: A Wrap predicate that guarantees that Trunc(Start) + i*Trunc(Accum)\n  //     fits within the truncated type (does not overflow) for i = 0 to n-1.\n  // P2: An Equal predicate that guarantees that\n  //     Start = (Ext ix (Trunc iy (Start) to ix) to iy)\n  // P3: An Equal predicate that guarantees that\n  //     Accum = (Ext ix (Trunc iy (Accum) to ix) to iy)\n  //\n  // As we next prove, the above predicates guarantee that:\n  //     Start + i*Accum = (Ext ix (Trunc iy ( Start + i*Accum ) to ix) to iy)\n  //\n  //\n  // More formally, we want to prove that:\n  //     Expr(i+1) = Start + (i+1) * Accum\n  //               = (Ext ix (Trunc iy (Expr(i)) to ix) to iy) + Accum\n  //\n  // Given that:\n  // 1) Expr(0) = Start\n  // 2) Expr(1) = Start + Accum\n  //            = (Ext ix (Trunc iy (Start) to ix) to iy) + Accum :: from P2\n  // 3) Induction hypothesis (step i):\n  //    Expr(i) = (Ext ix (Trunc iy (Expr(i-1)) to ix) to iy) + Accum\n  //\n  // Proof:\n  //  Expr(i+1) =\n  //   = Start + (i+1)*Accum\n  //   = (Start + i*Accum) + Accum\n  //   = Expr(i) + Accum\n  //   = (Ext ix (Trunc iy (Expr(i-1)) to ix) to iy) + Accum + Accum\n  //                                                             :: from step i\n  //\n  //   = (Ext ix (Trunc iy (Start + (i-1)*Accum) to ix) to iy) + Accum + Accum\n  //\n  //   = (Ext ix (Trunc iy (Start + (i-1)*Accum) to ix) to iy)\n  //     + (Ext ix (Trunc iy (Accum) to ix) to iy)\n  //     + Accum                                                     :: from P3\n  //\n  //   = (Ext ix (Trunc iy ((Start + (i-1)*Accum) + Accum) to ix) to iy)\n  //     + Accum                            :: from P1: Ext(x)+Ext(y)=>Ext(x+y)\n  //\n  //   = (Ext ix (Trunc iy (Start + i*Accum) to ix) to iy) + Accum\n  //   = (Ext ix (Trunc iy (Expr(i)) to ix) to iy) + Accum\n  //\n  // By induction, the same applies to all iterations 1<=i<n:\n  //\n\n  // Create a truncated addrec for which we will add a no overflow check (P1).\n  const SCEV *StartVal = getSCEV(StartValueV);\n  const SCEV *PHISCEV =\n      getAddRecExpr(getTruncateExpr(StartVal, TruncTy),\n                    getTruncateExpr(Accum, TruncTy), L, SCEV::FlagAnyWrap);\n\n  // PHISCEV can be either a SCEVConstant or a SCEVAddRecExpr.\n  // ex: If truncated Accum is 0 and StartVal is a constant, then PHISCEV\n  // will be constant.\n  //\n  //  If PHISCEV is a constant, then P1 degenerates into P2 or P3, so we don't\n  // add P1.\n  if (const auto *AR = dyn_cast<SCEVAddRecExpr>(PHISCEV)) {\n    SCEVWrapPredicate::IncrementWrapFlags AddedFlags =\n        Signed ? SCEVWrapPredicate::IncrementNSSW\n               : SCEVWrapPredicate::IncrementNUSW;\n    const SCEVPredicate *AddRecPred = getWrapPredicate(AR, AddedFlags);\n    Predicates.push_back(AddRecPred);\n  }\n\n  // Create the Equal Predicates P2,P3:\n\n  // It is possible that the predicates P2 and/or P3 are computable at\n  // compile time due to StartVal and/or Accum being constants.\n  // If either one is, then we can check that now and escape if either P2\n  // or P3 is false.\n\n  // Construct the extended SCEV: (Ext ix (Trunc iy (Expr) to ix) to iy)\n  // for each of StartVal and Accum\n  auto getExtendedExpr = [&](const SCEV *Expr,\n                             bool CreateSignExtend) -> const SCEV * {\n    assert(isLoopInvariant(Expr, L) && \"Expr is expected to be invariant\");\n    const SCEV *TruncatedExpr = getTruncateExpr(Expr, TruncTy);\n    const SCEV *ExtendedExpr =\n        CreateSignExtend ? getSignExtendExpr(TruncatedExpr, Expr->getType())\n                         : getZeroExtendExpr(TruncatedExpr, Expr->getType());\n    return ExtendedExpr;\n  };\n\n  // Given:\n  //  ExtendedExpr = (Ext ix (Trunc iy (Expr) to ix) to iy\n  //               = getExtendedExpr(Expr)\n  // Determine whether the predicate P: Expr == ExtendedExpr\n  // is known to be false at compile time\n  auto PredIsKnownFalse = [&](const SCEV *Expr,\n                              const SCEV *ExtendedExpr) -> bool {\n    return Expr != ExtendedExpr &&\n           isKnownPredicate(ICmpInst::ICMP_NE, Expr, ExtendedExpr);\n  };\n\n  const SCEV *StartExtended = getExtendedExpr(StartVal, Signed);\n  if (PredIsKnownFalse(StartVal, StartExtended)) {\n    LLVM_DEBUG(dbgs() << \"P2 is compile-time false\\n\";);\n    return None;\n  }\n\n  // The Step is always Signed (because the overflow checks are either\n  // NSSW or NUSW)\n  const SCEV *AccumExtended = getExtendedExpr(Accum, /*CreateSignExtend=*/true);\n  if (PredIsKnownFalse(Accum, AccumExtended)) {\n    LLVM_DEBUG(dbgs() << \"P3 is compile-time false\\n\";);\n    return None;\n  }\n\n  auto AppendPredicate = [&](const SCEV *Expr,\n                             const SCEV *ExtendedExpr) -> void {\n    if (Expr != ExtendedExpr &&\n        !isKnownPredicate(ICmpInst::ICMP_EQ, Expr, ExtendedExpr)) {\n      const SCEVPredicate *Pred = getEqualPredicate(Expr, ExtendedExpr);\n      LLVM_DEBUG(dbgs() << \"Added Predicate: \" << *Pred);\n      Predicates.push_back(Pred);\n    }\n  };\n\n  AppendPredicate(StartVal, StartExtended);\n  AppendPredicate(Accum, AccumExtended);\n\n  // *** Part3: Predicates are ready. Now go ahead and create the new addrec in\n  // which the casts had been folded away. The caller can rewrite SymbolicPHI\n  // into NewAR if it will also add the runtime overflow checks specified in\n  // Predicates.\n  auto *NewAR = getAddRecExpr(StartVal, Accum, L, SCEV::FlagAnyWrap);\n\n  std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>> PredRewrite =\n      std::make_pair(NewAR, Predicates);\n  // Remember the result of the analysis for this SCEV at this locayyytion.\n  PredicatedSCEVRewrites[{SymbolicPHI, L}] = PredRewrite;\n  return PredRewrite;\n}\n\nOptional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\nScalarEvolution::createAddRecFromPHIWithCasts(const SCEVUnknown *SymbolicPHI) {\n  auto *PN = cast<PHINode>(SymbolicPHI->getValue());\n  const Loop *L = isIntegerLoopHeaderPHI(PN, LI);\n  if (!L)\n    return None;\n\n  // Check to see if we already analyzed this PHI.\n  auto I = PredicatedSCEVRewrites.find({SymbolicPHI, L});\n  if (I != PredicatedSCEVRewrites.end()) {\n    std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>> Rewrite =\n        I->second;\n    // Analysis was done before and failed to create an AddRec:\n    if (Rewrite.first == SymbolicPHI)\n      return None;\n    // Analysis was done before and succeeded to create an AddRec under\n    // a predicate:\n    assert(isa<SCEVAddRecExpr>(Rewrite.first) && \"Expected an AddRec\");\n    assert(!(Rewrite.second).empty() && \"Expected to find Predicates\");\n    return Rewrite;\n  }\n\n  Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\n    Rewrite = createAddRecFromPHIWithCastsImpl(SymbolicPHI);\n\n  // Record in the cache that the analysis failed\n  if (!Rewrite) {\n    SmallVector<const SCEVPredicate *, 3> Predicates;\n    PredicatedSCEVRewrites[{SymbolicPHI, L}] = {SymbolicPHI, Predicates};\n    return None;\n  }\n\n  return Rewrite;\n}\n\n// FIXME: This utility is currently required because the Rewriter currently\n// does not rewrite this expression:\n// {0, +, (sext ix (trunc iy to ix) to iy)}\n// into {0, +, %step},\n// even when the following Equal predicate exists:\n// \"%step == (sext ix (trunc iy to ix) to iy)\".\nbool PredicatedScalarEvolution::areAddRecsEqualWithPreds(\n    const SCEVAddRecExpr *AR1, const SCEVAddRecExpr *AR2) const {\n  if (AR1 == AR2)\n    return true;\n\n  auto areExprsEqual = [&](const SCEV *Expr1, const SCEV *Expr2) -> bool {\n    if (Expr1 != Expr2 && !Preds.implies(SE.getEqualPredicate(Expr1, Expr2)) &&\n        !Preds.implies(SE.getEqualPredicate(Expr2, Expr1)))\n      return false;\n    return true;\n  };\n\n  if (!areExprsEqual(AR1->getStart(), AR2->getStart()) ||\n      !areExprsEqual(AR1->getStepRecurrence(SE), AR2->getStepRecurrence(SE)))\n    return false;\n  return true;\n}\n\n/// A helper function for createAddRecFromPHI to handle simple cases.\n///\n/// This function tries to find an AddRec expression for the simplest (yet most\n/// common) cases: PN = PHI(Start, OP(Self, LoopInvariant)).\n/// If it fails, createAddRecFromPHI will use a more general, but slow,\n/// technique for finding the AddRec expression.\nconst SCEV *ScalarEvolution::createSimpleAffineAddRec(PHINode *PN,\n                                                      Value *BEValueV,\n                                                      Value *StartValueV) {\n  const Loop *L = LI.getLoopFor(PN->getParent());\n  assert(L && L->getHeader() == PN->getParent());\n  assert(BEValueV && StartValueV);\n\n  auto BO = MatchBinaryOp(BEValueV, DT);\n  if (!BO)\n    return nullptr;\n\n  if (BO->Opcode != Instruction::Add)\n    return nullptr;\n\n  const SCEV *Accum = nullptr;\n  if (BO->LHS == PN && L->isLoopInvariant(BO->RHS))\n    Accum = getSCEV(BO->RHS);\n  else if (BO->RHS == PN && L->isLoopInvariant(BO->LHS))\n    Accum = getSCEV(BO->LHS);\n\n  if (!Accum)\n    return nullptr;\n\n  SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap;\n  if (BO->IsNUW)\n    Flags = setFlags(Flags, SCEV::FlagNUW);\n  if (BO->IsNSW)\n    Flags = setFlags(Flags, SCEV::FlagNSW);\n\n  const SCEV *StartVal = getSCEV(StartValueV);\n  const SCEV *PHISCEV = getAddRecExpr(StartVal, Accum, L, Flags);\n\n  ValueExprMap[SCEVCallbackVH(PN, this)] = PHISCEV;\n\n  // We can add Flags to the post-inc expression only if we\n  // know that it is *undefined behavior* for BEValueV to\n  // overflow.\n  if (auto *BEInst = dyn_cast<Instruction>(BEValueV))\n    if (isLoopInvariant(Accum, L) && isAddRecNeverPoison(BEInst, L))\n      (void)getAddRecExpr(getAddExpr(StartVal, Accum), Accum, L, Flags);\n\n  return PHISCEV;\n}\n\nconst SCEV *ScalarEvolution::createAddRecFromPHI(PHINode *PN) {\n  const Loop *L = LI.getLoopFor(PN->getParent());\n  if (!L || L->getHeader() != PN->getParent())\n    return nullptr;\n\n  // The loop may have multiple entrances or multiple exits; we can analyze\n  // this phi as an addrec if it has a unique entry value and a unique\n  // backedge value.\n  Value *BEValueV = nullptr, *StartValueV = nullptr;\n  for (unsigned i = 0, e = PN->getNumIncomingValues(); i != e; ++i) {\n    Value *V = PN->getIncomingValue(i);\n    if (L->contains(PN->getIncomingBlock(i))) {\n      if (!BEValueV) {\n        BEValueV = V;\n      } else if (BEValueV != V) {\n        BEValueV = nullptr;\n        break;\n      }\n    } else if (!StartValueV) {\n      StartValueV = V;\n    } else if (StartValueV != V) {\n      StartValueV = nullptr;\n      break;\n    }\n  }\n  if (!BEValueV || !StartValueV)\n    return nullptr;\n\n  assert(ValueExprMap.find_as(PN) == ValueExprMap.end() &&\n         \"PHI node already processed?\");\n\n  // First, try to find AddRec expression without creating a fictituos symbolic\n  // value for PN.\n  if (auto *S = createSimpleAffineAddRec(PN, BEValueV, StartValueV))\n    return S;\n\n  // Handle PHI node value symbolically.\n  const SCEV *SymbolicName = getUnknown(PN);\n  ValueExprMap.insert({SCEVCallbackVH(PN, this), SymbolicName});\n\n  // Using this symbolic name for the PHI, analyze the value coming around\n  // the back-edge.\n  const SCEV *BEValue = getSCEV(BEValueV);\n\n  // NOTE: If BEValue is loop invariant, we know that the PHI node just\n  // has a special value for the first iteration of the loop.\n\n  // If the value coming around the backedge is an add with the symbolic\n  // value we just inserted, then we found a simple induction variable!\n  if (const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(BEValue)) {\n    // If there is a single occurrence of the symbolic value, replace it\n    // with a recurrence.\n    unsigned FoundIndex = Add->getNumOperands();\n    for (unsigned i = 0, e = Add->getNumOperands(); i != e; ++i)\n      if (Add->getOperand(i) == SymbolicName)\n        if (FoundIndex == e) {\n          FoundIndex = i;\n          break;\n        }\n\n    if (FoundIndex != Add->getNumOperands()) {\n      // Create an add with everything but the specified operand.\n      SmallVector<const SCEV *, 8> Ops;\n      for (unsigned i = 0, e = Add->getNumOperands(); i != e; ++i)\n        if (i != FoundIndex)\n          Ops.push_back(SCEVBackedgeConditionFolder::rewrite(Add->getOperand(i),\n                                                             L, *this));\n      const SCEV *Accum = getAddExpr(Ops);\n\n      // This is not a valid addrec if the step amount is varying each\n      // loop iteration, but is not itself an addrec in this loop.\n      if (isLoopInvariant(Accum, L) ||\n          (isa<SCEVAddRecExpr>(Accum) &&\n           cast<SCEVAddRecExpr>(Accum)->getLoop() == L)) {\n        SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap;\n\n        if (auto BO = MatchBinaryOp(BEValueV, DT)) {\n          if (BO->Opcode == Instruction::Add && BO->LHS == PN) {\n            if (BO->IsNUW)\n              Flags = setFlags(Flags, SCEV::FlagNUW);\n            if (BO->IsNSW)\n              Flags = setFlags(Flags, SCEV::FlagNSW);\n          }\n        } else if (GEPOperator *GEP = dyn_cast<GEPOperator>(BEValueV)) {\n          // If the increment is an inbounds GEP, then we know the address\n          // space cannot be wrapped around. We cannot make any guarantee\n          // about signed or unsigned overflow because pointers are\n          // unsigned but we may have a negative index from the base\n          // pointer. We can guarantee that no unsigned wrap occurs if the\n          // indices form a positive value.\n          if (GEP->isInBounds() && GEP->getOperand(0) == PN) {\n            Flags = setFlags(Flags, SCEV::FlagNW);\n\n            const SCEV *Ptr = getSCEV(GEP->getPointerOperand());\n            if (isKnownPositive(getMinusSCEV(getSCEV(GEP), Ptr)))\n              Flags = setFlags(Flags, SCEV::FlagNUW);\n          }\n\n          // We cannot transfer nuw and nsw flags from subtraction\n          // operations -- sub nuw X, Y is not the same as add nuw X, -Y\n          // for instance.\n        }\n\n        const SCEV *StartVal = getSCEV(StartValueV);\n        const SCEV *PHISCEV = getAddRecExpr(StartVal, Accum, L, Flags);\n\n        // Okay, for the entire analysis of this edge we assumed the PHI\n        // to be symbolic.  We now need to go back and purge all of the\n        // entries for the scalars that use the symbolic expression.\n        forgetSymbolicName(PN, SymbolicName);\n        ValueExprMap[SCEVCallbackVH(PN, this)] = PHISCEV;\n\n        // We can add Flags to the post-inc expression only if we\n        // know that it is *undefined behavior* for BEValueV to\n        // overflow.\n        if (auto *BEInst = dyn_cast<Instruction>(BEValueV))\n          if (isLoopInvariant(Accum, L) && isAddRecNeverPoison(BEInst, L))\n            (void)getAddRecExpr(getAddExpr(StartVal, Accum), Accum, L, Flags);\n\n        return PHISCEV;\n      }\n    }\n  } else {\n    // Otherwise, this could be a loop like this:\n    //     i = 0;  for (j = 1; ..; ++j) { ....  i = j; }\n    // In this case, j = {1,+,1}  and BEValue is j.\n    // Because the other in-value of i (0) fits the evolution of BEValue\n    // i really is an addrec evolution.\n    //\n    // We can generalize this saying that i is the shifted value of BEValue\n    // by one iteration:\n    //   PHI(f(0), f({1,+,1})) --> f({0,+,1})\n    const SCEV *Shifted = SCEVShiftRewriter::rewrite(BEValue, L, *this);\n    const SCEV *Start = SCEVInitRewriter::rewrite(Shifted, L, *this, false);\n    if (Shifted != getCouldNotCompute() &&\n        Start != getCouldNotCompute()) {\n      const SCEV *StartVal = getSCEV(StartValueV);\n      if (Start == StartVal) {\n        // Okay, for the entire analysis of this edge we assumed the PHI\n        // to be symbolic.  We now need to go back and purge all of the\n        // entries for the scalars that use the symbolic expression.\n        forgetSymbolicName(PN, SymbolicName);\n        ValueExprMap[SCEVCallbackVH(PN, this)] = Shifted;\n        return Shifted;\n      }\n    }\n  }\n\n  // Remove the temporary PHI node SCEV that has been inserted while intending\n  // to create an AddRecExpr for this PHI node. We can not keep this temporary\n  // as it will prevent later (possibly simpler) SCEV expressions to be added\n  // to the ValueExprMap.\n  eraseValueFromMap(PN);\n\n  return nullptr;\n}\n\n// Checks if the SCEV S is available at BB.  S is considered available at BB\n// if S can be materialized at BB without introducing a fault.\nstatic bool IsAvailableOnEntry(const Loop *L, DominatorTree &DT, const SCEV *S,\n                               BasicBlock *BB) {\n  struct CheckAvailable {\n    bool TraversalDone = false;\n    bool Available = true;\n\n    const Loop *L = nullptr;  // The loop BB is in (can be nullptr)\n    BasicBlock *BB = nullptr;\n    DominatorTree &DT;\n\n    CheckAvailable(const Loop *L, BasicBlock *BB, DominatorTree &DT)\n      : L(L), BB(BB), DT(DT) {}\n\n    bool setUnavailable() {\n      TraversalDone = true;\n      Available = false;\n      return false;\n    }\n\n    bool follow(const SCEV *S) {\n      switch (S->getSCEVType()) {\n      case scConstant:\n      case scPtrToInt:\n      case scTruncate:\n      case scZeroExtend:\n      case scSignExtend:\n      case scAddExpr:\n      case scMulExpr:\n      case scUMaxExpr:\n      case scSMaxExpr:\n      case scUMinExpr:\n      case scSMinExpr:\n        // These expressions are available if their operand(s) is/are.\n        return true;\n\n      case scAddRecExpr: {\n        // We allow add recurrences that are on the loop BB is in, or some\n        // outer loop.  This guarantees availability because the value of the\n        // add recurrence at BB is simply the \"current\" value of the induction\n        // variable.  We can relax this in the future; for instance an add\n        // recurrence on a sibling dominating loop is also available at BB.\n        const auto *ARLoop = cast<SCEVAddRecExpr>(S)->getLoop();\n        if (L && (ARLoop == L || ARLoop->contains(L)))\n          return true;\n\n        return setUnavailable();\n      }\n\n      case scUnknown: {\n        // For SCEVUnknown, we check for simple dominance.\n        const auto *SU = cast<SCEVUnknown>(S);\n        Value *V = SU->getValue();\n\n        if (isa<Argument>(V))\n          return false;\n\n        if (isa<Instruction>(V) && DT.dominates(cast<Instruction>(V), BB))\n          return false;\n\n        return setUnavailable();\n      }\n\n      case scUDivExpr:\n      case scCouldNotCompute:\n        // We do not try to smart about these at all.\n        return setUnavailable();\n      }\n      llvm_unreachable(\"Unknown SCEV kind!\");\n    }\n\n    bool isDone() { return TraversalDone; }\n  };\n\n  CheckAvailable CA(L, BB, DT);\n  SCEVTraversal<CheckAvailable> ST(CA);\n\n  ST.visitAll(S);\n  return CA.Available;\n}\n\n// Try to match a control flow sequence that branches out at BI and merges back\n// at Merge into a \"C ? LHS : RHS\" select pattern.  Return true on a successful\n// match.\nstatic bool BrPHIToSelect(DominatorTree &DT, BranchInst *BI, PHINode *Merge,\n                          Value *&C, Value *&LHS, Value *&RHS) {\n  C = BI->getCondition();\n\n  BasicBlockEdge LeftEdge(BI->getParent(), BI->getSuccessor(0));\n  BasicBlockEdge RightEdge(BI->getParent(), BI->getSuccessor(1));\n\n  if (!LeftEdge.isSingleEdge())\n    return false;\n\n  assert(RightEdge.isSingleEdge() && \"Follows from LeftEdge.isSingleEdge()\");\n\n  Use &LeftUse = Merge->getOperandUse(0);\n  Use &RightUse = Merge->getOperandUse(1);\n\n  if (DT.dominates(LeftEdge, LeftUse) && DT.dominates(RightEdge, RightUse)) {\n    LHS = LeftUse;\n    RHS = RightUse;\n    return true;\n  }\n\n  if (DT.dominates(LeftEdge, RightUse) && DT.dominates(RightEdge, LeftUse)) {\n    LHS = RightUse;\n    RHS = LeftUse;\n    return true;\n  }\n\n  return false;\n}\n\nconst SCEV *ScalarEvolution::createNodeFromSelectLikePHI(PHINode *PN) {\n  auto IsReachable =\n      [&](BasicBlock *BB) { return DT.isReachableFromEntry(BB); };\n  if (PN->getNumIncomingValues() == 2 && all_of(PN->blocks(), IsReachable)) {\n    const Loop *L = LI.getLoopFor(PN->getParent());\n\n    // We don't want to break LCSSA, even in a SCEV expression tree.\n    for (unsigned i = 0, e = PN->getNumIncomingValues(); i != e; ++i)\n      if (LI.getLoopFor(PN->getIncomingBlock(i)) != L)\n        return nullptr;\n\n    // Try to match\n    //\n    //  br %cond, label %left, label %right\n    // left:\n    //  br label %merge\n    // right:\n    //  br label %merge\n    // merge:\n    //  V = phi [ %x, %left ], [ %y, %right ]\n    //\n    // as \"select %cond, %x, %y\"\n\n    BasicBlock *IDom = DT[PN->getParent()]->getIDom()->getBlock();\n    assert(IDom && \"At least the entry block should dominate PN\");\n\n    auto *BI = dyn_cast<BranchInst>(IDom->getTerminator());\n    Value *Cond = nullptr, *LHS = nullptr, *RHS = nullptr;\n\n    if (BI && BI->isConditional() &&\n        BrPHIToSelect(DT, BI, PN, Cond, LHS, RHS) &&\n        IsAvailableOnEntry(L, DT, getSCEV(LHS), PN->getParent()) &&\n        IsAvailableOnEntry(L, DT, getSCEV(RHS), PN->getParent()))\n      return createNodeForSelectOrPHI(PN, Cond, LHS, RHS);\n  }\n\n  return nullptr;\n}\n\nconst SCEV *ScalarEvolution::createNodeForPHI(PHINode *PN) {\n  if (const SCEV *S = createAddRecFromPHI(PN))\n    return S;\n\n  if (const SCEV *S = createNodeFromSelectLikePHI(PN))\n    return S;\n\n  // If the PHI has a single incoming value, follow that value, unless the\n  // PHI's incoming blocks are in a different loop, in which case doing so\n  // risks breaking LCSSA form. Instcombine would normally zap these, but\n  // it doesn't have DominatorTree information, so it may miss cases.\n  if (Value *V = SimplifyInstruction(PN, {getDataLayout(), &TLI, &DT, &AC}))\n    if (LI.replacementPreservesLCSSAForm(PN, V))\n      return getSCEV(V);\n\n  // If it's not a loop phi, we can't handle it yet.\n  return getUnknown(PN);\n}\n\nconst SCEV *ScalarEvolution::createNodeForSelectOrPHI(Instruction *I,\n                                                      Value *Cond,\n                                                      Value *TrueVal,\n                                                      Value *FalseVal) {\n  // Handle \"constant\" branch or select. This can occur for instance when a\n  // loop pass transforms an inner loop and moves on to process the outer loop.\n  if (auto *CI = dyn_cast<ConstantInt>(Cond))\n    return getSCEV(CI->isOne() ? TrueVal : FalseVal);\n\n  // Try to match some simple smax or umax patterns.\n  auto *ICI = dyn_cast<ICmpInst>(Cond);\n  if (!ICI)\n    return getUnknown(I);\n\n  Value *LHS = ICI->getOperand(0);\n  Value *RHS = ICI->getOperand(1);\n\n  switch (ICI->getPredicate()) {\n  case ICmpInst::ICMP_SLT:\n  case ICmpInst::ICMP_SLE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_SGT:\n  case ICmpInst::ICMP_SGE:\n    // a >s b ? a+x : b+x  ->  smax(a, b)+x\n    // a >s b ? b+x : a+x  ->  smin(a, b)+x\n    if (getTypeSizeInBits(LHS->getType()) <= getTypeSizeInBits(I->getType())) {\n      const SCEV *LS = getNoopOrSignExtend(getSCEV(LHS), I->getType());\n      const SCEV *RS = getNoopOrSignExtend(getSCEV(RHS), I->getType());\n      const SCEV *LA = getSCEV(TrueVal);\n      const SCEV *RA = getSCEV(FalseVal);\n      const SCEV *LDiff = getMinusSCEV(LA, LS);\n      const SCEV *RDiff = getMinusSCEV(RA, RS);\n      if (LDiff == RDiff)\n        return getAddExpr(getSMaxExpr(LS, RS), LDiff);\n      LDiff = getMinusSCEV(LA, RS);\n      RDiff = getMinusSCEV(RA, LS);\n      if (LDiff == RDiff)\n        return getAddExpr(getSMinExpr(LS, RS), LDiff);\n    }\n    break;\n  case ICmpInst::ICMP_ULT:\n  case ICmpInst::ICMP_ULE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_UGT:\n  case ICmpInst::ICMP_UGE:\n    // a >u b ? a+x : b+x  ->  umax(a, b)+x\n    // a >u b ? b+x : a+x  ->  umin(a, b)+x\n    if (getTypeSizeInBits(LHS->getType()) <= getTypeSizeInBits(I->getType())) {\n      const SCEV *LS = getNoopOrZeroExtend(getSCEV(LHS), I->getType());\n      const SCEV *RS = getNoopOrZeroExtend(getSCEV(RHS), I->getType());\n      const SCEV *LA = getSCEV(TrueVal);\n      const SCEV *RA = getSCEV(FalseVal);\n      const SCEV *LDiff = getMinusSCEV(LA, LS);\n      const SCEV *RDiff = getMinusSCEV(RA, RS);\n      if (LDiff == RDiff)\n        return getAddExpr(getUMaxExpr(LS, RS), LDiff);\n      LDiff = getMinusSCEV(LA, RS);\n      RDiff = getMinusSCEV(RA, LS);\n      if (LDiff == RDiff)\n        return getAddExpr(getUMinExpr(LS, RS), LDiff);\n    }\n    break;\n  case ICmpInst::ICMP_NE:\n    // n != 0 ? n+x : 1+x  ->  umax(n, 1)+x\n    if (getTypeSizeInBits(LHS->getType()) <= getTypeSizeInBits(I->getType()) &&\n        isa<ConstantInt>(RHS) && cast<ConstantInt>(RHS)->isZero()) {\n      const SCEV *One = getOne(I->getType());\n      const SCEV *LS = getNoopOrZeroExtend(getSCEV(LHS), I->getType());\n      const SCEV *LA = getSCEV(TrueVal);\n      const SCEV *RA = getSCEV(FalseVal);\n      const SCEV *LDiff = getMinusSCEV(LA, LS);\n      const SCEV *RDiff = getMinusSCEV(RA, One);\n      if (LDiff == RDiff)\n        return getAddExpr(getUMaxExpr(One, LS), LDiff);\n    }\n    break;\n  case ICmpInst::ICMP_EQ:\n    // n == 0 ? 1+x : n+x  ->  umax(n, 1)+x\n    if (getTypeSizeInBits(LHS->getType()) <= getTypeSizeInBits(I->getType()) &&\n        isa<ConstantInt>(RHS) && cast<ConstantInt>(RHS)->isZero()) {\n      const SCEV *One = getOne(I->getType());\n      const SCEV *LS = getNoopOrZeroExtend(getSCEV(LHS), I->getType());\n      const SCEV *LA = getSCEV(TrueVal);\n      const SCEV *RA = getSCEV(FalseVal);\n      const SCEV *LDiff = getMinusSCEV(LA, One);\n      const SCEV *RDiff = getMinusSCEV(RA, LS);\n      if (LDiff == RDiff)\n        return getAddExpr(getUMaxExpr(One, LS), LDiff);\n    }\n    break;\n  default:\n    break;\n  }\n\n  return getUnknown(I);\n}\n\n/// Expand GEP instructions into add and multiply operations. This allows them\n/// to be analyzed by regular SCEV code.\nconst SCEV *ScalarEvolution::createNodeForGEP(GEPOperator *GEP) {\n  // Don't attempt to analyze GEPs over unsized objects.\n  if (!GEP->getSourceElementType()->isSized())\n    return getUnknown(GEP);\n\n  SmallVector<const SCEV *, 4> IndexExprs;\n  for (auto Index = GEP->idx_begin(); Index != GEP->idx_end(); ++Index)\n    IndexExprs.push_back(getSCEV(*Index));\n  return getGEPExpr(GEP, IndexExprs);\n}\n\nuint32_t ScalarEvolution::GetMinTrailingZerosImpl(const SCEV *S) {\n  if (const SCEVConstant *C = dyn_cast<SCEVConstant>(S))\n    return C->getAPInt().countTrailingZeros();\n\n  if (const SCEVPtrToIntExpr *I = dyn_cast<SCEVPtrToIntExpr>(S))\n    return GetMinTrailingZeros(I->getOperand());\n\n  if (const SCEVTruncateExpr *T = dyn_cast<SCEVTruncateExpr>(S))\n    return std::min(GetMinTrailingZeros(T->getOperand()),\n                    (uint32_t)getTypeSizeInBits(T->getType()));\n\n  if (const SCEVZeroExtendExpr *E = dyn_cast<SCEVZeroExtendExpr>(S)) {\n    uint32_t OpRes = GetMinTrailingZeros(E->getOperand());\n    return OpRes == getTypeSizeInBits(E->getOperand()->getType())\n               ? getTypeSizeInBits(E->getType())\n               : OpRes;\n  }\n\n  if (const SCEVSignExtendExpr *E = dyn_cast<SCEVSignExtendExpr>(S)) {\n    uint32_t OpRes = GetMinTrailingZeros(E->getOperand());\n    return OpRes == getTypeSizeInBits(E->getOperand()->getType())\n               ? getTypeSizeInBits(E->getType())\n               : OpRes;\n  }\n\n  if (const SCEVAddExpr *A = dyn_cast<SCEVAddExpr>(S)) {\n    // The result is the min of all operands results.\n    uint32_t MinOpRes = GetMinTrailingZeros(A->getOperand(0));\n    for (unsigned i = 1, e = A->getNumOperands(); MinOpRes && i != e; ++i)\n      MinOpRes = std::min(MinOpRes, GetMinTrailingZeros(A->getOperand(i)));\n    return MinOpRes;\n  }\n\n  if (const SCEVMulExpr *M = dyn_cast<SCEVMulExpr>(S)) {\n    // The result is the sum of all operands results.\n    uint32_t SumOpRes = GetMinTrailingZeros(M->getOperand(0));\n    uint32_t BitWidth = getTypeSizeInBits(M->getType());\n    for (unsigned i = 1, e = M->getNumOperands();\n         SumOpRes != BitWidth && i != e; ++i)\n      SumOpRes =\n          std::min(SumOpRes + GetMinTrailingZeros(M->getOperand(i)), BitWidth);\n    return SumOpRes;\n  }\n\n  if (const SCEVAddRecExpr *A = dyn_cast<SCEVAddRecExpr>(S)) {\n    // The result is the min of all operands results.\n    uint32_t MinOpRes = GetMinTrailingZeros(A->getOperand(0));\n    for (unsigned i = 1, e = A->getNumOperands(); MinOpRes && i != e; ++i)\n      MinOpRes = std::min(MinOpRes, GetMinTrailingZeros(A->getOperand(i)));\n    return MinOpRes;\n  }\n\n  if (const SCEVSMaxExpr *M = dyn_cast<SCEVSMaxExpr>(S)) {\n    // The result is the min of all operands results.\n    uint32_t MinOpRes = GetMinTrailingZeros(M->getOperand(0));\n    for (unsigned i = 1, e = M->getNumOperands(); MinOpRes && i != e; ++i)\n      MinOpRes = std::min(MinOpRes, GetMinTrailingZeros(M->getOperand(i)));\n    return MinOpRes;\n  }\n\n  if (const SCEVUMaxExpr *M = dyn_cast<SCEVUMaxExpr>(S)) {\n    // The result is the min of all operands results.\n    uint32_t MinOpRes = GetMinTrailingZeros(M->getOperand(0));\n    for (unsigned i = 1, e = M->getNumOperands(); MinOpRes && i != e; ++i)\n      MinOpRes = std::min(MinOpRes, GetMinTrailingZeros(M->getOperand(i)));\n    return MinOpRes;\n  }\n\n  if (const SCEVUnknown *U = dyn_cast<SCEVUnknown>(S)) {\n    // For a SCEVUnknown, ask ValueTracking.\n    KnownBits Known = computeKnownBits(U->getValue(), getDataLayout(), 0, &AC, nullptr, &DT);\n    return Known.countMinTrailingZeros();\n  }\n\n  // SCEVUDivExpr\n  return 0;\n}\n\nuint32_t ScalarEvolution::GetMinTrailingZeros(const SCEV *S) {\n  auto I = MinTrailingZerosCache.find(S);\n  if (I != MinTrailingZerosCache.end())\n    return I->second;\n\n  uint32_t Result = GetMinTrailingZerosImpl(S);\n  auto InsertPair = MinTrailingZerosCache.insert({S, Result});\n  assert(InsertPair.second && \"Should insert a new key\");\n  return InsertPair.first->second;\n}\n\n/// Helper method to assign a range to V from metadata present in the IR.\nstatic Optional<ConstantRange> GetRangeFromMetadata(Value *V) {\n  if (Instruction *I = dyn_cast<Instruction>(V))\n    if (MDNode *MD = I->getMetadata(LLVMContext::MD_range))\n      return getConstantRangeFromMetadata(*MD);\n\n  return None;\n}\n\nvoid ScalarEvolution::setNoWrapFlags(SCEVAddRecExpr *AddRec,\n                                     SCEV::NoWrapFlags Flags) {\n  if (AddRec->getNoWrapFlags(Flags) != Flags) {\n    AddRec->setNoWrapFlags(Flags);\n    UnsignedRanges.erase(AddRec);\n    SignedRanges.erase(AddRec);\n  }\n}\n\n/// Determine the range for a particular SCEV.  If SignHint is\n/// HINT_RANGE_UNSIGNED (resp. HINT_RANGE_SIGNED) then getRange prefers ranges\n/// with a \"cleaner\" unsigned (resp. signed) representation.\nconst ConstantRange &\nScalarEvolution::getRangeRef(const SCEV *S,\n                             ScalarEvolution::RangeSignHint SignHint) {\n  DenseMap<const SCEV *, ConstantRange> &Cache =\n      SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED ? UnsignedRanges\n                                                       : SignedRanges;\n  ConstantRange::PreferredRangeType RangeType =\n      SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED\n          ? ConstantRange::Unsigned : ConstantRange::Signed;\n\n  // See if we've computed this range already.\n  DenseMap<const SCEV *, ConstantRange>::iterator I = Cache.find(S);\n  if (I != Cache.end())\n    return I->second;\n\n  if (const SCEVConstant *C = dyn_cast<SCEVConstant>(S))\n    return setRange(C, SignHint, ConstantRange(C->getAPInt()));\n\n  unsigned BitWidth = getTypeSizeInBits(S->getType());\n  ConstantRange ConservativeResult(BitWidth, /*isFullSet=*/true);\n  using OBO = OverflowingBinaryOperator;\n\n  // If the value has known zeros, the maximum value will have those known zeros\n  // as well.\n  uint32_t TZ = GetMinTrailingZeros(S);\n  if (TZ != 0) {\n    if (SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED)\n      ConservativeResult =\n          ConstantRange(APInt::getMinValue(BitWidth),\n                        APInt::getMaxValue(BitWidth).lshr(TZ).shl(TZ) + 1);\n    else\n      ConservativeResult = ConstantRange(\n          APInt::getSignedMinValue(BitWidth),\n          APInt::getSignedMaxValue(BitWidth).ashr(TZ).shl(TZ) + 1);\n  }\n\n  if (const SCEVAddExpr *Add = dyn_cast<SCEVAddExpr>(S)) {\n    ConstantRange X = getRangeRef(Add->getOperand(0), SignHint);\n    unsigned WrapType = OBO::AnyWrap;\n    if (Add->hasNoSignedWrap())\n      WrapType |= OBO::NoSignedWrap;\n    if (Add->hasNoUnsignedWrap())\n      WrapType |= OBO::NoUnsignedWrap;\n    for (unsigned i = 1, e = Add->getNumOperands(); i != e; ++i)\n      X = X.addWithNoWrap(getRangeRef(Add->getOperand(i), SignHint),\n                          WrapType, RangeType);\n    return setRange(Add, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(S)) {\n    ConstantRange X = getRangeRef(Mul->getOperand(0), SignHint);\n    for (unsigned i = 1, e = Mul->getNumOperands(); i != e; ++i)\n      X = X.multiply(getRangeRef(Mul->getOperand(i), SignHint));\n    return setRange(Mul, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVSMaxExpr *SMax = dyn_cast<SCEVSMaxExpr>(S)) {\n    ConstantRange X = getRangeRef(SMax->getOperand(0), SignHint);\n    for (unsigned i = 1, e = SMax->getNumOperands(); i != e; ++i)\n      X = X.smax(getRangeRef(SMax->getOperand(i), SignHint));\n    return setRange(SMax, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVUMaxExpr *UMax = dyn_cast<SCEVUMaxExpr>(S)) {\n    ConstantRange X = getRangeRef(UMax->getOperand(0), SignHint);\n    for (unsigned i = 1, e = UMax->getNumOperands(); i != e; ++i)\n      X = X.umax(getRangeRef(UMax->getOperand(i), SignHint));\n    return setRange(UMax, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVSMinExpr *SMin = dyn_cast<SCEVSMinExpr>(S)) {\n    ConstantRange X = getRangeRef(SMin->getOperand(0), SignHint);\n    for (unsigned i = 1, e = SMin->getNumOperands(); i != e; ++i)\n      X = X.smin(getRangeRef(SMin->getOperand(i), SignHint));\n    return setRange(SMin, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVUMinExpr *UMin = dyn_cast<SCEVUMinExpr>(S)) {\n    ConstantRange X = getRangeRef(UMin->getOperand(0), SignHint);\n    for (unsigned i = 1, e = UMin->getNumOperands(); i != e; ++i)\n      X = X.umin(getRangeRef(UMin->getOperand(i), SignHint));\n    return setRange(UMin, SignHint,\n                    ConservativeResult.intersectWith(X, RangeType));\n  }\n\n  if (const SCEVUDivExpr *UDiv = dyn_cast<SCEVUDivExpr>(S)) {\n    ConstantRange X = getRangeRef(UDiv->getLHS(), SignHint);\n    ConstantRange Y = getRangeRef(UDiv->getRHS(), SignHint);\n    return setRange(UDiv, SignHint,\n                    ConservativeResult.intersectWith(X.udiv(Y), RangeType));\n  }\n\n  if (const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(S)) {\n    ConstantRange X = getRangeRef(ZExt->getOperand(), SignHint);\n    return setRange(ZExt, SignHint,\n                    ConservativeResult.intersectWith(X.zeroExtend(BitWidth),\n                                                     RangeType));\n  }\n\n  if (const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(S)) {\n    ConstantRange X = getRangeRef(SExt->getOperand(), SignHint);\n    return setRange(SExt, SignHint,\n                    ConservativeResult.intersectWith(X.signExtend(BitWidth),\n                                                     RangeType));\n  }\n\n  if (const SCEVPtrToIntExpr *PtrToInt = dyn_cast<SCEVPtrToIntExpr>(S)) {\n    ConstantRange X = getRangeRef(PtrToInt->getOperand(), SignHint);\n    return setRange(PtrToInt, SignHint, X);\n  }\n\n  if (const SCEVTruncateExpr *Trunc = dyn_cast<SCEVTruncateExpr>(S)) {\n    ConstantRange X = getRangeRef(Trunc->getOperand(), SignHint);\n    return setRange(Trunc, SignHint,\n                    ConservativeResult.intersectWith(X.truncate(BitWidth),\n                                                     RangeType));\n  }\n\n  if (const SCEVAddRecExpr *AddRec = dyn_cast<SCEVAddRecExpr>(S)) {\n    // If there's no unsigned wrap, the value will never be less than its\n    // initial value.\n    if (AddRec->hasNoUnsignedWrap()) {\n      APInt UnsignedMinValue = getUnsignedRangeMin(AddRec->getStart());\n      if (!UnsignedMinValue.isNullValue())\n        ConservativeResult = ConservativeResult.intersectWith(\n            ConstantRange(UnsignedMinValue, APInt(BitWidth, 0)), RangeType);\n    }\n\n    // If there's no signed wrap, and all the operands except initial value have\n    // the same sign or zero, the value won't ever be:\n    // 1: smaller than initial value if operands are non negative,\n    // 2: bigger than initial value if operands are non positive.\n    // For both cases, value can not cross signed min/max boundary.\n    if (AddRec->hasNoSignedWrap()) {\n      bool AllNonNeg = true;\n      bool AllNonPos = true;\n      for (unsigned i = 1, e = AddRec->getNumOperands(); i != e; ++i) {\n        if (!isKnownNonNegative(AddRec->getOperand(i)))\n          AllNonNeg = false;\n        if (!isKnownNonPositive(AddRec->getOperand(i)))\n          AllNonPos = false;\n      }\n      if (AllNonNeg)\n        ConservativeResult = ConservativeResult.intersectWith(\n            ConstantRange::getNonEmpty(getSignedRangeMin(AddRec->getStart()),\n                                       APInt::getSignedMinValue(BitWidth)),\n            RangeType);\n      else if (AllNonPos)\n        ConservativeResult = ConservativeResult.intersectWith(\n            ConstantRange::getNonEmpty(\n                APInt::getSignedMinValue(BitWidth),\n                getSignedRangeMax(AddRec->getStart()) + 1),\n            RangeType);\n    }\n\n    // TODO: non-affine addrec\n    if (AddRec->isAffine()) {\n      const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(AddRec->getLoop());\n      if (!isa<SCEVCouldNotCompute>(MaxBECount) &&\n          getTypeSizeInBits(MaxBECount->getType()) <= BitWidth) {\n        auto RangeFromAffine = getRangeForAffineAR(\n            AddRec->getStart(), AddRec->getStepRecurrence(*this), MaxBECount,\n            BitWidth);\n        ConservativeResult =\n            ConservativeResult.intersectWith(RangeFromAffine, RangeType);\n\n        auto RangeFromFactoring = getRangeViaFactoring(\n            AddRec->getStart(), AddRec->getStepRecurrence(*this), MaxBECount,\n            BitWidth);\n        ConservativeResult =\n            ConservativeResult.intersectWith(RangeFromFactoring, RangeType);\n      }\n\n      // Now try symbolic BE count and more powerful methods.\n      if (UseExpensiveRangeSharpening) {\n        const SCEV *SymbolicMaxBECount =\n            getSymbolicMaxBackedgeTakenCount(AddRec->getLoop());\n        if (!isa<SCEVCouldNotCompute>(SymbolicMaxBECount) &&\n            getTypeSizeInBits(MaxBECount->getType()) <= BitWidth &&\n            AddRec->hasNoSelfWrap()) {\n          auto RangeFromAffineNew = getRangeForAffineNoSelfWrappingAR(\n              AddRec, SymbolicMaxBECount, BitWidth, SignHint);\n          ConservativeResult =\n              ConservativeResult.intersectWith(RangeFromAffineNew, RangeType);\n        }\n      }\n    }\n\n    return setRange(AddRec, SignHint, std::move(ConservativeResult));\n  }\n\n  if (const SCEVUnknown *U = dyn_cast<SCEVUnknown>(S)) {\n    // Check if the IR explicitly contains !range metadata.\n    Optional<ConstantRange> MDRange = GetRangeFromMetadata(U->getValue());\n    if (MDRange.hasValue())\n      ConservativeResult = ConservativeResult.intersectWith(MDRange.getValue(),\n                                                            RangeType);\n\n    // Split here to avoid paying the compile-time cost of calling both\n    // computeKnownBits and ComputeNumSignBits.  This restriction can be lifted\n    // if needed.\n    const DataLayout &DL = getDataLayout();\n    if (SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED) {\n      // For a SCEVUnknown, ask ValueTracking.\n      KnownBits Known = computeKnownBits(U->getValue(), DL, 0, &AC, nullptr, &DT);\n      if (Known.getBitWidth() != BitWidth)\n        Known = Known.zextOrTrunc(BitWidth);\n      // If Known does not result in full-set, intersect with it.\n      if (Known.getMinValue() != Known.getMaxValue() + 1)\n        ConservativeResult = ConservativeResult.intersectWith(\n            ConstantRange(Known.getMinValue(), Known.getMaxValue() + 1),\n            RangeType);\n    } else {\n      assert(SignHint == ScalarEvolution::HINT_RANGE_SIGNED &&\n             \"generalize as needed!\");\n      unsigned NS = ComputeNumSignBits(U->getValue(), DL, 0, &AC, nullptr, &DT);\n      // If the pointer size is larger than the index size type, this can cause\n      // NS to be larger than BitWidth. So compensate for this.\n      if (U->getType()->isPointerTy()) {\n        unsigned ptrSize = DL.getPointerTypeSizeInBits(U->getType());\n        int ptrIdxDiff = ptrSize - BitWidth;\n        if (ptrIdxDiff > 0 && ptrSize > BitWidth && NS > (unsigned)ptrIdxDiff)\n          NS -= ptrIdxDiff;\n      }\n\n      if (NS > 1)\n        ConservativeResult = ConservativeResult.intersectWith(\n            ConstantRange(APInt::getSignedMinValue(BitWidth).ashr(NS - 1),\n                          APInt::getSignedMaxValue(BitWidth).ashr(NS - 1) + 1),\n            RangeType);\n    }\n\n    // A range of Phi is a subset of union of all ranges of its input.\n    if (const PHINode *Phi = dyn_cast<PHINode>(U->getValue())) {\n      // Make sure that we do not run over cycled Phis.\n      if (PendingPhiRanges.insert(Phi).second) {\n        ConstantRange RangeFromOps(BitWidth, /*isFullSet=*/false);\n        for (auto &Op : Phi->operands()) {\n          auto OpRange = getRangeRef(getSCEV(Op), SignHint);\n          RangeFromOps = RangeFromOps.unionWith(OpRange);\n          // No point to continue if we already have a full set.\n          if (RangeFromOps.isFullSet())\n            break;\n        }\n        ConservativeResult =\n            ConservativeResult.intersectWith(RangeFromOps, RangeType);\n        bool Erased = PendingPhiRanges.erase(Phi);\n        assert(Erased && \"Failed to erase Phi properly?\");\n        (void) Erased;\n      }\n    }\n\n    return setRange(U, SignHint, std::move(ConservativeResult));\n  }\n\n  return setRange(S, SignHint, std::move(ConservativeResult));\n}\n\n// Given a StartRange, Step and MaxBECount for an expression compute a range of\n// values that the expression can take. Initially, the expression has a value\n// from StartRange and then is changed by Step up to MaxBECount times. Signed\n// argument defines if we treat Step as signed or unsigned.\nstatic ConstantRange getRangeForAffineARHelper(APInt Step,\n                                               const ConstantRange &StartRange,\n                                               const APInt &MaxBECount,\n                                               unsigned BitWidth, bool Signed) {\n  // If either Step or MaxBECount is 0, then the expression won't change, and we\n  // just need to return the initial range.\n  if (Step == 0 || MaxBECount == 0)\n    return StartRange;\n\n  // If we don't know anything about the initial value (i.e. StartRange is\n  // FullRange), then we don't know anything about the final range either.\n  // Return FullRange.\n  if (StartRange.isFullSet())\n    return ConstantRange::getFull(BitWidth);\n\n  // If Step is signed and negative, then we use its absolute value, but we also\n  // note that we're moving in the opposite direction.\n  bool Descending = Signed && Step.isNegative();\n\n  if (Signed)\n    // This is correct even for INT_SMIN. Let's look at i8 to illustrate this:\n    // abs(INT_SMIN) = abs(-128) = abs(0x80) = -0x80 = 0x80 = 128.\n    // This equations hold true due to the well-defined wrap-around behavior of\n    // APInt.\n    Step = Step.abs();\n\n  // Check if Offset is more than full span of BitWidth. If it is, the\n  // expression is guaranteed to overflow.\n  if (APInt::getMaxValue(StartRange.getBitWidth()).udiv(Step).ult(MaxBECount))\n    return ConstantRange::getFull(BitWidth);\n\n  // Offset is by how much the expression can change. Checks above guarantee no\n  // overflow here.\n  APInt Offset = Step * MaxBECount;\n\n  // Minimum value of the final range will match the minimal value of StartRange\n  // if the expression is increasing and will be decreased by Offset otherwise.\n  // Maximum value of the final range will match the maximal value of StartRange\n  // if the expression is decreasing and will be increased by Offset otherwise.\n  APInt StartLower = StartRange.getLower();\n  APInt StartUpper = StartRange.getUpper() - 1;\n  APInt MovedBoundary = Descending ? (StartLower - std::move(Offset))\n                                   : (StartUpper + std::move(Offset));\n\n  // It's possible that the new minimum/maximum value will fall into the initial\n  // range (due to wrap around). This means that the expression can take any\n  // value in this bitwidth, and we have to return full range.\n  if (StartRange.contains(MovedBoundary))\n    return ConstantRange::getFull(BitWidth);\n\n  APInt NewLower =\n      Descending ? std::move(MovedBoundary) : std::move(StartLower);\n  APInt NewUpper =\n      Descending ? std::move(StartUpper) : std::move(MovedBoundary);\n  NewUpper += 1;\n\n  // No overflow detected, return [StartLower, StartUpper + Offset + 1) range.\n  return ConstantRange::getNonEmpty(std::move(NewLower), std::move(NewUpper));\n}\n\nConstantRange ScalarEvolution::getRangeForAffineAR(const SCEV *Start,\n                                                   const SCEV *Step,\n                                                   const SCEV *MaxBECount,\n                                                   unsigned BitWidth) {\n  assert(!isa<SCEVCouldNotCompute>(MaxBECount) &&\n         getTypeSizeInBits(MaxBECount->getType()) <= BitWidth &&\n         \"Precondition!\");\n\n  MaxBECount = getNoopOrZeroExtend(MaxBECount, Start->getType());\n  APInt MaxBECountValue = getUnsignedRangeMax(MaxBECount);\n\n  // First, consider step signed.\n  ConstantRange StartSRange = getSignedRange(Start);\n  ConstantRange StepSRange = getSignedRange(Step);\n\n  // If Step can be both positive and negative, we need to find ranges for the\n  // maximum absolute step values in both directions and union them.\n  ConstantRange SR =\n      getRangeForAffineARHelper(StepSRange.getSignedMin(), StartSRange,\n                                MaxBECountValue, BitWidth, /* Signed = */ true);\n  SR = SR.unionWith(getRangeForAffineARHelper(StepSRange.getSignedMax(),\n                                              StartSRange, MaxBECountValue,\n                                              BitWidth, /* Signed = */ true));\n\n  // Next, consider step unsigned.\n  ConstantRange UR = getRangeForAffineARHelper(\n      getUnsignedRangeMax(Step), getUnsignedRange(Start),\n      MaxBECountValue, BitWidth, /* Signed = */ false);\n\n  // Finally, intersect signed and unsigned ranges.\n  return SR.intersectWith(UR, ConstantRange::Smallest);\n}\n\nConstantRange ScalarEvolution::getRangeForAffineNoSelfWrappingAR(\n    const SCEVAddRecExpr *AddRec, const SCEV *MaxBECount, unsigned BitWidth,\n    ScalarEvolution::RangeSignHint SignHint) {\n  assert(AddRec->isAffine() && \"Non-affine AddRecs are not suppored!\\n\");\n  assert(AddRec->hasNoSelfWrap() &&\n         \"This only works for non-self-wrapping AddRecs!\");\n  const bool IsSigned = SignHint == HINT_RANGE_SIGNED;\n  const SCEV *Step = AddRec->getStepRecurrence(*this);\n  // Only deal with constant step to save compile time.\n  if (!isa<SCEVConstant>(Step))\n    return ConstantRange::getFull(BitWidth);\n  // Let's make sure that we can prove that we do not self-wrap during\n  // MaxBECount iterations. We need this because MaxBECount is a maximum\n  // iteration count estimate, and we might infer nw from some exit for which we\n  // do not know max exit count (or any other side reasoning).\n  // TODO: Turn into assert at some point.\n  if (getTypeSizeInBits(MaxBECount->getType()) >\n      getTypeSizeInBits(AddRec->getType()))\n    return ConstantRange::getFull(BitWidth);\n  MaxBECount = getNoopOrZeroExtend(MaxBECount, AddRec->getType());\n  const SCEV *RangeWidth = getMinusOne(AddRec->getType());\n  const SCEV *StepAbs = getUMinExpr(Step, getNegativeSCEV(Step));\n  const SCEV *MaxItersWithoutWrap = getUDivExpr(RangeWidth, StepAbs);\n  if (!isKnownPredicateViaConstantRanges(ICmpInst::ICMP_ULE, MaxBECount,\n                                         MaxItersWithoutWrap))\n    return ConstantRange::getFull(BitWidth);\n\n  ICmpInst::Predicate LEPred =\n      IsSigned ? ICmpInst::ICMP_SLE : ICmpInst::ICMP_ULE;\n  ICmpInst::Predicate GEPred =\n      IsSigned ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE;\n  const SCEV *End = AddRec->evaluateAtIteration(MaxBECount, *this);\n\n  // We know that there is no self-wrap. Let's take Start and End values and\n  // look at all intermediate values V1, V2, ..., Vn that IndVar takes during\n  // the iteration. They either lie inside the range [Min(Start, End),\n  // Max(Start, End)] or outside it:\n  //\n  // Case 1:   RangeMin    ...    Start V1 ... VN End ...           RangeMax;\n  // Case 2:   RangeMin Vk ... V1 Start    ...    End Vn ... Vk + 1 RangeMax;\n  //\n  // No self wrap flag guarantees that the intermediate values cannot be BOTH\n  // outside and inside the range [Min(Start, End), Max(Start, End)]. Using that\n  // knowledge, let's try to prove that we are dealing with Case 1. It is so if\n  // Start <= End and step is positive, or Start >= End and step is negative.\n  const SCEV *Start = AddRec->getStart();\n  ConstantRange StartRange = getRangeRef(Start, SignHint);\n  ConstantRange EndRange = getRangeRef(End, SignHint);\n  ConstantRange RangeBetween = StartRange.unionWith(EndRange);\n  // If they already cover full iteration space, we will know nothing useful\n  // even if we prove what we want to prove.\n  if (RangeBetween.isFullSet())\n    return RangeBetween;\n  // Only deal with ranges that do not wrap (i.e. RangeMin < RangeMax).\n  bool IsWrappedSet = IsSigned ? RangeBetween.isSignWrappedSet()\n                               : RangeBetween.isWrappedSet();\n  if (IsWrappedSet)\n    return ConstantRange::getFull(BitWidth);\n\n  if (isKnownPositive(Step) &&\n      isKnownPredicateViaConstantRanges(LEPred, Start, End))\n    return RangeBetween;\n  else if (isKnownNegative(Step) &&\n           isKnownPredicateViaConstantRanges(GEPred, Start, End))\n    return RangeBetween;\n  return ConstantRange::getFull(BitWidth);\n}\n\nConstantRange ScalarEvolution::getRangeViaFactoring(const SCEV *Start,\n                                                    const SCEV *Step,\n                                                    const SCEV *MaxBECount,\n                                                    unsigned BitWidth) {\n  //    RangeOf({C?A:B,+,C?P:Q}) == RangeOf(C?{A,+,P}:{B,+,Q})\n  // == RangeOf({A,+,P}) union RangeOf({B,+,Q})\n\n  struct SelectPattern {\n    Value *Condition = nullptr;\n    APInt TrueValue;\n    APInt FalseValue;\n\n    explicit SelectPattern(ScalarEvolution &SE, unsigned BitWidth,\n                           const SCEV *S) {\n      Optional<unsigned> CastOp;\n      APInt Offset(BitWidth, 0);\n\n      assert(SE.getTypeSizeInBits(S->getType()) == BitWidth &&\n             \"Should be!\");\n\n      // Peel off a constant offset:\n      if (auto *SA = dyn_cast<SCEVAddExpr>(S)) {\n        // In the future we could consider being smarter here and handle\n        // {Start+Step,+,Step} too.\n        if (SA->getNumOperands() != 2 || !isa<SCEVConstant>(SA->getOperand(0)))\n          return;\n\n        Offset = cast<SCEVConstant>(SA->getOperand(0))->getAPInt();\n        S = SA->getOperand(1);\n      }\n\n      // Peel off a cast operation\n      if (auto *SCast = dyn_cast<SCEVIntegralCastExpr>(S)) {\n        CastOp = SCast->getSCEVType();\n        S = SCast->getOperand();\n      }\n\n      using namespace llvm::PatternMatch;\n\n      auto *SU = dyn_cast<SCEVUnknown>(S);\n      const APInt *TrueVal, *FalseVal;\n      if (!SU ||\n          !match(SU->getValue(), m_Select(m_Value(Condition), m_APInt(TrueVal),\n                                          m_APInt(FalseVal)))) {\n        Condition = nullptr;\n        return;\n      }\n\n      TrueValue = *TrueVal;\n      FalseValue = *FalseVal;\n\n      // Re-apply the cast we peeled off earlier\n      if (CastOp.hasValue())\n        switch (*CastOp) {\n        default:\n          llvm_unreachable(\"Unknown SCEV cast type!\");\n\n        case scTruncate:\n          TrueValue = TrueValue.trunc(BitWidth);\n          FalseValue = FalseValue.trunc(BitWidth);\n          break;\n        case scZeroExtend:\n          TrueValue = TrueValue.zext(BitWidth);\n          FalseValue = FalseValue.zext(BitWidth);\n          break;\n        case scSignExtend:\n          TrueValue = TrueValue.sext(BitWidth);\n          FalseValue = FalseValue.sext(BitWidth);\n          break;\n        }\n\n      // Re-apply the constant offset we peeled off earlier\n      TrueValue += Offset;\n      FalseValue += Offset;\n    }\n\n    bool isRecognized() { return Condition != nullptr; }\n  };\n\n  SelectPattern StartPattern(*this, BitWidth, Start);\n  if (!StartPattern.isRecognized())\n    return ConstantRange::getFull(BitWidth);\n\n  SelectPattern StepPattern(*this, BitWidth, Step);\n  if (!StepPattern.isRecognized())\n    return ConstantRange::getFull(BitWidth);\n\n  if (StartPattern.Condition != StepPattern.Condition) {\n    // We don't handle this case today; but we could, by considering four\n    // possibilities below instead of two. I'm not sure if there are cases where\n    // that will help over what getRange already does, though.\n    return ConstantRange::getFull(BitWidth);\n  }\n\n  // NB! Calling ScalarEvolution::getConstant is fine, but we should not try to\n  // construct arbitrary general SCEV expressions here.  This function is called\n  // from deep in the call stack, and calling getSCEV (on a sext instruction,\n  // say) can end up caching a suboptimal value.\n\n  // FIXME: without the explicit `this` receiver below, MSVC errors out with\n  // C2352 and C2512 (otherwise it isn't needed).\n\n  const SCEV *TrueStart = this->getConstant(StartPattern.TrueValue);\n  const SCEV *TrueStep = this->getConstant(StepPattern.TrueValue);\n  const SCEV *FalseStart = this->getConstant(StartPattern.FalseValue);\n  const SCEV *FalseStep = this->getConstant(StepPattern.FalseValue);\n\n  ConstantRange TrueRange =\n      this->getRangeForAffineAR(TrueStart, TrueStep, MaxBECount, BitWidth);\n  ConstantRange FalseRange =\n      this->getRangeForAffineAR(FalseStart, FalseStep, MaxBECount, BitWidth);\n\n  return TrueRange.unionWith(FalseRange);\n}\n\nSCEV::NoWrapFlags ScalarEvolution::getNoWrapFlagsFromUB(const Value *V) {\n  if (isa<ConstantExpr>(V)) return SCEV::FlagAnyWrap;\n  const BinaryOperator *BinOp = cast<BinaryOperator>(V);\n\n  // Return early if there are no flags to propagate to the SCEV.\n  SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap;\n  if (BinOp->hasNoUnsignedWrap())\n    Flags = ScalarEvolution::setFlags(Flags, SCEV::FlagNUW);\n  if (BinOp->hasNoSignedWrap())\n    Flags = ScalarEvolution::setFlags(Flags, SCEV::FlagNSW);\n  if (Flags == SCEV::FlagAnyWrap)\n    return SCEV::FlagAnyWrap;\n\n  return isSCEVExprNeverPoison(BinOp) ? Flags : SCEV::FlagAnyWrap;\n}\n\nbool ScalarEvolution::isSCEVExprNeverPoison(const Instruction *I) {\n  // Here we check that I is in the header of the innermost loop containing I,\n  // since we only deal with instructions in the loop header. The actual loop we\n  // need to check later will come from an add recurrence, but getting that\n  // requires computing the SCEV of the operands, which can be expensive. This\n  // check we can do cheaply to rule out some cases early.\n  Loop *InnermostContainingLoop = LI.getLoopFor(I->getParent());\n  if (InnermostContainingLoop == nullptr ||\n      InnermostContainingLoop->getHeader() != I->getParent())\n    return false;\n\n  // Only proceed if we can prove that I does not yield poison.\n  if (!programUndefinedIfPoison(I))\n    return false;\n\n  // At this point we know that if I is executed, then it does not wrap\n  // according to at least one of NSW or NUW. If I is not executed, then we do\n  // not know if the calculation that I represents would wrap. Multiple\n  // instructions can map to the same SCEV. If we apply NSW or NUW from I to\n  // the SCEV, we must guarantee no wrapping for that SCEV also when it is\n  // derived from other instructions that map to the same SCEV. We cannot make\n  // that guarantee for cases where I is not executed. So we need to find the\n  // loop that I is considered in relation to and prove that I is executed for\n  // every iteration of that loop. That implies that the value that I\n  // calculates does not wrap anywhere in the loop, so then we can apply the\n  // flags to the SCEV.\n  //\n  // We check isLoopInvariant to disambiguate in case we are adding recurrences\n  // from different loops, so that we know which loop to prove that I is\n  // executed in.\n  for (unsigned OpIndex = 0; OpIndex < I->getNumOperands(); ++OpIndex) {\n    // I could be an extractvalue from a call to an overflow intrinsic.\n    // TODO: We can do better here in some cases.\n    if (!isSCEVable(I->getOperand(OpIndex)->getType()))\n      return false;\n    const SCEV *Op = getSCEV(I->getOperand(OpIndex));\n    if (auto *AddRec = dyn_cast<SCEVAddRecExpr>(Op)) {\n      bool AllOtherOpsLoopInvariant = true;\n      for (unsigned OtherOpIndex = 0; OtherOpIndex < I->getNumOperands();\n           ++OtherOpIndex) {\n        if (OtherOpIndex != OpIndex) {\n          const SCEV *OtherOp = getSCEV(I->getOperand(OtherOpIndex));\n          if (!isLoopInvariant(OtherOp, AddRec->getLoop())) {\n            AllOtherOpsLoopInvariant = false;\n            break;\n          }\n        }\n      }\n      if (AllOtherOpsLoopInvariant &&\n          isGuaranteedToExecuteForEveryIteration(I, AddRec->getLoop()))\n        return true;\n    }\n  }\n  return false;\n}\n\nbool ScalarEvolution::isAddRecNeverPoison(const Instruction *I, const Loop *L) {\n  // If we know that \\c I can never be poison period, then that's enough.\n  if (isSCEVExprNeverPoison(I))\n    return true;\n\n  // For an add recurrence specifically, we assume that infinite loops without\n  // side effects are undefined behavior, and then reason as follows:\n  //\n  // If the add recurrence is poison in any iteration, it is poison on all\n  // future iterations (since incrementing poison yields poison). If the result\n  // of the add recurrence is fed into the loop latch condition and the loop\n  // does not contain any throws or exiting blocks other than the latch, we now\n  // have the ability to \"choose\" whether the backedge is taken or not (by\n  // choosing a sufficiently evil value for the poison feeding into the branch)\n  // for every iteration including and after the one in which \\p I first became\n  // poison.  There are two possibilities (let's call the iteration in which \\p\n  // I first became poison as K):\n  //\n  //  1. In the set of iterations including and after K, the loop body executes\n  //     no side effects.  In this case executing the backege an infinte number\n  //     of times will yield undefined behavior.\n  //\n  //  2. In the set of iterations including and after K, the loop body executes\n  //     at least one side effect.  In this case, that specific instance of side\n  //     effect is control dependent on poison, which also yields undefined\n  //     behavior.\n\n  auto *ExitingBB = L->getExitingBlock();\n  auto *LatchBB = L->getLoopLatch();\n  if (!ExitingBB || !LatchBB || ExitingBB != LatchBB)\n    return false;\n\n  SmallPtrSet<const Instruction *, 16> Pushed;\n  SmallVector<const Instruction *, 8> PoisonStack;\n\n  // We start by assuming \\c I, the post-inc add recurrence, is poison.  Only\n  // things that are known to be poison under that assumption go on the\n  // PoisonStack.\n  Pushed.insert(I);\n  PoisonStack.push_back(I);\n\n  bool LatchControlDependentOnPoison = false;\n  while (!PoisonStack.empty() && !LatchControlDependentOnPoison) {\n    const Instruction *Poison = PoisonStack.pop_back_val();\n\n    for (auto *PoisonUser : Poison->users()) {\n      if (propagatesPoison(cast<Operator>(PoisonUser))) {\n        if (Pushed.insert(cast<Instruction>(PoisonUser)).second)\n          PoisonStack.push_back(cast<Instruction>(PoisonUser));\n      } else if (auto *BI = dyn_cast<BranchInst>(PoisonUser)) {\n        assert(BI->isConditional() && \"Only possibility!\");\n        if (BI->getParent() == LatchBB) {\n          LatchControlDependentOnPoison = true;\n          break;\n        }\n      }\n    }\n  }\n\n  return LatchControlDependentOnPoison && loopHasNoAbnormalExits(L);\n}\n\nScalarEvolution::LoopProperties\nScalarEvolution::getLoopProperties(const Loop *L) {\n  using LoopProperties = ScalarEvolution::LoopProperties;\n\n  auto Itr = LoopPropertiesCache.find(L);\n  if (Itr == LoopPropertiesCache.end()) {\n    auto HasSideEffects = [](Instruction *I) {\n      if (auto *SI = dyn_cast<StoreInst>(I))\n        return !SI->isSimple();\n\n      return I->mayHaveSideEffects();\n    };\n\n    LoopProperties LP = {/* HasNoAbnormalExits */ true,\n                         /*HasNoSideEffects*/ true};\n\n    for (auto *BB : L->getBlocks())\n      for (auto &I : *BB) {\n        if (!isGuaranteedToTransferExecutionToSuccessor(&I))\n          LP.HasNoAbnormalExits = false;\n        if (HasSideEffects(&I))\n          LP.HasNoSideEffects = false;\n        if (!LP.HasNoAbnormalExits && !LP.HasNoSideEffects)\n          break; // We're already as pessimistic as we can get.\n      }\n\n    auto InsertPair = LoopPropertiesCache.insert({L, LP});\n    assert(InsertPair.second && \"We just checked!\");\n    Itr = InsertPair.first;\n  }\n\n  return Itr->second;\n}\n\nconst SCEV *ScalarEvolution::createSCEV(Value *V) {\n  if (!isSCEVable(V->getType()))\n    return getUnknown(V);\n\n  if (Instruction *I = dyn_cast<Instruction>(V)) {\n    // Don't attempt to analyze instructions in blocks that aren't\n    // reachable. Such instructions don't matter, and they aren't required\n    // to obey basic rules for definitions dominating uses which this\n    // analysis depends on.\n    if (!DT.isReachableFromEntry(I->getParent()))\n      return getUnknown(UndefValue::get(V->getType()));\n  } else if (ConstantInt *CI = dyn_cast<ConstantInt>(V))\n    return getConstant(CI);\n  else if (isa<ConstantPointerNull>(V))\n    // FIXME: we shouldn't special-case null pointer constant.\n    return getZero(V->getType());\n  else if (GlobalAlias *GA = dyn_cast<GlobalAlias>(V))\n    return GA->isInterposable() ? getUnknown(V) : getSCEV(GA->getAliasee());\n  else if (!isa<ConstantExpr>(V))\n    return getUnknown(V);\n\n  Operator *U = cast<Operator>(V);\n  if (auto BO = MatchBinaryOp(U, DT)) {\n    switch (BO->Opcode) {\n    case Instruction::Add: {\n      // The simple thing to do would be to just call getSCEV on both operands\n      // and call getAddExpr with the result. However if we're looking at a\n      // bunch of things all added together, this can be quite inefficient,\n      // because it leads to N-1 getAddExpr calls for N ultimate operands.\n      // Instead, gather up all the operands and make a single getAddExpr call.\n      // LLVM IR canonical form means we need only traverse the left operands.\n      SmallVector<const SCEV *, 4> AddOps;\n      do {\n        if (BO->Op) {\n          if (auto *OpSCEV = getExistingSCEV(BO->Op)) {\n            AddOps.push_back(OpSCEV);\n            break;\n          }\n\n          // If a NUW or NSW flag can be applied to the SCEV for this\n          // addition, then compute the SCEV for this addition by itself\n          // with a separate call to getAddExpr. We need to do that\n          // instead of pushing the operands of the addition onto AddOps,\n          // since the flags are only known to apply to this particular\n          // addition - they may not apply to other additions that can be\n          // formed with operands from AddOps.\n          const SCEV *RHS = getSCEV(BO->RHS);\n          SCEV::NoWrapFlags Flags = getNoWrapFlagsFromUB(BO->Op);\n          if (Flags != SCEV::FlagAnyWrap) {\n            const SCEV *LHS = getSCEV(BO->LHS);\n            if (BO->Opcode == Instruction::Sub)\n              AddOps.push_back(getMinusSCEV(LHS, RHS, Flags));\n            else\n              AddOps.push_back(getAddExpr(LHS, RHS, Flags));\n            break;\n          }\n        }\n\n        if (BO->Opcode == Instruction::Sub)\n          AddOps.push_back(getNegativeSCEV(getSCEV(BO->RHS)));\n        else\n          AddOps.push_back(getSCEV(BO->RHS));\n\n        auto NewBO = MatchBinaryOp(BO->LHS, DT);\n        if (!NewBO || (NewBO->Opcode != Instruction::Add &&\n                       NewBO->Opcode != Instruction::Sub)) {\n          AddOps.push_back(getSCEV(BO->LHS));\n          break;\n        }\n        BO = NewBO;\n      } while (true);\n\n      return getAddExpr(AddOps);\n    }\n\n    case Instruction::Mul: {\n      SmallVector<const SCEV *, 4> MulOps;\n      do {\n        if (BO->Op) {\n          if (auto *OpSCEV = getExistingSCEV(BO->Op)) {\n            MulOps.push_back(OpSCEV);\n            break;\n          }\n\n          SCEV::NoWrapFlags Flags = getNoWrapFlagsFromUB(BO->Op);\n          if (Flags != SCEV::FlagAnyWrap) {\n            MulOps.push_back(\n                getMulExpr(getSCEV(BO->LHS), getSCEV(BO->RHS), Flags));\n            break;\n          }\n        }\n\n        MulOps.push_back(getSCEV(BO->RHS));\n        auto NewBO = MatchBinaryOp(BO->LHS, DT);\n        if (!NewBO || NewBO->Opcode != Instruction::Mul) {\n          MulOps.push_back(getSCEV(BO->LHS));\n          break;\n        }\n        BO = NewBO;\n      } while (true);\n\n      return getMulExpr(MulOps);\n    }\n    case Instruction::UDiv:\n      return getUDivExpr(getSCEV(BO->LHS), getSCEV(BO->RHS));\n    case Instruction::URem:\n      return getURemExpr(getSCEV(BO->LHS), getSCEV(BO->RHS));\n    case Instruction::Sub: {\n      SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap;\n      if (BO->Op)\n        Flags = getNoWrapFlagsFromUB(BO->Op);\n      return getMinusSCEV(getSCEV(BO->LHS), getSCEV(BO->RHS), Flags);\n    }\n    case Instruction::And:\n      // For an expression like x&255 that merely masks off the high bits,\n      // use zext(trunc(x)) as the SCEV expression.\n      if (ConstantInt *CI = dyn_cast<ConstantInt>(BO->RHS)) {\n        if (CI->isZero())\n          return getSCEV(BO->RHS);\n        if (CI->isMinusOne())\n          return getSCEV(BO->LHS);\n        const APInt &A = CI->getValue();\n\n        // Instcombine's ShrinkDemandedConstant may strip bits out of\n        // constants, obscuring what would otherwise be a low-bits mask.\n        // Use computeKnownBits to compute what ShrinkDemandedConstant\n        // knew about to reconstruct a low-bits mask value.\n        unsigned LZ = A.countLeadingZeros();\n        unsigned TZ = A.countTrailingZeros();\n        unsigned BitWidth = A.getBitWidth();\n        KnownBits Known(BitWidth);\n        computeKnownBits(BO->LHS, Known, getDataLayout(),\n                         0, &AC, nullptr, &DT);\n\n        APInt EffectiveMask =\n            APInt::getLowBitsSet(BitWidth, BitWidth - LZ - TZ).shl(TZ);\n        if ((LZ != 0 || TZ != 0) && !((~A & ~Known.Zero) & EffectiveMask)) {\n          const SCEV *MulCount = getConstant(APInt::getOneBitSet(BitWidth, TZ));\n          const SCEV *LHS = getSCEV(BO->LHS);\n          const SCEV *ShiftedLHS = nullptr;\n          if (auto *LHSMul = dyn_cast<SCEVMulExpr>(LHS)) {\n            if (auto *OpC = dyn_cast<SCEVConstant>(LHSMul->getOperand(0))) {\n              // For an expression like (x * 8) & 8, simplify the multiply.\n              unsigned MulZeros = OpC->getAPInt().countTrailingZeros();\n              unsigned GCD = std::min(MulZeros, TZ);\n              APInt DivAmt = APInt::getOneBitSet(BitWidth, TZ - GCD);\n              SmallVector<const SCEV*, 4> MulOps;\n              MulOps.push_back(getConstant(OpC->getAPInt().lshr(GCD)));\n              MulOps.append(LHSMul->op_begin() + 1, LHSMul->op_end());\n              auto *NewMul = getMulExpr(MulOps, LHSMul->getNoWrapFlags());\n              ShiftedLHS = getUDivExpr(NewMul, getConstant(DivAmt));\n            }\n          }\n          if (!ShiftedLHS)\n            ShiftedLHS = getUDivExpr(LHS, MulCount);\n          return getMulExpr(\n              getZeroExtendExpr(\n                  getTruncateExpr(ShiftedLHS,\n                      IntegerType::get(getContext(), BitWidth - LZ - TZ)),\n                  BO->LHS->getType()),\n              MulCount);\n        }\n      }\n      break;\n\n    case Instruction::Or:\n      // If the RHS of the Or is a constant, we may have something like:\n      // X*4+1 which got turned into X*4|1.  Handle this as an Add so loop\n      // optimizations will transparently handle this case.\n      //\n      // In order for this transformation to be safe, the LHS must be of the\n      // form X*(2^n) and the Or constant must be less than 2^n.\n      if (ConstantInt *CI = dyn_cast<ConstantInt>(BO->RHS)) {\n        const SCEV *LHS = getSCEV(BO->LHS);\n        const APInt &CIVal = CI->getValue();\n        if (GetMinTrailingZeros(LHS) >=\n            (CIVal.getBitWidth() - CIVal.countLeadingZeros())) {\n          // Build a plain add SCEV.\n          return getAddExpr(LHS, getSCEV(CI),\n                            (SCEV::NoWrapFlags)(SCEV::FlagNUW | SCEV::FlagNSW));\n        }\n      }\n      break;\n\n    case Instruction::Xor:\n      if (ConstantInt *CI = dyn_cast<ConstantInt>(BO->RHS)) {\n        // If the RHS of xor is -1, then this is a not operation.\n        if (CI->isMinusOne())\n          return getNotSCEV(getSCEV(BO->LHS));\n\n        // Model xor(and(x, C), C) as and(~x, C), if C is a low-bits mask.\n        // This is a variant of the check for xor with -1, and it handles\n        // the case where instcombine has trimmed non-demanded bits out\n        // of an xor with -1.\n        if (auto *LBO = dyn_cast<BinaryOperator>(BO->LHS))\n          if (ConstantInt *LCI = dyn_cast<ConstantInt>(LBO->getOperand(1)))\n            if (LBO->getOpcode() == Instruction::And &&\n                LCI->getValue() == CI->getValue())\n              if (const SCEVZeroExtendExpr *Z =\n                      dyn_cast<SCEVZeroExtendExpr>(getSCEV(BO->LHS))) {\n                Type *UTy = BO->LHS->getType();\n                const SCEV *Z0 = Z->getOperand();\n                Type *Z0Ty = Z0->getType();\n                unsigned Z0TySize = getTypeSizeInBits(Z0Ty);\n\n                // If C is a low-bits mask, the zero extend is serving to\n                // mask off the high bits. Complement the operand and\n                // re-apply the zext.\n                if (CI->getValue().isMask(Z0TySize))\n                  return getZeroExtendExpr(getNotSCEV(Z0), UTy);\n\n                // If C is a single bit, it may be in the sign-bit position\n                // before the zero-extend. In this case, represent the xor\n                // using an add, which is equivalent, and re-apply the zext.\n                APInt Trunc = CI->getValue().trunc(Z0TySize);\n                if (Trunc.zext(getTypeSizeInBits(UTy)) == CI->getValue() &&\n                    Trunc.isSignMask())\n                  return getZeroExtendExpr(getAddExpr(Z0, getConstant(Trunc)),\n                                           UTy);\n              }\n      }\n      break;\n\n    case Instruction::Shl:\n      // Turn shift left of a constant amount into a multiply.\n      if (ConstantInt *SA = dyn_cast<ConstantInt>(BO->RHS)) {\n        uint32_t BitWidth = cast<IntegerType>(SA->getType())->getBitWidth();\n\n        // If the shift count is not less than the bitwidth, the result of\n        // the shift is undefined. Don't try to analyze it, because the\n        // resolution chosen here may differ from the resolution chosen in\n        // other parts of the compiler.\n        if (SA->getValue().uge(BitWidth))\n          break;\n\n        // We can safely preserve the nuw flag in all cases. It's also safe to\n        // turn a nuw nsw shl into a nuw nsw mul. However, nsw in isolation\n        // requires special handling. It can be preserved as long as we're not\n        // left shifting by bitwidth - 1.\n        auto Flags = SCEV::FlagAnyWrap;\n        if (BO->Op) {\n          auto MulFlags = getNoWrapFlagsFromUB(BO->Op);\n          if ((MulFlags & SCEV::FlagNSW) &&\n              ((MulFlags & SCEV::FlagNUW) || SA->getValue().ult(BitWidth - 1)))\n            Flags = (SCEV::NoWrapFlags)(Flags | SCEV::FlagNSW);\n          if (MulFlags & SCEV::FlagNUW)\n            Flags = (SCEV::NoWrapFlags)(Flags | SCEV::FlagNUW);\n        }\n\n        Constant *X = ConstantInt::get(\n            getContext(), APInt::getOneBitSet(BitWidth, SA->getZExtValue()));\n        return getMulExpr(getSCEV(BO->LHS), getSCEV(X), Flags);\n      }\n      break;\n\n    case Instruction::AShr: {\n      // AShr X, C, where C is a constant.\n      ConstantInt *CI = dyn_cast<ConstantInt>(BO->RHS);\n      if (!CI)\n        break;\n\n      Type *OuterTy = BO->LHS->getType();\n      uint64_t BitWidth = getTypeSizeInBits(OuterTy);\n      // If the shift count is not less than the bitwidth, the result of\n      // the shift is undefined. Don't try to analyze it, because the\n      // resolution chosen here may differ from the resolution chosen in\n      // other parts of the compiler.\n      if (CI->getValue().uge(BitWidth))\n        break;\n\n      if (CI->isZero())\n        return getSCEV(BO->LHS); // shift by zero --> noop\n\n      uint64_t AShrAmt = CI->getZExtValue();\n      Type *TruncTy = IntegerType::get(getContext(), BitWidth - AShrAmt);\n\n      Operator *L = dyn_cast<Operator>(BO->LHS);\n      if (L && L->getOpcode() == Instruction::Shl) {\n        // X = Shl A, n\n        // Y = AShr X, m\n        // Both n and m are constant.\n\n        const SCEV *ShlOp0SCEV = getSCEV(L->getOperand(0));\n        if (L->getOperand(1) == BO->RHS)\n          // For a two-shift sext-inreg, i.e. n = m,\n          // use sext(trunc(x)) as the SCEV expression.\n          return getSignExtendExpr(\n              getTruncateExpr(ShlOp0SCEV, TruncTy), OuterTy);\n\n        ConstantInt *ShlAmtCI = dyn_cast<ConstantInt>(L->getOperand(1));\n        if (ShlAmtCI && ShlAmtCI->getValue().ult(BitWidth)) {\n          uint64_t ShlAmt = ShlAmtCI->getZExtValue();\n          if (ShlAmt > AShrAmt) {\n            // When n > m, use sext(mul(trunc(x), 2^(n-m)))) as the SCEV\n            // expression. We already checked that ShlAmt < BitWidth, so\n            // the multiplier, 1 << (ShlAmt - AShrAmt), fits into TruncTy as\n            // ShlAmt - AShrAmt < Amt.\n            APInt Mul = APInt::getOneBitSet(BitWidth - AShrAmt,\n                                            ShlAmt - AShrAmt);\n            return getSignExtendExpr(\n                getMulExpr(getTruncateExpr(ShlOp0SCEV, TruncTy),\n                getConstant(Mul)), OuterTy);\n          }\n        }\n      }\n      if (BO->IsExact) {\n        // Given exact arithmetic in-bounds right-shift by a constant,\n        // we can lower it into:  (abs(x) EXACT/u (1<<C)) * signum(x)\n        const SCEV *X = getSCEV(BO->LHS);\n        const SCEV *AbsX = getAbsExpr(X, /*IsNSW=*/false);\n        APInt Mult = APInt::getOneBitSet(BitWidth, AShrAmt);\n        const SCEV *Div = getUDivExactExpr(AbsX, getConstant(Mult));\n        return getMulExpr(Div, getSignumExpr(X), SCEV::FlagNSW);\n      }\n      break;\n    }\n    }\n  }\n\n  switch (U->getOpcode()) {\n  case Instruction::Trunc:\n    return getTruncateExpr(getSCEV(U->getOperand(0)), U->getType());\n\n  case Instruction::ZExt:\n    return getZeroExtendExpr(getSCEV(U->getOperand(0)), U->getType());\n\n  case Instruction::SExt:\n    if (auto BO = MatchBinaryOp(U->getOperand(0), DT)) {\n      // The NSW flag of a subtract does not always survive the conversion to\n      // A + (-1)*B.  By pushing sign extension onto its operands we are much\n      // more likely to preserve NSW and allow later AddRec optimisations.\n      //\n      // NOTE: This is effectively duplicating this logic from getSignExtend:\n      //   sext((A + B + ...)<nsw>) --> (sext(A) + sext(B) + ...)<nsw>\n      // but by that point the NSW information has potentially been lost.\n      if (BO->Opcode == Instruction::Sub && BO->IsNSW) {\n        Type *Ty = U->getType();\n        auto *V1 = getSignExtendExpr(getSCEV(BO->LHS), Ty);\n        auto *V2 = getSignExtendExpr(getSCEV(BO->RHS), Ty);\n        return getMinusSCEV(V1, V2, SCEV::FlagNSW);\n      }\n    }\n    return getSignExtendExpr(getSCEV(U->getOperand(0)), U->getType());\n\n  case Instruction::BitCast:\n    // BitCasts are no-op casts so we just eliminate the cast.\n    if (isSCEVable(U->getType()) && isSCEVable(U->getOperand(0)->getType()))\n      return getSCEV(U->getOperand(0));\n    break;\n\n  case Instruction::PtrToInt: {\n    // Pointer to integer cast is straight-forward, so do model it.\n    Value *Ptr = U->getOperand(0);\n    const SCEV *Op = getSCEV(Ptr);\n    Type *DstIntTy = U->getType();\n    // SCEV doesn't have constant pointer expression type, but it supports\n    // nullptr constant (and only that one), which is modelled in SCEV as a\n    // zero integer constant. So just skip the ptrtoint cast for constants.\n    if (isa<SCEVConstant>(Op))\n      return getTruncateOrZeroExtend(Op, DstIntTy);\n    Type *PtrTy = Ptr->getType();\n    Type *IntPtrTy = getDataLayout().getIntPtrType(PtrTy);\n    // But only if effective SCEV (integer) type is wide enough to represent\n    // all possible pointer values.\n    if (getDataLayout().getTypeSizeInBits(getEffectiveSCEVType(PtrTy)) !=\n        getDataLayout().getTypeSizeInBits(IntPtrTy))\n      return getUnknown(V);\n    return getPtrToIntExpr(Op, DstIntTy);\n  }\n  case Instruction::IntToPtr:\n    // Just don't deal with inttoptr casts.\n    return getUnknown(V);\n\n  case Instruction::SDiv:\n    // If both operands are non-negative, this is just an udiv.\n    if (isKnownNonNegative(getSCEV(U->getOperand(0))) &&\n        isKnownNonNegative(getSCEV(U->getOperand(1))))\n      return getUDivExpr(getSCEV(U->getOperand(0)), getSCEV(U->getOperand(1)));\n    break;\n\n  case Instruction::SRem:\n    // If both operands are non-negative, this is just an urem.\n    if (isKnownNonNegative(getSCEV(U->getOperand(0))) &&\n        isKnownNonNegative(getSCEV(U->getOperand(1))))\n      return getURemExpr(getSCEV(U->getOperand(0)), getSCEV(U->getOperand(1)));\n    break;\n\n  case Instruction::GetElementPtr:\n    return createNodeForGEP(cast<GEPOperator>(U));\n\n  case Instruction::PHI:\n    return createNodeForPHI(cast<PHINode>(U));\n\n  case Instruction::Select:\n    // U can also be a select constant expr, which let fall through.  Since\n    // createNodeForSelect only works for a condition that is an `ICmpInst`, and\n    // constant expressions cannot have instructions as operands, we'd have\n    // returned getUnknown for a select constant expressions anyway.\n    if (isa<Instruction>(U))\n      return createNodeForSelectOrPHI(cast<Instruction>(U), U->getOperand(0),\n                                      U->getOperand(1), U->getOperand(2));\n    break;\n\n  case Instruction::Call:\n  case Instruction::Invoke:\n    if (Value *RV = cast<CallBase>(U)->getReturnedArgOperand())\n      return getSCEV(RV);\n\n    if (auto *II = dyn_cast<IntrinsicInst>(U)) {\n      switch (II->getIntrinsicID()) {\n      case Intrinsic::abs:\n        return getAbsExpr(\n            getSCEV(II->getArgOperand(0)),\n            /*IsNSW=*/cast<ConstantInt>(II->getArgOperand(1))->isOne());\n      case Intrinsic::umax:\n        return getUMaxExpr(getSCEV(II->getArgOperand(0)),\n                           getSCEV(II->getArgOperand(1)));\n      case Intrinsic::umin:\n        return getUMinExpr(getSCEV(II->getArgOperand(0)),\n                           getSCEV(II->getArgOperand(1)));\n      case Intrinsic::smax:\n        return getSMaxExpr(getSCEV(II->getArgOperand(0)),\n                           getSCEV(II->getArgOperand(1)));\n      case Intrinsic::smin:\n        return getSMinExpr(getSCEV(II->getArgOperand(0)),\n                           getSCEV(II->getArgOperand(1)));\n      case Intrinsic::usub_sat: {\n        const SCEV *X = getSCEV(II->getArgOperand(0));\n        const SCEV *Y = getSCEV(II->getArgOperand(1));\n        const SCEV *ClampedY = getUMinExpr(X, Y);\n        return getMinusSCEV(X, ClampedY, SCEV::FlagNUW);\n      }\n      case Intrinsic::uadd_sat: {\n        const SCEV *X = getSCEV(II->getArgOperand(0));\n        const SCEV *Y = getSCEV(II->getArgOperand(1));\n        const SCEV *ClampedX = getUMinExpr(X, getNotSCEV(Y));\n        return getAddExpr(ClampedX, Y, SCEV::FlagNUW);\n      }\n      case Intrinsic::start_loop_iterations:\n        // A start_loop_iterations is just equivalent to the first operand for\n        // SCEV purposes.\n        return getSCEV(II->getArgOperand(0));\n      default:\n        break;\n      }\n    }\n    break;\n  }\n\n  return getUnknown(V);\n}\n\n//===----------------------------------------------------------------------===//\n//                   Iteration Count Computation Code\n//\n\nstatic unsigned getConstantTripCount(const SCEVConstant *ExitCount) {\n  if (!ExitCount)\n    return 0;\n\n  ConstantInt *ExitConst = ExitCount->getValue();\n\n  // Guard against huge trip counts.\n  if (ExitConst->getValue().getActiveBits() > 32)\n    return 0;\n\n  // In case of integer overflow, this returns 0, which is correct.\n  return ((unsigned)ExitConst->getZExtValue()) + 1;\n}\n\nunsigned ScalarEvolution::getSmallConstantTripCount(const Loop *L) {\n  if (BasicBlock *ExitingBB = L->getExitingBlock())\n    return getSmallConstantTripCount(L, ExitingBB);\n\n  // No trip count information for multiple exits.\n  return 0;\n}\n\nunsigned\nScalarEvolution::getSmallConstantTripCount(const Loop *L,\n                                           const BasicBlock *ExitingBlock) {\n  assert(ExitingBlock && \"Must pass a non-null exiting block!\");\n  assert(L->isLoopExiting(ExitingBlock) &&\n         \"Exiting block must actually branch out of the loop!\");\n  const SCEVConstant *ExitCount =\n      dyn_cast<SCEVConstant>(getExitCount(L, ExitingBlock));\n  return getConstantTripCount(ExitCount);\n}\n\nunsigned ScalarEvolution::getSmallConstantMaxTripCount(const Loop *L) {\n  const auto *MaxExitCount =\n      dyn_cast<SCEVConstant>(getConstantMaxBackedgeTakenCount(L));\n  return getConstantTripCount(MaxExitCount);\n}\n\nunsigned ScalarEvolution::getSmallConstantTripMultiple(const Loop *L) {\n  if (BasicBlock *ExitingBB = L->getExitingBlock())\n    return getSmallConstantTripMultiple(L, ExitingBB);\n\n  // No trip multiple information for multiple exits.\n  return 0;\n}\n\n/// Returns the largest constant divisor of the trip count of this loop as a\n/// normal unsigned value, if possible. This means that the actual trip count is\n/// always a multiple of the returned value (don't forget the trip count could\n/// very well be zero as well!).\n///\n/// Returns 1 if the trip count is unknown or not guaranteed to be the\n/// multiple of a constant (which is also the case if the trip count is simply\n/// constant, use getSmallConstantTripCount for that case), Will also return 1\n/// if the trip count is very large (>= 2^32).\n///\n/// As explained in the comments for getSmallConstantTripCount, this assumes\n/// that control exits the loop via ExitingBlock.\nunsigned\nScalarEvolution::getSmallConstantTripMultiple(const Loop *L,\n                                              const BasicBlock *ExitingBlock) {\n  assert(ExitingBlock && \"Must pass a non-null exiting block!\");\n  assert(L->isLoopExiting(ExitingBlock) &&\n         \"Exiting block must actually branch out of the loop!\");\n  const SCEV *ExitCount = getExitCount(L, ExitingBlock);\n  if (ExitCount == getCouldNotCompute())\n    return 1;\n\n  // Get the trip count from the BE count by adding 1.\n  const SCEV *TCExpr = getAddExpr(ExitCount, getOne(ExitCount->getType()));\n\n  const SCEVConstant *TC = dyn_cast<SCEVConstant>(TCExpr);\n  if (!TC)\n    // Attempt to factor more general cases. Returns the greatest power of\n    // two divisor. If overflow happens, the trip count expression is still\n    // divisible by the greatest power of 2 divisor returned.\n    return 1U << std::min((uint32_t)31,\n                          GetMinTrailingZeros(applyLoopGuards(TCExpr, L)));\n\n  ConstantInt *Result = TC->getValue();\n\n  // Guard against huge trip counts (this requires checking\n  // for zero to handle the case where the trip count == -1 and the\n  // addition wraps).\n  if (!Result || Result->getValue().getActiveBits() > 32 ||\n      Result->getValue().getActiveBits() == 0)\n    return 1;\n\n  return (unsigned)Result->getZExtValue();\n}\n\nconst SCEV *ScalarEvolution::getExitCount(const Loop *L,\n                                          const BasicBlock *ExitingBlock,\n                                          ExitCountKind Kind) {\n  switch (Kind) {\n  case Exact:\n  case SymbolicMaximum:\n    return getBackedgeTakenInfo(L).getExact(ExitingBlock, this);\n  case ConstantMaximum:\n    return getBackedgeTakenInfo(L).getConstantMax(ExitingBlock, this);\n  };\n  llvm_unreachable(\"Invalid ExitCountKind!\");\n}\n\nconst SCEV *\nScalarEvolution::getPredicatedBackedgeTakenCount(const Loop *L,\n                                                 SCEVUnionPredicate &Preds) {\n  return getPredicatedBackedgeTakenInfo(L).getExact(L, this, &Preds);\n}\n\nconst SCEV *ScalarEvolution::getBackedgeTakenCount(const Loop *L,\n                                                   ExitCountKind Kind) {\n  switch (Kind) {\n  case Exact:\n    return getBackedgeTakenInfo(L).getExact(L, this);\n  case ConstantMaximum:\n    return getBackedgeTakenInfo(L).getConstantMax(this);\n  case SymbolicMaximum:\n    return getBackedgeTakenInfo(L).getSymbolicMax(L, this);\n  };\n  llvm_unreachable(\"Invalid ExitCountKind!\");\n}\n\nbool ScalarEvolution::isBackedgeTakenCountMaxOrZero(const Loop *L) {\n  return getBackedgeTakenInfo(L).isConstantMaxOrZero(this);\n}\n\n/// Push PHI nodes in the header of the given loop onto the given Worklist.\nstatic void\nPushLoopPHIs(const Loop *L, SmallVectorImpl<Instruction *> &Worklist) {\n  BasicBlock *Header = L->getHeader();\n\n  // Push all Loop-header PHIs onto the Worklist stack.\n  for (PHINode &PN : Header->phis())\n    Worklist.push_back(&PN);\n}\n\nconst ScalarEvolution::BackedgeTakenInfo &\nScalarEvolution::getPredicatedBackedgeTakenInfo(const Loop *L) {\n  auto &BTI = getBackedgeTakenInfo(L);\n  if (BTI.hasFullInfo())\n    return BTI;\n\n  auto Pair = PredicatedBackedgeTakenCounts.insert({L, BackedgeTakenInfo()});\n\n  if (!Pair.second)\n    return Pair.first->second;\n\n  BackedgeTakenInfo Result =\n      computeBackedgeTakenCount(L, /*AllowPredicates=*/true);\n\n  return PredicatedBackedgeTakenCounts.find(L)->second = std::move(Result);\n}\n\nScalarEvolution::BackedgeTakenInfo &\nScalarEvolution::getBackedgeTakenInfo(const Loop *L) {\n  // Initially insert an invalid entry for this loop. If the insertion\n  // succeeds, proceed to actually compute a backedge-taken count and\n  // update the value. The temporary CouldNotCompute value tells SCEV\n  // code elsewhere that it shouldn't attempt to request a new\n  // backedge-taken count, which could result in infinite recursion.\n  std::pair<DenseMap<const Loop *, BackedgeTakenInfo>::iterator, bool> Pair =\n      BackedgeTakenCounts.insert({L, BackedgeTakenInfo()});\n  if (!Pair.second)\n    return Pair.first->second;\n\n  // computeBackedgeTakenCount may allocate memory for its result. Inserting it\n  // into the BackedgeTakenCounts map transfers ownership. Otherwise, the result\n  // must be cleared in this scope.\n  BackedgeTakenInfo Result = computeBackedgeTakenCount(L);\n\n  // In product build, there are no usage of statistic.\n  (void)NumTripCountsComputed;\n  (void)NumTripCountsNotComputed;\n#if LLVM_ENABLE_STATS || !defined(NDEBUG)\n  const SCEV *BEExact = Result.getExact(L, this);\n  if (BEExact != getCouldNotCompute()) {\n    assert(isLoopInvariant(BEExact, L) &&\n           isLoopInvariant(Result.getConstantMax(this), L) &&\n           \"Computed backedge-taken count isn't loop invariant for loop!\");\n    ++NumTripCountsComputed;\n  } else if (Result.getConstantMax(this) == getCouldNotCompute() &&\n             isa<PHINode>(L->getHeader()->begin())) {\n    // Only count loops that have phi nodes as not being computable.\n    ++NumTripCountsNotComputed;\n  }\n#endif // LLVM_ENABLE_STATS || !defined(NDEBUG)\n\n  // Now that we know more about the trip count for this loop, forget any\n  // existing SCEV values for PHI nodes in this loop since they are only\n  // conservative estimates made without the benefit of trip count\n  // information. This is similar to the code in forgetLoop, except that\n  // it handles SCEVUnknown PHI nodes specially.\n  if (Result.hasAnyInfo()) {\n    SmallVector<Instruction *, 16> Worklist;\n    PushLoopPHIs(L, Worklist);\n\n    SmallPtrSet<Instruction *, 8> Discovered;\n    while (!Worklist.empty()) {\n      Instruction *I = Worklist.pop_back_val();\n\n      ValueExprMapType::iterator It =\n        ValueExprMap.find_as(static_cast<Value *>(I));\n      if (It != ValueExprMap.end()) {\n        const SCEV *Old = It->second;\n\n        // SCEVUnknown for a PHI either means that it has an unrecognized\n        // structure, or it's a PHI that's in the progress of being computed\n        // by createNodeForPHI.  In the former case, additional loop trip\n        // count information isn't going to change anything. In the later\n        // case, createNodeForPHI will perform the necessary updates on its\n        // own when it gets to that point.\n        if (!isa<PHINode>(I) || !isa<SCEVUnknown>(Old)) {\n          eraseValueFromMap(It->first);\n          forgetMemoizedResults(Old);\n        }\n        if (PHINode *PN = dyn_cast<PHINode>(I))\n          ConstantEvolutionLoopExitValue.erase(PN);\n      }\n\n      // Since we don't need to invalidate anything for correctness and we're\n      // only invalidating to make SCEV's results more precise, we get to stop\n      // early to avoid invalidating too much.  This is especially important in\n      // cases like:\n      //\n      //   %v = f(pn0, pn1) // pn0 and pn1 used through some other phi node\n      // loop0:\n      //   %pn0 = phi\n      //   ...\n      // loop1:\n      //   %pn1 = phi\n      //   ...\n      //\n      // where both loop0 and loop1's backedge taken count uses the SCEV\n      // expression for %v.  If we don't have the early stop below then in cases\n      // like the above, getBackedgeTakenInfo(loop1) will clear out the trip\n      // count for loop0 and getBackedgeTakenInfo(loop0) will clear out the trip\n      // count for loop1, effectively nullifying SCEV's trip count cache.\n      for (auto *U : I->users())\n        if (auto *I = dyn_cast<Instruction>(U)) {\n          auto *LoopForUser = LI.getLoopFor(I->getParent());\n          if (LoopForUser && L->contains(LoopForUser) &&\n              Discovered.insert(I).second)\n            Worklist.push_back(I);\n        }\n    }\n  }\n\n  // Re-lookup the insert position, since the call to\n  // computeBackedgeTakenCount above could result in a\n  // recusive call to getBackedgeTakenInfo (on a different\n  // loop), which would invalidate the iterator computed\n  // earlier.\n  return BackedgeTakenCounts.find(L)->second = std::move(Result);\n}\n\nvoid ScalarEvolution::forgetAllLoops() {\n  // This method is intended to forget all info about loops. It should\n  // invalidate caches as if the following happened:\n  // - The trip counts of all loops have changed arbitrarily\n  // - Every llvm::Value has been updated in place to produce a different\n  // result.\n  BackedgeTakenCounts.clear();\n  PredicatedBackedgeTakenCounts.clear();\n  LoopPropertiesCache.clear();\n  ConstantEvolutionLoopExitValue.clear();\n  ValueExprMap.clear();\n  ValuesAtScopes.clear();\n  LoopDispositions.clear();\n  BlockDispositions.clear();\n  UnsignedRanges.clear();\n  SignedRanges.clear();\n  ExprValueMap.clear();\n  HasRecMap.clear();\n  MinTrailingZerosCache.clear();\n  PredicatedSCEVRewrites.clear();\n}\n\nvoid ScalarEvolution::forgetLoop(const Loop *L) {\n  // Drop any stored trip count value.\n  auto RemoveLoopFromBackedgeMap =\n      [](DenseMap<const Loop *, BackedgeTakenInfo> &Map, const Loop *L) {\n        auto BTCPos = Map.find(L);\n        if (BTCPos != Map.end()) {\n          BTCPos->second.clear();\n          Map.erase(BTCPos);\n        }\n      };\n\n  SmallVector<const Loop *, 16> LoopWorklist(1, L);\n  SmallVector<Instruction *, 32> Worklist;\n  SmallPtrSet<Instruction *, 16> Visited;\n\n  // Iterate over all the loops and sub-loops to drop SCEV information.\n  while (!LoopWorklist.empty()) {\n    auto *CurrL = LoopWorklist.pop_back_val();\n\n    RemoveLoopFromBackedgeMap(BackedgeTakenCounts, CurrL);\n    RemoveLoopFromBackedgeMap(PredicatedBackedgeTakenCounts, CurrL);\n\n    // Drop information about predicated SCEV rewrites for this loop.\n    for (auto I = PredicatedSCEVRewrites.begin();\n         I != PredicatedSCEVRewrites.end();) {\n      std::pair<const SCEV *, const Loop *> Entry = I->first;\n      if (Entry.second == CurrL)\n        PredicatedSCEVRewrites.erase(I++);\n      else\n        ++I;\n    }\n\n    auto LoopUsersItr = LoopUsers.find(CurrL);\n    if (LoopUsersItr != LoopUsers.end()) {\n      for (auto *S : LoopUsersItr->second)\n        forgetMemoizedResults(S);\n      LoopUsers.erase(LoopUsersItr);\n    }\n\n    // Drop information about expressions based on loop-header PHIs.\n    PushLoopPHIs(CurrL, Worklist);\n\n    while (!Worklist.empty()) {\n      Instruction *I = Worklist.pop_back_val();\n      if (!Visited.insert(I).second)\n        continue;\n\n      ValueExprMapType::iterator It =\n          ValueExprMap.find_as(static_cast<Value *>(I));\n      if (It != ValueExprMap.end()) {\n        eraseValueFromMap(It->first);\n        forgetMemoizedResults(It->second);\n        if (PHINode *PN = dyn_cast<PHINode>(I))\n          ConstantEvolutionLoopExitValue.erase(PN);\n      }\n\n      PushDefUseChildren(I, Worklist);\n    }\n\n    LoopPropertiesCache.erase(CurrL);\n    // Forget all contained loops too, to avoid dangling entries in the\n    // ValuesAtScopes map.\n    LoopWorklist.append(CurrL->begin(), CurrL->end());\n  }\n}\n\nvoid ScalarEvolution::forgetTopmostLoop(const Loop *L) {\n  while (Loop *Parent = L->getParentLoop())\n    L = Parent;\n  forgetLoop(L);\n}\n\nvoid ScalarEvolution::forgetValue(Value *V) {\n  Instruction *I = dyn_cast<Instruction>(V);\n  if (!I) return;\n\n  // Drop information about expressions based on loop-header PHIs.\n  SmallVector<Instruction *, 16> Worklist;\n  Worklist.push_back(I);\n\n  SmallPtrSet<Instruction *, 8> Visited;\n  while (!Worklist.empty()) {\n    I = Worklist.pop_back_val();\n    if (!Visited.insert(I).second)\n      continue;\n\n    ValueExprMapType::iterator It =\n      ValueExprMap.find_as(static_cast<Value *>(I));\n    if (It != ValueExprMap.end()) {\n      eraseValueFromMap(It->first);\n      forgetMemoizedResults(It->second);\n      if (PHINode *PN = dyn_cast<PHINode>(I))\n        ConstantEvolutionLoopExitValue.erase(PN);\n    }\n\n    PushDefUseChildren(I, Worklist);\n  }\n}\n\nvoid ScalarEvolution::forgetLoopDispositions(const Loop *L) {\n  LoopDispositions.clear();\n}\n\n/// Get the exact loop backedge taken count considering all loop exits. A\n/// computable result can only be returned for loops with all exiting blocks\n/// dominating the latch. howFarToZero assumes that the limit of each loop test\n/// is never skipped. This is a valid assumption as long as the loop exits via\n/// that test. For precise results, it is the caller's responsibility to specify\n/// the relevant loop exiting block using getExact(ExitingBlock, SE).\nconst SCEV *\nScalarEvolution::BackedgeTakenInfo::getExact(const Loop *L, ScalarEvolution *SE,\n                                             SCEVUnionPredicate *Preds) const {\n  // If any exits were not computable, the loop is not computable.\n  if (!isComplete() || ExitNotTaken.empty())\n    return SE->getCouldNotCompute();\n\n  const BasicBlock *Latch = L->getLoopLatch();\n  // All exiting blocks we have collected must dominate the only backedge.\n  if (!Latch)\n    return SE->getCouldNotCompute();\n\n  // All exiting blocks we have gathered dominate loop's latch, so exact trip\n  // count is simply a minimum out of all these calculated exit counts.\n  SmallVector<const SCEV *, 2> Ops;\n  for (auto &ENT : ExitNotTaken) {\n    const SCEV *BECount = ENT.ExactNotTaken;\n    assert(BECount != SE->getCouldNotCompute() && \"Bad exit SCEV!\");\n    assert(SE->DT.dominates(ENT.ExitingBlock, Latch) &&\n           \"We should only have known counts for exiting blocks that dominate \"\n           \"latch!\");\n\n    Ops.push_back(BECount);\n\n    if (Preds && !ENT.hasAlwaysTruePredicate())\n      Preds->add(ENT.Predicate.get());\n\n    assert((Preds || ENT.hasAlwaysTruePredicate()) &&\n           \"Predicate should be always true!\");\n  }\n\n  return SE->getUMinFromMismatchedTypes(Ops);\n}\n\n/// Get the exact not taken count for this loop exit.\nconst SCEV *\nScalarEvolution::BackedgeTakenInfo::getExact(const BasicBlock *ExitingBlock,\n                                             ScalarEvolution *SE) const {\n  for (auto &ENT : ExitNotTaken)\n    if (ENT.ExitingBlock == ExitingBlock && ENT.hasAlwaysTruePredicate())\n      return ENT.ExactNotTaken;\n\n  return SE->getCouldNotCompute();\n}\n\nconst SCEV *ScalarEvolution::BackedgeTakenInfo::getConstantMax(\n    const BasicBlock *ExitingBlock, ScalarEvolution *SE) const {\n  for (auto &ENT : ExitNotTaken)\n    if (ENT.ExitingBlock == ExitingBlock && ENT.hasAlwaysTruePredicate())\n      return ENT.MaxNotTaken;\n\n  return SE->getCouldNotCompute();\n}\n\n/// getConstantMax - Get the constant max backedge taken count for the loop.\nconst SCEV *\nScalarEvolution::BackedgeTakenInfo::getConstantMax(ScalarEvolution *SE) const {\n  auto PredicateNotAlwaysTrue = [](const ExitNotTakenInfo &ENT) {\n    return !ENT.hasAlwaysTruePredicate();\n  };\n\n  if (any_of(ExitNotTaken, PredicateNotAlwaysTrue) || !getConstantMax())\n    return SE->getCouldNotCompute();\n\n  assert((isa<SCEVCouldNotCompute>(getConstantMax()) ||\n          isa<SCEVConstant>(getConstantMax())) &&\n         \"No point in having a non-constant max backedge taken count!\");\n  return getConstantMax();\n}\n\nconst SCEV *\nScalarEvolution::BackedgeTakenInfo::getSymbolicMax(const Loop *L,\n                                                   ScalarEvolution *SE) {\n  if (!SymbolicMax)\n    SymbolicMax = SE->computeSymbolicMaxBackedgeTakenCount(L);\n  return SymbolicMax;\n}\n\nbool ScalarEvolution::BackedgeTakenInfo::isConstantMaxOrZero(\n    ScalarEvolution *SE) const {\n  auto PredicateNotAlwaysTrue = [](const ExitNotTakenInfo &ENT) {\n    return !ENT.hasAlwaysTruePredicate();\n  };\n  return MaxOrZero && !any_of(ExitNotTaken, PredicateNotAlwaysTrue);\n}\n\nbool ScalarEvolution::BackedgeTakenInfo::hasOperand(const SCEV *S,\n                                                    ScalarEvolution *SE) const {\n  if (getConstantMax() && getConstantMax() != SE->getCouldNotCompute() &&\n      SE->hasOperand(getConstantMax(), S))\n    return true;\n\n  for (auto &ENT : ExitNotTaken)\n    if (ENT.ExactNotTaken != SE->getCouldNotCompute() &&\n        SE->hasOperand(ENT.ExactNotTaken, S))\n      return true;\n\n  return false;\n}\n\nScalarEvolution::ExitLimit::ExitLimit(const SCEV *E)\n    : ExactNotTaken(E), MaxNotTaken(E) {\n  assert((isa<SCEVCouldNotCompute>(MaxNotTaken) ||\n          isa<SCEVConstant>(MaxNotTaken)) &&\n         \"No point in having a non-constant max backedge taken count!\");\n}\n\nScalarEvolution::ExitLimit::ExitLimit(\n    const SCEV *E, const SCEV *M, bool MaxOrZero,\n    ArrayRef<const SmallPtrSetImpl<const SCEVPredicate *> *> PredSetList)\n    : ExactNotTaken(E), MaxNotTaken(M), MaxOrZero(MaxOrZero) {\n  assert((isa<SCEVCouldNotCompute>(ExactNotTaken) ||\n          !isa<SCEVCouldNotCompute>(MaxNotTaken)) &&\n         \"Exact is not allowed to be less precise than Max\");\n  assert((isa<SCEVCouldNotCompute>(MaxNotTaken) ||\n          isa<SCEVConstant>(MaxNotTaken)) &&\n         \"No point in having a non-constant max backedge taken count!\");\n  for (auto *PredSet : PredSetList)\n    for (auto *P : *PredSet)\n      addPredicate(P);\n}\n\nScalarEvolution::ExitLimit::ExitLimit(\n    const SCEV *E, const SCEV *M, bool MaxOrZero,\n    const SmallPtrSetImpl<const SCEVPredicate *> &PredSet)\n    : ExitLimit(E, M, MaxOrZero, {&PredSet}) {\n  assert((isa<SCEVCouldNotCompute>(MaxNotTaken) ||\n          isa<SCEVConstant>(MaxNotTaken)) &&\n         \"No point in having a non-constant max backedge taken count!\");\n}\n\nScalarEvolution::ExitLimit::ExitLimit(const SCEV *E, const SCEV *M,\n                                      bool MaxOrZero)\n    : ExitLimit(E, M, MaxOrZero, None) {\n  assert((isa<SCEVCouldNotCompute>(MaxNotTaken) ||\n          isa<SCEVConstant>(MaxNotTaken)) &&\n         \"No point in having a non-constant max backedge taken count!\");\n}\n\n/// Allocate memory for BackedgeTakenInfo and copy the not-taken count of each\n/// computable exit into a persistent ExitNotTakenInfo array.\nScalarEvolution::BackedgeTakenInfo::BackedgeTakenInfo(\n    ArrayRef<ScalarEvolution::BackedgeTakenInfo::EdgeExitInfo> ExitCounts,\n    bool IsComplete, const SCEV *ConstantMax, bool MaxOrZero)\n    : ConstantMax(ConstantMax), IsComplete(IsComplete), MaxOrZero(MaxOrZero) {\n  using EdgeExitInfo = ScalarEvolution::BackedgeTakenInfo::EdgeExitInfo;\n\n  ExitNotTaken.reserve(ExitCounts.size());\n  std::transform(\n      ExitCounts.begin(), ExitCounts.end(), std::back_inserter(ExitNotTaken),\n      [&](const EdgeExitInfo &EEI) {\n        BasicBlock *ExitBB = EEI.first;\n        const ExitLimit &EL = EEI.second;\n        if (EL.Predicates.empty())\n          return ExitNotTakenInfo(ExitBB, EL.ExactNotTaken, EL.MaxNotTaken,\n                                  nullptr);\n\n        std::unique_ptr<SCEVUnionPredicate> Predicate(new SCEVUnionPredicate);\n        for (auto *Pred : EL.Predicates)\n          Predicate->add(Pred);\n\n        return ExitNotTakenInfo(ExitBB, EL.ExactNotTaken, EL.MaxNotTaken,\n                                std::move(Predicate));\n      });\n  assert((isa<SCEVCouldNotCompute>(ConstantMax) ||\n          isa<SCEVConstant>(ConstantMax)) &&\n         \"No point in having a non-constant max backedge taken count!\");\n}\n\n/// Invalidate this result and free the ExitNotTakenInfo array.\nvoid ScalarEvolution::BackedgeTakenInfo::clear() {\n  ExitNotTaken.clear();\n}\n\n/// Compute the number of times the backedge of the specified loop will execute.\nScalarEvolution::BackedgeTakenInfo\nScalarEvolution::computeBackedgeTakenCount(const Loop *L,\n                                           bool AllowPredicates) {\n  SmallVector<BasicBlock *, 8> ExitingBlocks;\n  L->getExitingBlocks(ExitingBlocks);\n\n  using EdgeExitInfo = ScalarEvolution::BackedgeTakenInfo::EdgeExitInfo;\n\n  SmallVector<EdgeExitInfo, 4> ExitCounts;\n  bool CouldComputeBECount = true;\n  BasicBlock *Latch = L->getLoopLatch(); // may be NULL.\n  const SCEV *MustExitMaxBECount = nullptr;\n  const SCEV *MayExitMaxBECount = nullptr;\n  bool MustExitMaxOrZero = false;\n\n  // Compute the ExitLimit for each loop exit. Use this to populate ExitCounts\n  // and compute maxBECount.\n  // Do a union of all the predicates here.\n  for (unsigned i = 0, e = ExitingBlocks.size(); i != e; ++i) {\n    BasicBlock *ExitBB = ExitingBlocks[i];\n\n    // We canonicalize untaken exits to br (constant), ignore them so that\n    // proving an exit untaken doesn't negatively impact our ability to reason\n    // about the loop as whole.\n    if (auto *BI = dyn_cast<BranchInst>(ExitBB->getTerminator()))\n      if (auto *CI = dyn_cast<ConstantInt>(BI->getCondition())) {\n        bool ExitIfTrue = !L->contains(BI->getSuccessor(0));\n        if ((ExitIfTrue && CI->isZero()) || (!ExitIfTrue && CI->isOne()))\n          continue;\n      }\n\n    ExitLimit EL = computeExitLimit(L, ExitBB, AllowPredicates);\n\n    assert((AllowPredicates || EL.Predicates.empty()) &&\n           \"Predicated exit limit when predicates are not allowed!\");\n\n    // 1. For each exit that can be computed, add an entry to ExitCounts.\n    // CouldComputeBECount is true only if all exits can be computed.\n    if (EL.ExactNotTaken == getCouldNotCompute())\n      // We couldn't compute an exact value for this exit, so\n      // we won't be able to compute an exact value for the loop.\n      CouldComputeBECount = false;\n    else\n      ExitCounts.emplace_back(ExitBB, EL);\n\n    // 2. Derive the loop's MaxBECount from each exit's max number of\n    // non-exiting iterations. Partition the loop exits into two kinds:\n    // LoopMustExits and LoopMayExits.\n    //\n    // If the exit dominates the loop latch, it is a LoopMustExit otherwise it\n    // is a LoopMayExit.  If any computable LoopMustExit is found, then\n    // MaxBECount is the minimum EL.MaxNotTaken of computable\n    // LoopMustExits. Otherwise, MaxBECount is conservatively the maximum\n    // EL.MaxNotTaken, where CouldNotCompute is considered greater than any\n    // computable EL.MaxNotTaken.\n    if (EL.MaxNotTaken != getCouldNotCompute() && Latch &&\n        DT.dominates(ExitBB, Latch)) {\n      if (!MustExitMaxBECount) {\n        MustExitMaxBECount = EL.MaxNotTaken;\n        MustExitMaxOrZero = EL.MaxOrZero;\n      } else {\n        MustExitMaxBECount =\n            getUMinFromMismatchedTypes(MustExitMaxBECount, EL.MaxNotTaken);\n      }\n    } else if (MayExitMaxBECount != getCouldNotCompute()) {\n      if (!MayExitMaxBECount || EL.MaxNotTaken == getCouldNotCompute())\n        MayExitMaxBECount = EL.MaxNotTaken;\n      else {\n        MayExitMaxBECount =\n            getUMaxFromMismatchedTypes(MayExitMaxBECount, EL.MaxNotTaken);\n      }\n    }\n  }\n  const SCEV *MaxBECount = MustExitMaxBECount ? MustExitMaxBECount :\n    (MayExitMaxBECount ? MayExitMaxBECount : getCouldNotCompute());\n  // The loop backedge will be taken the maximum or zero times if there's\n  // a single exit that must be taken the maximum or zero times.\n  bool MaxOrZero = (MustExitMaxOrZero && ExitingBlocks.size() == 1);\n  return BackedgeTakenInfo(std::move(ExitCounts), CouldComputeBECount,\n                           MaxBECount, MaxOrZero);\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::computeExitLimit(const Loop *L, BasicBlock *ExitingBlock,\n                                      bool AllowPredicates) {\n  assert(L->contains(ExitingBlock) && \"Exit count for non-loop block?\");\n  // If our exiting block does not dominate the latch, then its connection with\n  // loop's exit limit may be far from trivial.\n  const BasicBlock *Latch = L->getLoopLatch();\n  if (!Latch || !DT.dominates(ExitingBlock, Latch))\n    return getCouldNotCompute();\n\n  bool IsOnlyExit = (L->getExitingBlock() != nullptr);\n  Instruction *Term = ExitingBlock->getTerminator();\n  if (BranchInst *BI = dyn_cast<BranchInst>(Term)) {\n    assert(BI->isConditional() && \"If unconditional, it can't be in loop!\");\n    bool ExitIfTrue = !L->contains(BI->getSuccessor(0));\n    assert(ExitIfTrue == L->contains(BI->getSuccessor(1)) &&\n           \"It should have one successor in loop and one exit block!\");\n    // Proceed to the next level to examine the exit condition expression.\n    return computeExitLimitFromCond(\n        L, BI->getCondition(), ExitIfTrue,\n        /*ControlsExit=*/IsOnlyExit, AllowPredicates);\n  }\n\n  if (SwitchInst *SI = dyn_cast<SwitchInst>(Term)) {\n    // For switch, make sure that there is a single exit from the loop.\n    BasicBlock *Exit = nullptr;\n    for (auto *SBB : successors(ExitingBlock))\n      if (!L->contains(SBB)) {\n        if (Exit) // Multiple exit successors.\n          return getCouldNotCompute();\n        Exit = SBB;\n      }\n    assert(Exit && \"Exiting block must have at least one exit\");\n    return computeExitLimitFromSingleExitSwitch(L, SI, Exit,\n                                                /*ControlsExit=*/IsOnlyExit);\n  }\n\n  return getCouldNotCompute();\n}\n\nScalarEvolution::ExitLimit ScalarEvolution::computeExitLimitFromCond(\n    const Loop *L, Value *ExitCond, bool ExitIfTrue,\n    bool ControlsExit, bool AllowPredicates) {\n  ScalarEvolution::ExitLimitCacheTy Cache(L, ExitIfTrue, AllowPredicates);\n  return computeExitLimitFromCondCached(Cache, L, ExitCond, ExitIfTrue,\n                                        ControlsExit, AllowPredicates);\n}\n\nOptional<ScalarEvolution::ExitLimit>\nScalarEvolution::ExitLimitCache::find(const Loop *L, Value *ExitCond,\n                                      bool ExitIfTrue, bool ControlsExit,\n                                      bool AllowPredicates) {\n  (void)this->L;\n  (void)this->ExitIfTrue;\n  (void)this->AllowPredicates;\n\n  assert(this->L == L && this->ExitIfTrue == ExitIfTrue &&\n         this->AllowPredicates == AllowPredicates &&\n         \"Variance in assumed invariant key components!\");\n  auto Itr = TripCountMap.find({ExitCond, ControlsExit});\n  if (Itr == TripCountMap.end())\n    return None;\n  return Itr->second;\n}\n\nvoid ScalarEvolution::ExitLimitCache::insert(const Loop *L, Value *ExitCond,\n                                             bool ExitIfTrue,\n                                             bool ControlsExit,\n                                             bool AllowPredicates,\n                                             const ExitLimit &EL) {\n  assert(this->L == L && this->ExitIfTrue == ExitIfTrue &&\n         this->AllowPredicates == AllowPredicates &&\n         \"Variance in assumed invariant key components!\");\n\n  auto InsertResult = TripCountMap.insert({{ExitCond, ControlsExit}, EL});\n  assert(InsertResult.second && \"Expected successful insertion!\");\n  (void)InsertResult;\n  (void)ExitIfTrue;\n}\n\nScalarEvolution::ExitLimit ScalarEvolution::computeExitLimitFromCondCached(\n    ExitLimitCacheTy &Cache, const Loop *L, Value *ExitCond, bool ExitIfTrue,\n    bool ControlsExit, bool AllowPredicates) {\n\n  if (auto MaybeEL =\n          Cache.find(L, ExitCond, ExitIfTrue, ControlsExit, AllowPredicates))\n    return *MaybeEL;\n\n  ExitLimit EL = computeExitLimitFromCondImpl(Cache, L, ExitCond, ExitIfTrue,\n                                              ControlsExit, AllowPredicates);\n  Cache.insert(L, ExitCond, ExitIfTrue, ControlsExit, AllowPredicates, EL);\n  return EL;\n}\n\nScalarEvolution::ExitLimit ScalarEvolution::computeExitLimitFromCondImpl(\n    ExitLimitCacheTy &Cache, const Loop *L, Value *ExitCond, bool ExitIfTrue,\n    bool ControlsExit, bool AllowPredicates) {\n  // Handle BinOp conditions (And, Or).\n  if (auto LimitFromBinOp = computeExitLimitFromCondFromBinOp(\n          Cache, L, ExitCond, ExitIfTrue, ControlsExit, AllowPredicates))\n    return *LimitFromBinOp;\n\n  // With an icmp, it may be feasible to compute an exact backedge-taken count.\n  // Proceed to the next level to examine the icmp.\n  if (ICmpInst *ExitCondICmp = dyn_cast<ICmpInst>(ExitCond)) {\n    ExitLimit EL =\n        computeExitLimitFromICmp(L, ExitCondICmp, ExitIfTrue, ControlsExit);\n    if (EL.hasFullInfo() || !AllowPredicates)\n      return EL;\n\n    // Try again, but use SCEV predicates this time.\n    return computeExitLimitFromICmp(L, ExitCondICmp, ExitIfTrue, ControlsExit,\n                                    /*AllowPredicates=*/true);\n  }\n\n  // Check for a constant condition. These are normally stripped out by\n  // SimplifyCFG, but ScalarEvolution may be used by a pass which wishes to\n  // preserve the CFG and is temporarily leaving constant conditions\n  // in place.\n  if (ConstantInt *CI = dyn_cast<ConstantInt>(ExitCond)) {\n    if (ExitIfTrue == !CI->getZExtValue())\n      // The backedge is always taken.\n      return getCouldNotCompute();\n    else\n      // The backedge is never taken.\n      return getZero(CI->getType());\n  }\n\n  // If it's not an integer or pointer comparison then compute it the hard way.\n  return computeExitCountExhaustively(L, ExitCond, ExitIfTrue);\n}\n\nOptional<ScalarEvolution::ExitLimit>\nScalarEvolution::computeExitLimitFromCondFromBinOp(\n    ExitLimitCacheTy &Cache, const Loop *L, Value *ExitCond, bool ExitIfTrue,\n    bool ControlsExit, bool AllowPredicates) {\n  // Check if the controlling expression for this loop is an And or Or.\n  Value *Op0, *Op1;\n  bool IsAnd = false;\n  if (match(ExitCond, m_LogicalAnd(m_Value(Op0), m_Value(Op1))))\n    IsAnd = true;\n  else if (match(ExitCond, m_LogicalOr(m_Value(Op0), m_Value(Op1))))\n    IsAnd = false;\n  else\n    return None;\n\n  // EitherMayExit is true in these two cases:\n  //   br (and Op0 Op1), loop, exit\n  //   br (or  Op0 Op1), exit, loop\n  bool EitherMayExit = IsAnd ^ ExitIfTrue;\n  ExitLimit EL0 = computeExitLimitFromCondCached(Cache, L, Op0, ExitIfTrue,\n                                                 ControlsExit && !EitherMayExit,\n                                                 AllowPredicates);\n  ExitLimit EL1 = computeExitLimitFromCondCached(Cache, L, Op1, ExitIfTrue,\n                                                 ControlsExit && !EitherMayExit,\n                                                 AllowPredicates);\n\n  // Be robust against unsimplified IR for the form \"op i1 X, NeutralElement\"\n  const Constant *NeutralElement = ConstantInt::get(ExitCond->getType(), IsAnd);\n  if (isa<ConstantInt>(Op1))\n    return Op1 == NeutralElement ? EL0 : EL1;\n  if (isa<ConstantInt>(Op0))\n    return Op0 == NeutralElement ? EL1 : EL0;\n\n  const SCEV *BECount = getCouldNotCompute();\n  const SCEV *MaxBECount = getCouldNotCompute();\n  if (EitherMayExit) {\n    // Both conditions must be same for the loop to continue executing.\n    // Choose the less conservative count.\n    // If ExitCond is a short-circuit form (select), using\n    // umin(EL0.ExactNotTaken, EL1.ExactNotTaken) is unsafe in general.\n    // To see the detailed examples, please see\n    // test/Analysis/ScalarEvolution/exit-count-select.ll\n    bool PoisonSafe = isa<BinaryOperator>(ExitCond);\n    if (!PoisonSafe)\n      // Even if ExitCond is select, we can safely derive BECount using both\n      // EL0 and EL1 in these cases:\n      // (1) EL0.ExactNotTaken is non-zero\n      // (2) EL1.ExactNotTaken is non-poison\n      // (3) EL0.ExactNotTaken is zero (BECount should be simply zero and\n      //     it cannot be umin(0, ..))\n      // The PoisonSafe assignment below is simplified and the assertion after\n      // BECount calculation fully guarantees the condition (3).\n      PoisonSafe = isa<SCEVConstant>(EL0.ExactNotTaken) ||\n                   isa<SCEVConstant>(EL1.ExactNotTaken);\n    if (EL0.ExactNotTaken != getCouldNotCompute() &&\n        EL1.ExactNotTaken != getCouldNotCompute() && PoisonSafe) {\n      BECount =\n          getUMinFromMismatchedTypes(EL0.ExactNotTaken, EL1.ExactNotTaken);\n\n      // If EL0.ExactNotTaken was zero and ExitCond was a short-circuit form,\n      // it should have been simplified to zero (see the condition (3) above)\n      assert(!isa<BinaryOperator>(ExitCond) || !EL0.ExactNotTaken->isZero() ||\n             BECount->isZero());\n    }\n    if (EL0.MaxNotTaken == getCouldNotCompute())\n      MaxBECount = EL1.MaxNotTaken;\n    else if (EL1.MaxNotTaken == getCouldNotCompute())\n      MaxBECount = EL0.MaxNotTaken;\n    else\n      MaxBECount = getUMinFromMismatchedTypes(EL0.MaxNotTaken, EL1.MaxNotTaken);\n  } else {\n    // Both conditions must be same at the same time for the loop to exit.\n    // For now, be conservative.\n    if (EL0.ExactNotTaken == EL1.ExactNotTaken)\n      BECount = EL0.ExactNotTaken;\n  }\n\n  // There are cases (e.g. PR26207) where computeExitLimitFromCond is able\n  // to be more aggressive when computing BECount than when computing\n  // MaxBECount.  In these cases it is possible for EL0.ExactNotTaken and\n  // EL1.ExactNotTaken to match, but for EL0.MaxNotTaken and EL1.MaxNotTaken\n  // to not.\n  if (isa<SCEVCouldNotCompute>(MaxBECount) &&\n      !isa<SCEVCouldNotCompute>(BECount))\n    MaxBECount = getConstant(getUnsignedRangeMax(BECount));\n\n  return ExitLimit(BECount, MaxBECount, false,\n                   { &EL0.Predicates, &EL1.Predicates });\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::computeExitLimitFromICmp(const Loop *L,\n                                          ICmpInst *ExitCond,\n                                          bool ExitIfTrue,\n                                          bool ControlsExit,\n                                          bool AllowPredicates) {\n  // If the condition was exit on true, convert the condition to exit on false\n  ICmpInst::Predicate Pred;\n  if (!ExitIfTrue)\n    Pred = ExitCond->getPredicate();\n  else\n    Pred = ExitCond->getInversePredicate();\n  const ICmpInst::Predicate OriginalPred = Pred;\n\n  // Handle common loops like: for (X = \"string\"; *X; ++X)\n  if (LoadInst *LI = dyn_cast<LoadInst>(ExitCond->getOperand(0)))\n    if (Constant *RHS = dyn_cast<Constant>(ExitCond->getOperand(1))) {\n      ExitLimit ItCnt =\n        computeLoadConstantCompareExitLimit(LI, RHS, L, Pred);\n      if (ItCnt.hasAnyInfo())\n        return ItCnt;\n    }\n\n  const SCEV *LHS = getSCEV(ExitCond->getOperand(0));\n  const SCEV *RHS = getSCEV(ExitCond->getOperand(1));\n\n  // Try to evaluate any dependencies out of the loop.\n  LHS = getSCEVAtScope(LHS, L);\n  RHS = getSCEVAtScope(RHS, L);\n\n  // At this point, we would like to compute how many iterations of the\n  // loop the predicate will return true for these inputs.\n  if (isLoopInvariant(LHS, L) && !isLoopInvariant(RHS, L)) {\n    // If there is a loop-invariant, force it into the RHS.\n    std::swap(LHS, RHS);\n    Pred = ICmpInst::getSwappedPredicate(Pred);\n  }\n\n  // Simplify the operands before analyzing them.\n  (void)SimplifyICmpOperands(Pred, LHS, RHS);\n\n  // If we have a comparison of a chrec against a constant, try to use value\n  // ranges to answer this query.\n  if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS))\n    if (const SCEVAddRecExpr *AddRec = dyn_cast<SCEVAddRecExpr>(LHS))\n      if (AddRec->getLoop() == L) {\n        // Form the constant range.\n        ConstantRange CompRange =\n            ConstantRange::makeExactICmpRegion(Pred, RHSC->getAPInt());\n\n        const SCEV *Ret = AddRec->getNumIterationsInRange(CompRange, *this);\n        if (!isa<SCEVCouldNotCompute>(Ret)) return Ret;\n      }\n\n  switch (Pred) {\n  case ICmpInst::ICMP_NE: {                     // while (X != Y)\n    // Convert to: while (X-Y != 0)\n    ExitLimit EL = howFarToZero(getMinusSCEV(LHS, RHS), L, ControlsExit,\n                                AllowPredicates);\n    if (EL.hasAnyInfo()) return EL;\n    break;\n  }\n  case ICmpInst::ICMP_EQ: {                     // while (X == Y)\n    // Convert to: while (X-Y == 0)\n    ExitLimit EL = howFarToNonZero(getMinusSCEV(LHS, RHS), L);\n    if (EL.hasAnyInfo()) return EL;\n    break;\n  }\n  case ICmpInst::ICMP_SLT:\n  case ICmpInst::ICMP_ULT: {                    // while (X < Y)\n    bool IsSigned = Pred == ICmpInst::ICMP_SLT;\n    ExitLimit EL = howManyLessThans(LHS, RHS, L, IsSigned, ControlsExit,\n                                    AllowPredicates);\n    if (EL.hasAnyInfo()) return EL;\n    break;\n  }\n  case ICmpInst::ICMP_SGT:\n  case ICmpInst::ICMP_UGT: {                    // while (X > Y)\n    bool IsSigned = Pred == ICmpInst::ICMP_SGT;\n    ExitLimit EL =\n        howManyGreaterThans(LHS, RHS, L, IsSigned, ControlsExit,\n                            AllowPredicates);\n    if (EL.hasAnyInfo()) return EL;\n    break;\n  }\n  default:\n    break;\n  }\n\n  auto *ExhaustiveCount =\n      computeExitCountExhaustively(L, ExitCond, ExitIfTrue);\n\n  if (!isa<SCEVCouldNotCompute>(ExhaustiveCount))\n    return ExhaustiveCount;\n\n  return computeShiftCompareExitLimit(ExitCond->getOperand(0),\n                                      ExitCond->getOperand(1), L, OriginalPred);\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::computeExitLimitFromSingleExitSwitch(const Loop *L,\n                                                      SwitchInst *Switch,\n                                                      BasicBlock *ExitingBlock,\n                                                      bool ControlsExit) {\n  assert(!L->contains(ExitingBlock) && \"Not an exiting block!\");\n\n  // Give up if the exit is the default dest of a switch.\n  if (Switch->getDefaultDest() == ExitingBlock)\n    return getCouldNotCompute();\n\n  assert(L->contains(Switch->getDefaultDest()) &&\n         \"Default case must not exit the loop!\");\n  const SCEV *LHS = getSCEVAtScope(Switch->getCondition(), L);\n  const SCEV *RHS = getConstant(Switch->findCaseDest(ExitingBlock));\n\n  // while (X != Y) --> while (X-Y != 0)\n  ExitLimit EL = howFarToZero(getMinusSCEV(LHS, RHS), L, ControlsExit);\n  if (EL.hasAnyInfo())\n    return EL;\n\n  return getCouldNotCompute();\n}\n\nstatic ConstantInt *\nEvaluateConstantChrecAtConstant(const SCEVAddRecExpr *AddRec, ConstantInt *C,\n                                ScalarEvolution &SE) {\n  const SCEV *InVal = SE.getConstant(C);\n  const SCEV *Val = AddRec->evaluateAtIteration(InVal, SE);\n  assert(isa<SCEVConstant>(Val) &&\n         \"Evaluation of SCEV at constant didn't fold correctly?\");\n  return cast<SCEVConstant>(Val)->getValue();\n}\n\n/// Given an exit condition of 'icmp op load X, cst', try to see if we can\n/// compute the backedge execution count.\nScalarEvolution::ExitLimit\nScalarEvolution::computeLoadConstantCompareExitLimit(\n  LoadInst *LI,\n  Constant *RHS,\n  const Loop *L,\n  ICmpInst::Predicate predicate) {\n  if (LI->isVolatile()) return getCouldNotCompute();\n\n  // Check to see if the loaded pointer is a getelementptr of a global.\n  // TODO: Use SCEV instead of manually grubbing with GEPs.\n  GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(LI->getOperand(0));\n  if (!GEP) return getCouldNotCompute();\n\n  // Make sure that it is really a constant global we are gepping, with an\n  // initializer, and make sure the first IDX is really 0.\n  GlobalVariable *GV = dyn_cast<GlobalVariable>(GEP->getOperand(0));\n  if (!GV || !GV->isConstant() || !GV->hasDefinitiveInitializer() ||\n      GEP->getNumOperands() < 3 || !isa<Constant>(GEP->getOperand(1)) ||\n      !cast<Constant>(GEP->getOperand(1))->isNullValue())\n    return getCouldNotCompute();\n\n  // Okay, we allow one non-constant index into the GEP instruction.\n  Value *VarIdx = nullptr;\n  std::vector<Constant*> Indexes;\n  unsigned VarIdxNum = 0;\n  for (unsigned i = 2, e = GEP->getNumOperands(); i != e; ++i)\n    if (ConstantInt *CI = dyn_cast<ConstantInt>(GEP->getOperand(i))) {\n      Indexes.push_back(CI);\n    } else if (!isa<ConstantInt>(GEP->getOperand(i))) {\n      if (VarIdx) return getCouldNotCompute();  // Multiple non-constant idx's.\n      VarIdx = GEP->getOperand(i);\n      VarIdxNum = i-2;\n      Indexes.push_back(nullptr);\n    }\n\n  // Loop-invariant loads may be a byproduct of loop optimization. Skip them.\n  if (!VarIdx)\n    return getCouldNotCompute();\n\n  // Okay, we know we have a (load (gep GV, 0, X)) comparison with a constant.\n  // Check to see if X is a loop variant variable value now.\n  const SCEV *Idx = getSCEV(VarIdx);\n  Idx = getSCEVAtScope(Idx, L);\n\n  // We can only recognize very limited forms of loop index expressions, in\n  // particular, only affine AddRec's like {C1,+,C2}<L>.\n  const SCEVAddRecExpr *IdxExpr = dyn_cast<SCEVAddRecExpr>(Idx);\n  if (!IdxExpr || IdxExpr->getLoop() != L || !IdxExpr->isAffine() ||\n      isLoopInvariant(IdxExpr, L) ||\n      !isa<SCEVConstant>(IdxExpr->getOperand(0)) ||\n      !isa<SCEVConstant>(IdxExpr->getOperand(1)))\n    return getCouldNotCompute();\n\n  unsigned MaxSteps = MaxBruteForceIterations;\n  for (unsigned IterationNum = 0; IterationNum != MaxSteps; ++IterationNum) {\n    ConstantInt *ItCst = ConstantInt::get(\n                           cast<IntegerType>(IdxExpr->getType()), IterationNum);\n    ConstantInt *Val = EvaluateConstantChrecAtConstant(IdxExpr, ItCst, *this);\n\n    // Form the GEP offset.\n    Indexes[VarIdxNum] = Val;\n\n    Constant *Result = ConstantFoldLoadThroughGEPIndices(GV->getInitializer(),\n                                                         Indexes);\n    if (!Result) break;  // Cannot compute!\n\n    // Evaluate the condition for this iteration.\n    Result = ConstantExpr::getICmp(predicate, Result, RHS);\n    if (!isa<ConstantInt>(Result)) break;  // Couldn't decide for sure\n    if (cast<ConstantInt>(Result)->getValue().isMinValue()) {\n      ++NumArrayLenItCounts;\n      return getConstant(ItCst);   // Found terminating iteration!\n    }\n  }\n  return getCouldNotCompute();\n}\n\nScalarEvolution::ExitLimit ScalarEvolution::computeShiftCompareExitLimit(\n    Value *LHS, Value *RHSV, const Loop *L, ICmpInst::Predicate Pred) {\n  ConstantInt *RHS = dyn_cast<ConstantInt>(RHSV);\n  if (!RHS)\n    return getCouldNotCompute();\n\n  const BasicBlock *Latch = L->getLoopLatch();\n  if (!Latch)\n    return getCouldNotCompute();\n\n  const BasicBlock *Predecessor = L->getLoopPredecessor();\n  if (!Predecessor)\n    return getCouldNotCompute();\n\n  // Return true if V is of the form \"LHS `shift_op` <positive constant>\".\n  // Return LHS in OutLHS and shift_opt in OutOpCode.\n  auto MatchPositiveShift =\n      [](Value *V, Value *&OutLHS, Instruction::BinaryOps &OutOpCode) {\n\n    using namespace PatternMatch;\n\n    ConstantInt *ShiftAmt;\n    if (match(V, m_LShr(m_Value(OutLHS), m_ConstantInt(ShiftAmt))))\n      OutOpCode = Instruction::LShr;\n    else if (match(V, m_AShr(m_Value(OutLHS), m_ConstantInt(ShiftAmt))))\n      OutOpCode = Instruction::AShr;\n    else if (match(V, m_Shl(m_Value(OutLHS), m_ConstantInt(ShiftAmt))))\n      OutOpCode = Instruction::Shl;\n    else\n      return false;\n\n    return ShiftAmt->getValue().isStrictlyPositive();\n  };\n\n  // Recognize a \"shift recurrence\" either of the form %iv or of %iv.shifted in\n  //\n  // loop:\n  //   %iv = phi i32 [ %iv.shifted, %loop ], [ %val, %preheader ]\n  //   %iv.shifted = lshr i32 %iv, <positive constant>\n  //\n  // Return true on a successful match.  Return the corresponding PHI node (%iv\n  // above) in PNOut and the opcode of the shift operation in OpCodeOut.\n  auto MatchShiftRecurrence =\n      [&](Value *V, PHINode *&PNOut, Instruction::BinaryOps &OpCodeOut) {\n    Optional<Instruction::BinaryOps> PostShiftOpCode;\n\n    {\n      Instruction::BinaryOps OpC;\n      Value *V;\n\n      // If we encounter a shift instruction, \"peel off\" the shift operation,\n      // and remember that we did so.  Later when we inspect %iv's backedge\n      // value, we will make sure that the backedge value uses the same\n      // operation.\n      //\n      // Note: the peeled shift operation does not have to be the same\n      // instruction as the one feeding into the PHI's backedge value.  We only\n      // really care about it being the same *kind* of shift instruction --\n      // that's all that is required for our later inferences to hold.\n      if (MatchPositiveShift(LHS, V, OpC)) {\n        PostShiftOpCode = OpC;\n        LHS = V;\n      }\n    }\n\n    PNOut = dyn_cast<PHINode>(LHS);\n    if (!PNOut || PNOut->getParent() != L->getHeader())\n      return false;\n\n    Value *BEValue = PNOut->getIncomingValueForBlock(Latch);\n    Value *OpLHS;\n\n    return\n        // The backedge value for the PHI node must be a shift by a positive\n        // amount\n        MatchPositiveShift(BEValue, OpLHS, OpCodeOut) &&\n\n        // of the PHI node itself\n        OpLHS == PNOut &&\n\n        // and the kind of shift should be match the kind of shift we peeled\n        // off, if any.\n        (!PostShiftOpCode.hasValue() || *PostShiftOpCode == OpCodeOut);\n  };\n\n  PHINode *PN;\n  Instruction::BinaryOps OpCode;\n  if (!MatchShiftRecurrence(LHS, PN, OpCode))\n    return getCouldNotCompute();\n\n  const DataLayout &DL = getDataLayout();\n\n  // The key rationale for this optimization is that for some kinds of shift\n  // recurrences, the value of the recurrence \"stabilizes\" to either 0 or -1\n  // within a finite number of iterations.  If the condition guarding the\n  // backedge (in the sense that the backedge is taken if the condition is true)\n  // is false for the value the shift recurrence stabilizes to, then we know\n  // that the backedge is taken only a finite number of times.\n\n  ConstantInt *StableValue = nullptr;\n  switch (OpCode) {\n  default:\n    llvm_unreachable(\"Impossible case!\");\n\n  case Instruction::AShr: {\n    // {K,ashr,<positive-constant>} stabilizes to signum(K) in at most\n    // bitwidth(K) iterations.\n    Value *FirstValue = PN->getIncomingValueForBlock(Predecessor);\n    KnownBits Known = computeKnownBits(FirstValue, DL, 0, &AC,\n                                       Predecessor->getTerminator(), &DT);\n    auto *Ty = cast<IntegerType>(RHS->getType());\n    if (Known.isNonNegative())\n      StableValue = ConstantInt::get(Ty, 0);\n    else if (Known.isNegative())\n      StableValue = ConstantInt::get(Ty, -1, true);\n    else\n      return getCouldNotCompute();\n\n    break;\n  }\n  case Instruction::LShr:\n  case Instruction::Shl:\n    // Both {K,lshr,<positive-constant>} and {K,shl,<positive-constant>}\n    // stabilize to 0 in at most bitwidth(K) iterations.\n    StableValue = ConstantInt::get(cast<IntegerType>(RHS->getType()), 0);\n    break;\n  }\n\n  auto *Result =\n      ConstantFoldCompareInstOperands(Pred, StableValue, RHS, DL, &TLI);\n  assert(Result->getType()->isIntegerTy(1) &&\n         \"Otherwise cannot be an operand to a branch instruction\");\n\n  if (Result->isZeroValue()) {\n    unsigned BitWidth = getTypeSizeInBits(RHS->getType());\n    const SCEV *UpperBound =\n        getConstant(getEffectiveSCEVType(RHS->getType()), BitWidth);\n    return ExitLimit(getCouldNotCompute(), UpperBound, false);\n  }\n\n  return getCouldNotCompute();\n}\n\n/// Return true if we can constant fold an instruction of the specified type,\n/// assuming that all operands were constants.\nstatic bool CanConstantFold(const Instruction *I) {\n  if (isa<BinaryOperator>(I) || isa<CmpInst>(I) ||\n      isa<SelectInst>(I) || isa<CastInst>(I) || isa<GetElementPtrInst>(I) ||\n      isa<LoadInst>(I) || isa<ExtractValueInst>(I))\n    return true;\n\n  if (const CallInst *CI = dyn_cast<CallInst>(I))\n    if (const Function *F = CI->getCalledFunction())\n      return canConstantFoldCallTo(CI, F);\n  return false;\n}\n\n/// Determine whether this instruction can constant evolve within this loop\n/// assuming its operands can all constant evolve.\nstatic bool canConstantEvolve(Instruction *I, const Loop *L) {\n  // An instruction outside of the loop can't be derived from a loop PHI.\n  if (!L->contains(I)) return false;\n\n  if (isa<PHINode>(I)) {\n    // We don't currently keep track of the control flow needed to evaluate\n    // PHIs, so we cannot handle PHIs inside of loops.\n    return L->getHeader() == I->getParent();\n  }\n\n  // If we won't be able to constant fold this expression even if the operands\n  // are constants, bail early.\n  return CanConstantFold(I);\n}\n\n/// getConstantEvolvingPHIOperands - Implement getConstantEvolvingPHI by\n/// recursing through each instruction operand until reaching a loop header phi.\nstatic PHINode *\ngetConstantEvolvingPHIOperands(Instruction *UseInst, const Loop *L,\n                               DenseMap<Instruction *, PHINode *> &PHIMap,\n                               unsigned Depth) {\n  if (Depth > MaxConstantEvolvingDepth)\n    return nullptr;\n\n  // Otherwise, we can evaluate this instruction if all of its operands are\n  // constant or derived from a PHI node themselves.\n  PHINode *PHI = nullptr;\n  for (Value *Op : UseInst->operands()) {\n    if (isa<Constant>(Op)) continue;\n\n    Instruction *OpInst = dyn_cast<Instruction>(Op);\n    if (!OpInst || !canConstantEvolve(OpInst, L)) return nullptr;\n\n    PHINode *P = dyn_cast<PHINode>(OpInst);\n    if (!P)\n      // If this operand is already visited, reuse the prior result.\n      // We may have P != PHI if this is the deepest point at which the\n      // inconsistent paths meet.\n      P = PHIMap.lookup(OpInst);\n    if (!P) {\n      // Recurse and memoize the results, whether a phi is found or not.\n      // This recursive call invalidates pointers into PHIMap.\n      P = getConstantEvolvingPHIOperands(OpInst, L, PHIMap, Depth + 1);\n      PHIMap[OpInst] = P;\n    }\n    if (!P)\n      return nullptr;  // Not evolving from PHI\n    if (PHI && PHI != P)\n      return nullptr;  // Evolving from multiple different PHIs.\n    PHI = P;\n  }\n  // This is a expression evolving from a constant PHI!\n  return PHI;\n}\n\n/// getConstantEvolvingPHI - Given an LLVM value and a loop, return a PHI node\n/// in the loop that V is derived from.  We allow arbitrary operations along the\n/// way, but the operands of an operation must either be constants or a value\n/// derived from a constant PHI.  If this expression does not fit with these\n/// constraints, return null.\nstatic PHINode *getConstantEvolvingPHI(Value *V, const Loop *L) {\n  Instruction *I = dyn_cast<Instruction>(V);\n  if (!I || !canConstantEvolve(I, L)) return nullptr;\n\n  if (PHINode *PN = dyn_cast<PHINode>(I))\n    return PN;\n\n  // Record non-constant instructions contained by the loop.\n  DenseMap<Instruction *, PHINode *> PHIMap;\n  return getConstantEvolvingPHIOperands(I, L, PHIMap, 0);\n}\n\n/// EvaluateExpression - Given an expression that passes the\n/// getConstantEvolvingPHI predicate, evaluate its value assuming the PHI node\n/// in the loop has the value PHIVal.  If we can't fold this expression for some\n/// reason, return null.\nstatic Constant *EvaluateExpression(Value *V, const Loop *L,\n                                    DenseMap<Instruction *, Constant *> &Vals,\n                                    const DataLayout &DL,\n                                    const TargetLibraryInfo *TLI) {\n  // Convenient constant check, but redundant for recursive calls.\n  if (Constant *C = dyn_cast<Constant>(V)) return C;\n  Instruction *I = dyn_cast<Instruction>(V);\n  if (!I) return nullptr;\n\n  if (Constant *C = Vals.lookup(I)) return C;\n\n  // An instruction inside the loop depends on a value outside the loop that we\n  // weren't given a mapping for, or a value such as a call inside the loop.\n  if (!canConstantEvolve(I, L)) return nullptr;\n\n  // An unmapped PHI can be due to a branch or another loop inside this loop,\n  // or due to this not being the initial iteration through a loop where we\n  // couldn't compute the evolution of this particular PHI last time.\n  if (isa<PHINode>(I)) return nullptr;\n\n  std::vector<Constant*> Operands(I->getNumOperands());\n\n  for (unsigned i = 0, e = I->getNumOperands(); i != e; ++i) {\n    Instruction *Operand = dyn_cast<Instruction>(I->getOperand(i));\n    if (!Operand) {\n      Operands[i] = dyn_cast<Constant>(I->getOperand(i));\n      if (!Operands[i]) return nullptr;\n      continue;\n    }\n    Constant *C = EvaluateExpression(Operand, L, Vals, DL, TLI);\n    Vals[Operand] = C;\n    if (!C) return nullptr;\n    Operands[i] = C;\n  }\n\n  if (CmpInst *CI = dyn_cast<CmpInst>(I))\n    return ConstantFoldCompareInstOperands(CI->getPredicate(), Operands[0],\n                                           Operands[1], DL, TLI);\n  if (LoadInst *LI = dyn_cast<LoadInst>(I)) {\n    if (!LI->isVolatile())\n      return ConstantFoldLoadFromConstPtr(Operands[0], LI->getType(), DL);\n  }\n  return ConstantFoldInstOperands(I, Operands, DL, TLI);\n}\n\n\n// If every incoming value to PN except the one for BB is a specific Constant,\n// return that, else return nullptr.\nstatic Constant *getOtherIncomingValue(PHINode *PN, BasicBlock *BB) {\n  Constant *IncomingVal = nullptr;\n\n  for (unsigned i = 0, e = PN->getNumIncomingValues(); i != e; ++i) {\n    if (PN->getIncomingBlock(i) == BB)\n      continue;\n\n    auto *CurrentVal = dyn_cast<Constant>(PN->getIncomingValue(i));\n    if (!CurrentVal)\n      return nullptr;\n\n    if (IncomingVal != CurrentVal) {\n      if (IncomingVal)\n        return nullptr;\n      IncomingVal = CurrentVal;\n    }\n  }\n\n  return IncomingVal;\n}\n\n/// getConstantEvolutionLoopExitValue - If we know that the specified Phi is\n/// in the header of its containing loop, we know the loop executes a\n/// constant number of times, and the PHI node is just a recurrence\n/// involving constants, fold it.\nConstant *\nScalarEvolution::getConstantEvolutionLoopExitValue(PHINode *PN,\n                                                   const APInt &BEs,\n                                                   const Loop *L) {\n  auto I = ConstantEvolutionLoopExitValue.find(PN);\n  if (I != ConstantEvolutionLoopExitValue.end())\n    return I->second;\n\n  if (BEs.ugt(MaxBruteForceIterations))\n    return ConstantEvolutionLoopExitValue[PN] = nullptr;  // Not going to evaluate it.\n\n  Constant *&RetVal = ConstantEvolutionLoopExitValue[PN];\n\n  DenseMap<Instruction *, Constant *> CurrentIterVals;\n  BasicBlock *Header = L->getHeader();\n  assert(PN->getParent() == Header && \"Can't evaluate PHI not in loop header!\");\n\n  BasicBlock *Latch = L->getLoopLatch();\n  if (!Latch)\n    return nullptr;\n\n  for (PHINode &PHI : Header->phis()) {\n    if (auto *StartCST = getOtherIncomingValue(&PHI, Latch))\n      CurrentIterVals[&PHI] = StartCST;\n  }\n  if (!CurrentIterVals.count(PN))\n    return RetVal = nullptr;\n\n  Value *BEValue = PN->getIncomingValueForBlock(Latch);\n\n  // Execute the loop symbolically to determine the exit value.\n  assert(BEs.getActiveBits() < CHAR_BIT * sizeof(unsigned) &&\n         \"BEs is <= MaxBruteForceIterations which is an 'unsigned'!\");\n\n  unsigned NumIterations = BEs.getZExtValue(); // must be in range\n  unsigned IterationNum = 0;\n  const DataLayout &DL = getDataLayout();\n  for (; ; ++IterationNum) {\n    if (IterationNum == NumIterations)\n      return RetVal = CurrentIterVals[PN];  // Got exit value!\n\n    // Compute the value of the PHIs for the next iteration.\n    // EvaluateExpression adds non-phi values to the CurrentIterVals map.\n    DenseMap<Instruction *, Constant *> NextIterVals;\n    Constant *NextPHI =\n        EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);\n    if (!NextPHI)\n      return nullptr;        // Couldn't evaluate!\n    NextIterVals[PN] = NextPHI;\n\n    bool StoppedEvolving = NextPHI == CurrentIterVals[PN];\n\n    // Also evaluate the other PHI nodes.  However, we don't get to stop if we\n    // cease to be able to evaluate one of them or if they stop evolving,\n    // because that doesn't necessarily prevent us from computing PN.\n    SmallVector<std::pair<PHINode *, Constant *>, 8> PHIsToCompute;\n    for (const auto &I : CurrentIterVals) {\n      PHINode *PHI = dyn_cast<PHINode>(I.first);\n      if (!PHI || PHI == PN || PHI->getParent() != Header) continue;\n      PHIsToCompute.emplace_back(PHI, I.second);\n    }\n    // We use two distinct loops because EvaluateExpression may invalidate any\n    // iterators into CurrentIterVals.\n    for (const auto &I : PHIsToCompute) {\n      PHINode *PHI = I.first;\n      Constant *&NextPHI = NextIterVals[PHI];\n      if (!NextPHI) {   // Not already computed.\n        Value *BEValue = PHI->getIncomingValueForBlock(Latch);\n        NextPHI = EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);\n      }\n      if (NextPHI != I.second)\n        StoppedEvolving = false;\n    }\n\n    // If all entries in CurrentIterVals == NextIterVals then we can stop\n    // iterating, the loop can't continue to change.\n    if (StoppedEvolving)\n      return RetVal = CurrentIterVals[PN];\n\n    CurrentIterVals.swap(NextIterVals);\n  }\n}\n\nconst SCEV *ScalarEvolution::computeExitCountExhaustively(const Loop *L,\n                                                          Value *Cond,\n                                                          bool ExitWhen) {\n  PHINode *PN = getConstantEvolvingPHI(Cond, L);\n  if (!PN) return getCouldNotCompute();\n\n  // If the loop is canonicalized, the PHI will have exactly two entries.\n  // That's the only form we support here.\n  if (PN->getNumIncomingValues() != 2) return getCouldNotCompute();\n\n  DenseMap<Instruction *, Constant *> CurrentIterVals;\n  BasicBlock *Header = L->getHeader();\n  assert(PN->getParent() == Header && \"Can't evaluate PHI not in loop header!\");\n\n  BasicBlock *Latch = L->getLoopLatch();\n  assert(Latch && \"Should follow from NumIncomingValues == 2!\");\n\n  for (PHINode &PHI : Header->phis()) {\n    if (auto *StartCST = getOtherIncomingValue(&PHI, Latch))\n      CurrentIterVals[&PHI] = StartCST;\n  }\n  if (!CurrentIterVals.count(PN))\n    return getCouldNotCompute();\n\n  // Okay, we find a PHI node that defines the trip count of this loop.  Execute\n  // the loop symbolically to determine when the condition gets a value of\n  // \"ExitWhen\".\n  unsigned MaxIterations = MaxBruteForceIterations;   // Limit analysis.\n  const DataLayout &DL = getDataLayout();\n  for (unsigned IterationNum = 0; IterationNum != MaxIterations;++IterationNum){\n    auto *CondVal = dyn_cast_or_null<ConstantInt>(\n        EvaluateExpression(Cond, L, CurrentIterVals, DL, &TLI));\n\n    // Couldn't symbolically evaluate.\n    if (!CondVal) return getCouldNotCompute();\n\n    if (CondVal->getValue() == uint64_t(ExitWhen)) {\n      ++NumBruteForceTripCountsComputed;\n      return getConstant(Type::getInt32Ty(getContext()), IterationNum);\n    }\n\n    // Update all the PHI nodes for the next iteration.\n    DenseMap<Instruction *, Constant *> NextIterVals;\n\n    // Create a list of which PHIs we need to compute. We want to do this before\n    // calling EvaluateExpression on them because that may invalidate iterators\n    // into CurrentIterVals.\n    SmallVector<PHINode *, 8> PHIsToCompute;\n    for (const auto &I : CurrentIterVals) {\n      PHINode *PHI = dyn_cast<PHINode>(I.first);\n      if (!PHI || PHI->getParent() != Header) continue;\n      PHIsToCompute.push_back(PHI);\n    }\n    for (PHINode *PHI : PHIsToCompute) {\n      Constant *&NextPHI = NextIterVals[PHI];\n      if (NextPHI) continue;    // Already computed!\n\n      Value *BEValue = PHI->getIncomingValueForBlock(Latch);\n      NextPHI = EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);\n    }\n    CurrentIterVals.swap(NextIterVals);\n  }\n\n  // Too many iterations were needed to evaluate.\n  return getCouldNotCompute();\n}\n\nconst SCEV *ScalarEvolution::getSCEVAtScope(const SCEV *V, const Loop *L) {\n  SmallVector<std::pair<const Loop *, const SCEV *>, 2> &Values =\n      ValuesAtScopes[V];\n  // Check to see if we've folded this expression at this loop before.\n  for (auto &LS : Values)\n    if (LS.first == L)\n      return LS.second ? LS.second : V;\n\n  Values.emplace_back(L, nullptr);\n\n  // Otherwise compute it.\n  const SCEV *C = computeSCEVAtScope(V, L);\n  for (auto &LS : reverse(ValuesAtScopes[V]))\n    if (LS.first == L) {\n      LS.second = C;\n      break;\n    }\n  return C;\n}\n\n/// This builds up a Constant using the ConstantExpr interface.  That way, we\n/// will return Constants for objects which aren't represented by a\n/// SCEVConstant, because SCEVConstant is restricted to ConstantInt.\n/// Returns NULL if the SCEV isn't representable as a Constant.\nstatic Constant *BuildConstantFromSCEV(const SCEV *V) {\n  switch (V->getSCEVType()) {\n  case scCouldNotCompute:\n  case scAddRecExpr:\n    return nullptr;\n  case scConstant:\n    return cast<SCEVConstant>(V)->getValue();\n  case scUnknown:\n    return dyn_cast<Constant>(cast<SCEVUnknown>(V)->getValue());\n  case scSignExtend: {\n    const SCEVSignExtendExpr *SS = cast<SCEVSignExtendExpr>(V);\n    if (Constant *CastOp = BuildConstantFromSCEV(SS->getOperand()))\n      return ConstantExpr::getSExt(CastOp, SS->getType());\n    return nullptr;\n  }\n  case scZeroExtend: {\n    const SCEVZeroExtendExpr *SZ = cast<SCEVZeroExtendExpr>(V);\n    if (Constant *CastOp = BuildConstantFromSCEV(SZ->getOperand()))\n      return ConstantExpr::getZExt(CastOp, SZ->getType());\n    return nullptr;\n  }\n  case scPtrToInt: {\n    const SCEVPtrToIntExpr *P2I = cast<SCEVPtrToIntExpr>(V);\n    if (Constant *CastOp = BuildConstantFromSCEV(P2I->getOperand()))\n      return ConstantExpr::getPtrToInt(CastOp, P2I->getType());\n\n    return nullptr;\n  }\n  case scTruncate: {\n    const SCEVTruncateExpr *ST = cast<SCEVTruncateExpr>(V);\n    if (Constant *CastOp = BuildConstantFromSCEV(ST->getOperand()))\n      return ConstantExpr::getTrunc(CastOp, ST->getType());\n    return nullptr;\n  }\n  case scAddExpr: {\n    const SCEVAddExpr *SA = cast<SCEVAddExpr>(V);\n    if (Constant *C = BuildConstantFromSCEV(SA->getOperand(0))) {\n      if (PointerType *PTy = dyn_cast<PointerType>(C->getType())) {\n        unsigned AS = PTy->getAddressSpace();\n        Type *DestPtrTy = Type::getInt8PtrTy(C->getContext(), AS);\n        C = ConstantExpr::getBitCast(C, DestPtrTy);\n      }\n      for (unsigned i = 1, e = SA->getNumOperands(); i != e; ++i) {\n        Constant *C2 = BuildConstantFromSCEV(SA->getOperand(i));\n        if (!C2)\n          return nullptr;\n\n        // First pointer!\n        if (!C->getType()->isPointerTy() && C2->getType()->isPointerTy()) {\n          unsigned AS = C2->getType()->getPointerAddressSpace();\n          std::swap(C, C2);\n          Type *DestPtrTy = Type::getInt8PtrTy(C->getContext(), AS);\n          // The offsets have been converted to bytes.  We can add bytes to an\n          // i8* by GEP with the byte count in the first index.\n          C = ConstantExpr::getBitCast(C, DestPtrTy);\n        }\n\n        // Don't bother trying to sum two pointers. We probably can't\n        // statically compute a load that results from it anyway.\n        if (C2->getType()->isPointerTy())\n          return nullptr;\n\n        if (PointerType *PTy = dyn_cast<PointerType>(C->getType())) {\n          if (PTy->getElementType()->isStructTy())\n            C2 = ConstantExpr::getIntegerCast(\n                C2, Type::getInt32Ty(C->getContext()), true);\n          C = ConstantExpr::getGetElementPtr(PTy->getElementType(), C, C2);\n        } else\n          C = ConstantExpr::getAdd(C, C2);\n      }\n      return C;\n    }\n    return nullptr;\n  }\n  case scMulExpr: {\n    const SCEVMulExpr *SM = cast<SCEVMulExpr>(V);\n    if (Constant *C = BuildConstantFromSCEV(SM->getOperand(0))) {\n      // Don't bother with pointers at all.\n      if (C->getType()->isPointerTy())\n        return nullptr;\n      for (unsigned i = 1, e = SM->getNumOperands(); i != e; ++i) {\n        Constant *C2 = BuildConstantFromSCEV(SM->getOperand(i));\n        if (!C2 || C2->getType()->isPointerTy())\n          return nullptr;\n        C = ConstantExpr::getMul(C, C2);\n      }\n      return C;\n    }\n    return nullptr;\n  }\n  case scUDivExpr: {\n    const SCEVUDivExpr *SU = cast<SCEVUDivExpr>(V);\n    if (Constant *LHS = BuildConstantFromSCEV(SU->getLHS()))\n      if (Constant *RHS = BuildConstantFromSCEV(SU->getRHS()))\n        if (LHS->getType() == RHS->getType())\n          return ConstantExpr::getUDiv(LHS, RHS);\n    return nullptr;\n  }\n  case scSMaxExpr:\n  case scUMaxExpr:\n  case scSMinExpr:\n  case scUMinExpr:\n    return nullptr; // TODO: smax, umax, smin, umax.\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\nconst SCEV *ScalarEvolution::computeSCEVAtScope(const SCEV *V, const Loop *L) {\n  if (isa<SCEVConstant>(V)) return V;\n\n  // If this instruction is evolved from a constant-evolving PHI, compute the\n  // exit value from the loop without using SCEVs.\n  if (const SCEVUnknown *SU = dyn_cast<SCEVUnknown>(V)) {\n    if (Instruction *I = dyn_cast<Instruction>(SU->getValue())) {\n      if (PHINode *PN = dyn_cast<PHINode>(I)) {\n        const Loop *CurrLoop = this->LI[I->getParent()];\n        // Looking for loop exit value.\n        if (CurrLoop && CurrLoop->getParentLoop() == L &&\n            PN->getParent() == CurrLoop->getHeader()) {\n          // Okay, there is no closed form solution for the PHI node.  Check\n          // to see if the loop that contains it has a known backedge-taken\n          // count.  If so, we may be able to force computation of the exit\n          // value.\n          const SCEV *BackedgeTakenCount = getBackedgeTakenCount(CurrLoop);\n          // This trivial case can show up in some degenerate cases where\n          // the incoming IR has not yet been fully simplified.\n          if (BackedgeTakenCount->isZero()) {\n            Value *InitValue = nullptr;\n            bool MultipleInitValues = false;\n            for (unsigned i = 0; i < PN->getNumIncomingValues(); i++) {\n              if (!CurrLoop->contains(PN->getIncomingBlock(i))) {\n                if (!InitValue)\n                  InitValue = PN->getIncomingValue(i);\n                else if (InitValue != PN->getIncomingValue(i)) {\n                  MultipleInitValues = true;\n                  break;\n                }\n              }\n            }\n            if (!MultipleInitValues && InitValue)\n              return getSCEV(InitValue);\n          }\n          // Do we have a loop invariant value flowing around the backedge\n          // for a loop which must execute the backedge?\n          if (!isa<SCEVCouldNotCompute>(BackedgeTakenCount) &&\n              isKnownPositive(BackedgeTakenCount) &&\n              PN->getNumIncomingValues() == 2) {\n\n            unsigned InLoopPred =\n                CurrLoop->contains(PN->getIncomingBlock(0)) ? 0 : 1;\n            Value *BackedgeVal = PN->getIncomingValue(InLoopPred);\n            if (CurrLoop->isLoopInvariant(BackedgeVal))\n              return getSCEV(BackedgeVal);\n          }\n          if (auto *BTCC = dyn_cast<SCEVConstant>(BackedgeTakenCount)) {\n            // Okay, we know how many times the containing loop executes.  If\n            // this is a constant evolving PHI node, get the final value at\n            // the specified iteration number.\n            Constant *RV = getConstantEvolutionLoopExitValue(\n                PN, BTCC->getAPInt(), CurrLoop);\n            if (RV) return getSCEV(RV);\n          }\n        }\n\n        // If there is a single-input Phi, evaluate it at our scope. If we can\n        // prove that this replacement does not break LCSSA form, use new value.\n        if (PN->getNumOperands() == 1) {\n          const SCEV *Input = getSCEV(PN->getOperand(0));\n          const SCEV *InputAtScope = getSCEVAtScope(Input, L);\n          // TODO: We can generalize it using LI.replacementPreservesLCSSAForm,\n          // for the simplest case just support constants.\n          if (isa<SCEVConstant>(InputAtScope)) return InputAtScope;\n        }\n      }\n\n      // Okay, this is an expression that we cannot symbolically evaluate\n      // into a SCEV.  Check to see if it's possible to symbolically evaluate\n      // the arguments into constants, and if so, try to constant propagate the\n      // result.  This is particularly useful for computing loop exit values.\n      if (CanConstantFold(I)) {\n        SmallVector<Constant *, 4> Operands;\n        bool MadeImprovement = false;\n        for (Value *Op : I->operands()) {\n          if (Constant *C = dyn_cast<Constant>(Op)) {\n            Operands.push_back(C);\n            continue;\n          }\n\n          // If any of the operands is non-constant and if they are\n          // non-integer and non-pointer, don't even try to analyze them\n          // with scev techniques.\n          if (!isSCEVable(Op->getType()))\n            return V;\n\n          const SCEV *OrigV = getSCEV(Op);\n          const SCEV *OpV = getSCEVAtScope(OrigV, L);\n          MadeImprovement |= OrigV != OpV;\n\n          Constant *C = BuildConstantFromSCEV(OpV);\n          if (!C) return V;\n          if (C->getType() != Op->getType())\n            C = ConstantExpr::getCast(CastInst::getCastOpcode(C, false,\n                                                              Op->getType(),\n                                                              false),\n                                      C, Op->getType());\n          Operands.push_back(C);\n        }\n\n        // Check to see if getSCEVAtScope actually made an improvement.\n        if (MadeImprovement) {\n          Constant *C = nullptr;\n          const DataLayout &DL = getDataLayout();\n          if (const CmpInst *CI = dyn_cast<CmpInst>(I))\n            C = ConstantFoldCompareInstOperands(CI->getPredicate(), Operands[0],\n                                                Operands[1], DL, &TLI);\n          else if (const LoadInst *Load = dyn_cast<LoadInst>(I)) {\n            if (!Load->isVolatile())\n              C = ConstantFoldLoadFromConstPtr(Operands[0], Load->getType(),\n                                               DL);\n          } else\n            C = ConstantFoldInstOperands(I, Operands, DL, &TLI);\n          if (!C) return V;\n          return getSCEV(C);\n        }\n      }\n    }\n\n    // This is some other type of SCEVUnknown, just return it.\n    return V;\n  }\n\n  if (const SCEVCommutativeExpr *Comm = dyn_cast<SCEVCommutativeExpr>(V)) {\n    // Avoid performing the look-up in the common case where the specified\n    // expression has no loop-variant portions.\n    for (unsigned i = 0, e = Comm->getNumOperands(); i != e; ++i) {\n      const SCEV *OpAtScope = getSCEVAtScope(Comm->getOperand(i), L);\n      if (OpAtScope != Comm->getOperand(i)) {\n        // Okay, at least one of these operands is loop variant but might be\n        // foldable.  Build a new instance of the folded commutative expression.\n        SmallVector<const SCEV *, 8> NewOps(Comm->op_begin(),\n                                            Comm->op_begin()+i);\n        NewOps.push_back(OpAtScope);\n\n        for (++i; i != e; ++i) {\n          OpAtScope = getSCEVAtScope(Comm->getOperand(i), L);\n          NewOps.push_back(OpAtScope);\n        }\n        if (isa<SCEVAddExpr>(Comm))\n          return getAddExpr(NewOps, Comm->getNoWrapFlags());\n        if (isa<SCEVMulExpr>(Comm))\n          return getMulExpr(NewOps, Comm->getNoWrapFlags());\n        if (isa<SCEVMinMaxExpr>(Comm))\n          return getMinMaxExpr(Comm->getSCEVType(), NewOps);\n        llvm_unreachable(\"Unknown commutative SCEV type!\");\n      }\n    }\n    // If we got here, all operands are loop invariant.\n    return Comm;\n  }\n\n  if (const SCEVUDivExpr *Div = dyn_cast<SCEVUDivExpr>(V)) {\n    const SCEV *LHS = getSCEVAtScope(Div->getLHS(), L);\n    const SCEV *RHS = getSCEVAtScope(Div->getRHS(), L);\n    if (LHS == Div->getLHS() && RHS == Div->getRHS())\n      return Div;   // must be loop invariant\n    return getUDivExpr(LHS, RHS);\n  }\n\n  // If this is a loop recurrence for a loop that does not contain L, then we\n  // are dealing with the final value computed by the loop.\n  if (const SCEVAddRecExpr *AddRec = dyn_cast<SCEVAddRecExpr>(V)) {\n    // First, attempt to evaluate each operand.\n    // Avoid performing the look-up in the common case where the specified\n    // expression has no loop-variant portions.\n    for (unsigned i = 0, e = AddRec->getNumOperands(); i != e; ++i) {\n      const SCEV *OpAtScope = getSCEVAtScope(AddRec->getOperand(i), L);\n      if (OpAtScope == AddRec->getOperand(i))\n        continue;\n\n      // Okay, at least one of these operands is loop variant but might be\n      // foldable.  Build a new instance of the folded commutative expression.\n      SmallVector<const SCEV *, 8> NewOps(AddRec->op_begin(),\n                                          AddRec->op_begin()+i);\n      NewOps.push_back(OpAtScope);\n      for (++i; i != e; ++i)\n        NewOps.push_back(getSCEVAtScope(AddRec->getOperand(i), L));\n\n      const SCEV *FoldedRec =\n        getAddRecExpr(NewOps, AddRec->getLoop(),\n                      AddRec->getNoWrapFlags(SCEV::FlagNW));\n      AddRec = dyn_cast<SCEVAddRecExpr>(FoldedRec);\n      // The addrec may be folded to a nonrecurrence, for example, if the\n      // induction variable is multiplied by zero after constant folding. Go\n      // ahead and return the folded value.\n      if (!AddRec)\n        return FoldedRec;\n      break;\n    }\n\n    // If the scope is outside the addrec's loop, evaluate it by using the\n    // loop exit value of the addrec.\n    if (!AddRec->getLoop()->contains(L)) {\n      // To evaluate this recurrence, we need to know how many times the AddRec\n      // loop iterates.  Compute this now.\n      const SCEV *BackedgeTakenCount = getBackedgeTakenCount(AddRec->getLoop());\n      if (BackedgeTakenCount == getCouldNotCompute()) return AddRec;\n\n      // Then, evaluate the AddRec.\n      return AddRec->evaluateAtIteration(BackedgeTakenCount, *this);\n    }\n\n    return AddRec;\n  }\n\n  if (const SCEVZeroExtendExpr *Cast = dyn_cast<SCEVZeroExtendExpr>(V)) {\n    const SCEV *Op = getSCEVAtScope(Cast->getOperand(), L);\n    if (Op == Cast->getOperand())\n      return Cast;  // must be loop invariant\n    return getZeroExtendExpr(Op, Cast->getType());\n  }\n\n  if (const SCEVSignExtendExpr *Cast = dyn_cast<SCEVSignExtendExpr>(V)) {\n    const SCEV *Op = getSCEVAtScope(Cast->getOperand(), L);\n    if (Op == Cast->getOperand())\n      return Cast;  // must be loop invariant\n    return getSignExtendExpr(Op, Cast->getType());\n  }\n\n  if (const SCEVTruncateExpr *Cast = dyn_cast<SCEVTruncateExpr>(V)) {\n    const SCEV *Op = getSCEVAtScope(Cast->getOperand(), L);\n    if (Op == Cast->getOperand())\n      return Cast;  // must be loop invariant\n    return getTruncateExpr(Op, Cast->getType());\n  }\n\n  if (const SCEVPtrToIntExpr *Cast = dyn_cast<SCEVPtrToIntExpr>(V)) {\n    const SCEV *Op = getSCEVAtScope(Cast->getOperand(), L);\n    if (Op == Cast->getOperand())\n      return Cast; // must be loop invariant\n    return getPtrToIntExpr(Op, Cast->getType());\n  }\n\n  llvm_unreachable(\"Unknown SCEV type!\");\n}\n\nconst SCEV *ScalarEvolution::getSCEVAtScope(Value *V, const Loop *L) {\n  return getSCEVAtScope(getSCEV(V), L);\n}\n\nconst SCEV *ScalarEvolution::stripInjectiveFunctions(const SCEV *S) const {\n  if (const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(S))\n    return stripInjectiveFunctions(ZExt->getOperand());\n  if (const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(S))\n    return stripInjectiveFunctions(SExt->getOperand());\n  return S;\n}\n\n/// Finds the minimum unsigned root of the following equation:\n///\n///     A * X = B (mod N)\n///\n/// where N = 2^BW and BW is the common bit width of A and B. The signedness of\n/// A and B isn't important.\n///\n/// If the equation does not have a solution, SCEVCouldNotCompute is returned.\nstatic const SCEV *SolveLinEquationWithOverflow(const APInt &A, const SCEV *B,\n                                               ScalarEvolution &SE) {\n  uint32_t BW = A.getBitWidth();\n  assert(BW == SE.getTypeSizeInBits(B->getType()));\n  assert(A != 0 && \"A must be non-zero.\");\n\n  // 1. D = gcd(A, N)\n  //\n  // The gcd of A and N may have only one prime factor: 2. The number of\n  // trailing zeros in A is its multiplicity\n  uint32_t Mult2 = A.countTrailingZeros();\n  // D = 2^Mult2\n\n  // 2. Check if B is divisible by D.\n  //\n  // B is divisible by D if and only if the multiplicity of prime factor 2 for B\n  // is not less than multiplicity of this prime factor for D.\n  if (SE.GetMinTrailingZeros(B) < Mult2)\n    return SE.getCouldNotCompute();\n\n  // 3. Compute I: the multiplicative inverse of (A / D) in arithmetic\n  // modulo (N / D).\n  //\n  // If D == 1, (N / D) == N == 2^BW, so we need one extra bit to represent\n  // (N / D) in general. The inverse itself always fits into BW bits, though,\n  // so we immediately truncate it.\n  APInt AD = A.lshr(Mult2).zext(BW + 1);  // AD = A / D\n  APInt Mod(BW + 1, 0);\n  Mod.setBit(BW - Mult2);  // Mod = N / D\n  APInt I = AD.multiplicativeInverse(Mod).trunc(BW);\n\n  // 4. Compute the minimum unsigned root of the equation:\n  // I * (B / D) mod (N / D)\n  // To simplify the computation, we factor out the divide by D:\n  // (I * B mod N) / D\n  const SCEV *D = SE.getConstant(APInt::getOneBitSet(BW, Mult2));\n  return SE.getUDivExactExpr(SE.getMulExpr(B, SE.getConstant(I)), D);\n}\n\n/// For a given quadratic addrec, generate coefficients of the corresponding\n/// quadratic equation, multiplied by a common value to ensure that they are\n/// integers.\n/// The returned value is a tuple { A, B, C, M, BitWidth }, where\n/// Ax^2 + Bx + C is the quadratic function, M is the value that A, B and C\n/// were multiplied by, and BitWidth is the bit width of the original addrec\n/// coefficients.\n/// This function returns None if the addrec coefficients are not compile-\n/// time constants.\nstatic Optional<std::tuple<APInt, APInt, APInt, APInt, unsigned>>\nGetQuadraticEquation(const SCEVAddRecExpr *AddRec) {\n  assert(AddRec->getNumOperands() == 3 && \"This is not a quadratic chrec!\");\n  const SCEVConstant *LC = dyn_cast<SCEVConstant>(AddRec->getOperand(0));\n  const SCEVConstant *MC = dyn_cast<SCEVConstant>(AddRec->getOperand(1));\n  const SCEVConstant *NC = dyn_cast<SCEVConstant>(AddRec->getOperand(2));\n  LLVM_DEBUG(dbgs() << __func__ << \": analyzing quadratic addrec: \"\n                    << *AddRec << '\\n');\n\n  // We currently can only solve this if the coefficients are constants.\n  if (!LC || !MC || !NC) {\n    LLVM_DEBUG(dbgs() << __func__ << \": coefficients are not constant\\n\");\n    return None;\n  }\n\n  APInt L = LC->getAPInt();\n  APInt M = MC->getAPInt();\n  APInt N = NC->getAPInt();\n  assert(!N.isNullValue() && \"This is not a quadratic addrec\");\n\n  unsigned BitWidth = LC->getAPInt().getBitWidth();\n  unsigned NewWidth = BitWidth + 1;\n  LLVM_DEBUG(dbgs() << __func__ << \": addrec coeff bw: \"\n                    << BitWidth << '\\n');\n  // The sign-extension (as opposed to a zero-extension) here matches the\n  // extension used in SolveQuadraticEquationWrap (with the same motivation).\n  N = N.sext(NewWidth);\n  M = M.sext(NewWidth);\n  L = L.sext(NewWidth);\n\n  // The increments are M, M+N, M+2N, ..., so the accumulated values are\n  //   L+M, (L+M)+(M+N), (L+M)+(M+N)+(M+2N), ..., that is,\n  //   L+M, L+2M+N, L+3M+3N, ...\n  // After n iterations the accumulated value Acc is L + nM + n(n-1)/2 N.\n  //\n  // The equation Acc = 0 is then\n  //   L + nM + n(n-1)/2 N = 0,  or  2L + 2M n + n(n-1) N = 0.\n  // In a quadratic form it becomes:\n  //   N n^2 + (2M-N) n + 2L = 0.\n\n  APInt A = N;\n  APInt B = 2 * M - A;\n  APInt C = 2 * L;\n  APInt T = APInt(NewWidth, 2);\n  LLVM_DEBUG(dbgs() << __func__ << \": equation \" << A << \"x^2 + \" << B\n                    << \"x + \" << C << \", coeff bw: \" << NewWidth\n                    << \", multiplied by \" << T << '\\n');\n  return std::make_tuple(A, B, C, T, BitWidth);\n}\n\n/// Helper function to compare optional APInts:\n/// (a) if X and Y both exist, return min(X, Y),\n/// (b) if neither X nor Y exist, return None,\n/// (c) if exactly one of X and Y exists, return that value.\nstatic Optional<APInt> MinOptional(Optional<APInt> X, Optional<APInt> Y) {\n  if (X.hasValue() && Y.hasValue()) {\n    unsigned W = std::max(X->getBitWidth(), Y->getBitWidth());\n    APInt XW = X->sextOrSelf(W);\n    APInt YW = Y->sextOrSelf(W);\n    return XW.slt(YW) ? *X : *Y;\n  }\n  if (!X.hasValue() && !Y.hasValue())\n    return None;\n  return X.hasValue() ? *X : *Y;\n}\n\n/// Helper function to truncate an optional APInt to a given BitWidth.\n/// When solving addrec-related equations, it is preferable to return a value\n/// that has the same bit width as the original addrec's coefficients. If the\n/// solution fits in the original bit width, truncate it (except for i1).\n/// Returning a value of a different bit width may inhibit some optimizations.\n///\n/// In general, a solution to a quadratic equation generated from an addrec\n/// may require BW+1 bits, where BW is the bit width of the addrec's\n/// coefficients. The reason is that the coefficients of the quadratic\n/// equation are BW+1 bits wide (to avoid truncation when converting from\n/// the addrec to the equation).\nstatic Optional<APInt> TruncIfPossible(Optional<APInt> X, unsigned BitWidth) {\n  if (!X.hasValue())\n    return None;\n  unsigned W = X->getBitWidth();\n  if (BitWidth > 1 && BitWidth < W && X->isIntN(BitWidth))\n    return X->trunc(BitWidth);\n  return X;\n}\n\n/// Let c(n) be the value of the quadratic chrec {L,+,M,+,N} after n\n/// iterations. The values L, M, N are assumed to be signed, and they\n/// should all have the same bit widths.\n/// Find the least n >= 0 such that c(n) = 0 in the arithmetic modulo 2^BW,\n/// where BW is the bit width of the addrec's coefficients.\n/// If the calculated value is a BW-bit integer (for BW > 1), it will be\n/// returned as such, otherwise the bit width of the returned value may\n/// be greater than BW.\n///\n/// This function returns None if\n/// (a) the addrec coefficients are not constant, or\n/// (b) SolveQuadraticEquationWrap was unable to find a solution. For cases\n///     like x^2 = 5, no integer solutions exist, in other cases an integer\n///     solution may exist, but SolveQuadraticEquationWrap may fail to find it.\nstatic Optional<APInt>\nSolveQuadraticAddRecExact(const SCEVAddRecExpr *AddRec, ScalarEvolution &SE) {\n  APInt A, B, C, M;\n  unsigned BitWidth;\n  auto T = GetQuadraticEquation(AddRec);\n  if (!T.hasValue())\n    return None;\n\n  std::tie(A, B, C, M, BitWidth) = *T;\n  LLVM_DEBUG(dbgs() << __func__ << \": solving for unsigned overflow\\n\");\n  Optional<APInt> X = APIntOps::SolveQuadraticEquationWrap(A, B, C, BitWidth+1);\n  if (!X.hasValue())\n    return None;\n\n  ConstantInt *CX = ConstantInt::get(SE.getContext(), *X);\n  ConstantInt *V = EvaluateConstantChrecAtConstant(AddRec, CX, SE);\n  if (!V->isZero())\n    return None;\n\n  return TruncIfPossible(X, BitWidth);\n}\n\n/// Let c(n) be the value of the quadratic chrec {0,+,M,+,N} after n\n/// iterations. The values M, N are assumed to be signed, and they\n/// should all have the same bit widths.\n/// Find the least n such that c(n) does not belong to the given range,\n/// while c(n-1) does.\n///\n/// This function returns None if\n/// (a) the addrec coefficients are not constant, or\n/// (b) SolveQuadraticEquationWrap was unable to find a solution for the\n///     bounds of the range.\nstatic Optional<APInt>\nSolveQuadraticAddRecRange(const SCEVAddRecExpr *AddRec,\n                          const ConstantRange &Range, ScalarEvolution &SE) {\n  assert(AddRec->getOperand(0)->isZero() &&\n         \"Starting value of addrec should be 0\");\n  LLVM_DEBUG(dbgs() << __func__ << \": solving boundary crossing for range \"\n                    << Range << \", addrec \" << *AddRec << '\\n');\n  // This case is handled in getNumIterationsInRange. Here we can assume that\n  // we start in the range.\n  assert(Range.contains(APInt(SE.getTypeSizeInBits(AddRec->getType()), 0)) &&\n         \"Addrec's initial value should be in range\");\n\n  APInt A, B, C, M;\n  unsigned BitWidth;\n  auto T = GetQuadraticEquation(AddRec);\n  if (!T.hasValue())\n    return None;\n\n  // Be careful about the return value: there can be two reasons for not\n  // returning an actual number. First, if no solutions to the equations\n  // were found, and second, if the solutions don't leave the given range.\n  // The first case means that the actual solution is \"unknown\", the second\n  // means that it's known, but not valid. If the solution is unknown, we\n  // cannot make any conclusions.\n  // Return a pair: the optional solution and a flag indicating if the\n  // solution was found.\n  auto SolveForBoundary = [&](APInt Bound) -> std::pair<Optional<APInt>,bool> {\n    // Solve for signed overflow and unsigned overflow, pick the lower\n    // solution.\n    LLVM_DEBUG(dbgs() << \"SolveQuadraticAddRecRange: checking boundary \"\n                      << Bound << \" (before multiplying by \" << M << \")\\n\");\n    Bound *= M; // The quadratic equation multiplier.\n\n    Optional<APInt> SO = None;\n    if (BitWidth > 1) {\n      LLVM_DEBUG(dbgs() << \"SolveQuadraticAddRecRange: solving for \"\n                           \"signed overflow\\n\");\n      SO = APIntOps::SolveQuadraticEquationWrap(A, B, -Bound, BitWidth);\n    }\n    LLVM_DEBUG(dbgs() << \"SolveQuadraticAddRecRange: solving for \"\n                         \"unsigned overflow\\n\");\n    Optional<APInt> UO = APIntOps::SolveQuadraticEquationWrap(A, B, -Bound,\n                                                              BitWidth+1);\n\n    auto LeavesRange = [&] (const APInt &X) {\n      ConstantInt *C0 = ConstantInt::get(SE.getContext(), X);\n      ConstantInt *V0 = EvaluateConstantChrecAtConstant(AddRec, C0, SE);\n      if (Range.contains(V0->getValue()))\n        return false;\n      // X should be at least 1, so X-1 is non-negative.\n      ConstantInt *C1 = ConstantInt::get(SE.getContext(), X-1);\n      ConstantInt *V1 = EvaluateConstantChrecAtConstant(AddRec, C1, SE);\n      if (Range.contains(V1->getValue()))\n        return true;\n      return false;\n    };\n\n    // If SolveQuadraticEquationWrap returns None, it means that there can\n    // be a solution, but the function failed to find it. We cannot treat it\n    // as \"no solution\".\n    if (!SO.hasValue() || !UO.hasValue())\n      return { None, false };\n\n    // Check the smaller value first to see if it leaves the range.\n    // At this point, both SO and UO must have values.\n    Optional<APInt> Min = MinOptional(SO, UO);\n    if (LeavesRange(*Min))\n      return { Min, true };\n    Optional<APInt> Max = Min == SO ? UO : SO;\n    if (LeavesRange(*Max))\n      return { Max, true };\n\n    // Solutions were found, but were eliminated, hence the \"true\".\n    return { None, true };\n  };\n\n  std::tie(A, B, C, M, BitWidth) = *T;\n  // Lower bound is inclusive, subtract 1 to represent the exiting value.\n  APInt Lower = Range.getLower().sextOrSelf(A.getBitWidth()) - 1;\n  APInt Upper = Range.getUpper().sextOrSelf(A.getBitWidth());\n  auto SL = SolveForBoundary(Lower);\n  auto SU = SolveForBoundary(Upper);\n  // If any of the solutions was unknown, no meaninigful conclusions can\n  // be made.\n  if (!SL.second || !SU.second)\n    return None;\n\n  // Claim: The correct solution is not some value between Min and Max.\n  //\n  // Justification: Assuming that Min and Max are different values, one of\n  // them is when the first signed overflow happens, the other is when the\n  // first unsigned overflow happens. Crossing the range boundary is only\n  // possible via an overflow (treating 0 as a special case of it, modeling\n  // an overflow as crossing k*2^W for some k).\n  //\n  // The interesting case here is when Min was eliminated as an invalid\n  // solution, but Max was not. The argument is that if there was another\n  // overflow between Min and Max, it would also have been eliminated if\n  // it was considered.\n  //\n  // For a given boundary, it is possible to have two overflows of the same\n  // type (signed/unsigned) without having the other type in between: this\n  // can happen when the vertex of the parabola is between the iterations\n  // corresponding to the overflows. This is only possible when the two\n  // overflows cross k*2^W for the same k. In such case, if the second one\n  // left the range (and was the first one to do so), the first overflow\n  // would have to enter the range, which would mean that either we had left\n  // the range before or that we started outside of it. Both of these cases\n  // are contradictions.\n  //\n  // Claim: In the case where SolveForBoundary returns None, the correct\n  // solution is not some value between the Max for this boundary and the\n  // Min of the other boundary.\n  //\n  // Justification: Assume that we had such Max_A and Min_B corresponding\n  // to range boundaries A and B and such that Max_A < Min_B. If there was\n  // a solution between Max_A and Min_B, it would have to be caused by an\n  // overflow corresponding to either A or B. It cannot correspond to B,\n  // since Min_B is the first occurrence of such an overflow. If it\n  // corresponded to A, it would have to be either a signed or an unsigned\n  // overflow that is larger than both eliminated overflows for A. But\n  // between the eliminated overflows and this overflow, the values would\n  // cover the entire value space, thus crossing the other boundary, which\n  // is a contradiction.\n\n  return TruncIfPossible(MinOptional(SL.first, SU.first), BitWidth);\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::howFarToZero(const SCEV *V, const Loop *L, bool ControlsExit,\n                              bool AllowPredicates) {\n\n  // This is only used for loops with a \"x != y\" exit test. The exit condition\n  // is now expressed as a single expression, V = x-y. So the exit test is\n  // effectively V != 0.  We know and take advantage of the fact that this\n  // expression only being used in a comparison by zero context.\n\n  SmallPtrSet<const SCEVPredicate *, 4> Predicates;\n  // If the value is a constant\n  if (const SCEVConstant *C = dyn_cast<SCEVConstant>(V)) {\n    // If the value is already zero, the branch will execute zero times.\n    if (C->getValue()->isZero()) return C;\n    return getCouldNotCompute();  // Otherwise it will loop infinitely.\n  }\n\n  const SCEVAddRecExpr *AddRec =\n      dyn_cast<SCEVAddRecExpr>(stripInjectiveFunctions(V));\n\n  if (!AddRec && AllowPredicates)\n    // Try to make this an AddRec using runtime tests, in the first X\n    // iterations of this loop, where X is the SCEV expression found by the\n    // algorithm below.\n    AddRec = convertSCEVToAddRecWithPredicates(V, L, Predicates);\n\n  if (!AddRec || AddRec->getLoop() != L)\n    return getCouldNotCompute();\n\n  // If this is a quadratic (3-term) AddRec {L,+,M,+,N}, find the roots of\n  // the quadratic equation to solve it.\n  if (AddRec->isQuadratic() && AddRec->getType()->isIntegerTy()) {\n    // We can only use this value if the chrec ends up with an exact zero\n    // value at this index.  When solving for \"X*X != 5\", for example, we\n    // should not accept a root of 2.\n    if (auto S = SolveQuadraticAddRecExact(AddRec, *this)) {\n      const auto *R = cast<SCEVConstant>(getConstant(S.getValue()));\n      return ExitLimit(R, R, false, Predicates);\n    }\n    return getCouldNotCompute();\n  }\n\n  // Otherwise we can only handle this if it is affine.\n  if (!AddRec->isAffine())\n    return getCouldNotCompute();\n\n  // If this is an affine expression, the execution count of this branch is\n  // the minimum unsigned root of the following equation:\n  //\n  //     Start + Step*N = 0 (mod 2^BW)\n  //\n  // equivalent to:\n  //\n  //             Step*N = -Start (mod 2^BW)\n  //\n  // where BW is the common bit width of Start and Step.\n\n  // Get the initial value for the loop.\n  const SCEV *Start = getSCEVAtScope(AddRec->getStart(), L->getParentLoop());\n  const SCEV *Step = getSCEVAtScope(AddRec->getOperand(1), L->getParentLoop());\n\n  // For now we handle only constant steps.\n  //\n  // TODO: Handle a nonconstant Step given AddRec<NUW>. If the\n  // AddRec is NUW, then (in an unsigned sense) it cannot be counting up to wrap\n  // to 0, it must be counting down to equal 0. Consequently, N = Start / -Step.\n  // We have not yet seen any such cases.\n  const SCEVConstant *StepC = dyn_cast<SCEVConstant>(Step);\n  if (!StepC || StepC->getValue()->isZero())\n    return getCouldNotCompute();\n\n  // For positive steps (counting up until unsigned overflow):\n  //   N = -Start/Step (as unsigned)\n  // For negative steps (counting down to zero):\n  //   N = Start/-Step\n  // First compute the unsigned distance from zero in the direction of Step.\n  bool CountDown = StepC->getAPInt().isNegative();\n  const SCEV *Distance = CountDown ? Start : getNegativeSCEV(Start);\n\n  // Handle unitary steps, which cannot wraparound.\n  // 1*N = -Start; -1*N = Start (mod 2^BW), so:\n  //   N = Distance (as unsigned)\n  if (StepC->getValue()->isOne() || StepC->getValue()->isMinusOne()) {\n    APInt MaxBECount = getUnsignedRangeMax(applyLoopGuards(Distance, L));\n    APInt MaxBECountBase = getUnsignedRangeMax(Distance);\n    if (MaxBECountBase.ult(MaxBECount))\n      MaxBECount = MaxBECountBase;\n\n    // When a loop like \"for (int i = 0; i != n; ++i) { /* body */ }\" is rotated,\n    // we end up with a loop whose backedge-taken count is n - 1.  Detect this\n    // case, and see if we can improve the bound.\n    //\n    // Explicitly handling this here is necessary because getUnsignedRange\n    // isn't context-sensitive; it doesn't know that we only care about the\n    // range inside the loop.\n    const SCEV *Zero = getZero(Distance->getType());\n    const SCEV *One = getOne(Distance->getType());\n    const SCEV *DistancePlusOne = getAddExpr(Distance, One);\n    if (isLoopEntryGuardedByCond(L, ICmpInst::ICMP_NE, DistancePlusOne, Zero)) {\n      // If Distance + 1 doesn't overflow, we can compute the maximum distance\n      // as \"unsigned_max(Distance + 1) - 1\".\n      ConstantRange CR = getUnsignedRange(DistancePlusOne);\n      MaxBECount = APIntOps::umin(MaxBECount, CR.getUnsignedMax() - 1);\n    }\n    return ExitLimit(Distance, getConstant(MaxBECount), false, Predicates);\n  }\n\n  // If the condition controls loop exit (the loop exits only if the expression\n  // is true) and the addition is no-wrap we can use unsigned divide to\n  // compute the backedge count.  In this case, the step may not divide the\n  // distance, but we don't care because if the condition is \"missed\" the loop\n  // will have undefined behavior due to wrapping.\n  if (ControlsExit && AddRec->hasNoSelfWrap() &&\n      loopHasNoAbnormalExits(AddRec->getLoop())) {\n    const SCEV *Exact =\n        getUDivExpr(Distance, CountDown ? getNegativeSCEV(Step) : Step);\n    const SCEV *Max =\n        Exact == getCouldNotCompute()\n            ? Exact\n            : getConstant(getUnsignedRangeMax(Exact));\n    return ExitLimit(Exact, Max, false, Predicates);\n  }\n\n  // Solve the general equation.\n  const SCEV *E = SolveLinEquationWithOverflow(StepC->getAPInt(),\n                                               getNegativeSCEV(Start), *this);\n  const SCEV *M = E == getCouldNotCompute()\n                      ? E\n                      : getConstant(getUnsignedRangeMax(E));\n  return ExitLimit(E, M, false, Predicates);\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::howFarToNonZero(const SCEV *V, const Loop *L) {\n  // Loops that look like: while (X == 0) are very strange indeed.  We don't\n  // handle them yet except for the trivial case.  This could be expanded in the\n  // future as needed.\n\n  // If the value is a constant, check to see if it is known to be non-zero\n  // already.  If so, the backedge will execute zero times.\n  if (const SCEVConstant *C = dyn_cast<SCEVConstant>(V)) {\n    if (!C->getValue()->isZero())\n      return getZero(C->getType());\n    return getCouldNotCompute();  // Otherwise it will loop infinitely.\n  }\n\n  // We could implement others, but I really doubt anyone writes loops like\n  // this, and if they did, they would already be constant folded.\n  return getCouldNotCompute();\n}\n\nstd::pair<const BasicBlock *, const BasicBlock *>\nScalarEvolution::getPredecessorWithUniqueSuccessorForBB(const BasicBlock *BB)\n    const {\n  // If the block has a unique predecessor, then there is no path from the\n  // predecessor to the block that does not go through the direct edge\n  // from the predecessor to the block.\n  if (const BasicBlock *Pred = BB->getSinglePredecessor())\n    return {Pred, BB};\n\n  // A loop's header is defined to be a block that dominates the loop.\n  // If the header has a unique predecessor outside the loop, it must be\n  // a block that has exactly one successor that can reach the loop.\n  if (const Loop *L = LI.getLoopFor(BB))\n    return {L->getLoopPredecessor(), L->getHeader()};\n\n  return {nullptr, nullptr};\n}\n\n/// SCEV structural equivalence is usually sufficient for testing whether two\n/// expressions are equal, however for the purposes of looking for a condition\n/// guarding a loop, it can be useful to be a little more general, since a\n/// front-end may have replicated the controlling expression.\nstatic bool HasSameValue(const SCEV *A, const SCEV *B) {\n  // Quick check to see if they are the same SCEV.\n  if (A == B) return true;\n\n  auto ComputesEqualValues = [](const Instruction *A, const Instruction *B) {\n    // Not all instructions that are \"identical\" compute the same value.  For\n    // instance, two distinct alloca instructions allocating the same type are\n    // identical and do not read memory; but compute distinct values.\n    return A->isIdenticalTo(B) && (isa<BinaryOperator>(A) || isa<GetElementPtrInst>(A));\n  };\n\n  // Otherwise, if they're both SCEVUnknown, it's possible that they hold\n  // two different instructions with the same value. Check for this case.\n  if (const SCEVUnknown *AU = dyn_cast<SCEVUnknown>(A))\n    if (const SCEVUnknown *BU = dyn_cast<SCEVUnknown>(B))\n      if (const Instruction *AI = dyn_cast<Instruction>(AU->getValue()))\n        if (const Instruction *BI = dyn_cast<Instruction>(BU->getValue()))\n          if (ComputesEqualValues(AI, BI))\n            return true;\n\n  // Otherwise assume they may have a different value.\n  return false;\n}\n\nbool ScalarEvolution::SimplifyICmpOperands(ICmpInst::Predicate &Pred,\n                                           const SCEV *&LHS, const SCEV *&RHS,\n                                           unsigned Depth) {\n  bool Changed = false;\n  // Simplifies ICMP to trivial true or false by turning it into '0 == 0' or\n  // '0 != 0'.\n  auto TrivialCase = [&](bool TriviallyTrue) {\n    LHS = RHS = getConstant(ConstantInt::getFalse(getContext()));\n    Pred = TriviallyTrue ? ICmpInst::ICMP_EQ : ICmpInst::ICMP_NE;\n    return true;\n  };\n  // If we hit the max recursion limit bail out.\n  if (Depth >= 3)\n    return false;\n\n  // Canonicalize a constant to the right side.\n  if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(LHS)) {\n    // Check for both operands constant.\n    if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS)) {\n      if (ConstantExpr::getICmp(Pred,\n                                LHSC->getValue(),\n                                RHSC->getValue())->isNullValue())\n        return TrivialCase(false);\n      else\n        return TrivialCase(true);\n    }\n    // Otherwise swap the operands to put the constant on the right.\n    std::swap(LHS, RHS);\n    Pred = ICmpInst::getSwappedPredicate(Pred);\n    Changed = true;\n  }\n\n  // If we're comparing an addrec with a value which is loop-invariant in the\n  // addrec's loop, put the addrec on the left. Also make a dominance check,\n  // as both operands could be addrecs loop-invariant in each other's loop.\n  if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(RHS)) {\n    const Loop *L = AR->getLoop();\n    if (isLoopInvariant(LHS, L) && properlyDominates(LHS, L->getHeader())) {\n      std::swap(LHS, RHS);\n      Pred = ICmpInst::getSwappedPredicate(Pred);\n      Changed = true;\n    }\n  }\n\n  // If there's a constant operand, canonicalize comparisons with boundary\n  // cases, and canonicalize *-or-equal comparisons to regular comparisons.\n  if (const SCEVConstant *RC = dyn_cast<SCEVConstant>(RHS)) {\n    const APInt &RA = RC->getAPInt();\n\n    bool SimplifiedByConstantRange = false;\n\n    if (!ICmpInst::isEquality(Pred)) {\n      ConstantRange ExactCR = ConstantRange::makeExactICmpRegion(Pred, RA);\n      if (ExactCR.isFullSet())\n        return TrivialCase(true);\n      else if (ExactCR.isEmptySet())\n        return TrivialCase(false);\n\n      APInt NewRHS;\n      CmpInst::Predicate NewPred;\n      if (ExactCR.getEquivalentICmp(NewPred, NewRHS) &&\n          ICmpInst::isEquality(NewPred)) {\n        // We were able to convert an inequality to an equality.\n        Pred = NewPred;\n        RHS = getConstant(NewRHS);\n        Changed = SimplifiedByConstantRange = true;\n      }\n    }\n\n    if (!SimplifiedByConstantRange) {\n      switch (Pred) {\n      default:\n        break;\n      case ICmpInst::ICMP_EQ:\n      case ICmpInst::ICMP_NE:\n        // Fold ((-1) * %a) + %b == 0 (equivalent to %b-%a == 0) into %a == %b.\n        if (!RA)\n          if (const SCEVAddExpr *AE = dyn_cast<SCEVAddExpr>(LHS))\n            if (const SCEVMulExpr *ME =\n                    dyn_cast<SCEVMulExpr>(AE->getOperand(0)))\n              if (AE->getNumOperands() == 2 && ME->getNumOperands() == 2 &&\n                  ME->getOperand(0)->isAllOnesValue()) {\n                RHS = AE->getOperand(1);\n                LHS = ME->getOperand(1);\n                Changed = true;\n              }\n        break;\n\n\n        // The \"Should have been caught earlier!\" messages refer to the fact\n        // that the ExactCR.isFullSet() or ExactCR.isEmptySet() check above\n        // should have fired on the corresponding cases, and canonicalized the\n        // check to trivial case.\n\n      case ICmpInst::ICMP_UGE:\n        assert(!RA.isMinValue() && \"Should have been caught earlier!\");\n        Pred = ICmpInst::ICMP_UGT;\n        RHS = getConstant(RA - 1);\n        Changed = true;\n        break;\n      case ICmpInst::ICMP_ULE:\n        assert(!RA.isMaxValue() && \"Should have been caught earlier!\");\n        Pred = ICmpInst::ICMP_ULT;\n        RHS = getConstant(RA + 1);\n        Changed = true;\n        break;\n      case ICmpInst::ICMP_SGE:\n        assert(!RA.isMinSignedValue() && \"Should have been caught earlier!\");\n        Pred = ICmpInst::ICMP_SGT;\n        RHS = getConstant(RA - 1);\n        Changed = true;\n        break;\n      case ICmpInst::ICMP_SLE:\n        assert(!RA.isMaxSignedValue() && \"Should have been caught earlier!\");\n        Pred = ICmpInst::ICMP_SLT;\n        RHS = getConstant(RA + 1);\n        Changed = true;\n        break;\n      }\n    }\n  }\n\n  // Check for obvious equality.\n  if (HasSameValue(LHS, RHS)) {\n    if (ICmpInst::isTrueWhenEqual(Pred))\n      return TrivialCase(true);\n    if (ICmpInst::isFalseWhenEqual(Pred))\n      return TrivialCase(false);\n  }\n\n  // If possible, canonicalize GE/LE comparisons to GT/LT comparisons, by\n  // adding or subtracting 1 from one of the operands.\n  switch (Pred) {\n  case ICmpInst::ICMP_SLE:\n    if (!getSignedRangeMax(RHS).isMaxSignedValue()) {\n      RHS = getAddExpr(getConstant(RHS->getType(), 1, true), RHS,\n                       SCEV::FlagNSW);\n      Pred = ICmpInst::ICMP_SLT;\n      Changed = true;\n    } else if (!getSignedRangeMin(LHS).isMinSignedValue()) {\n      LHS = getAddExpr(getConstant(RHS->getType(), (uint64_t)-1, true), LHS,\n                       SCEV::FlagNSW);\n      Pred = ICmpInst::ICMP_SLT;\n      Changed = true;\n    }\n    break;\n  case ICmpInst::ICMP_SGE:\n    if (!getSignedRangeMin(RHS).isMinSignedValue()) {\n      RHS = getAddExpr(getConstant(RHS->getType(), (uint64_t)-1, true), RHS,\n                       SCEV::FlagNSW);\n      Pred = ICmpInst::ICMP_SGT;\n      Changed = true;\n    } else if (!getSignedRangeMax(LHS).isMaxSignedValue()) {\n      LHS = getAddExpr(getConstant(RHS->getType(), 1, true), LHS,\n                       SCEV::FlagNSW);\n      Pred = ICmpInst::ICMP_SGT;\n      Changed = true;\n    }\n    break;\n  case ICmpInst::ICMP_ULE:\n    if (!getUnsignedRangeMax(RHS).isMaxValue()) {\n      RHS = getAddExpr(getConstant(RHS->getType(), 1, true), RHS,\n                       SCEV::FlagNUW);\n      Pred = ICmpInst::ICMP_ULT;\n      Changed = true;\n    } else if (!getUnsignedRangeMin(LHS).isMinValue()) {\n      LHS = getAddExpr(getConstant(RHS->getType(), (uint64_t)-1, true), LHS);\n      Pred = ICmpInst::ICMP_ULT;\n      Changed = true;\n    }\n    break;\n  case ICmpInst::ICMP_UGE:\n    if (!getUnsignedRangeMin(RHS).isMinValue()) {\n      RHS = getAddExpr(getConstant(RHS->getType(), (uint64_t)-1, true), RHS);\n      Pred = ICmpInst::ICMP_UGT;\n      Changed = true;\n    } else if (!getUnsignedRangeMax(LHS).isMaxValue()) {\n      LHS = getAddExpr(getConstant(RHS->getType(), 1, true), LHS,\n                       SCEV::FlagNUW);\n      Pred = ICmpInst::ICMP_UGT;\n      Changed = true;\n    }\n    break;\n  default:\n    break;\n  }\n\n  // TODO: More simplifications are possible here.\n\n  // Recursively simplify until we either hit a recursion limit or nothing\n  // changes.\n  if (Changed)\n    return SimplifyICmpOperands(Pred, LHS, RHS, Depth+1);\n\n  return Changed;\n}\n\nbool ScalarEvolution::isKnownNegative(const SCEV *S) {\n  return getSignedRangeMax(S).isNegative();\n}\n\nbool ScalarEvolution::isKnownPositive(const SCEV *S) {\n  return getSignedRangeMin(S).isStrictlyPositive();\n}\n\nbool ScalarEvolution::isKnownNonNegative(const SCEV *S) {\n  return !getSignedRangeMin(S).isNegative();\n}\n\nbool ScalarEvolution::isKnownNonPositive(const SCEV *S) {\n  return !getSignedRangeMax(S).isStrictlyPositive();\n}\n\nbool ScalarEvolution::isKnownNonZero(const SCEV *S) {\n  return isKnownNegative(S) || isKnownPositive(S);\n}\n\nstd::pair<const SCEV *, const SCEV *>\nScalarEvolution::SplitIntoInitAndPostInc(const Loop *L, const SCEV *S) {\n  // Compute SCEV on entry of loop L.\n  const SCEV *Start = SCEVInitRewriter::rewrite(S, L, *this);\n  if (Start == getCouldNotCompute())\n    return { Start, Start };\n  // Compute post increment SCEV for loop L.\n  const SCEV *PostInc = SCEVPostIncRewriter::rewrite(S, L, *this);\n  assert(PostInc != getCouldNotCompute() && \"Unexpected could not compute\");\n  return { Start, PostInc };\n}\n\nbool ScalarEvolution::isKnownViaInduction(ICmpInst::Predicate Pred,\n                                          const SCEV *LHS, const SCEV *RHS) {\n  // First collect all loops.\n  SmallPtrSet<const Loop *, 8> LoopsUsed;\n  getUsedLoops(LHS, LoopsUsed);\n  getUsedLoops(RHS, LoopsUsed);\n\n  if (LoopsUsed.empty())\n    return false;\n\n  // Domination relationship must be a linear order on collected loops.\n#ifndef NDEBUG\n  for (auto *L1 : LoopsUsed)\n    for (auto *L2 : LoopsUsed)\n      assert((DT.dominates(L1->getHeader(), L2->getHeader()) ||\n              DT.dominates(L2->getHeader(), L1->getHeader())) &&\n             \"Domination relationship is not a linear order\");\n#endif\n\n  const Loop *MDL =\n      *std::max_element(LoopsUsed.begin(), LoopsUsed.end(),\n                        [&](const Loop *L1, const Loop *L2) {\n         return DT.properlyDominates(L1->getHeader(), L2->getHeader());\n       });\n\n  // Get init and post increment value for LHS.\n  auto SplitLHS = SplitIntoInitAndPostInc(MDL, LHS);\n  // if LHS contains unknown non-invariant SCEV then bail out.\n  if (SplitLHS.first == getCouldNotCompute())\n    return false;\n  assert (SplitLHS.second != getCouldNotCompute() && \"Unexpected CNC\");\n  // Get init and post increment value for RHS.\n  auto SplitRHS = SplitIntoInitAndPostInc(MDL, RHS);\n  // if RHS contains unknown non-invariant SCEV then bail out.\n  if (SplitRHS.first == getCouldNotCompute())\n    return false;\n  assert (SplitRHS.second != getCouldNotCompute() && \"Unexpected CNC\");\n  // It is possible that init SCEV contains an invariant load but it does\n  // not dominate MDL and is not available at MDL loop entry, so we should\n  // check it here.\n  if (!isAvailableAtLoopEntry(SplitLHS.first, MDL) ||\n      !isAvailableAtLoopEntry(SplitRHS.first, MDL))\n    return false;\n\n  // It seems backedge guard check is faster than entry one so in some cases\n  // it can speed up whole estimation by short circuit\n  return isLoopBackedgeGuardedByCond(MDL, Pred, SplitLHS.second,\n                                     SplitRHS.second) &&\n         isLoopEntryGuardedByCond(MDL, Pred, SplitLHS.first, SplitRHS.first);\n}\n\nbool ScalarEvolution::isKnownPredicate(ICmpInst::Predicate Pred,\n                                       const SCEV *LHS, const SCEV *RHS) {\n  // Canonicalize the inputs first.\n  (void)SimplifyICmpOperands(Pred, LHS, RHS);\n\n  if (isKnownViaInduction(Pred, LHS, RHS))\n    return true;\n\n  if (isKnownPredicateViaSplitting(Pred, LHS, RHS))\n    return true;\n\n  // Otherwise see what can be done with some simple reasoning.\n  return isKnownViaNonRecursiveReasoning(Pred, LHS, RHS);\n}\n\nbool ScalarEvolution::isKnownPredicateAt(ICmpInst::Predicate Pred,\n                                         const SCEV *LHS, const SCEV *RHS,\n                                         const Instruction *Context) {\n  // TODO: Analyze guards and assumes from Context's block.\n  return isKnownPredicate(Pred, LHS, RHS) ||\n         isBasicBlockEntryGuardedByCond(Context->getParent(), Pred, LHS, RHS);\n}\n\nbool ScalarEvolution::isKnownOnEveryIteration(ICmpInst::Predicate Pred,\n                                              const SCEVAddRecExpr *LHS,\n                                              const SCEV *RHS) {\n  const Loop *L = LHS->getLoop();\n  return isLoopEntryGuardedByCond(L, Pred, LHS->getStart(), RHS) &&\n         isLoopBackedgeGuardedByCond(L, Pred, LHS->getPostIncExpr(*this), RHS);\n}\n\nOptional<ScalarEvolution::MonotonicPredicateType>\nScalarEvolution::getMonotonicPredicateType(const SCEVAddRecExpr *LHS,\n                                           ICmpInst::Predicate Pred) {\n  auto Result = getMonotonicPredicateTypeImpl(LHS, Pred);\n\n#ifndef NDEBUG\n  // Verify an invariant: inverting the predicate should turn a monotonically\n  // increasing change to a monotonically decreasing one, and vice versa.\n  if (Result) {\n    auto ResultSwapped =\n        getMonotonicPredicateTypeImpl(LHS, ICmpInst::getSwappedPredicate(Pred));\n\n    assert(ResultSwapped.hasValue() && \"should be able to analyze both!\");\n    assert(ResultSwapped.getValue() != Result.getValue() &&\n           \"monotonicity should flip as we flip the predicate\");\n  }\n#endif\n\n  return Result;\n}\n\nOptional<ScalarEvolution::MonotonicPredicateType>\nScalarEvolution::getMonotonicPredicateTypeImpl(const SCEVAddRecExpr *LHS,\n                                               ICmpInst::Predicate Pred) {\n  // A zero step value for LHS means the induction variable is essentially a\n  // loop invariant value. We don't really depend on the predicate actually\n  // flipping from false to true (for increasing predicates, and the other way\n  // around for decreasing predicates), all we care about is that *if* the\n  // predicate changes then it only changes from false to true.\n  //\n  // A zero step value in itself is not very useful, but there may be places\n  // where SCEV can prove X >= 0 but not prove X > 0, so it is helpful to be\n  // as general as possible.\n\n  // Only handle LE/LT/GE/GT predicates.\n  if (!ICmpInst::isRelational(Pred))\n    return None;\n\n  bool IsGreater = ICmpInst::isGE(Pred) || ICmpInst::isGT(Pred);\n  assert((IsGreater || ICmpInst::isLE(Pred) || ICmpInst::isLT(Pred)) &&\n         \"Should be greater or less!\");\n\n  // Check that AR does not wrap.\n  if (ICmpInst::isUnsigned(Pred)) {\n    if (!LHS->hasNoUnsignedWrap())\n      return None;\n    return IsGreater ? MonotonicallyIncreasing : MonotonicallyDecreasing;\n  } else {\n    assert(ICmpInst::isSigned(Pred) &&\n           \"Relational predicate is either signed or unsigned!\");\n    if (!LHS->hasNoSignedWrap())\n      return None;\n\n    const SCEV *Step = LHS->getStepRecurrence(*this);\n\n    if (isKnownNonNegative(Step))\n      return IsGreater ? MonotonicallyIncreasing : MonotonicallyDecreasing;\n\n    if (isKnownNonPositive(Step))\n      return !IsGreater ? MonotonicallyIncreasing : MonotonicallyDecreasing;\n\n    return None;\n  }\n}\n\nOptional<ScalarEvolution::LoopInvariantPredicate>\nScalarEvolution::getLoopInvariantPredicate(ICmpInst::Predicate Pred,\n                                           const SCEV *LHS, const SCEV *RHS,\n                                           const Loop *L) {\n\n  // If there is a loop-invariant, force it into the RHS, otherwise bail out.\n  if (!isLoopInvariant(RHS, L)) {\n    if (!isLoopInvariant(LHS, L))\n      return None;\n\n    std::swap(LHS, RHS);\n    Pred = ICmpInst::getSwappedPredicate(Pred);\n  }\n\n  const SCEVAddRecExpr *ArLHS = dyn_cast<SCEVAddRecExpr>(LHS);\n  if (!ArLHS || ArLHS->getLoop() != L)\n    return None;\n\n  auto MonotonicType = getMonotonicPredicateType(ArLHS, Pred);\n  if (!MonotonicType)\n    return None;\n  // If the predicate \"ArLHS `Pred` RHS\" monotonically increases from false to\n  // true as the loop iterates, and the backedge is control dependent on\n  // \"ArLHS `Pred` RHS\" == true then we can reason as follows:\n  //\n  //   * if the predicate was false in the first iteration then the predicate\n  //     is never evaluated again, since the loop exits without taking the\n  //     backedge.\n  //   * if the predicate was true in the first iteration then it will\n  //     continue to be true for all future iterations since it is\n  //     monotonically increasing.\n  //\n  // For both the above possibilities, we can replace the loop varying\n  // predicate with its value on the first iteration of the loop (which is\n  // loop invariant).\n  //\n  // A similar reasoning applies for a monotonically decreasing predicate, by\n  // replacing true with false and false with true in the above two bullets.\n  bool Increasing = *MonotonicType == ScalarEvolution::MonotonicallyIncreasing;\n  auto P = Increasing ? Pred : ICmpInst::getInversePredicate(Pred);\n\n  if (!isLoopBackedgeGuardedByCond(L, P, LHS, RHS))\n    return None;\n\n  return ScalarEvolution::LoopInvariantPredicate(Pred, ArLHS->getStart(), RHS);\n}\n\nOptional<ScalarEvolution::LoopInvariantPredicate>\nScalarEvolution::getLoopInvariantExitCondDuringFirstIterations(\n    ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS, const Loop *L,\n    const Instruction *Context, const SCEV *MaxIter) {\n  // Try to prove the following set of facts:\n  // - The predicate is monotonic in the iteration space.\n  // - If the check does not fail on the 1st iteration:\n  //   - No overflow will happen during first MaxIter iterations;\n  //   - It will not fail on the MaxIter'th iteration.\n  // If the check does fail on the 1st iteration, we leave the loop and no\n  // other checks matter.\n\n  // If there is a loop-invariant, force it into the RHS, otherwise bail out.\n  if (!isLoopInvariant(RHS, L)) {\n    if (!isLoopInvariant(LHS, L))\n      return None;\n\n    std::swap(LHS, RHS);\n    Pred = ICmpInst::getSwappedPredicate(Pred);\n  }\n\n  auto *AR = dyn_cast<SCEVAddRecExpr>(LHS);\n  if (!AR || AR->getLoop() != L)\n    return None;\n\n  // The predicate must be relational (i.e. <, <=, >=, >).\n  if (!ICmpInst::isRelational(Pred))\n    return None;\n\n  // TODO: Support steps other than +/- 1.\n  const SCEV *Step = AR->getStepRecurrence(*this);\n  auto *One = getOne(Step->getType());\n  auto *MinusOne = getNegativeSCEV(One);\n  if (Step != One && Step != MinusOne)\n    return None;\n\n  // Type mismatch here means that MaxIter is potentially larger than max\n  // unsigned value in start type, which mean we cannot prove no wrap for the\n  // indvar.\n  if (AR->getType() != MaxIter->getType())\n    return None;\n\n  // Value of IV on suggested last iteration.\n  const SCEV *Last = AR->evaluateAtIteration(MaxIter, *this);\n  // Does it still meet the requirement?\n  if (!isLoopBackedgeGuardedByCond(L, Pred, Last, RHS))\n    return None;\n  // Because step is +/- 1 and MaxIter has same type as Start (i.e. it does\n  // not exceed max unsigned value of this type), this effectively proves\n  // that there is no wrap during the iteration. To prove that there is no\n  // signed/unsigned wrap, we need to check that\n  // Start <= Last for step = 1 or Start >= Last for step = -1.\n  ICmpInst::Predicate NoOverflowPred =\n      CmpInst::isSigned(Pred) ? ICmpInst::ICMP_SLE : ICmpInst::ICMP_ULE;\n  if (Step == MinusOne)\n    NoOverflowPred = CmpInst::getSwappedPredicate(NoOverflowPred);\n  const SCEV *Start = AR->getStart();\n  if (!isKnownPredicateAt(NoOverflowPred, Start, Last, Context))\n    return None;\n\n  // Everything is fine.\n  return ScalarEvolution::LoopInvariantPredicate(Pred, Start, RHS);\n}\n\nbool ScalarEvolution::isKnownPredicateViaConstantRanges(\n    ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS) {\n  if (HasSameValue(LHS, RHS))\n    return ICmpInst::isTrueWhenEqual(Pred);\n\n  // This code is split out from isKnownPredicate because it is called from\n  // within isLoopEntryGuardedByCond.\n\n  auto CheckRanges =\n      [&](const ConstantRange &RangeLHS, const ConstantRange &RangeRHS) {\n    return ConstantRange::makeSatisfyingICmpRegion(Pred, RangeRHS)\n        .contains(RangeLHS);\n  };\n\n  // The check at the top of the function catches the case where the values are\n  // known to be equal.\n  if (Pred == CmpInst::ICMP_EQ)\n    return false;\n\n  if (Pred == CmpInst::ICMP_NE)\n    return CheckRanges(getSignedRange(LHS), getSignedRange(RHS)) ||\n           CheckRanges(getUnsignedRange(LHS), getUnsignedRange(RHS)) ||\n           isKnownNonZero(getMinusSCEV(LHS, RHS));\n\n  if (CmpInst::isSigned(Pred))\n    return CheckRanges(getSignedRange(LHS), getSignedRange(RHS));\n\n  return CheckRanges(getUnsignedRange(LHS), getUnsignedRange(RHS));\n}\n\nbool ScalarEvolution::isKnownPredicateViaNoOverflow(ICmpInst::Predicate Pred,\n                                                    const SCEV *LHS,\n                                                    const SCEV *RHS) {\n  // Match Result to (X + Y)<ExpectedFlags> where Y is a constant integer.\n  // Return Y via OutY.\n  auto MatchBinaryAddToConst =\n      [this](const SCEV *Result, const SCEV *X, APInt &OutY,\n             SCEV::NoWrapFlags ExpectedFlags) {\n    const SCEV *NonConstOp, *ConstOp;\n    SCEV::NoWrapFlags FlagsPresent;\n\n    if (!splitBinaryAdd(Result, ConstOp, NonConstOp, FlagsPresent) ||\n        !isa<SCEVConstant>(ConstOp) || NonConstOp != X)\n      return false;\n\n    OutY = cast<SCEVConstant>(ConstOp)->getAPInt();\n    return (FlagsPresent & ExpectedFlags) == ExpectedFlags;\n  };\n\n  APInt C;\n\n  switch (Pred) {\n  default:\n    break;\n\n  case ICmpInst::ICMP_SGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_SLE:\n    // X s<= (X + C)<nsw> if C >= 0\n    if (MatchBinaryAddToConst(RHS, LHS, C, SCEV::FlagNSW) && C.isNonNegative())\n      return true;\n\n    // (X + C)<nsw> s<= X if C <= 0\n    if (MatchBinaryAddToConst(LHS, RHS, C, SCEV::FlagNSW) &&\n        !C.isStrictlyPositive())\n      return true;\n    break;\n\n  case ICmpInst::ICMP_SGT:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_SLT:\n    // X s< (X + C)<nsw> if C > 0\n    if (MatchBinaryAddToConst(RHS, LHS, C, SCEV::FlagNSW) &&\n        C.isStrictlyPositive())\n      return true;\n\n    // (X + C)<nsw> s< X if C < 0\n    if (MatchBinaryAddToConst(LHS, RHS, C, SCEV::FlagNSW) && C.isNegative())\n      return true;\n    break;\n\n  case ICmpInst::ICMP_UGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_ULE:\n    // X u<= (X + C)<nuw> for any C\n    if (MatchBinaryAddToConst(RHS, LHS, C, SCEV::FlagNUW))\n      return true;\n    break;\n\n  case ICmpInst::ICMP_UGT:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_ULT:\n    // X u< (X + C)<nuw> if C != 0\n    if (MatchBinaryAddToConst(RHS, LHS, C, SCEV::FlagNUW) && !C.isNullValue())\n      return true;\n    break;\n  }\n\n  return false;\n}\n\nbool ScalarEvolution::isKnownPredicateViaSplitting(ICmpInst::Predicate Pred,\n                                                   const SCEV *LHS,\n                                                   const SCEV *RHS) {\n  if (Pred != ICmpInst::ICMP_ULT || ProvingSplitPredicate)\n    return false;\n\n  // Allowing arbitrary number of activations of isKnownPredicateViaSplitting on\n  // the stack can result in exponential time complexity.\n  SaveAndRestore<bool> Restore(ProvingSplitPredicate, true);\n\n  // If L >= 0 then I `ult` L <=> I >= 0 && I `slt` L\n  //\n  // To prove L >= 0 we use isKnownNonNegative whereas to prove I >= 0 we use\n  // isKnownPredicate.  isKnownPredicate is more powerful, but also more\n  // expensive; and using isKnownNonNegative(RHS) is sufficient for most of the\n  // interesting cases seen in practice.  We can consider \"upgrading\" L >= 0 to\n  // use isKnownPredicate later if needed.\n  return isKnownNonNegative(RHS) &&\n         isKnownPredicate(CmpInst::ICMP_SGE, LHS, getZero(LHS->getType())) &&\n         isKnownPredicate(CmpInst::ICMP_SLT, LHS, RHS);\n}\n\nbool ScalarEvolution::isImpliedViaGuard(const BasicBlock *BB,\n                                        ICmpInst::Predicate Pred,\n                                        const SCEV *LHS, const SCEV *RHS) {\n  // No need to even try if we know the module has no guards.\n  if (!HasGuards)\n    return false;\n\n  return any_of(*BB, [&](const Instruction &I) {\n    using namespace llvm::PatternMatch;\n\n    Value *Condition;\n    return match(&I, m_Intrinsic<Intrinsic::experimental_guard>(\n                         m_Value(Condition))) &&\n           isImpliedCond(Pred, LHS, RHS, Condition, false);\n  });\n}\n\n/// isLoopBackedgeGuardedByCond - Test whether the backedge of the loop is\n/// protected by a conditional between LHS and RHS.  This is used to\n/// to eliminate casts.\nbool\nScalarEvolution::isLoopBackedgeGuardedByCond(const Loop *L,\n                                             ICmpInst::Predicate Pred,\n                                             const SCEV *LHS, const SCEV *RHS) {\n  // Interpret a null as meaning no loop, where there is obviously no guard\n  // (interprocedural conditions notwithstanding).\n  if (!L) return true;\n\n  if (VerifyIR)\n    assert(!verifyFunction(*L->getHeader()->getParent(), &dbgs()) &&\n           \"This cannot be done on broken IR!\");\n\n\n  if (isKnownViaNonRecursiveReasoning(Pred, LHS, RHS))\n    return true;\n\n  BasicBlock *Latch = L->getLoopLatch();\n  if (!Latch)\n    return false;\n\n  BranchInst *LoopContinuePredicate =\n    dyn_cast<BranchInst>(Latch->getTerminator());\n  if (LoopContinuePredicate && LoopContinuePredicate->isConditional() &&\n      isImpliedCond(Pred, LHS, RHS,\n                    LoopContinuePredicate->getCondition(),\n                    LoopContinuePredicate->getSuccessor(0) != L->getHeader()))\n    return true;\n\n  // We don't want more than one activation of the following loops on the stack\n  // -- that can lead to O(n!) time complexity.\n  if (WalkingBEDominatingConds)\n    return false;\n\n  SaveAndRestore<bool> ClearOnExit(WalkingBEDominatingConds, true);\n\n  // See if we can exploit a trip count to prove the predicate.\n  const auto &BETakenInfo = getBackedgeTakenInfo(L);\n  const SCEV *LatchBECount = BETakenInfo.getExact(Latch, this);\n  if (LatchBECount != getCouldNotCompute()) {\n    // We know that Latch branches back to the loop header exactly\n    // LatchBECount times.  This means the backdege condition at Latch is\n    // equivalent to  \"{0,+,1} u< LatchBECount\".\n    Type *Ty = LatchBECount->getType();\n    auto NoWrapFlags = SCEV::NoWrapFlags(SCEV::FlagNUW | SCEV::FlagNW);\n    const SCEV *LoopCounter =\n      getAddRecExpr(getZero(Ty), getOne(Ty), L, NoWrapFlags);\n    if (isImpliedCond(Pred, LHS, RHS, ICmpInst::ICMP_ULT, LoopCounter,\n                      LatchBECount))\n      return true;\n  }\n\n  // Check conditions due to any @llvm.assume intrinsics.\n  for (auto &AssumeVH : AC.assumptions()) {\n    auto *CI = AssumeVH.getAssumeCI();\n    if (!DT.dominates(CI, Latch->getTerminator()))\n      continue;\n\n    if (isImpliedCond(Pred, LHS, RHS, CI->getArgOperand(0), false))\n      return true;\n  }\n\n  // If the loop is not reachable from the entry block, we risk running into an\n  // infinite loop as we walk up into the dom tree.  These loops do not matter\n  // anyway, so we just return a conservative answer when we see them.\n  if (!DT.isReachableFromEntry(L->getHeader()))\n    return false;\n\n  if (isImpliedViaGuard(Latch, Pred, LHS, RHS))\n    return true;\n\n  for (DomTreeNode *DTN = DT[Latch], *HeaderDTN = DT[L->getHeader()];\n       DTN != HeaderDTN; DTN = DTN->getIDom()) {\n    assert(DTN && \"should reach the loop header before reaching the root!\");\n\n    BasicBlock *BB = DTN->getBlock();\n    if (isImpliedViaGuard(BB, Pred, LHS, RHS))\n      return true;\n\n    BasicBlock *PBB = BB->getSinglePredecessor();\n    if (!PBB)\n      continue;\n\n    BranchInst *ContinuePredicate = dyn_cast<BranchInst>(PBB->getTerminator());\n    if (!ContinuePredicate || !ContinuePredicate->isConditional())\n      continue;\n\n    Value *Condition = ContinuePredicate->getCondition();\n\n    // If we have an edge `E` within the loop body that dominates the only\n    // latch, the condition guarding `E` also guards the backedge.  This\n    // reasoning works only for loops with a single latch.\n\n    BasicBlockEdge DominatingEdge(PBB, BB);\n    if (DominatingEdge.isSingleEdge()) {\n      // We're constructively (and conservatively) enumerating edges within the\n      // loop body that dominate the latch.  The dominator tree better agree\n      // with us on this:\n      assert(DT.dominates(DominatingEdge, Latch) && \"should be!\");\n\n      if (isImpliedCond(Pred, LHS, RHS, Condition,\n                        BB != ContinuePredicate->getSuccessor(0)))\n        return true;\n    }\n  }\n\n  return false;\n}\n\nbool ScalarEvolution::isBasicBlockEntryGuardedByCond(const BasicBlock *BB,\n                                                     ICmpInst::Predicate Pred,\n                                                     const SCEV *LHS,\n                                                     const SCEV *RHS) {\n  if (VerifyIR)\n    assert(!verifyFunction(*BB->getParent(), &dbgs()) &&\n           \"This cannot be done on broken IR!\");\n\n  if (isKnownViaNonRecursiveReasoning(Pred, LHS, RHS))\n    return true;\n\n  // If we cannot prove strict comparison (e.g. a > b), maybe we can prove\n  // the facts (a >= b && a != b) separately. A typical situation is when the\n  // non-strict comparison is known from ranges and non-equality is known from\n  // dominating predicates. If we are proving strict comparison, we always try\n  // to prove non-equality and non-strict comparison separately.\n  auto NonStrictPredicate = ICmpInst::getNonStrictPredicate(Pred);\n  const bool ProvingStrictComparison = (Pred != NonStrictPredicate);\n  bool ProvedNonStrictComparison = false;\n  bool ProvedNonEquality = false;\n\n  if (ProvingStrictComparison) {\n    ProvedNonStrictComparison =\n        isKnownViaNonRecursiveReasoning(NonStrictPredicate, LHS, RHS);\n    ProvedNonEquality =\n        isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_NE, LHS, RHS);\n    if (ProvedNonStrictComparison && ProvedNonEquality)\n      return true;\n  }\n\n  // Try to prove (Pred, LHS, RHS) using isImpliedViaGuard.\n  auto ProveViaGuard = [&](const BasicBlock *Block) {\n    if (isImpliedViaGuard(Block, Pred, LHS, RHS))\n      return true;\n    if (ProvingStrictComparison) {\n      if (!ProvedNonStrictComparison)\n        ProvedNonStrictComparison =\n            isImpliedViaGuard(Block, NonStrictPredicate, LHS, RHS);\n      if (!ProvedNonEquality)\n        ProvedNonEquality =\n            isImpliedViaGuard(Block, ICmpInst::ICMP_NE, LHS, RHS);\n      if (ProvedNonStrictComparison && ProvedNonEquality)\n        return true;\n    }\n    return false;\n  };\n\n  // Try to prove (Pred, LHS, RHS) using isImpliedCond.\n  auto ProveViaCond = [&](const Value *Condition, bool Inverse) {\n    const Instruction *Context = &BB->front();\n    if (isImpliedCond(Pred, LHS, RHS, Condition, Inverse, Context))\n      return true;\n    if (ProvingStrictComparison) {\n      if (!ProvedNonStrictComparison)\n        ProvedNonStrictComparison = isImpliedCond(NonStrictPredicate, LHS, RHS,\n                                                  Condition, Inverse, Context);\n      if (!ProvedNonEquality)\n        ProvedNonEquality = isImpliedCond(ICmpInst::ICMP_NE, LHS, RHS,\n                                          Condition, Inverse, Context);\n      if (ProvedNonStrictComparison && ProvedNonEquality)\n        return true;\n    }\n    return false;\n  };\n\n  // Starting at the block's predecessor, climb up the predecessor chain, as long\n  // as there are predecessors that can be found that have unique successors\n  // leading to the original block.\n  const Loop *ContainingLoop = LI.getLoopFor(BB);\n  const BasicBlock *PredBB;\n  if (ContainingLoop && ContainingLoop->getHeader() == BB)\n    PredBB = ContainingLoop->getLoopPredecessor();\n  else\n    PredBB = BB->getSinglePredecessor();\n  for (std::pair<const BasicBlock *, const BasicBlock *> Pair(PredBB, BB);\n       Pair.first; Pair = getPredecessorWithUniqueSuccessorForBB(Pair.first)) {\n    if (ProveViaGuard(Pair.first))\n      return true;\n\n    const BranchInst *LoopEntryPredicate =\n        dyn_cast<BranchInst>(Pair.first->getTerminator());\n    if (!LoopEntryPredicate ||\n        LoopEntryPredicate->isUnconditional())\n      continue;\n\n    if (ProveViaCond(LoopEntryPredicate->getCondition(),\n                     LoopEntryPredicate->getSuccessor(0) != Pair.second))\n      return true;\n  }\n\n  // Check conditions due to any @llvm.assume intrinsics.\n  for (auto &AssumeVH : AC.assumptions()) {\n    auto *CI = AssumeVH.getAssumeCI();\n    if (!DT.dominates(CI, BB))\n      continue;\n\n    if (ProveViaCond(CI->getArgOperand(0), false))\n      return true;\n  }\n\n  return false;\n}\n\nbool ScalarEvolution::isLoopEntryGuardedByCond(const Loop *L,\n                                               ICmpInst::Predicate Pred,\n                                               const SCEV *LHS,\n                                               const SCEV *RHS) {\n  // Interpret a null as meaning no loop, where there is obviously no guard\n  // (interprocedural conditions notwithstanding).\n  if (!L)\n    return false;\n\n  // Both LHS and RHS must be available at loop entry.\n  assert(isAvailableAtLoopEntry(LHS, L) &&\n         \"LHS is not available at Loop Entry\");\n  assert(isAvailableAtLoopEntry(RHS, L) &&\n         \"RHS is not available at Loop Entry\");\n  return isBasicBlockEntryGuardedByCond(L->getHeader(), Pred, LHS, RHS);\n}\n\nbool ScalarEvolution::isImpliedCond(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                    const SCEV *RHS,\n                                    const Value *FoundCondValue, bool Inverse,\n                                    const Instruction *Context) {\n  if (!PendingLoopPredicates.insert(FoundCondValue).second)\n    return false;\n\n  auto ClearOnExit =\n      make_scope_exit([&]() { PendingLoopPredicates.erase(FoundCondValue); });\n\n  // Recursively handle And and Or conditions.\n  if (const BinaryOperator *BO = dyn_cast<BinaryOperator>(FoundCondValue)) {\n    if (BO->getOpcode() == Instruction::And) {\n      if (!Inverse)\n        return isImpliedCond(Pred, LHS, RHS, BO->getOperand(0), Inverse,\n                             Context) ||\n               isImpliedCond(Pred, LHS, RHS, BO->getOperand(1), Inverse,\n                             Context);\n    } else if (BO->getOpcode() == Instruction::Or) {\n      if (Inverse)\n        return isImpliedCond(Pred, LHS, RHS, BO->getOperand(0), Inverse,\n                             Context) ||\n               isImpliedCond(Pred, LHS, RHS, BO->getOperand(1), Inverse,\n                             Context);\n    }\n  }\n\n  const ICmpInst *ICI = dyn_cast<ICmpInst>(FoundCondValue);\n  if (!ICI) return false;\n\n  // Now that we found a conditional branch that dominates the loop or controls\n  // the loop latch. Check to see if it is the comparison we are looking for.\n  ICmpInst::Predicate FoundPred;\n  if (Inverse)\n    FoundPred = ICI->getInversePredicate();\n  else\n    FoundPred = ICI->getPredicate();\n\n  const SCEV *FoundLHS = getSCEV(ICI->getOperand(0));\n  const SCEV *FoundRHS = getSCEV(ICI->getOperand(1));\n\n  return isImpliedCond(Pred, LHS, RHS, FoundPred, FoundLHS, FoundRHS, Context);\n}\n\nbool ScalarEvolution::isImpliedCond(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                    const SCEV *RHS,\n                                    ICmpInst::Predicate FoundPred,\n                                    const SCEV *FoundLHS, const SCEV *FoundRHS,\n                                    const Instruction *Context) {\n  // Balance the types.\n  if (getTypeSizeInBits(LHS->getType()) <\n      getTypeSizeInBits(FoundLHS->getType())) {\n    // For unsigned and equality predicates, try to prove that both found\n    // operands fit into narrow unsigned range. If so, try to prove facts in\n    // narrow types.\n    if (!CmpInst::isSigned(FoundPred)) {\n      auto *NarrowType = LHS->getType();\n      auto *WideType = FoundLHS->getType();\n      auto BitWidth = getTypeSizeInBits(NarrowType);\n      const SCEV *MaxValue = getZeroExtendExpr(\n          getConstant(APInt::getMaxValue(BitWidth)), WideType);\n      if (isKnownPredicate(ICmpInst::ICMP_ULE, FoundLHS, MaxValue) &&\n          isKnownPredicate(ICmpInst::ICMP_ULE, FoundRHS, MaxValue)) {\n        const SCEV *TruncFoundLHS = getTruncateExpr(FoundLHS, NarrowType);\n        const SCEV *TruncFoundRHS = getTruncateExpr(FoundRHS, NarrowType);\n        if (isImpliedCondBalancedTypes(Pred, LHS, RHS, FoundPred, TruncFoundLHS,\n                                       TruncFoundRHS, Context))\n          return true;\n      }\n    }\n\n    if (CmpInst::isSigned(Pred)) {\n      LHS = getSignExtendExpr(LHS, FoundLHS->getType());\n      RHS = getSignExtendExpr(RHS, FoundLHS->getType());\n    } else {\n      LHS = getZeroExtendExpr(LHS, FoundLHS->getType());\n      RHS = getZeroExtendExpr(RHS, FoundLHS->getType());\n    }\n  } else if (getTypeSizeInBits(LHS->getType()) >\n      getTypeSizeInBits(FoundLHS->getType())) {\n    if (CmpInst::isSigned(FoundPred)) {\n      FoundLHS = getSignExtendExpr(FoundLHS, LHS->getType());\n      FoundRHS = getSignExtendExpr(FoundRHS, LHS->getType());\n    } else {\n      FoundLHS = getZeroExtendExpr(FoundLHS, LHS->getType());\n      FoundRHS = getZeroExtendExpr(FoundRHS, LHS->getType());\n    }\n  }\n  return isImpliedCondBalancedTypes(Pred, LHS, RHS, FoundPred, FoundLHS,\n                                    FoundRHS, Context);\n}\n\nbool ScalarEvolution::isImpliedCondBalancedTypes(\n    ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,\n    ICmpInst::Predicate FoundPred, const SCEV *FoundLHS, const SCEV *FoundRHS,\n    const Instruction *Context) {\n  assert(getTypeSizeInBits(LHS->getType()) ==\n             getTypeSizeInBits(FoundLHS->getType()) &&\n         \"Types should be balanced!\");\n  // Canonicalize the query to match the way instcombine will have\n  // canonicalized the comparison.\n  if (SimplifyICmpOperands(Pred, LHS, RHS))\n    if (LHS == RHS)\n      return CmpInst::isTrueWhenEqual(Pred);\n  if (SimplifyICmpOperands(FoundPred, FoundLHS, FoundRHS))\n    if (FoundLHS == FoundRHS)\n      return CmpInst::isFalseWhenEqual(FoundPred);\n\n  // Check to see if we can make the LHS or RHS match.\n  if (LHS == FoundRHS || RHS == FoundLHS) {\n    if (isa<SCEVConstant>(RHS)) {\n      std::swap(FoundLHS, FoundRHS);\n      FoundPred = ICmpInst::getSwappedPredicate(FoundPred);\n    } else {\n      std::swap(LHS, RHS);\n      Pred = ICmpInst::getSwappedPredicate(Pred);\n    }\n  }\n\n  // Check whether the found predicate is the same as the desired predicate.\n  if (FoundPred == Pred)\n    return isImpliedCondOperands(Pred, LHS, RHS, FoundLHS, FoundRHS, Context);\n\n  // Check whether swapping the found predicate makes it the same as the\n  // desired predicate.\n  if (ICmpInst::getSwappedPredicate(FoundPred) == Pred) {\n    if (isa<SCEVConstant>(RHS))\n      return isImpliedCondOperands(Pred, LHS, RHS, FoundRHS, FoundLHS, Context);\n    else\n      return isImpliedCondOperands(ICmpInst::getSwappedPredicate(Pred), RHS,\n                                   LHS, FoundLHS, FoundRHS, Context);\n  }\n\n  // Unsigned comparison is the same as signed comparison when both the operands\n  // are non-negative.\n  if (CmpInst::isUnsigned(FoundPred) &&\n      CmpInst::getSignedPredicate(FoundPred) == Pred &&\n      isKnownNonNegative(FoundLHS) && isKnownNonNegative(FoundRHS))\n    return isImpliedCondOperands(Pred, LHS, RHS, FoundLHS, FoundRHS, Context);\n\n  // Check if we can make progress by sharpening ranges.\n  if (FoundPred == ICmpInst::ICMP_NE &&\n      (isa<SCEVConstant>(FoundLHS) || isa<SCEVConstant>(FoundRHS))) {\n\n    const SCEVConstant *C = nullptr;\n    const SCEV *V = nullptr;\n\n    if (isa<SCEVConstant>(FoundLHS)) {\n      C = cast<SCEVConstant>(FoundLHS);\n      V = FoundRHS;\n    } else {\n      C = cast<SCEVConstant>(FoundRHS);\n      V = FoundLHS;\n    }\n\n    // The guarding predicate tells us that C != V. If the known range\n    // of V is [C, t), we can sharpen the range to [C + 1, t).  The\n    // range we consider has to correspond to same signedness as the\n    // predicate we're interested in folding.\n\n    APInt Min = ICmpInst::isSigned(Pred) ?\n        getSignedRangeMin(V) : getUnsignedRangeMin(V);\n\n    if (Min == C->getAPInt()) {\n      // Given (V >= Min && V != Min) we conclude V >= (Min + 1).\n      // This is true even if (Min + 1) wraps around -- in case of\n      // wraparound, (Min + 1) < Min, so (V >= Min => V >= (Min + 1)).\n\n      APInt SharperMin = Min + 1;\n\n      switch (Pred) {\n        case ICmpInst::ICMP_SGE:\n        case ICmpInst::ICMP_UGE:\n          // We know V `Pred` SharperMin.  If this implies LHS `Pred`\n          // RHS, we're done.\n          if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(SharperMin),\n                                    Context))\n            return true;\n          LLVM_FALLTHROUGH;\n\n        case ICmpInst::ICMP_SGT:\n        case ICmpInst::ICMP_UGT:\n          // We know from the range information that (V `Pred` Min ||\n          // V == Min).  We know from the guarding condition that !(V\n          // == Min).  This gives us\n          //\n          //       V `Pred` Min || V == Min && !(V == Min)\n          //   =>  V `Pred` Min\n          //\n          // If V `Pred` Min implies LHS `Pred` RHS, we're done.\n\n          if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(Min),\n                                    Context))\n            return true;\n          break;\n\n        // `LHS < RHS` and `LHS <= RHS` are handled in the same way as `RHS > LHS` and `RHS >= LHS` respectively.\n        case ICmpInst::ICMP_SLE:\n        case ICmpInst::ICMP_ULE:\n          if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS,\n                                    LHS, V, getConstant(SharperMin), Context))\n            return true;\n          LLVM_FALLTHROUGH;\n\n        case ICmpInst::ICMP_SLT:\n        case ICmpInst::ICMP_ULT:\n          if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS,\n                                    LHS, V, getConstant(Min), Context))\n            return true;\n          break;\n\n        default:\n          // No change\n          break;\n      }\n    }\n  }\n\n  // Check whether the actual condition is beyond sufficient.\n  if (FoundPred == ICmpInst::ICMP_EQ)\n    if (ICmpInst::isTrueWhenEqual(Pred))\n      if (isImpliedCondOperands(Pred, LHS, RHS, FoundLHS, FoundRHS, Context))\n        return true;\n  if (Pred == ICmpInst::ICMP_NE)\n    if (!ICmpInst::isTrueWhenEqual(FoundPred))\n      if (isImpliedCondOperands(FoundPred, LHS, RHS, FoundLHS, FoundRHS,\n                                Context))\n        return true;\n\n  // Otherwise assume the worst.\n  return false;\n}\n\nbool ScalarEvolution::splitBinaryAdd(const SCEV *Expr,\n                                     const SCEV *&L, const SCEV *&R,\n                                     SCEV::NoWrapFlags &Flags) {\n  const auto *AE = dyn_cast<SCEVAddExpr>(Expr);\n  if (!AE || AE->getNumOperands() != 2)\n    return false;\n\n  L = AE->getOperand(0);\n  R = AE->getOperand(1);\n  Flags = AE->getNoWrapFlags();\n  return true;\n}\n\nOptional<APInt> ScalarEvolution::computeConstantDifference(const SCEV *More,\n                                                           const SCEV *Less) {\n  // We avoid subtracting expressions here because this function is usually\n  // fairly deep in the call stack (i.e. is called many times).\n\n  // X - X = 0.\n  if (More == Less)\n    return APInt(getTypeSizeInBits(More->getType()), 0);\n\n  if (isa<SCEVAddRecExpr>(Less) && isa<SCEVAddRecExpr>(More)) {\n    const auto *LAR = cast<SCEVAddRecExpr>(Less);\n    const auto *MAR = cast<SCEVAddRecExpr>(More);\n\n    if (LAR->getLoop() != MAR->getLoop())\n      return None;\n\n    // We look at affine expressions only; not for correctness but to keep\n    // getStepRecurrence cheap.\n    if (!LAR->isAffine() || !MAR->isAffine())\n      return None;\n\n    if (LAR->getStepRecurrence(*this) != MAR->getStepRecurrence(*this))\n      return None;\n\n    Less = LAR->getStart();\n    More = MAR->getStart();\n\n    // fall through\n  }\n\n  if (isa<SCEVConstant>(Less) && isa<SCEVConstant>(More)) {\n    const auto &M = cast<SCEVConstant>(More)->getAPInt();\n    const auto &L = cast<SCEVConstant>(Less)->getAPInt();\n    return M - L;\n  }\n\n  SCEV::NoWrapFlags Flags;\n  const SCEV *LLess = nullptr, *RLess = nullptr;\n  const SCEV *LMore = nullptr, *RMore = nullptr;\n  const SCEVConstant *C1 = nullptr, *C2 = nullptr;\n  // Compare (X + C1) vs X.\n  if (splitBinaryAdd(Less, LLess, RLess, Flags))\n    if ((C1 = dyn_cast<SCEVConstant>(LLess)))\n      if (RLess == More)\n        return -(C1->getAPInt());\n\n  // Compare X vs (X + C2).\n  if (splitBinaryAdd(More, LMore, RMore, Flags))\n    if ((C2 = dyn_cast<SCEVConstant>(LMore)))\n      if (RMore == Less)\n        return C2->getAPInt();\n\n  // Compare (X + C1) vs (X + C2).\n  if (C1 && C2 && RLess == RMore)\n    return C2->getAPInt() - C1->getAPInt();\n\n  return None;\n}\n\nbool ScalarEvolution::isImpliedCondOperandsViaAddRecStart(\n    ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,\n    const SCEV *FoundLHS, const SCEV *FoundRHS, const Instruction *Context) {\n  // Try to recognize the following pattern:\n  //\n  //   FoundRHS = ...\n  // ...\n  // loop:\n  //   FoundLHS = {Start,+,W}\n  // context_bb: // Basic block from the same loop\n  //   known(Pred, FoundLHS, FoundRHS)\n  //\n  // If some predicate is known in the context of a loop, it is also known on\n  // each iteration of this loop, including the first iteration. Therefore, in\n  // this case, `FoundLHS Pred FoundRHS` implies `Start Pred FoundRHS`. Try to\n  // prove the original pred using this fact.\n  if (!Context)\n    return false;\n  const BasicBlock *ContextBB = Context->getParent();\n  // Make sure AR varies in the context block.\n  if (auto *AR = dyn_cast<SCEVAddRecExpr>(FoundLHS)) {\n    const Loop *L = AR->getLoop();\n    // Make sure that context belongs to the loop and executes on 1st iteration\n    // (if it ever executes at all).\n    if (!L->contains(ContextBB) || !DT.dominates(ContextBB, L->getLoopLatch()))\n      return false;\n    if (!isAvailableAtLoopEntry(FoundRHS, AR->getLoop()))\n      return false;\n    return isImpliedCondOperands(Pred, LHS, RHS, AR->getStart(), FoundRHS);\n  }\n\n  if (auto *AR = dyn_cast<SCEVAddRecExpr>(FoundRHS)) {\n    const Loop *L = AR->getLoop();\n    // Make sure that context belongs to the loop and executes on 1st iteration\n    // (if it ever executes at all).\n    if (!L->contains(ContextBB) || !DT.dominates(ContextBB, L->getLoopLatch()))\n      return false;\n    if (!isAvailableAtLoopEntry(FoundLHS, AR->getLoop()))\n      return false;\n    return isImpliedCondOperands(Pred, LHS, RHS, FoundLHS, AR->getStart());\n  }\n\n  return false;\n}\n\nbool ScalarEvolution::isImpliedCondOperandsViaNoOverflow(\n    ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,\n    const SCEV *FoundLHS, const SCEV *FoundRHS) {\n  if (Pred != CmpInst::ICMP_SLT && Pred != CmpInst::ICMP_ULT)\n    return false;\n\n  const auto *AddRecLHS = dyn_cast<SCEVAddRecExpr>(LHS);\n  if (!AddRecLHS)\n    return false;\n\n  const auto *AddRecFoundLHS = dyn_cast<SCEVAddRecExpr>(FoundLHS);\n  if (!AddRecFoundLHS)\n    return false;\n\n  // We'd like to let SCEV reason about control dependencies, so we constrain\n  // both the inequalities to be about add recurrences on the same loop.  This\n  // way we can use isLoopEntryGuardedByCond later.\n\n  const Loop *L = AddRecFoundLHS->getLoop();\n  if (L != AddRecLHS->getLoop())\n    return false;\n\n  //  FoundLHS u< FoundRHS u< -C =>  (FoundLHS + C) u< (FoundRHS + C) ... (1)\n  //\n  //  FoundLHS s< FoundRHS s< INT_MIN - C => (FoundLHS + C) s< (FoundRHS + C)\n  //                                                                  ... (2)\n  //\n  // Informal proof for (2), assuming (1) [*]:\n  //\n  // We'll also assume (A s< B) <=> ((A + INT_MIN) u< (B + INT_MIN)) ... (3)[**]\n  //\n  // Then\n  //\n  //       FoundLHS s< FoundRHS s< INT_MIN - C\n  // <=>  (FoundLHS + INT_MIN) u< (FoundRHS + INT_MIN) u< -C   [ using (3) ]\n  // <=>  (FoundLHS + INT_MIN + C) u< (FoundRHS + INT_MIN + C) [ using (1) ]\n  // <=>  (FoundLHS + INT_MIN + C + INT_MIN) s<\n  //                        (FoundRHS + INT_MIN + C + INT_MIN) [ using (3) ]\n  // <=>  FoundLHS + C s< FoundRHS + C\n  //\n  // [*]: (1) can be proved by ruling out overflow.\n  //\n  // [**]: This can be proved by analyzing all the four possibilities:\n  //    (A s< 0, B s< 0), (A s< 0, B s>= 0), (A s>= 0, B s< 0) and\n  //    (A s>= 0, B s>= 0).\n  //\n  // Note:\n  // Despite (2), \"FoundRHS s< INT_MIN - C\" does not mean that \"FoundRHS + C\"\n  // will not sign underflow.  For instance, say FoundLHS = (i8 -128), FoundRHS\n  // = (i8 -127) and C = (i8 -100).  Then INT_MIN - C = (i8 -28), and FoundRHS\n  // s< (INT_MIN - C).  Lack of sign overflow / underflow in \"FoundRHS + C\" is\n  // neither necessary nor sufficient to prove \"(FoundLHS + C) s< (FoundRHS +\n  // C)\".\n\n  Optional<APInt> LDiff = computeConstantDifference(LHS, FoundLHS);\n  Optional<APInt> RDiff = computeConstantDifference(RHS, FoundRHS);\n  if (!LDiff || !RDiff || *LDiff != *RDiff)\n    return false;\n\n  if (LDiff->isMinValue())\n    return true;\n\n  APInt FoundRHSLimit;\n\n  if (Pred == CmpInst::ICMP_ULT) {\n    FoundRHSLimit = -(*RDiff);\n  } else {\n    assert(Pred == CmpInst::ICMP_SLT && \"Checked above!\");\n    FoundRHSLimit = APInt::getSignedMinValue(getTypeSizeInBits(RHS->getType())) - *RDiff;\n  }\n\n  // Try to prove (1) or (2), as needed.\n  return isAvailableAtLoopEntry(FoundRHS, L) &&\n         isLoopEntryGuardedByCond(L, Pred, FoundRHS,\n                                  getConstant(FoundRHSLimit));\n}\n\nbool ScalarEvolution::isImpliedViaMerge(ICmpInst::Predicate Pred,\n                                        const SCEV *LHS, const SCEV *RHS,\n                                        const SCEV *FoundLHS,\n                                        const SCEV *FoundRHS, unsigned Depth) {\n  const PHINode *LPhi = nullptr, *RPhi = nullptr;\n\n  auto ClearOnExit = make_scope_exit([&]() {\n    if (LPhi) {\n      bool Erased = PendingMerges.erase(LPhi);\n      assert(Erased && \"Failed to erase LPhi!\");\n      (void)Erased;\n    }\n    if (RPhi) {\n      bool Erased = PendingMerges.erase(RPhi);\n      assert(Erased && \"Failed to erase RPhi!\");\n      (void)Erased;\n    }\n  });\n\n  // Find respective Phis and check that they are not being pending.\n  if (const SCEVUnknown *LU = dyn_cast<SCEVUnknown>(LHS))\n    if (auto *Phi = dyn_cast<PHINode>(LU->getValue())) {\n      if (!PendingMerges.insert(Phi).second)\n        return false;\n      LPhi = Phi;\n    }\n  if (const SCEVUnknown *RU = dyn_cast<SCEVUnknown>(RHS))\n    if (auto *Phi = dyn_cast<PHINode>(RU->getValue())) {\n      // If we detect a loop of Phi nodes being processed by this method, for\n      // example:\n      //\n      //   %a = phi i32 [ %some1, %preheader ], [ %b, %latch ]\n      //   %b = phi i32 [ %some2, %preheader ], [ %a, %latch ]\n      //\n      // we don't want to deal with a case that complex, so return conservative\n      // answer false.\n      if (!PendingMerges.insert(Phi).second)\n        return false;\n      RPhi = Phi;\n    }\n\n  // If none of LHS, RHS is a Phi, nothing to do here.\n  if (!LPhi && !RPhi)\n    return false;\n\n  // If there is a SCEVUnknown Phi we are interested in, make it left.\n  if (!LPhi) {\n    std::swap(LHS, RHS);\n    std::swap(FoundLHS, FoundRHS);\n    std::swap(LPhi, RPhi);\n    Pred = ICmpInst::getSwappedPredicate(Pred);\n  }\n\n  assert(LPhi && \"LPhi should definitely be a SCEVUnknown Phi!\");\n  const BasicBlock *LBB = LPhi->getParent();\n  const SCEVAddRecExpr *RAR = dyn_cast<SCEVAddRecExpr>(RHS);\n\n  auto ProvedEasily = [&](const SCEV *S1, const SCEV *S2) {\n    return isKnownViaNonRecursiveReasoning(Pred, S1, S2) ||\n           isImpliedCondOperandsViaRanges(Pred, S1, S2, FoundLHS, FoundRHS) ||\n           isImpliedViaOperations(Pred, S1, S2, FoundLHS, FoundRHS, Depth);\n  };\n\n  if (RPhi && RPhi->getParent() == LBB) {\n    // Case one: RHS is also a SCEVUnknown Phi from the same basic block.\n    // If we compare two Phis from the same block, and for each entry block\n    // the predicate is true for incoming values from this block, then the\n    // predicate is also true for the Phis.\n    for (const BasicBlock *IncBB : predecessors(LBB)) {\n      const SCEV *L = getSCEV(LPhi->getIncomingValueForBlock(IncBB));\n      const SCEV *R = getSCEV(RPhi->getIncomingValueForBlock(IncBB));\n      if (!ProvedEasily(L, R))\n        return false;\n    }\n  } else if (RAR && RAR->getLoop()->getHeader() == LBB) {\n    // Case two: RHS is also a Phi from the same basic block, and it is an\n    // AddRec. It means that there is a loop which has both AddRec and Unknown\n    // PHIs, for it we can compare incoming values of AddRec from above the loop\n    // and latch with their respective incoming values of LPhi.\n    // TODO: Generalize to handle loops with many inputs in a header.\n    if (LPhi->getNumIncomingValues() != 2) return false;\n\n    auto *RLoop = RAR->getLoop();\n    auto *Predecessor = RLoop->getLoopPredecessor();\n    assert(Predecessor && \"Loop with AddRec with no predecessor?\");\n    const SCEV *L1 = getSCEV(LPhi->getIncomingValueForBlock(Predecessor));\n    if (!ProvedEasily(L1, RAR->getStart()))\n      return false;\n    auto *Latch = RLoop->getLoopLatch();\n    assert(Latch && \"Loop with AddRec with no latch?\");\n    const SCEV *L2 = getSCEV(LPhi->getIncomingValueForBlock(Latch));\n    if (!ProvedEasily(L2, RAR->getPostIncExpr(*this)))\n      return false;\n  } else {\n    // In all other cases go over inputs of LHS and compare each of them to RHS,\n    // the predicate is true for (LHS, RHS) if it is true for all such pairs.\n    // At this point RHS is either a non-Phi, or it is a Phi from some block\n    // different from LBB.\n    for (const BasicBlock *IncBB : predecessors(LBB)) {\n      // Check that RHS is available in this block.\n      if (!dominates(RHS, IncBB))\n        return false;\n      const SCEV *L = getSCEV(LPhi->getIncomingValueForBlock(IncBB));\n      if (!ProvedEasily(L, RHS))\n        return false;\n    }\n  }\n  return true;\n}\n\nbool ScalarEvolution::isImpliedCondOperands(ICmpInst::Predicate Pred,\n                                            const SCEV *LHS, const SCEV *RHS,\n                                            const SCEV *FoundLHS,\n                                            const SCEV *FoundRHS,\n                                            const Instruction *Context) {\n  if (isImpliedCondOperandsViaRanges(Pred, LHS, RHS, FoundLHS, FoundRHS))\n    return true;\n\n  if (isImpliedCondOperandsViaNoOverflow(Pred, LHS, RHS, FoundLHS, FoundRHS))\n    return true;\n\n  if (isImpliedCondOperandsViaAddRecStart(Pred, LHS, RHS, FoundLHS, FoundRHS,\n                                          Context))\n    return true;\n\n  return isImpliedCondOperandsHelper(Pred, LHS, RHS,\n                                     FoundLHS, FoundRHS) ||\n         // ~x < ~y --> x > y\n         isImpliedCondOperandsHelper(Pred, LHS, RHS,\n                                     getNotSCEV(FoundRHS),\n                                     getNotSCEV(FoundLHS));\n}\n\n/// Is MaybeMinMaxExpr an (U|S)(Min|Max) of Candidate and some other values?\ntemplate <typename MinMaxExprType>\nstatic bool IsMinMaxConsistingOf(const SCEV *MaybeMinMaxExpr,\n                                 const SCEV *Candidate) {\n  const MinMaxExprType *MinMaxExpr = dyn_cast<MinMaxExprType>(MaybeMinMaxExpr);\n  if (!MinMaxExpr)\n    return false;\n\n  return is_contained(MinMaxExpr->operands(), Candidate);\n}\n\nstatic bool IsKnownPredicateViaAddRecStart(ScalarEvolution &SE,\n                                           ICmpInst::Predicate Pred,\n                                           const SCEV *LHS, const SCEV *RHS) {\n  // If both sides are affine addrecs for the same loop, with equal\n  // steps, and we know the recurrences don't wrap, then we only\n  // need to check the predicate on the starting values.\n\n  if (!ICmpInst::isRelational(Pred))\n    return false;\n\n  const SCEVAddRecExpr *LAR = dyn_cast<SCEVAddRecExpr>(LHS);\n  if (!LAR)\n    return false;\n  const SCEVAddRecExpr *RAR = dyn_cast<SCEVAddRecExpr>(RHS);\n  if (!RAR)\n    return false;\n  if (LAR->getLoop() != RAR->getLoop())\n    return false;\n  if (!LAR->isAffine() || !RAR->isAffine())\n    return false;\n\n  if (LAR->getStepRecurrence(SE) != RAR->getStepRecurrence(SE))\n    return false;\n\n  SCEV::NoWrapFlags NW = ICmpInst::isSigned(Pred) ?\n                         SCEV::FlagNSW : SCEV::FlagNUW;\n  if (!LAR->getNoWrapFlags(NW) || !RAR->getNoWrapFlags(NW))\n    return false;\n\n  return SE.isKnownPredicate(Pred, LAR->getStart(), RAR->getStart());\n}\n\n/// Is LHS `Pred` RHS true on the virtue of LHS or RHS being a Min or Max\n/// expression?\nstatic bool IsKnownPredicateViaMinOrMax(ScalarEvolution &SE,\n                                        ICmpInst::Predicate Pred,\n                                        const SCEV *LHS, const SCEV *RHS) {\n  switch (Pred) {\n  default:\n    return false;\n\n  case ICmpInst::ICMP_SGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_SLE:\n    return\n        // min(A, ...) <= A\n        IsMinMaxConsistingOf<SCEVSMinExpr>(LHS, RHS) ||\n        // A <= max(A, ...)\n        IsMinMaxConsistingOf<SCEVSMaxExpr>(RHS, LHS);\n\n  case ICmpInst::ICMP_UGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_ULE:\n    return\n        // min(A, ...) <= A\n        IsMinMaxConsistingOf<SCEVUMinExpr>(LHS, RHS) ||\n        // A <= max(A, ...)\n        IsMinMaxConsistingOf<SCEVUMaxExpr>(RHS, LHS);\n  }\n\n  llvm_unreachable(\"covered switch fell through?!\");\n}\n\nbool ScalarEvolution::isImpliedViaOperations(ICmpInst::Predicate Pred,\n                                             const SCEV *LHS, const SCEV *RHS,\n                                             const SCEV *FoundLHS,\n                                             const SCEV *FoundRHS,\n                                             unsigned Depth) {\n  assert(getTypeSizeInBits(LHS->getType()) ==\n             getTypeSizeInBits(RHS->getType()) &&\n         \"LHS and RHS have different sizes?\");\n  assert(getTypeSizeInBits(FoundLHS->getType()) ==\n             getTypeSizeInBits(FoundRHS->getType()) &&\n         \"FoundLHS and FoundRHS have different sizes?\");\n  // We want to avoid hurting the compile time with analysis of too big trees.\n  if (Depth > MaxSCEVOperationsImplicationDepth)\n    return false;\n\n  // We only want to work with GT comparison so far.\n  if (Pred == ICmpInst::ICMP_ULT || Pred == ICmpInst::ICMP_SLT) {\n    Pred = CmpInst::getSwappedPredicate(Pred);\n    std::swap(LHS, RHS);\n    std::swap(FoundLHS, FoundRHS);\n  }\n\n  // For unsigned, try to reduce it to corresponding signed comparison.\n  if (Pred == ICmpInst::ICMP_UGT)\n    // We can replace unsigned predicate with its signed counterpart if all\n    // involved values are non-negative.\n    // TODO: We could have better support for unsigned.\n    if (isKnownNonNegative(FoundLHS) && isKnownNonNegative(FoundRHS)) {\n      // Knowing that both FoundLHS and FoundRHS are non-negative, and knowing\n      // FoundLHS >u FoundRHS, we also know that FoundLHS >s FoundRHS. Let us\n      // use this fact to prove that LHS and RHS are non-negative.\n      const SCEV *MinusOne = getMinusOne(LHS->getType());\n      if (isImpliedCondOperands(ICmpInst::ICMP_SGT, LHS, MinusOne, FoundLHS,\n                                FoundRHS) &&\n          isImpliedCondOperands(ICmpInst::ICMP_SGT, RHS, MinusOne, FoundLHS,\n                                FoundRHS))\n        Pred = ICmpInst::ICMP_SGT;\n    }\n\n  if (Pred != ICmpInst::ICMP_SGT)\n    return false;\n\n  auto GetOpFromSExt = [&](const SCEV *S) {\n    if (auto *Ext = dyn_cast<SCEVSignExtendExpr>(S))\n      return Ext->getOperand();\n    // TODO: If S is a SCEVConstant then you can cheaply \"strip\" the sext off\n    // the constant in some cases.\n    return S;\n  };\n\n  // Acquire values from extensions.\n  auto *OrigLHS = LHS;\n  auto *OrigFoundLHS = FoundLHS;\n  LHS = GetOpFromSExt(LHS);\n  FoundLHS = GetOpFromSExt(FoundLHS);\n\n  // Is the SGT predicate can be proved trivially or using the found context.\n  auto IsSGTViaContext = [&](const SCEV *S1, const SCEV *S2) {\n    return isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_SGT, S1, S2) ||\n           isImpliedViaOperations(ICmpInst::ICMP_SGT, S1, S2, OrigFoundLHS,\n                                  FoundRHS, Depth + 1);\n  };\n\n  if (auto *LHSAddExpr = dyn_cast<SCEVAddExpr>(LHS)) {\n    // We want to avoid creation of any new non-constant SCEV. Since we are\n    // going to compare the operands to RHS, we should be certain that we don't\n    // need any size extensions for this. So let's decline all cases when the\n    // sizes of types of LHS and RHS do not match.\n    // TODO: Maybe try to get RHS from sext to catch more cases?\n    if (getTypeSizeInBits(LHS->getType()) != getTypeSizeInBits(RHS->getType()))\n      return false;\n\n    // Should not overflow.\n    if (!LHSAddExpr->hasNoSignedWrap())\n      return false;\n\n    auto *LL = LHSAddExpr->getOperand(0);\n    auto *LR = LHSAddExpr->getOperand(1);\n    auto *MinusOne = getMinusOne(RHS->getType());\n\n    // Checks that S1 >= 0 && S2 > RHS, trivially or using the found context.\n    auto IsSumGreaterThanRHS = [&](const SCEV *S1, const SCEV *S2) {\n      return IsSGTViaContext(S1, MinusOne) && IsSGTViaContext(S2, RHS);\n    };\n    // Try to prove the following rule:\n    // (LHS = LL + LR) && (LL >= 0) && (LR > RHS) => (LHS > RHS).\n    // (LHS = LL + LR) && (LR >= 0) && (LL > RHS) => (LHS > RHS).\n    if (IsSumGreaterThanRHS(LL, LR) || IsSumGreaterThanRHS(LR, LL))\n      return true;\n  } else if (auto *LHSUnknownExpr = dyn_cast<SCEVUnknown>(LHS)) {\n    Value *LL, *LR;\n    // FIXME: Once we have SDiv implemented, we can get rid of this matching.\n\n    using namespace llvm::PatternMatch;\n\n    if (match(LHSUnknownExpr->getValue(), m_SDiv(m_Value(LL), m_Value(LR)))) {\n      // Rules for division.\n      // We are going to perform some comparisons with Denominator and its\n      // derivative expressions. In general case, creating a SCEV for it may\n      // lead to a complex analysis of the entire graph, and in particular it\n      // can request trip count recalculation for the same loop. This would\n      // cache as SCEVCouldNotCompute to avoid the infinite recursion. To avoid\n      // this, we only want to create SCEVs that are constants in this section.\n      // So we bail if Denominator is not a constant.\n      if (!isa<ConstantInt>(LR))\n        return false;\n\n      auto *Denominator = cast<SCEVConstant>(getSCEV(LR));\n\n      // We want to make sure that LHS = FoundLHS / Denominator. If it is so,\n      // then a SCEV for the numerator already exists and matches with FoundLHS.\n      auto *Numerator = getExistingSCEV(LL);\n      if (!Numerator || Numerator->getType() != FoundLHS->getType())\n        return false;\n\n      // Make sure that the numerator matches with FoundLHS and the denominator\n      // is positive.\n      if (!HasSameValue(Numerator, FoundLHS) || !isKnownPositive(Denominator))\n        return false;\n\n      auto *DTy = Denominator->getType();\n      auto *FRHSTy = FoundRHS->getType();\n      if (DTy->isPointerTy() != FRHSTy->isPointerTy())\n        // One of types is a pointer and another one is not. We cannot extend\n        // them properly to a wider type, so let us just reject this case.\n        // TODO: Usage of getEffectiveSCEVType for DTy, FRHSTy etc should help\n        // to avoid this check.\n        return false;\n\n      // Given that:\n      // FoundLHS > FoundRHS, LHS = FoundLHS / Denominator, Denominator > 0.\n      auto *WTy = getWiderType(DTy, FRHSTy);\n      auto *DenominatorExt = getNoopOrSignExtend(Denominator, WTy);\n      auto *FoundRHSExt = getNoopOrSignExtend(FoundRHS, WTy);\n\n      // Try to prove the following rule:\n      // (FoundRHS > Denominator - 2) && (RHS <= 0) => (LHS > RHS).\n      // For example, given that FoundLHS > 2. It means that FoundLHS is at\n      // least 3. If we divide it by Denominator < 4, we will have at least 1.\n      auto *DenomMinusTwo = getMinusSCEV(DenominatorExt, getConstant(WTy, 2));\n      if (isKnownNonPositive(RHS) &&\n          IsSGTViaContext(FoundRHSExt, DenomMinusTwo))\n        return true;\n\n      // Try to prove the following rule:\n      // (FoundRHS > -1 - Denominator) && (RHS < 0) => (LHS > RHS).\n      // For example, given that FoundLHS > -3. Then FoundLHS is at least -2.\n      // If we divide it by Denominator > 2, then:\n      // 1. If FoundLHS is negative, then the result is 0.\n      // 2. If FoundLHS is non-negative, then the result is non-negative.\n      // Anyways, the result is non-negative.\n      auto *MinusOne = getMinusOne(WTy);\n      auto *NegDenomMinusOne = getMinusSCEV(MinusOne, DenominatorExt);\n      if (isKnownNegative(RHS) &&\n          IsSGTViaContext(FoundRHSExt, NegDenomMinusOne))\n        return true;\n    }\n  }\n\n  // If our expression contained SCEVUnknown Phis, and we split it down and now\n  // need to prove something for them, try to prove the predicate for every\n  // possible incoming values of those Phis.\n  if (isImpliedViaMerge(Pred, OrigLHS, RHS, OrigFoundLHS, FoundRHS, Depth + 1))\n    return true;\n\n  return false;\n}\n\nstatic bool isKnownPredicateExtendIdiom(ICmpInst::Predicate Pred,\n                                        const SCEV *LHS, const SCEV *RHS) {\n  // zext x u<= sext x, sext x s<= zext x\n  switch (Pred) {\n  case ICmpInst::ICMP_SGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_SLE: {\n    // If operand >=s 0 then ZExt == SExt.  If operand <s 0 then SExt <s ZExt.\n    const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(LHS);\n    const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(RHS);\n    if (SExt && ZExt && SExt->getOperand() == ZExt->getOperand())\n      return true;\n    break;\n  }\n  case ICmpInst::ICMP_UGE:\n    std::swap(LHS, RHS);\n    LLVM_FALLTHROUGH;\n  case ICmpInst::ICMP_ULE: {\n    // If operand >=s 0 then ZExt == SExt.  If operand <s 0 then ZExt <u SExt.\n    const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(LHS);\n    const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(RHS);\n    if (SExt && ZExt && SExt->getOperand() == ZExt->getOperand())\n      return true;\n    break;\n  }\n  default:\n    break;\n  };\n  return false;\n}\n\nbool\nScalarEvolution::isKnownViaNonRecursiveReasoning(ICmpInst::Predicate Pred,\n                                           const SCEV *LHS, const SCEV *RHS) {\n  return isKnownPredicateExtendIdiom(Pred, LHS, RHS) ||\n         isKnownPredicateViaConstantRanges(Pred, LHS, RHS) ||\n         IsKnownPredicateViaMinOrMax(*this, Pred, LHS, RHS) ||\n         IsKnownPredicateViaAddRecStart(*this, Pred, LHS, RHS) ||\n         isKnownPredicateViaNoOverflow(Pred, LHS, RHS);\n}\n\nbool\nScalarEvolution::isImpliedCondOperandsHelper(ICmpInst::Predicate Pred,\n                                             const SCEV *LHS, const SCEV *RHS,\n                                             const SCEV *FoundLHS,\n                                             const SCEV *FoundRHS) {\n  switch (Pred) {\n  default: llvm_unreachable(\"Unexpected ICmpInst::Predicate value!\");\n  case ICmpInst::ICMP_EQ:\n  case ICmpInst::ICMP_NE:\n    if (HasSameValue(LHS, FoundLHS) && HasSameValue(RHS, FoundRHS))\n      return true;\n    break;\n  case ICmpInst::ICMP_SLT:\n  case ICmpInst::ICMP_SLE:\n    if (isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_SLE, LHS, FoundLHS) &&\n        isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_SGE, RHS, FoundRHS))\n      return true;\n    break;\n  case ICmpInst::ICMP_SGT:\n  case ICmpInst::ICMP_SGE:\n    if (isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_SGE, LHS, FoundLHS) &&\n        isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_SLE, RHS, FoundRHS))\n      return true;\n    break;\n  case ICmpInst::ICMP_ULT:\n  case ICmpInst::ICMP_ULE:\n    if (isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_ULE, LHS, FoundLHS) &&\n        isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_UGE, RHS, FoundRHS))\n      return true;\n    break;\n  case ICmpInst::ICMP_UGT:\n  case ICmpInst::ICMP_UGE:\n    if (isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_UGE, LHS, FoundLHS) &&\n        isKnownViaNonRecursiveReasoning(ICmpInst::ICMP_ULE, RHS, FoundRHS))\n      return true;\n    break;\n  }\n\n  // Maybe it can be proved via operations?\n  if (isImpliedViaOperations(Pred, LHS, RHS, FoundLHS, FoundRHS))\n    return true;\n\n  return false;\n}\n\nbool ScalarEvolution::isImpliedCondOperandsViaRanges(ICmpInst::Predicate Pred,\n                                                     const SCEV *LHS,\n                                                     const SCEV *RHS,\n                                                     const SCEV *FoundLHS,\n                                                     const SCEV *FoundRHS) {\n  if (!isa<SCEVConstant>(RHS) || !isa<SCEVConstant>(FoundRHS))\n    // The restriction on `FoundRHS` be lifted easily -- it exists only to\n    // reduce the compile time impact of this optimization.\n    return false;\n\n  Optional<APInt> Addend = computeConstantDifference(LHS, FoundLHS);\n  if (!Addend)\n    return false;\n\n  const APInt &ConstFoundRHS = cast<SCEVConstant>(FoundRHS)->getAPInt();\n\n  // `FoundLHSRange` is the range we know `FoundLHS` to be in by virtue of the\n  // antecedent \"`FoundLHS` `Pred` `FoundRHS`\".\n  ConstantRange FoundLHSRange =\n      ConstantRange::makeAllowedICmpRegion(Pred, ConstFoundRHS);\n\n  // Since `LHS` is `FoundLHS` + `Addend`, we can compute a range for `LHS`:\n  ConstantRange LHSRange = FoundLHSRange.add(ConstantRange(*Addend));\n\n  // We can also compute the range of values for `LHS` that satisfy the\n  // consequent, \"`LHS` `Pred` `RHS`\":\n  const APInt &ConstRHS = cast<SCEVConstant>(RHS)->getAPInt();\n  ConstantRange SatisfyingLHSRange =\n      ConstantRange::makeSatisfyingICmpRegion(Pred, ConstRHS);\n\n  // The antecedent implies the consequent if every value of `LHS` that\n  // satisfies the antecedent also satisfies the consequent.\n  return SatisfyingLHSRange.contains(LHSRange);\n}\n\nbool ScalarEvolution::doesIVOverflowOnLT(const SCEV *RHS, const SCEV *Stride,\n                                         bool IsSigned, bool NoWrap) {\n  assert(isKnownPositive(Stride) && \"Positive stride expected!\");\n\n  if (NoWrap) return false;\n\n  unsigned BitWidth = getTypeSizeInBits(RHS->getType());\n  const SCEV *One = getOne(Stride->getType());\n\n  if (IsSigned) {\n    APInt MaxRHS = getSignedRangeMax(RHS);\n    APInt MaxValue = APInt::getSignedMaxValue(BitWidth);\n    APInt MaxStrideMinusOne = getSignedRangeMax(getMinusSCEV(Stride, One));\n\n    // SMaxRHS + SMaxStrideMinusOne > SMaxValue => overflow!\n    return (std::move(MaxValue) - MaxStrideMinusOne).slt(MaxRHS);\n  }\n\n  APInt MaxRHS = getUnsignedRangeMax(RHS);\n  APInt MaxValue = APInt::getMaxValue(BitWidth);\n  APInt MaxStrideMinusOne = getUnsignedRangeMax(getMinusSCEV(Stride, One));\n\n  // UMaxRHS + UMaxStrideMinusOne > UMaxValue => overflow!\n  return (std::move(MaxValue) - MaxStrideMinusOne).ult(MaxRHS);\n}\n\nbool ScalarEvolution::doesIVOverflowOnGT(const SCEV *RHS, const SCEV *Stride,\n                                         bool IsSigned, bool NoWrap) {\n  if (NoWrap) return false;\n\n  unsigned BitWidth = getTypeSizeInBits(RHS->getType());\n  const SCEV *One = getOne(Stride->getType());\n\n  if (IsSigned) {\n    APInt MinRHS = getSignedRangeMin(RHS);\n    APInt MinValue = APInt::getSignedMinValue(BitWidth);\n    APInt MaxStrideMinusOne = getSignedRangeMax(getMinusSCEV(Stride, One));\n\n    // SMinRHS - SMaxStrideMinusOne < SMinValue => overflow!\n    return (std::move(MinValue) + MaxStrideMinusOne).sgt(MinRHS);\n  }\n\n  APInt MinRHS = getUnsignedRangeMin(RHS);\n  APInt MinValue = APInt::getMinValue(BitWidth);\n  APInt MaxStrideMinusOne = getUnsignedRangeMax(getMinusSCEV(Stride, One));\n\n  // UMinRHS - UMaxStrideMinusOne < UMinValue => overflow!\n  return (std::move(MinValue) + MaxStrideMinusOne).ugt(MinRHS);\n}\n\nconst SCEV *ScalarEvolution::computeBECount(const SCEV *Delta, const SCEV *Step,\n                                            bool Equality) {\n  const SCEV *One = getOne(Step->getType());\n  Delta = Equality ? getAddExpr(Delta, Step)\n                   : getAddExpr(Delta, getMinusSCEV(Step, One));\n  return getUDivExpr(Delta, Step);\n}\n\nconst SCEV *ScalarEvolution::computeMaxBECountForLT(const SCEV *Start,\n                                                    const SCEV *Stride,\n                                                    const SCEV *End,\n                                                    unsigned BitWidth,\n                                                    bool IsSigned) {\n\n  assert(!isKnownNonPositive(Stride) &&\n         \"Stride is expected strictly positive!\");\n  // Calculate the maximum backedge count based on the range of values\n  // permitted by Start, End, and Stride.\n  const SCEV *MaxBECount;\n  APInt MinStart =\n      IsSigned ? getSignedRangeMin(Start) : getUnsignedRangeMin(Start);\n\n  APInt StrideForMaxBECount =\n      IsSigned ? getSignedRangeMin(Stride) : getUnsignedRangeMin(Stride);\n\n  // We already know that the stride is positive, so we paper over conservatism\n  // in our range computation by forcing StrideForMaxBECount to be at least one.\n  // In theory this is unnecessary, but we expect MaxBECount to be a\n  // SCEVConstant, and (udiv <constant> 0) is not constant folded by SCEV (there\n  // is nothing to constant fold it to).\n  APInt One(BitWidth, 1, IsSigned);\n  StrideForMaxBECount = APIntOps::smax(One, StrideForMaxBECount);\n\n  APInt MaxValue = IsSigned ? APInt::getSignedMaxValue(BitWidth)\n                            : APInt::getMaxValue(BitWidth);\n  APInt Limit = MaxValue - (StrideForMaxBECount - 1);\n\n  // Although End can be a MAX expression we estimate MaxEnd considering only\n  // the case End = RHS of the loop termination condition. This is safe because\n  // in the other case (End - Start) is zero, leading to a zero maximum backedge\n  // taken count.\n  APInt MaxEnd = IsSigned ? APIntOps::smin(getSignedRangeMax(End), Limit)\n                          : APIntOps::umin(getUnsignedRangeMax(End), Limit);\n\n  MaxBECount = computeBECount(getConstant(MaxEnd - MinStart) /* Delta */,\n                              getConstant(StrideForMaxBECount) /* Step */,\n                              false /* Equality */);\n\n  return MaxBECount;\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::howManyLessThans(const SCEV *LHS, const SCEV *RHS,\n                                  const Loop *L, bool IsSigned,\n                                  bool ControlsExit, bool AllowPredicates) {\n  SmallPtrSet<const SCEVPredicate *, 4> Predicates;\n\n  const SCEVAddRecExpr *IV = dyn_cast<SCEVAddRecExpr>(LHS);\n  bool PredicatedIV = false;\n\n  if (!IV && AllowPredicates) {\n    // Try to make this an AddRec using runtime tests, in the first X\n    // iterations of this loop, where X is the SCEV expression found by the\n    // algorithm below.\n    IV = convertSCEVToAddRecWithPredicates(LHS, L, Predicates);\n    PredicatedIV = true;\n  }\n\n  // Avoid weird loops\n  if (!IV || IV->getLoop() != L || !IV->isAffine())\n    return getCouldNotCompute();\n\n  bool NoWrap = ControlsExit &&\n                IV->getNoWrapFlags(IsSigned ? SCEV::FlagNSW : SCEV::FlagNUW);\n\n  const SCEV *Stride = IV->getStepRecurrence(*this);\n\n  bool PositiveStride = isKnownPositive(Stride);\n\n  // Avoid negative or zero stride values.\n  if (!PositiveStride) {\n    // We can compute the correct backedge taken count for loops with unknown\n    // strides if we can prove that the loop is not an infinite loop with side\n    // effects. Here's the loop structure we are trying to handle -\n    //\n    // i = start\n    // do {\n    //   A[i] = i;\n    //   i += s;\n    // } while (i < end);\n    //\n    // The backedge taken count for such loops is evaluated as -\n    // (max(end, start + stride) - start - 1) /u stride\n    //\n    // The additional preconditions that we need to check to prove correctness\n    // of the above formula is as follows -\n    //\n    // a) IV is either nuw or nsw depending upon signedness (indicated by the\n    //    NoWrap flag).\n    // b) loop is single exit with no side effects.\n    //\n    //\n    // Precondition a) implies that if the stride is negative, this is a single\n    // trip loop. The backedge taken count formula reduces to zero in this case.\n    //\n    // Precondition b) implies that the unknown stride cannot be zero otherwise\n    // we have UB.\n    //\n    // The positive stride case is the same as isKnownPositive(Stride) returning\n    // true (original behavior of the function).\n    //\n    // We want to make sure that the stride is truly unknown as there are edge\n    // cases where ScalarEvolution propagates no wrap flags to the\n    // post-increment/decrement IV even though the increment/decrement operation\n    // itself is wrapping. The computed backedge taken count may be wrong in\n    // such cases. This is prevented by checking that the stride is not known to\n    // be either positive or non-positive. For example, no wrap flags are\n    // propagated to the post-increment IV of this loop with a trip count of 2 -\n    //\n    // unsigned char i;\n    // for(i=127; i<128; i+=129)\n    //   A[i] = i;\n    //\n    if (PredicatedIV || !NoWrap || isKnownNonPositive(Stride) ||\n        !loopHasNoSideEffects(L))\n      return getCouldNotCompute();\n  } else if (!Stride->isOne() &&\n             doesIVOverflowOnLT(RHS, Stride, IsSigned, NoWrap))\n    // Avoid proven overflow cases: this will ensure that the backedge taken\n    // count will not generate any unsigned overflow. Relaxed no-overflow\n    // conditions exploit NoWrapFlags, allowing to optimize in presence of\n    // undefined behaviors like the case of C language.\n    return getCouldNotCompute();\n\n  ICmpInst::Predicate Cond = IsSigned ? ICmpInst::ICMP_SLT\n                                      : ICmpInst::ICMP_ULT;\n  const SCEV *Start = IV->getStart();\n  const SCEV *End = RHS;\n  // When the RHS is not invariant, we do not know the end bound of the loop and\n  // cannot calculate the ExactBECount needed by ExitLimit. However, we can\n  // calculate the MaxBECount, given the start, stride and max value for the end\n  // bound of the loop (RHS), and the fact that IV does not overflow (which is\n  // checked above).\n  if (!isLoopInvariant(RHS, L)) {\n    const SCEV *MaxBECount = computeMaxBECountForLT(\n        Start, Stride, RHS, getTypeSizeInBits(LHS->getType()), IsSigned);\n    return ExitLimit(getCouldNotCompute() /* ExactNotTaken */, MaxBECount,\n                     false /*MaxOrZero*/, Predicates);\n  }\n  // If the backedge is taken at least once, then it will be taken\n  // (End-Start)/Stride times (rounded up to a multiple of Stride), where Start\n  // is the LHS value of the less-than comparison the first time it is evaluated\n  // and End is the RHS.\n  const SCEV *BECountIfBackedgeTaken =\n    computeBECount(getMinusSCEV(End, Start), Stride, false);\n  // If the loop entry is guarded by the result of the backedge test of the\n  // first loop iteration, then we know the backedge will be taken at least\n  // once and so the backedge taken count is as above. If not then we use the\n  // expression (max(End,Start)-Start)/Stride to describe the backedge count,\n  // as if the backedge is taken at least once max(End,Start) is End and so the\n  // result is as above, and if not max(End,Start) is Start so we get a backedge\n  // count of zero.\n  const SCEV *BECount;\n  if (isLoopEntryGuardedByCond(L, Cond, getMinusSCEV(Start, Stride), RHS))\n    BECount = BECountIfBackedgeTaken;\n  else {\n    // If we know that RHS >= Start in the context of loop, then we know that\n    // max(RHS, Start) = RHS at this point.\n    if (isLoopEntryGuardedByCond(\n            L, IsSigned ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE, RHS, Start))\n      End = RHS;\n    else\n      End = IsSigned ? getSMaxExpr(RHS, Start) : getUMaxExpr(RHS, Start);\n    BECount = computeBECount(getMinusSCEV(End, Start), Stride, false);\n  }\n\n  const SCEV *MaxBECount;\n  bool MaxOrZero = false;\n  if (isa<SCEVConstant>(BECount))\n    MaxBECount = BECount;\n  else if (isa<SCEVConstant>(BECountIfBackedgeTaken)) {\n    // If we know exactly how many times the backedge will be taken if it's\n    // taken at least once, then the backedge count will either be that or\n    // zero.\n    MaxBECount = BECountIfBackedgeTaken;\n    MaxOrZero = true;\n  } else {\n    MaxBECount = computeMaxBECountForLT(\n        Start, Stride, RHS, getTypeSizeInBits(LHS->getType()), IsSigned);\n  }\n\n  if (isa<SCEVCouldNotCompute>(MaxBECount) &&\n      !isa<SCEVCouldNotCompute>(BECount))\n    MaxBECount = getConstant(getUnsignedRangeMax(BECount));\n\n  return ExitLimit(BECount, MaxBECount, MaxOrZero, Predicates);\n}\n\nScalarEvolution::ExitLimit\nScalarEvolution::howManyGreaterThans(const SCEV *LHS, const SCEV *RHS,\n                                     const Loop *L, bool IsSigned,\n                                     bool ControlsExit, bool AllowPredicates) {\n  SmallPtrSet<const SCEVPredicate *, 4> Predicates;\n  // We handle only IV > Invariant\n  if (!isLoopInvariant(RHS, L))\n    return getCouldNotCompute();\n\n  const SCEVAddRecExpr *IV = dyn_cast<SCEVAddRecExpr>(LHS);\n  if (!IV && AllowPredicates)\n    // Try to make this an AddRec using runtime tests, in the first X\n    // iterations of this loop, where X is the SCEV expression found by the\n    // algorithm below.\n    IV = convertSCEVToAddRecWithPredicates(LHS, L, Predicates);\n\n  // Avoid weird loops\n  if (!IV || IV->getLoop() != L || !IV->isAffine())\n    return getCouldNotCompute();\n\n  bool NoWrap = ControlsExit &&\n                IV->getNoWrapFlags(IsSigned ? SCEV::FlagNSW : SCEV::FlagNUW);\n\n  const SCEV *Stride = getNegativeSCEV(IV->getStepRecurrence(*this));\n\n  // Avoid negative or zero stride values\n  if (!isKnownPositive(Stride))\n    return getCouldNotCompute();\n\n  // Avoid proven overflow cases: this will ensure that the backedge taken count\n  // will not generate any unsigned overflow. Relaxed no-overflow conditions\n  // exploit NoWrapFlags, allowing to optimize in presence of undefined\n  // behaviors like the case of C language.\n  if (!Stride->isOne() && doesIVOverflowOnGT(RHS, Stride, IsSigned, NoWrap))\n    return getCouldNotCompute();\n\n  ICmpInst::Predicate Cond = IsSigned ? ICmpInst::ICMP_SGT\n                                      : ICmpInst::ICMP_UGT;\n\n  const SCEV *Start = IV->getStart();\n  const SCEV *End = RHS;\n  if (!isLoopEntryGuardedByCond(L, Cond, getAddExpr(Start, Stride), RHS)) {\n    // If we know that Start >= RHS in the context of loop, then we know that\n    // min(RHS, Start) = RHS at this point.\n    if (isLoopEntryGuardedByCond(\n            L, IsSigned ? ICmpInst::ICMP_SGE : ICmpInst::ICMP_UGE, Start, RHS))\n      End = RHS;\n    else\n      End = IsSigned ? getSMinExpr(RHS, Start) : getUMinExpr(RHS, Start);\n  }\n\n  const SCEV *BECount = computeBECount(getMinusSCEV(Start, End), Stride, false);\n\n  APInt MaxStart = IsSigned ? getSignedRangeMax(Start)\n                            : getUnsignedRangeMax(Start);\n\n  APInt MinStride = IsSigned ? getSignedRangeMin(Stride)\n                             : getUnsignedRangeMin(Stride);\n\n  unsigned BitWidth = getTypeSizeInBits(LHS->getType());\n  APInt Limit = IsSigned ? APInt::getSignedMinValue(BitWidth) + (MinStride - 1)\n                         : APInt::getMinValue(BitWidth) + (MinStride - 1);\n\n  // Although End can be a MIN expression we estimate MinEnd considering only\n  // the case End = RHS. This is safe because in the other case (Start - End)\n  // is zero, leading to a zero maximum backedge taken count.\n  APInt MinEnd =\n    IsSigned ? APIntOps::smax(getSignedRangeMin(RHS), Limit)\n             : APIntOps::umax(getUnsignedRangeMin(RHS), Limit);\n\n  const SCEV *MaxBECount = isa<SCEVConstant>(BECount)\n                               ? BECount\n                               : computeBECount(getConstant(MaxStart - MinEnd),\n                                                getConstant(MinStride), false);\n\n  if (isa<SCEVCouldNotCompute>(MaxBECount))\n    MaxBECount = BECount;\n\n  return ExitLimit(BECount, MaxBECount, false, Predicates);\n}\n\nconst SCEV *SCEVAddRecExpr::getNumIterationsInRange(const ConstantRange &Range,\n                                                    ScalarEvolution &SE) const {\n  if (Range.isFullSet())  // Infinite loop.\n    return SE.getCouldNotCompute();\n\n  // If the start is a non-zero constant, shift the range to simplify things.\n  if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(getStart()))\n    if (!SC->getValue()->isZero()) {\n      SmallVector<const SCEV *, 4> Operands(operands());\n      Operands[0] = SE.getZero(SC->getType());\n      const SCEV *Shifted = SE.getAddRecExpr(Operands, getLoop(),\n                                             getNoWrapFlags(FlagNW));\n      if (const auto *ShiftedAddRec = dyn_cast<SCEVAddRecExpr>(Shifted))\n        return ShiftedAddRec->getNumIterationsInRange(\n            Range.subtract(SC->getAPInt()), SE);\n      // This is strange and shouldn't happen.\n      return SE.getCouldNotCompute();\n    }\n\n  // The only time we can solve this is when we have all constant indices.\n  // Otherwise, we cannot determine the overflow conditions.\n  if (any_of(operands(), [](const SCEV *Op) { return !isa<SCEVConstant>(Op); }))\n    return SE.getCouldNotCompute();\n\n  // Okay at this point we know that all elements of the chrec are constants and\n  // that the start element is zero.\n\n  // First check to see if the range contains zero.  If not, the first\n  // iteration exits.\n  unsigned BitWidth = SE.getTypeSizeInBits(getType());\n  if (!Range.contains(APInt(BitWidth, 0)))\n    return SE.getZero(getType());\n\n  if (isAffine()) {\n    // If this is an affine expression then we have this situation:\n    //   Solve {0,+,A} in Range  ===  Ax in Range\n\n    // We know that zero is in the range.  If A is positive then we know that\n    // the upper value of the range must be the first possible exit value.\n    // If A is negative then the lower of the range is the last possible loop\n    // value.  Also note that we already checked for a full range.\n    APInt A = cast<SCEVConstant>(getOperand(1))->getAPInt();\n    APInt End = A.sge(1) ? (Range.getUpper() - 1) : Range.getLower();\n\n    // The exit value should be (End+A)/A.\n    APInt ExitVal = (End + A).udiv(A);\n    ConstantInt *ExitValue = ConstantInt::get(SE.getContext(), ExitVal);\n\n    // Evaluate at the exit value.  If we really did fall out of the valid\n    // range, then we computed our trip count, otherwise wrap around or other\n    // things must have happened.\n    ConstantInt *Val = EvaluateConstantChrecAtConstant(this, ExitValue, SE);\n    if (Range.contains(Val->getValue()))\n      return SE.getCouldNotCompute();  // Something strange happened\n\n    // Ensure that the previous value is in the range.  This is a sanity check.\n    assert(Range.contains(\n           EvaluateConstantChrecAtConstant(this,\n           ConstantInt::get(SE.getContext(), ExitVal - 1), SE)->getValue()) &&\n           \"Linear scev computation is off in a bad way!\");\n    return SE.getConstant(ExitValue);\n  }\n\n  if (isQuadratic()) {\n    if (auto S = SolveQuadraticAddRecRange(this, Range, SE))\n      return SE.getConstant(S.getValue());\n  }\n\n  return SE.getCouldNotCompute();\n}\n\nconst SCEVAddRecExpr *\nSCEVAddRecExpr::getPostIncExpr(ScalarEvolution &SE) const {\n  assert(getNumOperands() > 1 && \"AddRec with zero step?\");\n  // There is a temptation to just call getAddExpr(this, getStepRecurrence(SE)),\n  // but in this case we cannot guarantee that the value returned will be an\n  // AddRec because SCEV does not have a fixed point where it stops\n  // simplification: it is legal to return ({rec1} + {rec2}). For example, it\n  // may happen if we reach arithmetic depth limit while simplifying. So we\n  // construct the returned value explicitly.\n  SmallVector<const SCEV *, 3> Ops;\n  // If this is {A,+,B,+,C,...,+,N}, then its step is {B,+,C,+,...,+,N}, and\n  // (this + Step) is {A+B,+,B+C,+...,+,N}.\n  for (unsigned i = 0, e = getNumOperands() - 1; i < e; ++i)\n    Ops.push_back(SE.getAddExpr(getOperand(i), getOperand(i + 1)));\n  // We know that the last operand is not a constant zero (otherwise it would\n  // have been popped out earlier). This guarantees us that if the result has\n  // the same last operand, then it will also not be popped out, meaning that\n  // the returned value will be an AddRec.\n  const SCEV *Last = getOperand(getNumOperands() - 1);\n  assert(!Last->isZero() && \"Recurrency with zero step?\");\n  Ops.push_back(Last);\n  return cast<SCEVAddRecExpr>(SE.getAddRecExpr(Ops, getLoop(),\n                                               SCEV::FlagAnyWrap));\n}\n\n// Return true when S contains at least an undef value.\nstatic inline bool containsUndefs(const SCEV *S) {\n  return SCEVExprContains(S, [](const SCEV *S) {\n    if (const auto *SU = dyn_cast<SCEVUnknown>(S))\n      return isa<UndefValue>(SU->getValue());\n    return false;\n  });\n}\n\nnamespace {\n\n// Collect all steps of SCEV expressions.\nstruct SCEVCollectStrides {\n  ScalarEvolution &SE;\n  SmallVectorImpl<const SCEV *> &Strides;\n\n  SCEVCollectStrides(ScalarEvolution &SE, SmallVectorImpl<const SCEV *> &S)\n      : SE(SE), Strides(S) {}\n\n  bool follow(const SCEV *S) {\n    if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(S))\n      Strides.push_back(AR->getStepRecurrence(SE));\n    return true;\n  }\n\n  bool isDone() const { return false; }\n};\n\n// Collect all SCEVUnknown and SCEVMulExpr expressions.\nstruct SCEVCollectTerms {\n  SmallVectorImpl<const SCEV *> &Terms;\n\n  SCEVCollectTerms(SmallVectorImpl<const SCEV *> &T) : Terms(T) {}\n\n  bool follow(const SCEV *S) {\n    if (isa<SCEVUnknown>(S) || isa<SCEVMulExpr>(S) ||\n        isa<SCEVSignExtendExpr>(S)) {\n      if (!containsUndefs(S))\n        Terms.push_back(S);\n\n      // Stop recursion: once we collected a term, do not walk its operands.\n      return false;\n    }\n\n    // Keep looking.\n    return true;\n  }\n\n  bool isDone() const { return false; }\n};\n\n// Check if a SCEV contains an AddRecExpr.\nstruct SCEVHasAddRec {\n  bool &ContainsAddRec;\n\n  SCEVHasAddRec(bool &ContainsAddRec) : ContainsAddRec(ContainsAddRec) {\n    ContainsAddRec = false;\n  }\n\n  bool follow(const SCEV *S) {\n    if (isa<SCEVAddRecExpr>(S)) {\n      ContainsAddRec = true;\n\n      // Stop recursion: once we collected a term, do not walk its operands.\n      return false;\n    }\n\n    // Keep looking.\n    return true;\n  }\n\n  bool isDone() const { return false; }\n};\n\n// Find factors that are multiplied with an expression that (possibly as a\n// subexpression) contains an AddRecExpr. In the expression:\n//\n//  8 * (100 +  %p * %q * (%a + {0, +, 1}_loop))\n//\n// \"%p * %q\" are factors multiplied by the expression \"(%a + {0, +, 1}_loop)\"\n// that contains the AddRec {0, +, 1}_loop. %p * %q are likely to be array size\n// parameters as they form a product with an induction variable.\n//\n// This collector expects all array size parameters to be in the same MulExpr.\n// It might be necessary to later add support for collecting parameters that are\n// spread over different nested MulExpr.\nstruct SCEVCollectAddRecMultiplies {\n  SmallVectorImpl<const SCEV *> &Terms;\n  ScalarEvolution &SE;\n\n  SCEVCollectAddRecMultiplies(SmallVectorImpl<const SCEV *> &T, ScalarEvolution &SE)\n      : Terms(T), SE(SE) {}\n\n  bool follow(const SCEV *S) {\n    if (auto *Mul = dyn_cast<SCEVMulExpr>(S)) {\n      bool HasAddRec = false;\n      SmallVector<const SCEV *, 0> Operands;\n      for (auto Op : Mul->operands()) {\n        const SCEVUnknown *Unknown = dyn_cast<SCEVUnknown>(Op);\n        if (Unknown && !isa<CallInst>(Unknown->getValue())) {\n          Operands.push_back(Op);\n        } else if (Unknown) {\n          HasAddRec = true;\n        } else {\n          bool ContainsAddRec = false;\n          SCEVHasAddRec ContiansAddRec(ContainsAddRec);\n          visitAll(Op, ContiansAddRec);\n          HasAddRec |= ContainsAddRec;\n        }\n      }\n      if (Operands.size() == 0)\n        return true;\n\n      if (!HasAddRec)\n        return false;\n\n      Terms.push_back(SE.getMulExpr(Operands));\n      // Stop recursion: once we collected a term, do not walk its operands.\n      return false;\n    }\n\n    // Keep looking.\n    return true;\n  }\n\n  bool isDone() const { return false; }\n};\n\n} // end anonymous namespace\n\n/// Find parametric terms in this SCEVAddRecExpr. We first for parameters in\n/// two places:\n///   1) The strides of AddRec expressions.\n///   2) Unknowns that are multiplied with AddRec expressions.\nvoid ScalarEvolution::collectParametricTerms(const SCEV *Expr,\n    SmallVectorImpl<const SCEV *> &Terms) {\n  SmallVector<const SCEV *, 4> Strides;\n  SCEVCollectStrides StrideCollector(*this, Strides);\n  visitAll(Expr, StrideCollector);\n\n  LLVM_DEBUG({\n    dbgs() << \"Strides:\\n\";\n    for (const SCEV *S : Strides)\n      dbgs() << *S << \"\\n\";\n  });\n\n  for (const SCEV *S : Strides) {\n    SCEVCollectTerms TermCollector(Terms);\n    visitAll(S, TermCollector);\n  }\n\n  LLVM_DEBUG({\n    dbgs() << \"Terms:\\n\";\n    for (const SCEV *T : Terms)\n      dbgs() << *T << \"\\n\";\n  });\n\n  SCEVCollectAddRecMultiplies MulCollector(Terms, *this);\n  visitAll(Expr, MulCollector);\n}\n\nstatic bool findArrayDimensionsRec(ScalarEvolution &SE,\n                                   SmallVectorImpl<const SCEV *> &Terms,\n                                   SmallVectorImpl<const SCEV *> &Sizes) {\n  int Last = Terms.size() - 1;\n  const SCEV *Step = Terms[Last];\n\n  // End of recursion.\n  if (Last == 0) {\n    if (const SCEVMulExpr *M = dyn_cast<SCEVMulExpr>(Step)) {\n      SmallVector<const SCEV *, 2> Qs;\n      for (const SCEV *Op : M->operands())\n        if (!isa<SCEVConstant>(Op))\n          Qs.push_back(Op);\n\n      Step = SE.getMulExpr(Qs);\n    }\n\n    Sizes.push_back(Step);\n    return true;\n  }\n\n  for (const SCEV *&Term : Terms) {\n    // Normalize the terms before the next call to findArrayDimensionsRec.\n    const SCEV *Q, *R;\n    SCEVDivision::divide(SE, Term, Step, &Q, &R);\n\n    // Bail out when GCD does not evenly divide one of the terms.\n    if (!R->isZero())\n      return false;\n\n    Term = Q;\n  }\n\n  // Remove all SCEVConstants.\n  erase_if(Terms, [](const SCEV *E) { return isa<SCEVConstant>(E); });\n\n  if (Terms.size() > 0)\n    if (!findArrayDimensionsRec(SE, Terms, Sizes))\n      return false;\n\n  Sizes.push_back(Step);\n  return true;\n}\n\n// Returns true when one of the SCEVs of Terms contains a SCEVUnknown parameter.\nstatic inline bool containsParameters(SmallVectorImpl<const SCEV *> &Terms) {\n  for (const SCEV *T : Terms)\n    if (SCEVExprContains(T, [](const SCEV *S) { return isa<SCEVUnknown>(S); }))\n      return true;\n\n  return false;\n}\n\n// Return the number of product terms in S.\nstatic inline int numberOfTerms(const SCEV *S) {\n  if (const SCEVMulExpr *Expr = dyn_cast<SCEVMulExpr>(S))\n    return Expr->getNumOperands();\n  return 1;\n}\n\nstatic const SCEV *removeConstantFactors(ScalarEvolution &SE, const SCEV *T) {\n  if (isa<SCEVConstant>(T))\n    return nullptr;\n\n  if (isa<SCEVUnknown>(T))\n    return T;\n\n  if (const SCEVMulExpr *M = dyn_cast<SCEVMulExpr>(T)) {\n    SmallVector<const SCEV *, 2> Factors;\n    for (const SCEV *Op : M->operands())\n      if (!isa<SCEVConstant>(Op))\n        Factors.push_back(Op);\n\n    return SE.getMulExpr(Factors);\n  }\n\n  return T;\n}\n\n/// Return the size of an element read or written by Inst.\nconst SCEV *ScalarEvolution::getElementSize(Instruction *Inst) {\n  Type *Ty;\n  if (StoreInst *Store = dyn_cast<StoreInst>(Inst))\n    Ty = Store->getValueOperand()->getType();\n  else if (LoadInst *Load = dyn_cast<LoadInst>(Inst))\n    Ty = Load->getType();\n  else\n    return nullptr;\n\n  Type *ETy = getEffectiveSCEVType(PointerType::getUnqual(Ty));\n  return getSizeOfExpr(ETy, Ty);\n}\n\nvoid ScalarEvolution::findArrayDimensions(SmallVectorImpl<const SCEV *> &Terms,\n                                          SmallVectorImpl<const SCEV *> &Sizes,\n                                          const SCEV *ElementSize) {\n  if (Terms.size() < 1 || !ElementSize)\n    return;\n\n  // Early return when Terms do not contain parameters: we do not delinearize\n  // non parametric SCEVs.\n  if (!containsParameters(Terms))\n    return;\n\n  LLVM_DEBUG({\n    dbgs() << \"Terms:\\n\";\n    for (const SCEV *T : Terms)\n      dbgs() << *T << \"\\n\";\n  });\n\n  // Remove duplicates.\n  array_pod_sort(Terms.begin(), Terms.end());\n  Terms.erase(std::unique(Terms.begin(), Terms.end()), Terms.end());\n\n  // Put larger terms first.\n  llvm::sort(Terms, [](const SCEV *LHS, const SCEV *RHS) {\n    return numberOfTerms(LHS) > numberOfTerms(RHS);\n  });\n\n  // Try to divide all terms by the element size. If term is not divisible by\n  // element size, proceed with the original term.\n  for (const SCEV *&Term : Terms) {\n    const SCEV *Q, *R;\n    SCEVDivision::divide(*this, Term, ElementSize, &Q, &R);\n    if (!Q->isZero())\n      Term = Q;\n  }\n\n  SmallVector<const SCEV *, 4> NewTerms;\n\n  // Remove constant factors.\n  for (const SCEV *T : Terms)\n    if (const SCEV *NewT = removeConstantFactors(*this, T))\n      NewTerms.push_back(NewT);\n\n  LLVM_DEBUG({\n    dbgs() << \"Terms after sorting:\\n\";\n    for (const SCEV *T : NewTerms)\n      dbgs() << *T << \"\\n\";\n  });\n\n  if (NewTerms.empty() || !findArrayDimensionsRec(*this, NewTerms, Sizes)) {\n    Sizes.clear();\n    return;\n  }\n\n  // The last element to be pushed into Sizes is the size of an element.\n  Sizes.push_back(ElementSize);\n\n  LLVM_DEBUG({\n    dbgs() << \"Sizes:\\n\";\n    for (const SCEV *S : Sizes)\n      dbgs() << *S << \"\\n\";\n  });\n}\n\nvoid ScalarEvolution::computeAccessFunctions(\n    const SCEV *Expr, SmallVectorImpl<const SCEV *> &Subscripts,\n    SmallVectorImpl<const SCEV *> &Sizes) {\n  // Early exit in case this SCEV is not an affine multivariate function.\n  if (Sizes.empty())\n    return;\n\n  if (auto *AR = dyn_cast<SCEVAddRecExpr>(Expr))\n    if (!AR->isAffine())\n      return;\n\n  const SCEV *Res = Expr;\n  int Last = Sizes.size() - 1;\n  for (int i = Last; i >= 0; i--) {\n    const SCEV *Q, *R;\n    SCEVDivision::divide(*this, Res, Sizes[i], &Q, &R);\n\n    LLVM_DEBUG({\n      dbgs() << \"Res: \" << *Res << \"\\n\";\n      dbgs() << \"Sizes[i]: \" << *Sizes[i] << \"\\n\";\n      dbgs() << \"Res divided by Sizes[i]:\\n\";\n      dbgs() << \"Quotient: \" << *Q << \"\\n\";\n      dbgs() << \"Remainder: \" << *R << \"\\n\";\n    });\n\n    Res = Q;\n\n    // Do not record the last subscript corresponding to the size of elements in\n    // the array.\n    if (i == Last) {\n\n      // Bail out if the remainder is too complex.\n      if (isa<SCEVAddRecExpr>(R)) {\n        Subscripts.clear();\n        Sizes.clear();\n        return;\n      }\n\n      continue;\n    }\n\n    // Record the access function for the current subscript.\n    Subscripts.push_back(R);\n  }\n\n  // Also push in last position the remainder of the last division: it will be\n  // the access function of the innermost dimension.\n  Subscripts.push_back(Res);\n\n  std::reverse(Subscripts.begin(), Subscripts.end());\n\n  LLVM_DEBUG({\n    dbgs() << \"Subscripts:\\n\";\n    for (const SCEV *S : Subscripts)\n      dbgs() << *S << \"\\n\";\n  });\n}\n\n/// Splits the SCEV into two vectors of SCEVs representing the subscripts and\n/// sizes of an array access. Returns the remainder of the delinearization that\n/// is the offset start of the array.  The SCEV->delinearize algorithm computes\n/// the multiples of SCEV coefficients: that is a pattern matching of sub\n/// expressions in the stride and base of a SCEV corresponding to the\n/// computation of a GCD (greatest common divisor) of base and stride.  When\n/// SCEV->delinearize fails, it returns the SCEV unchanged.\n///\n/// For example: when analyzing the memory access A[i][j][k] in this loop nest\n///\n///  void foo(long n, long m, long o, double A[n][m][o]) {\n///\n///    for (long i = 0; i < n; i++)\n///      for (long j = 0; j < m; j++)\n///        for (long k = 0; k < o; k++)\n///          A[i][j][k] = 1.0;\n///  }\n///\n/// the delinearization input is the following AddRec SCEV:\n///\n///  AddRec: {{{%A,+,(8 * %m * %o)}<%for.i>,+,(8 * %o)}<%for.j>,+,8}<%for.k>\n///\n/// From this SCEV, we are able to say that the base offset of the access is %A\n/// because it appears as an offset that does not divide any of the strides in\n/// the loops:\n///\n///  CHECK: Base offset: %A\n///\n/// and then SCEV->delinearize determines the size of some of the dimensions of\n/// the array as these are the multiples by which the strides are happening:\n///\n///  CHECK: ArrayDecl[UnknownSize][%m][%o] with elements of sizeof(double) bytes.\n///\n/// Note that the outermost dimension remains of UnknownSize because there are\n/// no strides that would help identifying the size of the last dimension: when\n/// the array has been statically allocated, one could compute the size of that\n/// dimension by dividing the overall size of the array by the size of the known\n/// dimensions: %m * %o * 8.\n///\n/// Finally delinearize provides the access functions for the array reference\n/// that does correspond to A[i][j][k] of the above C testcase:\n///\n///  CHECK: ArrayRef[{0,+,1}<%for.i>][{0,+,1}<%for.j>][{0,+,1}<%for.k>]\n///\n/// The testcases are checking the output of a function pass:\n/// DelinearizationPass that walks through all loads and stores of a function\n/// asking for the SCEV of the memory access with respect to all enclosing\n/// loops, calling SCEV->delinearize on that and printing the results.\nvoid ScalarEvolution::delinearize(const SCEV *Expr,\n                                 SmallVectorImpl<const SCEV *> &Subscripts,\n                                 SmallVectorImpl<const SCEV *> &Sizes,\n                                 const SCEV *ElementSize) {\n  // First step: collect parametric terms.\n  SmallVector<const SCEV *, 4> Terms;\n  collectParametricTerms(Expr, Terms);\n\n  if (Terms.empty())\n    return;\n\n  // Second step: find subscript sizes.\n  findArrayDimensions(Terms, Sizes, ElementSize);\n\n  if (Sizes.empty())\n    return;\n\n  // Third step: compute the access functions for each subscript.\n  computeAccessFunctions(Expr, Subscripts, Sizes);\n\n  if (Subscripts.empty())\n    return;\n\n  LLVM_DEBUG({\n    dbgs() << \"succeeded to delinearize \" << *Expr << \"\\n\";\n    dbgs() << \"ArrayDecl[UnknownSize]\";\n    for (const SCEV *S : Sizes)\n      dbgs() << \"[\" << *S << \"]\";\n\n    dbgs() << \"\\nArrayRef\";\n    for (const SCEV *S : Subscripts)\n      dbgs() << \"[\" << *S << \"]\";\n    dbgs() << \"\\n\";\n  });\n}\n\nbool ScalarEvolution::getIndexExpressionsFromGEP(\n    const GetElementPtrInst *GEP, SmallVectorImpl<const SCEV *> &Subscripts,\n    SmallVectorImpl<int> &Sizes) {\n  assert(Subscripts.empty() && Sizes.empty() &&\n         \"Expected output lists to be empty on entry to this function.\");\n  assert(GEP && \"getIndexExpressionsFromGEP called with a null GEP\");\n  Type *Ty = GEP->getPointerOperandType();\n  bool DroppedFirstDim = false;\n  for (unsigned i = 1; i < GEP->getNumOperands(); i++) {\n    const SCEV *Expr = getSCEV(GEP->getOperand(i));\n    if (i == 1) {\n      if (auto *PtrTy = dyn_cast<PointerType>(Ty)) {\n        Ty = PtrTy->getElementType();\n      } else if (auto *ArrayTy = dyn_cast<ArrayType>(Ty)) {\n        Ty = ArrayTy->getElementType();\n      } else {\n        Subscripts.clear();\n        Sizes.clear();\n        return false;\n      }\n      if (auto *Const = dyn_cast<SCEVConstant>(Expr))\n        if (Const->getValue()->isZero()) {\n          DroppedFirstDim = true;\n          continue;\n        }\n      Subscripts.push_back(Expr);\n      continue;\n    }\n\n    auto *ArrayTy = dyn_cast<ArrayType>(Ty);\n    if (!ArrayTy) {\n      Subscripts.clear();\n      Sizes.clear();\n      return false;\n    }\n\n    Subscripts.push_back(Expr);\n    if (!(DroppedFirstDim && i == 2))\n      Sizes.push_back(ArrayTy->getNumElements());\n\n    Ty = ArrayTy->getElementType();\n  }\n  return !Subscripts.empty();\n}\n\n//===----------------------------------------------------------------------===//\n//                   SCEVCallbackVH Class Implementation\n//===----------------------------------------------------------------------===//\n\nvoid ScalarEvolution::SCEVCallbackVH::deleted() {\n  assert(SE && \"SCEVCallbackVH called with a null ScalarEvolution!\");\n  if (PHINode *PN = dyn_cast<PHINode>(getValPtr()))\n    SE->ConstantEvolutionLoopExitValue.erase(PN);\n  SE->eraseValueFromMap(getValPtr());\n  // this now dangles!\n}\n\nvoid ScalarEvolution::SCEVCallbackVH::allUsesReplacedWith(Value *V) {\n  assert(SE && \"SCEVCallbackVH called with a null ScalarEvolution!\");\n\n  // Forget all the expressions associated with users of the old value,\n  // so that future queries will recompute the expressions using the new\n  // value.\n  Value *Old = getValPtr();\n  SmallVector<User *, 16> Worklist(Old->users());\n  SmallPtrSet<User *, 8> Visited;\n  while (!Worklist.empty()) {\n    User *U = Worklist.pop_back_val();\n    // Deleting the Old value will cause this to dangle. Postpone\n    // that until everything else is done.\n    if (U == Old)\n      continue;\n    if (!Visited.insert(U).second)\n      continue;\n    if (PHINode *PN = dyn_cast<PHINode>(U))\n      SE->ConstantEvolutionLoopExitValue.erase(PN);\n    SE->eraseValueFromMap(U);\n    llvm::append_range(Worklist, U->users());\n  }\n  // Delete the Old value.\n  if (PHINode *PN = dyn_cast<PHINode>(Old))\n    SE->ConstantEvolutionLoopExitValue.erase(PN);\n  SE->eraseValueFromMap(Old);\n  // this now dangles!\n}\n\nScalarEvolution::SCEVCallbackVH::SCEVCallbackVH(Value *V, ScalarEvolution *se)\n  : CallbackVH(V), SE(se) {}\n\n//===----------------------------------------------------------------------===//\n//                   ScalarEvolution Class Implementation\n//===----------------------------------------------------------------------===//\n\nScalarEvolution::ScalarEvolution(Function &F, TargetLibraryInfo &TLI,\n                                 AssumptionCache &AC, DominatorTree &DT,\n                                 LoopInfo &LI)\n    : F(F), TLI(TLI), AC(AC), DT(DT), LI(LI),\n      CouldNotCompute(new SCEVCouldNotCompute()), ValuesAtScopes(64),\n      LoopDispositions(64), BlockDispositions(64) {\n  // To use guards for proving predicates, we need to scan every instruction in\n  // relevant basic blocks, and not just terminators.  Doing this is a waste of\n  // time if the IR does not actually contain any calls to\n  // @llvm.experimental.guard, so do a quick check and remember this beforehand.\n  //\n  // This pessimizes the case where a pass that preserves ScalarEvolution wants\n  // to _add_ guards to the module when there weren't any before, and wants\n  // ScalarEvolution to optimize based on those guards.  For now we prefer to be\n  // efficient in lieu of being smart in that rather obscure case.\n\n  auto *GuardDecl = F.getParent()->getFunction(\n      Intrinsic::getName(Intrinsic::experimental_guard));\n  HasGuards = GuardDecl && !GuardDecl->use_empty();\n}\n\nScalarEvolution::ScalarEvolution(ScalarEvolution &&Arg)\n    : F(Arg.F), HasGuards(Arg.HasGuards), TLI(Arg.TLI), AC(Arg.AC), DT(Arg.DT),\n      LI(Arg.LI), CouldNotCompute(std::move(Arg.CouldNotCompute)),\n      ValueExprMap(std::move(Arg.ValueExprMap)),\n      PendingLoopPredicates(std::move(Arg.PendingLoopPredicates)),\n      PendingPhiRanges(std::move(Arg.PendingPhiRanges)),\n      PendingMerges(std::move(Arg.PendingMerges)),\n      MinTrailingZerosCache(std::move(Arg.MinTrailingZerosCache)),\n      BackedgeTakenCounts(std::move(Arg.BackedgeTakenCounts)),\n      PredicatedBackedgeTakenCounts(\n          std::move(Arg.PredicatedBackedgeTakenCounts)),\n      ConstantEvolutionLoopExitValue(\n          std::move(Arg.ConstantEvolutionLoopExitValue)),\n      ValuesAtScopes(std::move(Arg.ValuesAtScopes)),\n      LoopDispositions(std::move(Arg.LoopDispositions)),\n      LoopPropertiesCache(std::move(Arg.LoopPropertiesCache)),\n      BlockDispositions(std::move(Arg.BlockDispositions)),\n      UnsignedRanges(std::move(Arg.UnsignedRanges)),\n      SignedRanges(std::move(Arg.SignedRanges)),\n      UniqueSCEVs(std::move(Arg.UniqueSCEVs)),\n      UniquePreds(std::move(Arg.UniquePreds)),\n      SCEVAllocator(std::move(Arg.SCEVAllocator)),\n      LoopUsers(std::move(Arg.LoopUsers)),\n      PredicatedSCEVRewrites(std::move(Arg.PredicatedSCEVRewrites)),\n      FirstUnknown(Arg.FirstUnknown) {\n  Arg.FirstUnknown = nullptr;\n}\n\nScalarEvolution::~ScalarEvolution() {\n  // Iterate through all the SCEVUnknown instances and call their\n  // destructors, so that they release their references to their values.\n  for (SCEVUnknown *U = FirstUnknown; U;) {\n    SCEVUnknown *Tmp = U;\n    U = U->Next;\n    Tmp->~SCEVUnknown();\n  }\n  FirstUnknown = nullptr;\n\n  ExprValueMap.clear();\n  ValueExprMap.clear();\n  HasRecMap.clear();\n\n  // Free any extra memory created for ExitNotTakenInfo in the unlikely event\n  // that a loop had multiple computable exits.\n  for (auto &BTCI : BackedgeTakenCounts)\n    BTCI.second.clear();\n  for (auto &BTCI : PredicatedBackedgeTakenCounts)\n    BTCI.second.clear();\n\n  assert(PendingLoopPredicates.empty() && \"isImpliedCond garbage\");\n  assert(PendingPhiRanges.empty() && \"getRangeRef garbage\");\n  assert(PendingMerges.empty() && \"isImpliedViaMerge garbage\");\n  assert(!WalkingBEDominatingConds && \"isLoopBackedgeGuardedByCond garbage!\");\n  assert(!ProvingSplitPredicate && \"ProvingSplitPredicate garbage!\");\n}\n\nbool ScalarEvolution::hasLoopInvariantBackedgeTakenCount(const Loop *L) {\n  return !isa<SCEVCouldNotCompute>(getBackedgeTakenCount(L));\n}\n\nstatic void PrintLoopInfo(raw_ostream &OS, ScalarEvolution *SE,\n                          const Loop *L) {\n  // Print all inner loops first\n  for (Loop *I : *L)\n    PrintLoopInfo(OS, SE, I);\n\n  OS << \"Loop \";\n  L->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n  OS << \": \";\n\n  SmallVector<BasicBlock *, 8> ExitingBlocks;\n  L->getExitingBlocks(ExitingBlocks);\n  if (ExitingBlocks.size() != 1)\n    OS << \"<multiple exits> \";\n\n  if (SE->hasLoopInvariantBackedgeTakenCount(L))\n    OS << \"backedge-taken count is \" << *SE->getBackedgeTakenCount(L) << \"\\n\";\n  else\n    OS << \"Unpredictable backedge-taken count.\\n\";\n\n  if (ExitingBlocks.size() > 1)\n    for (BasicBlock *ExitingBlock : ExitingBlocks) {\n      OS << \"  exit count for \" << ExitingBlock->getName() << \": \"\n         << *SE->getExitCount(L, ExitingBlock) << \"\\n\";\n    }\n\n  OS << \"Loop \";\n  L->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n  OS << \": \";\n\n  if (!isa<SCEVCouldNotCompute>(SE->getConstantMaxBackedgeTakenCount(L))) {\n    OS << \"max backedge-taken count is \" << *SE->getConstantMaxBackedgeTakenCount(L);\n    if (SE->isBackedgeTakenCountMaxOrZero(L))\n      OS << \", actual taken count either this or zero.\";\n  } else {\n    OS << \"Unpredictable max backedge-taken count. \";\n  }\n\n  OS << \"\\n\"\n        \"Loop \";\n  L->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n  OS << \": \";\n\n  SCEVUnionPredicate Pred;\n  auto PBT = SE->getPredicatedBackedgeTakenCount(L, Pred);\n  if (!isa<SCEVCouldNotCompute>(PBT)) {\n    OS << \"Predicated backedge-taken count is \" << *PBT << \"\\n\";\n    OS << \" Predicates:\\n\";\n    Pred.print(OS, 4);\n  } else {\n    OS << \"Unpredictable predicated backedge-taken count. \";\n  }\n  OS << \"\\n\";\n\n  if (SE->hasLoopInvariantBackedgeTakenCount(L)) {\n    OS << \"Loop \";\n    L->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n    OS << \": \";\n    OS << \"Trip multiple is \" << SE->getSmallConstantTripMultiple(L) << \"\\n\";\n  }\n}\n\nstatic StringRef loopDispositionToStr(ScalarEvolution::LoopDisposition LD) {\n  switch (LD) {\n  case ScalarEvolution::LoopVariant:\n    return \"Variant\";\n  case ScalarEvolution::LoopInvariant:\n    return \"Invariant\";\n  case ScalarEvolution::LoopComputable:\n    return \"Computable\";\n  }\n  llvm_unreachable(\"Unknown ScalarEvolution::LoopDisposition kind!\");\n}\n\nvoid ScalarEvolution::print(raw_ostream &OS) const {\n  // ScalarEvolution's implementation of the print method is to print\n  // out SCEV values of all instructions that are interesting. Doing\n  // this potentially causes it to create new SCEV objects though,\n  // which technically conflicts with the const qualifier. This isn't\n  // observable from outside the class though, so casting away the\n  // const isn't dangerous.\n  ScalarEvolution &SE = *const_cast<ScalarEvolution *>(this);\n\n  if (ClassifyExpressions) {\n    OS << \"Classifying expressions for: \";\n    F.printAsOperand(OS, /*PrintType=*/false);\n    OS << \"\\n\";\n    for (Instruction &I : instructions(F))\n      if (isSCEVable(I.getType()) && !isa<CmpInst>(I)) {\n        OS << I << '\\n';\n        OS << \"  -->  \";\n        const SCEV *SV = SE.getSCEV(&I);\n        SV->print(OS);\n        if (!isa<SCEVCouldNotCompute>(SV)) {\n          OS << \" U: \";\n          SE.getUnsignedRange(SV).print(OS);\n          OS << \" S: \";\n          SE.getSignedRange(SV).print(OS);\n        }\n\n        const Loop *L = LI.getLoopFor(I.getParent());\n\n        const SCEV *AtUse = SE.getSCEVAtScope(SV, L);\n        if (AtUse != SV) {\n          OS << \"  -->  \";\n          AtUse->print(OS);\n          if (!isa<SCEVCouldNotCompute>(AtUse)) {\n            OS << \" U: \";\n            SE.getUnsignedRange(AtUse).print(OS);\n            OS << \" S: \";\n            SE.getSignedRange(AtUse).print(OS);\n          }\n        }\n\n        if (L) {\n          OS << \"\\t\\t\" \"Exits: \";\n          const SCEV *ExitValue = SE.getSCEVAtScope(SV, L->getParentLoop());\n          if (!SE.isLoopInvariant(ExitValue, L)) {\n            OS << \"<<Unknown>>\";\n          } else {\n            OS << *ExitValue;\n          }\n\n          bool First = true;\n          for (auto *Iter = L; Iter; Iter = Iter->getParentLoop()) {\n            if (First) {\n              OS << \"\\t\\t\" \"LoopDispositions: { \";\n              First = false;\n            } else {\n              OS << \", \";\n            }\n\n            Iter->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n            OS << \": \" << loopDispositionToStr(SE.getLoopDisposition(SV, Iter));\n          }\n\n          for (auto *InnerL : depth_first(L)) {\n            if (InnerL == L)\n              continue;\n            if (First) {\n              OS << \"\\t\\t\" \"LoopDispositions: { \";\n              First = false;\n            } else {\n              OS << \", \";\n            }\n\n            InnerL->getHeader()->printAsOperand(OS, /*PrintType=*/false);\n            OS << \": \" << loopDispositionToStr(SE.getLoopDisposition(SV, InnerL));\n          }\n\n          OS << \" }\";\n        }\n\n        OS << \"\\n\";\n      }\n  }\n\n  OS << \"Determining loop execution counts for: \";\n  F.printAsOperand(OS, /*PrintType=*/false);\n  OS << \"\\n\";\n  for (Loop *I : LI)\n    PrintLoopInfo(OS, &SE, I);\n}\n\nScalarEvolution::LoopDisposition\nScalarEvolution::getLoopDisposition(const SCEV *S, const Loop *L) {\n  auto &Values = LoopDispositions[S];\n  for (auto &V : Values) {\n    if (V.getPointer() == L)\n      return V.getInt();\n  }\n  Values.emplace_back(L, LoopVariant);\n  LoopDisposition D = computeLoopDisposition(S, L);\n  auto &Values2 = LoopDispositions[S];\n  for (auto &V : make_range(Values2.rbegin(), Values2.rend())) {\n    if (V.getPointer() == L) {\n      V.setInt(D);\n      break;\n    }\n  }\n  return D;\n}\n\nScalarEvolution::LoopDisposition\nScalarEvolution::computeLoopDisposition(const SCEV *S, const Loop *L) {\n  switch (S->getSCEVType()) {\n  case scConstant:\n    return LoopInvariant;\n  case scPtrToInt:\n  case scTruncate:\n  case scZeroExtend:\n  case scSignExtend:\n    return getLoopDisposition(cast<SCEVCastExpr>(S)->getOperand(), L);\n  case scAddRecExpr: {\n    const SCEVAddRecExpr *AR = cast<SCEVAddRecExpr>(S);\n\n    // If L is the addrec's loop, it's computable.\n    if (AR->getLoop() == L)\n      return LoopComputable;\n\n    // Add recurrences are never invariant in the function-body (null loop).\n    if (!L)\n      return LoopVariant;\n\n    // Everything that is not defined at loop entry is variant.\n    if (DT.dominates(L->getHeader(), AR->getLoop()->getHeader()))\n      return LoopVariant;\n    assert(!L->contains(AR->getLoop()) && \"Containing loop's header does not\"\n           \" dominate the contained loop's header?\");\n\n    // This recurrence is invariant w.r.t. L if AR's loop contains L.\n    if (AR->getLoop()->contains(L))\n      return LoopInvariant;\n\n    // This recurrence is variant w.r.t. L if any of its operands\n    // are variant.\n    for (auto *Op : AR->operands())\n      if (!isLoopInvariant(Op, L))\n        return LoopVariant;\n\n    // Otherwise it's loop-invariant.\n    return LoopInvariant;\n  }\n  case scAddExpr:\n  case scMulExpr:\n  case scUMaxExpr:\n  case scSMaxExpr:\n  case scUMinExpr:\n  case scSMinExpr: {\n    bool HasVarying = false;\n    for (auto *Op : cast<SCEVNAryExpr>(S)->operands()) {\n      LoopDisposition D = getLoopDisposition(Op, L);\n      if (D == LoopVariant)\n        return LoopVariant;\n      if (D == LoopComputable)\n        HasVarying = true;\n    }\n    return HasVarying ? LoopComputable : LoopInvariant;\n  }\n  case scUDivExpr: {\n    const SCEVUDivExpr *UDiv = cast<SCEVUDivExpr>(S);\n    LoopDisposition LD = getLoopDisposition(UDiv->getLHS(), L);\n    if (LD == LoopVariant)\n      return LoopVariant;\n    LoopDisposition RD = getLoopDisposition(UDiv->getRHS(), L);\n    if (RD == LoopVariant)\n      return LoopVariant;\n    return (LD == LoopInvariant && RD == LoopInvariant) ?\n           LoopInvariant : LoopComputable;\n  }\n  case scUnknown:\n    // All non-instruction values are loop invariant.  All instructions are loop\n    // invariant if they are not contained in the specified loop.\n    // Instructions are never considered invariant in the function body\n    // (null loop) because they are defined within the \"loop\".\n    if (auto *I = dyn_cast<Instruction>(cast<SCEVUnknown>(S)->getValue()))\n      return (L && !L->contains(I)) ? LoopInvariant : LoopVariant;\n    return LoopInvariant;\n  case scCouldNotCompute:\n    llvm_unreachable(\"Attempt to use a SCEVCouldNotCompute object!\");\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\nbool ScalarEvolution::isLoopInvariant(const SCEV *S, const Loop *L) {\n  return getLoopDisposition(S, L) == LoopInvariant;\n}\n\nbool ScalarEvolution::hasComputableLoopEvolution(const SCEV *S, const Loop *L) {\n  return getLoopDisposition(S, L) == LoopComputable;\n}\n\nScalarEvolution::BlockDisposition\nScalarEvolution::getBlockDisposition(const SCEV *S, const BasicBlock *BB) {\n  auto &Values = BlockDispositions[S];\n  for (auto &V : Values) {\n    if (V.getPointer() == BB)\n      return V.getInt();\n  }\n  Values.emplace_back(BB, DoesNotDominateBlock);\n  BlockDisposition D = computeBlockDisposition(S, BB);\n  auto &Values2 = BlockDispositions[S];\n  for (auto &V : make_range(Values2.rbegin(), Values2.rend())) {\n    if (V.getPointer() == BB) {\n      V.setInt(D);\n      break;\n    }\n  }\n  return D;\n}\n\nScalarEvolution::BlockDisposition\nScalarEvolution::computeBlockDisposition(const SCEV *S, const BasicBlock *BB) {\n  switch (S->getSCEVType()) {\n  case scConstant:\n    return ProperlyDominatesBlock;\n  case scPtrToInt:\n  case scTruncate:\n  case scZeroExtend:\n  case scSignExtend:\n    return getBlockDisposition(cast<SCEVCastExpr>(S)->getOperand(), BB);\n  case scAddRecExpr: {\n    // This uses a \"dominates\" query instead of \"properly dominates\" query\n    // to test for proper dominance too, because the instruction which\n    // produces the addrec's value is a PHI, and a PHI effectively properly\n    // dominates its entire containing block.\n    const SCEVAddRecExpr *AR = cast<SCEVAddRecExpr>(S);\n    if (!DT.dominates(AR->getLoop()->getHeader(), BB))\n      return DoesNotDominateBlock;\n\n    // Fall through into SCEVNAryExpr handling.\n    LLVM_FALLTHROUGH;\n  }\n  case scAddExpr:\n  case scMulExpr:\n  case scUMaxExpr:\n  case scSMaxExpr:\n  case scUMinExpr:\n  case scSMinExpr: {\n    const SCEVNAryExpr *NAry = cast<SCEVNAryExpr>(S);\n    bool Proper = true;\n    for (const SCEV *NAryOp : NAry->operands()) {\n      BlockDisposition D = getBlockDisposition(NAryOp, BB);\n      if (D == DoesNotDominateBlock)\n        return DoesNotDominateBlock;\n      if (D == DominatesBlock)\n        Proper = false;\n    }\n    return Proper ? ProperlyDominatesBlock : DominatesBlock;\n  }\n  case scUDivExpr: {\n    const SCEVUDivExpr *UDiv = cast<SCEVUDivExpr>(S);\n    const SCEV *LHS = UDiv->getLHS(), *RHS = UDiv->getRHS();\n    BlockDisposition LD = getBlockDisposition(LHS, BB);\n    if (LD == DoesNotDominateBlock)\n      return DoesNotDominateBlock;\n    BlockDisposition RD = getBlockDisposition(RHS, BB);\n    if (RD == DoesNotDominateBlock)\n      return DoesNotDominateBlock;\n    return (LD == ProperlyDominatesBlock && RD == ProperlyDominatesBlock) ?\n      ProperlyDominatesBlock : DominatesBlock;\n  }\n  case scUnknown:\n    if (Instruction *I =\n          dyn_cast<Instruction>(cast<SCEVUnknown>(S)->getValue())) {\n      if (I->getParent() == BB)\n        return DominatesBlock;\n      if (DT.properlyDominates(I->getParent(), BB))\n        return ProperlyDominatesBlock;\n      return DoesNotDominateBlock;\n    }\n    return ProperlyDominatesBlock;\n  case scCouldNotCompute:\n    llvm_unreachable(\"Attempt to use a SCEVCouldNotCompute object!\");\n  }\n  llvm_unreachable(\"Unknown SCEV kind!\");\n}\n\nbool ScalarEvolution::dominates(const SCEV *S, const BasicBlock *BB) {\n  return getBlockDisposition(S, BB) >= DominatesBlock;\n}\n\nbool ScalarEvolution::properlyDominates(const SCEV *S, const BasicBlock *BB) {\n  return getBlockDisposition(S, BB) == ProperlyDominatesBlock;\n}\n\nbool ScalarEvolution::hasOperand(const SCEV *S, const SCEV *Op) const {\n  return SCEVExprContains(S, [&](const SCEV *Expr) { return Expr == Op; });\n}\n\nbool ScalarEvolution::ExitLimit::hasOperand(const SCEV *S) const {\n  auto IsS = [&](const SCEV *X) { return S == X; };\n  auto ContainsS = [&](const SCEV *X) {\n    return !isa<SCEVCouldNotCompute>(X) && SCEVExprContains(X, IsS);\n  };\n  return ContainsS(ExactNotTaken) || ContainsS(MaxNotTaken);\n}\n\nvoid\nScalarEvolution::forgetMemoizedResults(const SCEV *S) {\n  ValuesAtScopes.erase(S);\n  LoopDispositions.erase(S);\n  BlockDispositions.erase(S);\n  UnsignedRanges.erase(S);\n  SignedRanges.erase(S);\n  ExprValueMap.erase(S);\n  HasRecMap.erase(S);\n  MinTrailingZerosCache.erase(S);\n\n  for (auto I = PredicatedSCEVRewrites.begin();\n       I != PredicatedSCEVRewrites.end();) {\n    std::pair<const SCEV *, const Loop *> Entry = I->first;\n    if (Entry.first == S)\n      PredicatedSCEVRewrites.erase(I++);\n    else\n      ++I;\n  }\n\n  auto RemoveSCEVFromBackedgeMap =\n      [S, this](DenseMap<const Loop *, BackedgeTakenInfo> &Map) {\n        for (auto I = Map.begin(), E = Map.end(); I != E;) {\n          BackedgeTakenInfo &BEInfo = I->second;\n          if (BEInfo.hasOperand(S, this)) {\n            BEInfo.clear();\n            Map.erase(I++);\n          } else\n            ++I;\n        }\n      };\n\n  RemoveSCEVFromBackedgeMap(BackedgeTakenCounts);\n  RemoveSCEVFromBackedgeMap(PredicatedBackedgeTakenCounts);\n}\n\nvoid\nScalarEvolution::getUsedLoops(const SCEV *S,\n                              SmallPtrSetImpl<const Loop *> &LoopsUsed) {\n  struct FindUsedLoops {\n    FindUsedLoops(SmallPtrSetImpl<const Loop *> &LoopsUsed)\n        : LoopsUsed(LoopsUsed) {}\n    SmallPtrSetImpl<const Loop *> &LoopsUsed;\n    bool follow(const SCEV *S) {\n      if (auto *AR = dyn_cast<SCEVAddRecExpr>(S))\n        LoopsUsed.insert(AR->getLoop());\n      return true;\n    }\n\n    bool isDone() const { return false; }\n  };\n\n  FindUsedLoops F(LoopsUsed);\n  SCEVTraversal<FindUsedLoops>(F).visitAll(S);\n}\n\nvoid ScalarEvolution::addToLoopUseLists(const SCEV *S) {\n  SmallPtrSet<const Loop *, 8> LoopsUsed;\n  getUsedLoops(S, LoopsUsed);\n  for (auto *L : LoopsUsed)\n    LoopUsers[L].push_back(S);\n}\n\nvoid ScalarEvolution::verify() const {\n  ScalarEvolution &SE = *const_cast<ScalarEvolution *>(this);\n  ScalarEvolution SE2(F, TLI, AC, DT, LI);\n\n  SmallVector<Loop *, 8> LoopStack(LI.begin(), LI.end());\n\n  // Map's SCEV expressions from one ScalarEvolution \"universe\" to another.\n  struct SCEVMapper : public SCEVRewriteVisitor<SCEVMapper> {\n    SCEVMapper(ScalarEvolution &SE) : SCEVRewriteVisitor<SCEVMapper>(SE) {}\n\n    const SCEV *visitConstant(const SCEVConstant *Constant) {\n      return SE.getConstant(Constant->getAPInt());\n    }\n\n    const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n      return SE.getUnknown(Expr->getValue());\n    }\n\n    const SCEV *visitCouldNotCompute(const SCEVCouldNotCompute *Expr) {\n      return SE.getCouldNotCompute();\n    }\n  };\n\n  SCEVMapper SCM(SE2);\n\n  while (!LoopStack.empty()) {\n    auto *L = LoopStack.pop_back_val();\n    llvm::append_range(LoopStack, *L);\n\n    auto *CurBECount = SCM.visit(\n        const_cast<ScalarEvolution *>(this)->getBackedgeTakenCount(L));\n    auto *NewBECount = SE2.getBackedgeTakenCount(L);\n\n    if (CurBECount == SE2.getCouldNotCompute() ||\n        NewBECount == SE2.getCouldNotCompute()) {\n      // NB! This situation is legal, but is very suspicious -- whatever pass\n      // change the loop to make a trip count go from could not compute to\n      // computable or vice-versa *should have* invalidated SCEV.  However, we\n      // choose not to assert here (for now) since we don't want false\n      // positives.\n      continue;\n    }\n\n    if (containsUndefs(CurBECount) || containsUndefs(NewBECount)) {\n      // SCEV treats \"undef\" as an unknown but consistent value (i.e. it does\n      // not propagate undef aggressively).  This means we can (and do) fail\n      // verification in cases where a transform makes the trip count of a loop\n      // go from \"undef\" to \"undef+1\" (say).  The transform is fine, since in\n      // both cases the loop iterates \"undef\" times, but SCEV thinks we\n      // increased the trip count of the loop by 1 incorrectly.\n      continue;\n    }\n\n    if (SE.getTypeSizeInBits(CurBECount->getType()) >\n        SE.getTypeSizeInBits(NewBECount->getType()))\n      NewBECount = SE2.getZeroExtendExpr(NewBECount, CurBECount->getType());\n    else if (SE.getTypeSizeInBits(CurBECount->getType()) <\n             SE.getTypeSizeInBits(NewBECount->getType()))\n      CurBECount = SE2.getZeroExtendExpr(CurBECount, NewBECount->getType());\n\n    const SCEV *Delta = SE2.getMinusSCEV(CurBECount, NewBECount);\n\n    // Unless VerifySCEVStrict is set, we only compare constant deltas.\n    if ((VerifySCEVStrict || isa<SCEVConstant>(Delta)) && !Delta->isZero()) {\n      dbgs() << \"Trip Count for \" << *L << \" Changed!\\n\";\n      dbgs() << \"Old: \" << *CurBECount << \"\\n\";\n      dbgs() << \"New: \" << *NewBECount << \"\\n\";\n      dbgs() << \"Delta: \" << *Delta << \"\\n\";\n      std::abort();\n    }\n  }\n\n  // Collect all valid loops currently in LoopInfo.\n  SmallPtrSet<Loop *, 32> ValidLoops;\n  SmallVector<Loop *, 32> Worklist(LI.begin(), LI.end());\n  while (!Worklist.empty()) {\n    Loop *L = Worklist.pop_back_val();\n    if (ValidLoops.contains(L))\n      continue;\n    ValidLoops.insert(L);\n    Worklist.append(L->begin(), L->end());\n  }\n  // Check for SCEV expressions referencing invalid/deleted loops.\n  for (auto &KV : ValueExprMap) {\n    auto *AR = dyn_cast<SCEVAddRecExpr>(KV.second);\n    if (!AR)\n      continue;\n    assert(ValidLoops.contains(AR->getLoop()) &&\n           \"AddRec references invalid loop\");\n  }\n}\n\nbool ScalarEvolution::invalidate(\n    Function &F, const PreservedAnalyses &PA,\n    FunctionAnalysisManager::Invalidator &Inv) {\n  // Invalidate the ScalarEvolution object whenever it isn't preserved or one\n  // of its dependencies is invalidated.\n  auto PAC = PA.getChecker<ScalarEvolutionAnalysis>();\n  return !(PAC.preserved() || PAC.preservedSet<AllAnalysesOn<Function>>()) ||\n         Inv.invalidate<AssumptionAnalysis>(F, PA) ||\n         Inv.invalidate<DominatorTreeAnalysis>(F, PA) ||\n         Inv.invalidate<LoopAnalysis>(F, PA);\n}\n\nAnalysisKey ScalarEvolutionAnalysis::Key;\n\nScalarEvolution ScalarEvolutionAnalysis::run(Function &F,\n                                             FunctionAnalysisManager &AM) {\n  return ScalarEvolution(F, AM.getResult<TargetLibraryAnalysis>(F),\n                         AM.getResult<AssumptionAnalysis>(F),\n                         AM.getResult<DominatorTreeAnalysis>(F),\n                         AM.getResult<LoopAnalysis>(F));\n}\n\nPreservedAnalyses\nScalarEvolutionVerifierPass::run(Function &F, FunctionAnalysisManager &AM) {\n  AM.getResult<ScalarEvolutionAnalysis>(F).verify();\n  return PreservedAnalyses::all();\n}\n\nPreservedAnalyses\nScalarEvolutionPrinterPass::run(Function &F, FunctionAnalysisManager &AM) {\n  // For compatibility with opt's -analyze feature under legacy pass manager\n  // which was not ported to NPM. This keeps tests using\n  // update_analyze_test_checks.py working.\n  OS << \"Printing analysis 'Scalar Evolution Analysis' for function '\"\n     << F.getName() << \"':\\n\";\n  AM.getResult<ScalarEvolutionAnalysis>(F).print(OS);\n  return PreservedAnalyses::all();\n}\n\nINITIALIZE_PASS_BEGIN(ScalarEvolutionWrapperPass, \"scalar-evolution\",\n                      \"Scalar Evolution Analysis\", false, true)\nINITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)\nINITIALIZE_PASS_DEPENDENCY(LoopInfoWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)\nINITIALIZE_PASS_DEPENDENCY(TargetLibraryInfoWrapperPass)\nINITIALIZE_PASS_END(ScalarEvolutionWrapperPass, \"scalar-evolution\",\n                    \"Scalar Evolution Analysis\", false, true)\n\nchar ScalarEvolutionWrapperPass::ID = 0;\n\nScalarEvolutionWrapperPass::ScalarEvolutionWrapperPass() : FunctionPass(ID) {\n  initializeScalarEvolutionWrapperPassPass(*PassRegistry::getPassRegistry());\n}\n\nbool ScalarEvolutionWrapperPass::runOnFunction(Function &F) {\n  SE.reset(new ScalarEvolution(\n      F, getAnalysis<TargetLibraryInfoWrapperPass>().getTLI(F),\n      getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F),\n      getAnalysis<DominatorTreeWrapperPass>().getDomTree(),\n      getAnalysis<LoopInfoWrapperPass>().getLoopInfo()));\n  return false;\n}\n\nvoid ScalarEvolutionWrapperPass::releaseMemory() { SE.reset(); }\n\nvoid ScalarEvolutionWrapperPass::print(raw_ostream &OS, const Module *) const {\n  SE->print(OS);\n}\n\nvoid ScalarEvolutionWrapperPass::verifyAnalysis() const {\n  if (!VerifySCEV)\n    return;\n\n  SE->verify();\n}\n\nvoid ScalarEvolutionWrapperPass::getAnalysisUsage(AnalysisUsage &AU) const {\n  AU.setPreservesAll();\n  AU.addRequiredTransitive<AssumptionCacheTracker>();\n  AU.addRequiredTransitive<LoopInfoWrapperPass>();\n  AU.addRequiredTransitive<DominatorTreeWrapperPass>();\n  AU.addRequiredTransitive<TargetLibraryInfoWrapperPass>();\n}\n\nconst SCEVPredicate *ScalarEvolution::getEqualPredicate(const SCEV *LHS,\n                                                        const SCEV *RHS) {\n  FoldingSetNodeID ID;\n  assert(LHS->getType() == RHS->getType() &&\n         \"Type mismatch between LHS and RHS\");\n  // Unique this node based on the arguments\n  ID.AddInteger(SCEVPredicate::P_Equal);\n  ID.AddPointer(LHS);\n  ID.AddPointer(RHS);\n  void *IP = nullptr;\n  if (const auto *S = UniquePreds.FindNodeOrInsertPos(ID, IP))\n    return S;\n  SCEVEqualPredicate *Eq = new (SCEVAllocator)\n      SCEVEqualPredicate(ID.Intern(SCEVAllocator), LHS, RHS);\n  UniquePreds.InsertNode(Eq, IP);\n  return Eq;\n}\n\nconst SCEVPredicate *ScalarEvolution::getWrapPredicate(\n    const SCEVAddRecExpr *AR,\n    SCEVWrapPredicate::IncrementWrapFlags AddedFlags) {\n  FoldingSetNodeID ID;\n  // Unique this node based on the arguments\n  ID.AddInteger(SCEVPredicate::P_Wrap);\n  ID.AddPointer(AR);\n  ID.AddInteger(AddedFlags);\n  void *IP = nullptr;\n  if (const auto *S = UniquePreds.FindNodeOrInsertPos(ID, IP))\n    return S;\n  auto *OF = new (SCEVAllocator)\n      SCEVWrapPredicate(ID.Intern(SCEVAllocator), AR, AddedFlags);\n  UniquePreds.InsertNode(OF, IP);\n  return OF;\n}\n\nnamespace {\n\nclass SCEVPredicateRewriter : public SCEVRewriteVisitor<SCEVPredicateRewriter> {\npublic:\n\n  /// Rewrites \\p S in the context of a loop L and the SCEV predication\n  /// infrastructure.\n  ///\n  /// If \\p Pred is non-null, the SCEV expression is rewritten to respect the\n  /// equivalences present in \\p Pred.\n  ///\n  /// If \\p NewPreds is non-null, rewrite is free to add further predicates to\n  /// \\p NewPreds such that the result will be an AddRecExpr.\n  static const SCEV *rewrite(const SCEV *S, const Loop *L, ScalarEvolution &SE,\n                             SmallPtrSetImpl<const SCEVPredicate *> *NewPreds,\n                             SCEVUnionPredicate *Pred) {\n    SCEVPredicateRewriter Rewriter(L, SE, NewPreds, Pred);\n    return Rewriter.visit(S);\n  }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    if (Pred) {\n      auto ExprPreds = Pred->getPredicatesForExpr(Expr);\n      for (auto *Pred : ExprPreds)\n        if (const auto *IPred = dyn_cast<SCEVEqualPredicate>(Pred))\n          if (IPred->getLHS() == Expr)\n            return IPred->getRHS();\n    }\n    return convertToAddRecWithPreds(Expr);\n  }\n\n  const SCEV *visitZeroExtendExpr(const SCEVZeroExtendExpr *Expr) {\n    const SCEV *Operand = visit(Expr->getOperand());\n    const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(Operand);\n    if (AR && AR->getLoop() == L && AR->isAffine()) {\n      // This couldn't be folded because the operand didn't have the nuw\n      // flag. Add the nusw flag as an assumption that we could make.\n      const SCEV *Step = AR->getStepRecurrence(SE);\n      Type *Ty = Expr->getType();\n      if (addOverflowAssumption(AR, SCEVWrapPredicate::IncrementNUSW))\n        return SE.getAddRecExpr(SE.getZeroExtendExpr(AR->getStart(), Ty),\n                                SE.getSignExtendExpr(Step, Ty), L,\n                                AR->getNoWrapFlags());\n    }\n    return SE.getZeroExtendExpr(Operand, Expr->getType());\n  }\n\n  const SCEV *visitSignExtendExpr(const SCEVSignExtendExpr *Expr) {\n    const SCEV *Operand = visit(Expr->getOperand());\n    const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(Operand);\n    if (AR && AR->getLoop() == L && AR->isAffine()) {\n      // This couldn't be folded because the operand didn't have the nsw\n      // flag. Add the nssw flag as an assumption that we could make.\n      const SCEV *Step = AR->getStepRecurrence(SE);\n      Type *Ty = Expr->getType();\n      if (addOverflowAssumption(AR, SCEVWrapPredicate::IncrementNSSW))\n        return SE.getAddRecExpr(SE.getSignExtendExpr(AR->getStart(), Ty),\n                                SE.getSignExtendExpr(Step, Ty), L,\n                                AR->getNoWrapFlags());\n    }\n    return SE.getSignExtendExpr(Operand, Expr->getType());\n  }\n\nprivate:\n  explicit SCEVPredicateRewriter(const Loop *L, ScalarEvolution &SE,\n                        SmallPtrSetImpl<const SCEVPredicate *> *NewPreds,\n                        SCEVUnionPredicate *Pred)\n      : SCEVRewriteVisitor(SE), NewPreds(NewPreds), Pred(Pred), L(L) {}\n\n  bool addOverflowAssumption(const SCEVPredicate *P) {\n    if (!NewPreds) {\n      // Check if we've already made this assumption.\n      return Pred && Pred->implies(P);\n    }\n    NewPreds->insert(P);\n    return true;\n  }\n\n  bool addOverflowAssumption(const SCEVAddRecExpr *AR,\n                             SCEVWrapPredicate::IncrementWrapFlags AddedFlags) {\n    auto *A = SE.getWrapPredicate(AR, AddedFlags);\n    return addOverflowAssumption(A);\n  }\n\n  // If \\p Expr represents a PHINode, we try to see if it can be represented\n  // as an AddRec, possibly under a predicate (PHISCEVPred). If it is possible\n  // to add this predicate as a runtime overflow check, we return the AddRec.\n  // If \\p Expr does not meet these conditions (is not a PHI node, or we\n  // couldn't create an AddRec for it, or couldn't add the predicate), we just\n  // return \\p Expr.\n  const SCEV *convertToAddRecWithPreds(const SCEVUnknown *Expr) {\n    if (!isa<PHINode>(Expr->getValue()))\n      return Expr;\n    Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\n    PredicatedRewrite = SE.createAddRecFromPHIWithCasts(Expr);\n    if (!PredicatedRewrite)\n      return Expr;\n    for (auto *P : PredicatedRewrite->second){\n      // Wrap predicates from outer loops are not supported.\n      if (auto *WP = dyn_cast<const SCEVWrapPredicate>(P)) {\n        auto *AR = cast<const SCEVAddRecExpr>(WP->getExpr());\n        if (L != AR->getLoop())\n          return Expr;\n      }\n      if (!addOverflowAssumption(P))\n        return Expr;\n    }\n    return PredicatedRewrite->first;\n  }\n\n  SmallPtrSetImpl<const SCEVPredicate *> *NewPreds;\n  SCEVUnionPredicate *Pred;\n  const Loop *L;\n};\n\n} // end anonymous namespace\n\nconst SCEV *ScalarEvolution::rewriteUsingPredicate(const SCEV *S, const Loop *L,\n                                                   SCEVUnionPredicate &Preds) {\n  return SCEVPredicateRewriter::rewrite(S, L, *this, nullptr, &Preds);\n}\n\nconst SCEVAddRecExpr *ScalarEvolution::convertSCEVToAddRecWithPredicates(\n    const SCEV *S, const Loop *L,\n    SmallPtrSetImpl<const SCEVPredicate *> &Preds) {\n  SmallPtrSet<const SCEVPredicate *, 4> TransformPreds;\n  S = SCEVPredicateRewriter::rewrite(S, L, *this, &TransformPreds, nullptr);\n  auto *AddRec = dyn_cast<SCEVAddRecExpr>(S);\n\n  if (!AddRec)\n    return nullptr;\n\n  // Since the transformation was successful, we can now transfer the SCEV\n  // predicates.\n  for (auto *P : TransformPreds)\n    Preds.insert(P);\n\n  return AddRec;\n}\n\n/// SCEV predicates\nSCEVPredicate::SCEVPredicate(const FoldingSetNodeIDRef ID,\n                             SCEVPredicateKind Kind)\n    : FastID(ID), Kind(Kind) {}\n\nSCEVEqualPredicate::SCEVEqualPredicate(const FoldingSetNodeIDRef ID,\n                                       const SCEV *LHS, const SCEV *RHS)\n    : SCEVPredicate(ID, P_Equal), LHS(LHS), RHS(RHS) {\n  assert(LHS->getType() == RHS->getType() && \"LHS and RHS types don't match\");\n  assert(LHS != RHS && \"LHS and RHS are the same SCEV\");\n}\n\nbool SCEVEqualPredicate::implies(const SCEVPredicate *N) const {\n  const auto *Op = dyn_cast<SCEVEqualPredicate>(N);\n\n  if (!Op)\n    return false;\n\n  return Op->LHS == LHS && Op->RHS == RHS;\n}\n\nbool SCEVEqualPredicate::isAlwaysTrue() const { return false; }\n\nconst SCEV *SCEVEqualPredicate::getExpr() const { return LHS; }\n\nvoid SCEVEqualPredicate::print(raw_ostream &OS, unsigned Depth) const {\n  OS.indent(Depth) << \"Equal predicate: \" << *LHS << \" == \" << *RHS << \"\\n\";\n}\n\nSCEVWrapPredicate::SCEVWrapPredicate(const FoldingSetNodeIDRef ID,\n                                     const SCEVAddRecExpr *AR,\n                                     IncrementWrapFlags Flags)\n    : SCEVPredicate(ID, P_Wrap), AR(AR), Flags(Flags) {}\n\nconst SCEV *SCEVWrapPredicate::getExpr() const { return AR; }\n\nbool SCEVWrapPredicate::implies(const SCEVPredicate *N) const {\n  const auto *Op = dyn_cast<SCEVWrapPredicate>(N);\n\n  return Op && Op->AR == AR && setFlags(Flags, Op->Flags) == Flags;\n}\n\nbool SCEVWrapPredicate::isAlwaysTrue() const {\n  SCEV::NoWrapFlags ScevFlags = AR->getNoWrapFlags();\n  IncrementWrapFlags IFlags = Flags;\n\n  if (ScalarEvolution::setFlags(ScevFlags, SCEV::FlagNSW) == ScevFlags)\n    IFlags = clearFlags(IFlags, IncrementNSSW);\n\n  return IFlags == IncrementAnyWrap;\n}\n\nvoid SCEVWrapPredicate::print(raw_ostream &OS, unsigned Depth) const {\n  OS.indent(Depth) << *getExpr() << \" Added Flags: \";\n  if (SCEVWrapPredicate::IncrementNUSW & getFlags())\n    OS << \"<nusw>\";\n  if (SCEVWrapPredicate::IncrementNSSW & getFlags())\n    OS << \"<nssw>\";\n  OS << \"\\n\";\n}\n\nSCEVWrapPredicate::IncrementWrapFlags\nSCEVWrapPredicate::getImpliedFlags(const SCEVAddRecExpr *AR,\n                                   ScalarEvolution &SE) {\n  IncrementWrapFlags ImpliedFlags = IncrementAnyWrap;\n  SCEV::NoWrapFlags StaticFlags = AR->getNoWrapFlags();\n\n  // We can safely transfer the NSW flag as NSSW.\n  if (ScalarEvolution::setFlags(StaticFlags, SCEV::FlagNSW) == StaticFlags)\n    ImpliedFlags = IncrementNSSW;\n\n  if (ScalarEvolution::setFlags(StaticFlags, SCEV::FlagNUW) == StaticFlags) {\n    // If the increment is positive, the SCEV NUW flag will also imply the\n    // WrapPredicate NUSW flag.\n    if (const auto *Step = dyn_cast<SCEVConstant>(AR->getStepRecurrence(SE)))\n      if (Step->getValue()->getValue().isNonNegative())\n        ImpliedFlags = setFlags(ImpliedFlags, IncrementNUSW);\n  }\n\n  return ImpliedFlags;\n}\n\n/// Union predicates don't get cached so create a dummy set ID for it.\nSCEVUnionPredicate::SCEVUnionPredicate()\n    : SCEVPredicate(FoldingSetNodeIDRef(nullptr, 0), P_Union) {}\n\nbool SCEVUnionPredicate::isAlwaysTrue() const {\n  return all_of(Preds,\n                [](const SCEVPredicate *I) { return I->isAlwaysTrue(); });\n}\n\nArrayRef<const SCEVPredicate *>\nSCEVUnionPredicate::getPredicatesForExpr(const SCEV *Expr) {\n  auto I = SCEVToPreds.find(Expr);\n  if (I == SCEVToPreds.end())\n    return ArrayRef<const SCEVPredicate *>();\n  return I->second;\n}\n\nbool SCEVUnionPredicate::implies(const SCEVPredicate *N) const {\n  if (const auto *Set = dyn_cast<SCEVUnionPredicate>(N))\n    return all_of(Set->Preds,\n                  [this](const SCEVPredicate *I) { return this->implies(I); });\n\n  auto ScevPredsIt = SCEVToPreds.find(N->getExpr());\n  if (ScevPredsIt == SCEVToPreds.end())\n    return false;\n  auto &SCEVPreds = ScevPredsIt->second;\n\n  return any_of(SCEVPreds,\n                [N](const SCEVPredicate *I) { return I->implies(N); });\n}\n\nconst SCEV *SCEVUnionPredicate::getExpr() const { return nullptr; }\n\nvoid SCEVUnionPredicate::print(raw_ostream &OS, unsigned Depth) const {\n  for (auto Pred : Preds)\n    Pred->print(OS, Depth);\n}\n\nvoid SCEVUnionPredicate::add(const SCEVPredicate *N) {\n  if (const auto *Set = dyn_cast<SCEVUnionPredicate>(N)) {\n    for (auto Pred : Set->Preds)\n      add(Pred);\n    return;\n  }\n\n  if (implies(N))\n    return;\n\n  const SCEV *Key = N->getExpr();\n  assert(Key && \"Only SCEVUnionPredicate doesn't have an \"\n                \" associated expression!\");\n\n  SCEVToPreds[Key].push_back(N);\n  Preds.push_back(N);\n}\n\nPredicatedScalarEvolution::PredicatedScalarEvolution(ScalarEvolution &SE,\n                                                     Loop &L)\n    : SE(SE), L(L) {}\n\nconst SCEV *PredicatedScalarEvolution::getSCEV(Value *V) {\n  const SCEV *Expr = SE.getSCEV(V);\n  RewriteEntry &Entry = RewriteMap[Expr];\n\n  // If we already have an entry and the version matches, return it.\n  if (Entry.second && Generation == Entry.first)\n    return Entry.second;\n\n  // We found an entry but it's stale. Rewrite the stale entry\n  // according to the current predicate.\n  if (Entry.second)\n    Expr = Entry.second;\n\n  const SCEV *NewSCEV = SE.rewriteUsingPredicate(Expr, &L, Preds);\n  Entry = {Generation, NewSCEV};\n\n  return NewSCEV;\n}\n\nconst SCEV *PredicatedScalarEvolution::getBackedgeTakenCount() {\n  if (!BackedgeCount) {\n    SCEVUnionPredicate BackedgePred;\n    BackedgeCount = SE.getPredicatedBackedgeTakenCount(&L, BackedgePred);\n    addPredicate(BackedgePred);\n  }\n  return BackedgeCount;\n}\n\nvoid PredicatedScalarEvolution::addPredicate(const SCEVPredicate &Pred) {\n  if (Preds.implies(&Pred))\n    return;\n  Preds.add(&Pred);\n  updateGeneration();\n}\n\nconst SCEVUnionPredicate &PredicatedScalarEvolution::getUnionPredicate() const {\n  return Preds;\n}\n\nvoid PredicatedScalarEvolution::updateGeneration() {\n  // If the generation number wrapped recompute everything.\n  if (++Generation == 0) {\n    for (auto &II : RewriteMap) {\n      const SCEV *Rewritten = II.second.second;\n      II.second = {Generation, SE.rewriteUsingPredicate(Rewritten, &L, Preds)};\n    }\n  }\n}\n\nvoid PredicatedScalarEvolution::setNoOverflow(\n    Value *V, SCEVWrapPredicate::IncrementWrapFlags Flags) {\n  const SCEV *Expr = getSCEV(V);\n  const auto *AR = cast<SCEVAddRecExpr>(Expr);\n\n  auto ImpliedFlags = SCEVWrapPredicate::getImpliedFlags(AR, SE);\n\n  // Clear the statically implied flags.\n  Flags = SCEVWrapPredicate::clearFlags(Flags, ImpliedFlags);\n  addPredicate(*SE.getWrapPredicate(AR, Flags));\n\n  auto II = FlagsMap.insert({V, Flags});\n  if (!II.second)\n    II.first->second = SCEVWrapPredicate::setFlags(Flags, II.first->second);\n}\n\nbool PredicatedScalarEvolution::hasNoOverflow(\n    Value *V, SCEVWrapPredicate::IncrementWrapFlags Flags) {\n  const SCEV *Expr = getSCEV(V);\n  const auto *AR = cast<SCEVAddRecExpr>(Expr);\n\n  Flags = SCEVWrapPredicate::clearFlags(\n      Flags, SCEVWrapPredicate::getImpliedFlags(AR, SE));\n\n  auto II = FlagsMap.find(V);\n\n  if (II != FlagsMap.end())\n    Flags = SCEVWrapPredicate::clearFlags(Flags, II->second);\n\n  return Flags == SCEVWrapPredicate::IncrementAnyWrap;\n}\n\nconst SCEVAddRecExpr *PredicatedScalarEvolution::getAsAddRec(Value *V) {\n  const SCEV *Expr = this->getSCEV(V);\n  SmallPtrSet<const SCEVPredicate *, 4> NewPreds;\n  auto *New = SE.convertSCEVToAddRecWithPredicates(Expr, &L, NewPreds);\n\n  if (!New)\n    return nullptr;\n\n  for (auto *P : NewPreds)\n    Preds.add(P);\n\n  updateGeneration();\n  RewriteMap[SE.getSCEV(V)] = {Generation, New};\n  return New;\n}\n\nPredicatedScalarEvolution::PredicatedScalarEvolution(\n    const PredicatedScalarEvolution &Init)\n    : RewriteMap(Init.RewriteMap), SE(Init.SE), L(Init.L), Preds(Init.Preds),\n      Generation(Init.Generation), BackedgeCount(Init.BackedgeCount) {\n  for (auto I : Init.FlagsMap)\n    FlagsMap.insert(I);\n}\n\nvoid PredicatedScalarEvolution::print(raw_ostream &OS, unsigned Depth) const {\n  // For each block.\n  for (auto *BB : L.getBlocks())\n    for (auto &I : *BB) {\n      if (!SE.isSCEVable(I.getType()))\n        continue;\n\n      auto *Expr = SE.getSCEV(&I);\n      auto II = RewriteMap.find(Expr);\n\n      if (II == RewriteMap.end())\n        continue;\n\n      // Don't print things that are not interesting.\n      if (II->second.second == Expr)\n        continue;\n\n      OS.indent(Depth) << \"[PSE]\" << I << \":\\n\";\n      OS.indent(Depth + 2) << *Expr << \"\\n\";\n      OS.indent(Depth + 2) << \"--> \" << *II->second.second << \"\\n\";\n    }\n}\n\n// Match the mathematical pattern A - (A / B) * B, where A and B can be\n// arbitrary expressions. Also match zext (trunc A to iB) to iY, which is used\n// for URem with constant power-of-2 second operands.\n// It's not always easy, as A and B can be folded (imagine A is X / 2, and B is\n// 4, A / B becomes X / 8).\nbool ScalarEvolution::matchURem(const SCEV *Expr, const SCEV *&LHS,\n                                const SCEV *&RHS) {\n  // Try to match 'zext (trunc A to iB) to iY', which is used\n  // for URem with constant power-of-2 second operands. Make sure the size of\n  // the operand A matches the size of the whole expressions.\n  if (const auto *ZExt = dyn_cast<SCEVZeroExtendExpr>(Expr))\n    if (const auto *Trunc = dyn_cast<SCEVTruncateExpr>(ZExt->getOperand(0))) {\n      LHS = Trunc->getOperand();\n      // Bail out if the type of the LHS is larger than the type of the\n      // expression for now.\n      if (getTypeSizeInBits(LHS->getType()) >\n          getTypeSizeInBits(Expr->getType()))\n        return false;\n      if (LHS->getType() != Expr->getType())\n        LHS = getZeroExtendExpr(LHS, Expr->getType());\n      RHS = getConstant(APInt(getTypeSizeInBits(Expr->getType()), 1)\n                        << getTypeSizeInBits(Trunc->getType()));\n      return true;\n    }\n  const auto *Add = dyn_cast<SCEVAddExpr>(Expr);\n  if (Add == nullptr || Add->getNumOperands() != 2)\n    return false;\n\n  const SCEV *A = Add->getOperand(1);\n  const auto *Mul = dyn_cast<SCEVMulExpr>(Add->getOperand(0));\n\n  if (Mul == nullptr)\n    return false;\n\n  const auto MatchURemWithDivisor = [&](const SCEV *B) {\n    // (SomeExpr + (-(SomeExpr / B) * B)).\n    if (Expr == getURemExpr(A, B)) {\n      LHS = A;\n      RHS = B;\n      return true;\n    }\n    return false;\n  };\n\n  // (SomeExpr + (-1 * (SomeExpr / B) * B)).\n  if (Mul->getNumOperands() == 3 && isa<SCEVConstant>(Mul->getOperand(0)))\n    return MatchURemWithDivisor(Mul->getOperand(1)) ||\n           MatchURemWithDivisor(Mul->getOperand(2));\n\n  // (SomeExpr + ((-SomeExpr / B) * B)) or (SomeExpr + ((SomeExpr / B) * -B)).\n  if (Mul->getNumOperands() == 2)\n    return MatchURemWithDivisor(Mul->getOperand(1)) ||\n           MatchURemWithDivisor(Mul->getOperand(0)) ||\n           MatchURemWithDivisor(getNegativeSCEV(Mul->getOperand(1))) ||\n           MatchURemWithDivisor(getNegativeSCEV(Mul->getOperand(0)));\n  return false;\n}\n\nconst SCEV *\nScalarEvolution::computeSymbolicMaxBackedgeTakenCount(const Loop *L) {\n  SmallVector<BasicBlock*, 16> ExitingBlocks;\n  L->getExitingBlocks(ExitingBlocks);\n\n  // Form an expression for the maximum exit count possible for this loop. We\n  // merge the max and exact information to approximate a version of\n  // getConstantMaxBackedgeTakenCount which isn't restricted to just constants.\n  SmallVector<const SCEV*, 4> ExitCounts;\n  for (BasicBlock *ExitingBB : ExitingBlocks) {\n    const SCEV *ExitCount = getExitCount(L, ExitingBB);\n    if (isa<SCEVCouldNotCompute>(ExitCount))\n      ExitCount = getExitCount(L, ExitingBB,\n                                  ScalarEvolution::ConstantMaximum);\n    if (!isa<SCEVCouldNotCompute>(ExitCount)) {\n      assert(DT.dominates(ExitingBB, L->getLoopLatch()) &&\n             \"We should only have known counts for exiting blocks that \"\n             \"dominate latch!\");\n      ExitCounts.push_back(ExitCount);\n    }\n  }\n  if (ExitCounts.empty())\n    return getCouldNotCompute();\n  return getUMinFromMismatchedTypes(ExitCounts);\n}\n\n/// This rewriter is similar to SCEVParameterRewriter (it replaces SCEVUnknown\n/// components following the Map (Value -> SCEV)), but skips AddRecExpr because\n/// we cannot guarantee that the replacement is loop invariant in the loop of\n/// the AddRec.\nclass SCEVLoopGuardRewriter : public SCEVRewriteVisitor<SCEVLoopGuardRewriter> {\n  ValueToSCEVMapTy &Map;\n\npublic:\n  SCEVLoopGuardRewriter(ScalarEvolution &SE, ValueToSCEVMapTy &M)\n      : SCEVRewriteVisitor(SE), Map(M) {}\n\n  const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) { return Expr; }\n\n  const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n    auto I = Map.find(Expr->getValue());\n    if (I == Map.end())\n      return Expr;\n    return I->second;\n  }\n};\n\nconst SCEV *ScalarEvolution::applyLoopGuards(const SCEV *Expr, const Loop *L) {\n  auto CollectCondition = [&](ICmpInst::Predicate Predicate, const SCEV *LHS,\n                              const SCEV *RHS, ValueToSCEVMapTy &RewriteMap) {\n    // If we have LHS == 0, check if LHS is computing a property of some unknown\n    // SCEV %v which we can rewrite %v to express explicitly.\n    const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS);\n    if (Predicate == CmpInst::ICMP_EQ && RHSC &&\n        RHSC->getValue()->isNullValue()) {\n      // If LHS is A % B, i.e. A % B == 0, rewrite A to (A /u B) * B to\n      // explicitly express that.\n      const SCEV *URemLHS = nullptr;\n      const SCEV *URemRHS = nullptr;\n      if (matchURem(LHS, URemLHS, URemRHS)) {\n        if (const SCEVUnknown *LHSUnknown = dyn_cast<SCEVUnknown>(URemLHS)) {\n          Value *V = LHSUnknown->getValue();\n          auto Multiple =\n              getMulExpr(getUDivExpr(URemLHS, URemRHS), URemRHS,\n                         (SCEV::NoWrapFlags)(SCEV::FlagNUW | SCEV::FlagNSW));\n          RewriteMap[V] = Multiple;\n          return;\n        }\n      }\n    }\n\n    if (!isa<SCEVUnknown>(LHS)) {\n      std::swap(LHS, RHS);\n      Predicate = CmpInst::getSwappedPredicate(Predicate);\n    }\n\n    // For now, limit to conditions that provide information about unknown\n    // expressions.\n    auto *LHSUnknown = dyn_cast<SCEVUnknown>(LHS);\n    if (!LHSUnknown)\n      return;\n\n    // TODO: use information from more predicates.\n    switch (Predicate) {\n    case CmpInst::ICMP_ULT: {\n      if (!containsAddRecurrence(RHS)) {\n        const SCEV *Base = LHS;\n        auto I = RewriteMap.find(LHSUnknown->getValue());\n        if (I != RewriteMap.end())\n          Base = I->second;\n\n        RewriteMap[LHSUnknown->getValue()] =\n            getUMinExpr(Base, getMinusSCEV(RHS, getOne(RHS->getType())));\n      }\n      break;\n    }\n    case CmpInst::ICMP_ULE: {\n      if (!containsAddRecurrence(RHS)) {\n        const SCEV *Base = LHS;\n        auto I = RewriteMap.find(LHSUnknown->getValue());\n        if (I != RewriteMap.end())\n          Base = I->second;\n        RewriteMap[LHSUnknown->getValue()] = getUMinExpr(Base, RHS);\n      }\n      break;\n    }\n    case CmpInst::ICMP_EQ:\n      if (isa<SCEVConstant>(RHS))\n        RewriteMap[LHSUnknown->getValue()] = RHS;\n      break;\n    case CmpInst::ICMP_NE:\n      if (isa<SCEVConstant>(RHS) &&\n          cast<SCEVConstant>(RHS)->getValue()->isNullValue())\n        RewriteMap[LHSUnknown->getValue()] =\n            getUMaxExpr(LHS, getOne(RHS->getType()));\n      break;\n    default:\n      break;\n    }\n  };\n  // Starting at the loop predecessor, climb up the predecessor chain, as long\n  // as there are predecessors that can be found that have unique successors\n  // leading to the original header.\n  // TODO: share this logic with isLoopEntryGuardedByCond.\n  ValueToSCEVMapTy RewriteMap;\n  for (std::pair<const BasicBlock *, const BasicBlock *> Pair(\n           L->getLoopPredecessor(), L->getHeader());\n       Pair.first; Pair = getPredecessorWithUniqueSuccessorForBB(Pair.first)) {\n\n    const BranchInst *LoopEntryPredicate =\n        dyn_cast<BranchInst>(Pair.first->getTerminator());\n    if (!LoopEntryPredicate || LoopEntryPredicate->isUnconditional())\n      continue;\n\n    // TODO: use information from more complex conditions, e.g. AND expressions.\n    auto *Cmp = dyn_cast<ICmpInst>(LoopEntryPredicate->getCondition());\n    if (!Cmp)\n      continue;\n\n    auto Predicate = Cmp->getPredicate();\n    if (LoopEntryPredicate->getSuccessor(1) == Pair.second)\n      Predicate = CmpInst::getInversePredicate(Predicate);\n    CollectCondition(Predicate, getSCEV(Cmp->getOperand(0)),\n                     getSCEV(Cmp->getOperand(1)), RewriteMap);\n  }\n\n  // Also collect information from assumptions dominating the loop.\n  for (auto &AssumeVH : AC.assumptions()) {\n    auto *AssumeI = AssumeVH.getAssumeCI();\n    auto *Cmp = dyn_cast<ICmpInst>(AssumeI->getOperand(0));\n    if (!Cmp || !DT.dominates(AssumeI, L->getHeader()))\n      continue;\n    CollectCondition(Cmp->getPredicate(), getSCEV(Cmp->getOperand(0)),\n                     getSCEV(Cmp->getOperand(1)), RewriteMap);\n  }\n\n  if (RewriteMap.empty())\n    return Expr;\n  SCEVLoopGuardRewriter Rewriter(*this, RewriteMap);\n  return Rewriter.visit(Expr);\n}\n"}, "1": {"id": 1, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "content": "//===- llvm/Analysis/ScalarEvolution.h - Scalar Evolution -------*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// The ScalarEvolution class is an LLVM pass which can be used to analyze and\n// categorize scalar expressions in loops.  It specializes in recognizing\n// general induction variables, representing them with the abstract and opaque\n// SCEV class.  Given this analysis, trip counts of loops and other important\n// properties can be obtained.\n//\n// This analysis is primarily useful for induction variable substitution and\n// strength reduction.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ANALYSIS_SCALAREVOLUTION_H\n#define LLVM_ANALYSIS_SCALAREVOLUTION_H\n\n#include \"llvm/ADT/APInt.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/DenseMapInfo.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/Hashing.h\"\n#include \"llvm/ADT/Optional.h\"\n#include \"llvm/ADT/PointerIntPair.h\"\n#include \"llvm/ADT/SetVector.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/IR/ConstantRange.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/InstrTypes.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/Operator.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/IR/ValueMap.h\"\n#include \"llvm/Pass.h\"\n#include \"llvm/Support/Allocator.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/Compiler.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <memory>\n#include <utility>\n\nnamespace llvm {\n\nclass AssumptionCache;\nclass BasicBlock;\nclass Constant;\nclass ConstantInt;\nclass DataLayout;\nclass DominatorTree;\nclass GEPOperator;\nclass Instruction;\nclass LLVMContext;\nclass Loop;\nclass LoopInfo;\nclass raw_ostream;\nclass ScalarEvolution;\nclass SCEVAddRecExpr;\nclass SCEVUnknown;\nclass StructType;\nclass TargetLibraryInfo;\nclass Type;\nclass Value;\nenum SCEVTypes : unsigned short;\n\n/// This class represents an analyzed expression in the program.  These are\n/// opaque objects that the client is not allowed to do much with directly.\n///\nclass SCEV : public FoldingSetNode {\n  friend struct FoldingSetTrait<SCEV>;\n\n  /// A reference to an Interned FoldingSetNodeID for this node.  The\n  /// ScalarEvolution's BumpPtrAllocator holds the data.\n  FoldingSetNodeIDRef FastID;\n\n  // The SCEV baseclass this node corresponds to\n  const SCEVTypes SCEVType;\n\nprotected:\n  // Estimated complexity of this node's expression tree size.\n  const unsigned short ExpressionSize;\n\n  /// This field is initialized to zero and may be used in subclasses to store\n  /// miscellaneous information.\n  unsigned short SubclassData = 0;\n\npublic:\n  /// NoWrapFlags are bitfield indices into SubclassData.\n  ///\n  /// Add and Mul expressions may have no-unsigned-wrap <NUW> or\n  /// no-signed-wrap <NSW> properties, which are derived from the IR\n  /// operator. NSW is a misnomer that we use to mean no signed overflow or\n  /// underflow.\n  ///\n  /// AddRec expressions may have a no-self-wraparound <NW> property if, in\n  /// the integer domain, abs(step) * max-iteration(loop) <=\n  /// unsigned-max(bitwidth).  This means that the recurrence will never reach\n  /// its start value if the step is non-zero.  Computing the same value on\n  /// each iteration is not considered wrapping, and recurrences with step = 0\n  /// are trivially <NW>.  <NW> is independent of the sign of step and the\n  /// value the add recurrence starts with.\n  ///\n  /// Note that NUW and NSW are also valid properties of a recurrence, and\n  /// either implies NW. For convenience, NW will be set for a recurrence\n  /// whenever either NUW or NSW are set.\n  enum NoWrapFlags {\n    FlagAnyWrap = 0,    // No guarantee.\n    FlagNW = (1 << 0),  // No self-wrap.\n    FlagNUW = (1 << 1), // No unsigned wrap.\n    FlagNSW = (1 << 2), // No signed wrap.\n    NoWrapMask = (1 << 3) - 1\n  };\n\n  explicit SCEV(const FoldingSetNodeIDRef ID, SCEVTypes SCEVTy,\n                unsigned short ExpressionSize)\n      : FastID(ID), SCEVType(SCEVTy), ExpressionSize(ExpressionSize) {}\n  SCEV(const SCEV &) = delete;\n  SCEV &operator=(const SCEV &) = delete;\n\n  SCEVTypes getSCEVType() const { return SCEVType; }\n\n  /// Return the LLVM type of this SCEV expression.\n  Type *getType() const;\n\n  /// Return true if the expression is a constant zero.\n  bool isZero() const;\n\n  /// Return true if the expression is a constant one.\n  bool isOne() const;\n\n  /// Return true if the expression is a constant all-ones value.\n  bool isAllOnesValue() const;\n\n  /// Return true if the specified scev is negated, but not a constant.\n  bool isNonConstantNegative() const;\n\n  // Returns estimated size of the mathematical expression represented by this\n  // SCEV. The rules of its calculation are following:\n  // 1) Size of a SCEV without operands (like constants and SCEVUnknown) is 1;\n  // 2) Size SCEV with operands Op1, Op2, ..., OpN is calculated by formula:\n  //    (1 + Size(Op1) + ... + Size(OpN)).\n  // This value gives us an estimation of time we need to traverse through this\n  // SCEV and all its operands recursively. We may use it to avoid performing\n  // heavy transformations on SCEVs of excessive size for sake of saving the\n  // compilation time.\n  unsigned short getExpressionSize() const {\n    return ExpressionSize;\n  }\n\n  /// Print out the internal representation of this scalar to the specified\n  /// stream.  This should really only be used for debugging purposes.\n  void print(raw_ostream &OS) const;\n\n  /// This method is used for debugging.\n  void dump() const;\n};\n\n// Specialize FoldingSetTrait for SCEV to avoid needing to compute\n// temporary FoldingSetNodeID values.\ntemplate <> struct FoldingSetTrait<SCEV> : DefaultFoldingSetTrait<SCEV> {\n  static void Profile(const SCEV &X, FoldingSetNodeID &ID) { ID = X.FastID; }\n\n  static bool Equals(const SCEV &X, const FoldingSetNodeID &ID, unsigned IDHash,\n                     FoldingSetNodeID &TempID) {\n    return ID == X.FastID;\n  }\n\n  static unsigned ComputeHash(const SCEV &X, FoldingSetNodeID &TempID) {\n    return X.FastID.ComputeHash();\n  }\n};\n\ninline raw_ostream &operator<<(raw_ostream &OS, const SCEV &S) {\n  S.print(OS);\n  return OS;\n}\n\n/// An object of this class is returned by queries that could not be answered.\n/// For example, if you ask for the number of iterations of a linked-list\n/// traversal loop, you will get one of these.  None of the standard SCEV\n/// operations are valid on this class, it is just a marker.\nstruct SCEVCouldNotCompute : public SCEV {\n  SCEVCouldNotCompute();\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const SCEV *S);\n};\n\n/// This class represents an assumption made using SCEV expressions which can\n/// be checked at run-time.\nclass SCEVPredicate : public FoldingSetNode {\n  friend struct FoldingSetTrait<SCEVPredicate>;\n\n  /// A reference to an Interned FoldingSetNodeID for this node.  The\n  /// ScalarEvolution's BumpPtrAllocator holds the data.\n  FoldingSetNodeIDRef FastID;\n\npublic:\n  enum SCEVPredicateKind { P_Union, P_Equal, P_Wrap };\n\nprotected:\n  SCEVPredicateKind Kind;\n  ~SCEVPredicate() = default;\n  SCEVPredicate(const SCEVPredicate &) = default;\n  SCEVPredicate &operator=(const SCEVPredicate &) = default;\n\npublic:\n  SCEVPredicate(const FoldingSetNodeIDRef ID, SCEVPredicateKind Kind);\n\n  SCEVPredicateKind getKind() const { return Kind; }\n\n  /// Returns the estimated complexity of this predicate.  This is roughly\n  /// measured in the number of run-time checks required.\n  virtual unsigned getComplexity() const { return 1; }\n\n  /// Returns true if the predicate is always true. This means that no\n  /// assumptions were made and nothing needs to be checked at run-time.\n  virtual bool isAlwaysTrue() const = 0;\n\n  /// Returns true if this predicate implies \\p N.\n  virtual bool implies(const SCEVPredicate *N) const = 0;\n\n  /// Prints a textual representation of this predicate with an indentation of\n  /// \\p Depth.\n  virtual void print(raw_ostream &OS, unsigned Depth = 0) const = 0;\n\n  /// Returns the SCEV to which this predicate applies, or nullptr if this is\n  /// a SCEVUnionPredicate.\n  virtual const SCEV *getExpr() const = 0;\n};\n\ninline raw_ostream &operator<<(raw_ostream &OS, const SCEVPredicate &P) {\n  P.print(OS);\n  return OS;\n}\n\n// Specialize FoldingSetTrait for SCEVPredicate to avoid needing to compute\n// temporary FoldingSetNodeID values.\ntemplate <>\nstruct FoldingSetTrait<SCEVPredicate> : DefaultFoldingSetTrait<SCEVPredicate> {\n  static void Profile(const SCEVPredicate &X, FoldingSetNodeID &ID) {\n    ID = X.FastID;\n  }\n\n  static bool Equals(const SCEVPredicate &X, const FoldingSetNodeID &ID,\n                     unsigned IDHash, FoldingSetNodeID &TempID) {\n    return ID == X.FastID;\n  }\n\n  static unsigned ComputeHash(const SCEVPredicate &X,\n                              FoldingSetNodeID &TempID) {\n    return X.FastID.ComputeHash();\n  }\n};\n\n/// This class represents an assumption that two SCEV expressions are equal,\n/// and this can be checked at run-time.\nclass SCEVEqualPredicate final : public SCEVPredicate {\n  /// We assume that LHS == RHS.\n  const SCEV *LHS;\n  const SCEV *RHS;\n\npublic:\n  SCEVEqualPredicate(const FoldingSetNodeIDRef ID, const SCEV *LHS,\n                     const SCEV *RHS);\n\n  /// Implementation of the SCEVPredicate interface\n  bool implies(const SCEVPredicate *N) const override;\n  void print(raw_ostream &OS, unsigned Depth = 0) const override;\n  bool isAlwaysTrue() const override;\n  const SCEV *getExpr() const override;\n\n  /// Returns the left hand side of the equality.\n  const SCEV *getLHS() const { return LHS; }\n\n  /// Returns the right hand side of the equality.\n  const SCEV *getRHS() const { return RHS; }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const SCEVPredicate *P) {\n    return P->getKind() == P_Equal;\n  }\n};\n\n/// This class represents an assumption made on an AddRec expression. Given an\n/// affine AddRec expression {a,+,b}, we assume that it has the nssw or nusw\n/// flags (defined below) in the first X iterations of the loop, where X is a\n/// SCEV expression returned by getPredicatedBackedgeTakenCount).\n///\n/// Note that this does not imply that X is equal to the backedge taken\n/// count. This means that if we have a nusw predicate for i32 {0,+,1} with a\n/// predicated backedge taken count of X, we only guarantee that {0,+,1} has\n/// nusw in the first X iterations. {0,+,1} may still wrap in the loop if we\n/// have more than X iterations.\nclass SCEVWrapPredicate final : public SCEVPredicate {\npublic:\n  /// Similar to SCEV::NoWrapFlags, but with slightly different semantics\n  /// for FlagNUSW. The increment is considered to be signed, and a + b\n  /// (where b is the increment) is considered to wrap if:\n  ///    zext(a + b) != zext(a) + sext(b)\n  ///\n  /// If Signed is a function that takes an n-bit tuple and maps to the\n  /// integer domain as the tuples value interpreted as twos complement,\n  /// and Unsigned a function that takes an n-bit tuple and maps to the\n  /// integer domain as as the base two value of input tuple, then a + b\n  /// has IncrementNUSW iff:\n  ///\n  /// 0 <= Unsigned(a) + Signed(b) < 2^n\n  ///\n  /// The IncrementNSSW flag has identical semantics with SCEV::FlagNSW.\n  ///\n  /// Note that the IncrementNUSW flag is not commutative: if base + inc\n  /// has IncrementNUSW, then inc + base doesn't neccessarily have this\n  /// property. The reason for this is that this is used for sign/zero\n  /// extending affine AddRec SCEV expressions when a SCEVWrapPredicate is\n  /// assumed. A {base,+,inc} expression is already non-commutative with\n  /// regards to base and inc, since it is interpreted as:\n  ///     (((base + inc) + inc) + inc) ...\n  enum IncrementWrapFlags {\n    IncrementAnyWrap = 0,     // No guarantee.\n    IncrementNUSW = (1 << 0), // No unsigned with signed increment wrap.\n    IncrementNSSW = (1 << 1), // No signed with signed increment wrap\n                              // (equivalent with SCEV::NSW)\n    IncrementNoWrapMask = (1 << 2) - 1\n  };\n\n  /// Convenient IncrementWrapFlags manipulation methods.\n  LLVM_NODISCARD static SCEVWrapPredicate::IncrementWrapFlags\n  clearFlags(SCEVWrapPredicate::IncrementWrapFlags Flags,\n             SCEVWrapPredicate::IncrementWrapFlags OffFlags) {\n    assert((Flags & IncrementNoWrapMask) == Flags && \"Invalid flags value!\");\n    assert((OffFlags & IncrementNoWrapMask) == OffFlags &&\n           \"Invalid flags value!\");\n    return (SCEVWrapPredicate::IncrementWrapFlags)(Flags & ~OffFlags);\n  }\n\n  LLVM_NODISCARD static SCEVWrapPredicate::IncrementWrapFlags\n  maskFlags(SCEVWrapPredicate::IncrementWrapFlags Flags, int Mask) {\n    assert((Flags & IncrementNoWrapMask) == Flags && \"Invalid flags value!\");\n    assert((Mask & IncrementNoWrapMask) == Mask && \"Invalid mask value!\");\n\n    return (SCEVWrapPredicate::IncrementWrapFlags)(Flags & Mask);\n  }\n\n  LLVM_NODISCARD static SCEVWrapPredicate::IncrementWrapFlags\n  setFlags(SCEVWrapPredicate::IncrementWrapFlags Flags,\n           SCEVWrapPredicate::IncrementWrapFlags OnFlags) {\n    assert((Flags & IncrementNoWrapMask) == Flags && \"Invalid flags value!\");\n    assert((OnFlags & IncrementNoWrapMask) == OnFlags &&\n           \"Invalid flags value!\");\n\n    return (SCEVWrapPredicate::IncrementWrapFlags)(Flags | OnFlags);\n  }\n\n  /// Returns the set of SCEVWrapPredicate no wrap flags implied by a\n  /// SCEVAddRecExpr.\n  LLVM_NODISCARD static SCEVWrapPredicate::IncrementWrapFlags\n  getImpliedFlags(const SCEVAddRecExpr *AR, ScalarEvolution &SE);\n\nprivate:\n  const SCEVAddRecExpr *AR;\n  IncrementWrapFlags Flags;\n\npublic:\n  explicit SCEVWrapPredicate(const FoldingSetNodeIDRef ID,\n                             const SCEVAddRecExpr *AR,\n                             IncrementWrapFlags Flags);\n\n  /// Returns the set assumed no overflow flags.\n  IncrementWrapFlags getFlags() const { return Flags; }\n\n  /// Implementation of the SCEVPredicate interface\n  const SCEV *getExpr() const override;\n  bool implies(const SCEVPredicate *N) const override;\n  void print(raw_ostream &OS, unsigned Depth = 0) const override;\n  bool isAlwaysTrue() const override;\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const SCEVPredicate *P) {\n    return P->getKind() == P_Wrap;\n  }\n};\n\n/// This class represents a composition of other SCEV predicates, and is the\n/// class that most clients will interact with.  This is equivalent to a\n/// logical \"AND\" of all the predicates in the union.\n///\n/// NB! Unlike other SCEVPredicate sub-classes this class does not live in the\n/// ScalarEvolution::Preds folding set.  This is why the \\c add function is sound.\nclass SCEVUnionPredicate final : public SCEVPredicate {\nprivate:\n  using PredicateMap =\n      DenseMap<const SCEV *, SmallVector<const SCEVPredicate *, 4>>;\n\n  /// Vector with references to all predicates in this union.\n  SmallVector<const SCEVPredicate *, 16> Preds;\n\n  /// Maps SCEVs to predicates for quick look-ups.\n  PredicateMap SCEVToPreds;\n\npublic:\n  SCEVUnionPredicate();\n\n  const SmallVectorImpl<const SCEVPredicate *> &getPredicates() const {\n    return Preds;\n  }\n\n  /// Adds a predicate to this union.\n  void add(const SCEVPredicate *N);\n\n  /// Returns a reference to a vector containing all predicates which apply to\n  /// \\p Expr.\n  ArrayRef<const SCEVPredicate *> getPredicatesForExpr(const SCEV *Expr);\n\n  /// Implementation of the SCEVPredicate interface\n  bool isAlwaysTrue() const override;\n  bool implies(const SCEVPredicate *N) const override;\n  void print(raw_ostream &OS, unsigned Depth) const override;\n  const SCEV *getExpr() const override;\n\n  /// We estimate the complexity of a union predicate as the size number of\n  /// predicates in the union.\n  unsigned getComplexity() const override { return Preds.size(); }\n\n  /// Methods for support type inquiry through isa, cast, and dyn_cast:\n  static bool classof(const SCEVPredicate *P) {\n    return P->getKind() == P_Union;\n  }\n};\n\n/// The main scalar evolution driver. Because client code (intentionally)\n/// can't do much with the SCEV objects directly, they must ask this class\n/// for services.\nclass ScalarEvolution {\n  friend class ScalarEvolutionsTest;\n\npublic:\n  /// An enum describing the relationship between a SCEV and a loop.\n  enum LoopDisposition {\n    LoopVariant,   ///< The SCEV is loop-variant (unknown).\n    LoopInvariant, ///< The SCEV is loop-invariant.\n    LoopComputable ///< The SCEV varies predictably with the loop.\n  };\n\n  /// An enum describing the relationship between a SCEV and a basic block.\n  enum BlockDisposition {\n    DoesNotDominateBlock,  ///< The SCEV does not dominate the block.\n    DominatesBlock,        ///< The SCEV dominates the block.\n    ProperlyDominatesBlock ///< The SCEV properly dominates the block.\n  };\n\n  /// Convenient NoWrapFlags manipulation that hides enum casts and is\n  /// visible in the ScalarEvolution name space.\n  LLVM_NODISCARD static SCEV::NoWrapFlags maskFlags(SCEV::NoWrapFlags Flags,\n                                                    int Mask) {\n    return (SCEV::NoWrapFlags)(Flags & Mask);\n  }\n  LLVM_NODISCARD static SCEV::NoWrapFlags setFlags(SCEV::NoWrapFlags Flags,\n                                                   SCEV::NoWrapFlags OnFlags) {\n    return (SCEV::NoWrapFlags)(Flags | OnFlags);\n  }\n  LLVM_NODISCARD static SCEV::NoWrapFlags\n  clearFlags(SCEV::NoWrapFlags Flags, SCEV::NoWrapFlags OffFlags) {\n    return (SCEV::NoWrapFlags)(Flags & ~OffFlags);\n  }\n\n  ScalarEvolution(Function &F, TargetLibraryInfo &TLI, AssumptionCache &AC,\n                  DominatorTree &DT, LoopInfo &LI);\n  ScalarEvolution(ScalarEvolution &&Arg);\n  ~ScalarEvolution();\n\n  LLVMContext &getContext() const { return F.getContext(); }\n\n  /// Test if values of the given type are analyzable within the SCEV\n  /// framework. This primarily includes integer types, and it can optionally\n  /// include pointer types if the ScalarEvolution class has access to\n  /// target-specific information.\n  bool isSCEVable(Type *Ty) const;\n\n  /// Return the size in bits of the specified type, for which isSCEVable must\n  /// return true.\n  uint64_t getTypeSizeInBits(Type *Ty) const;\n\n  /// Return a type with the same bitwidth as the given type and which\n  /// represents how SCEV will treat the given type, for which isSCEVable must\n  /// return true. For pointer types, this is the pointer-sized integer type.\n  Type *getEffectiveSCEVType(Type *Ty) const;\n\n  // Returns a wider type among {Ty1, Ty2}.\n  Type *getWiderType(Type *Ty1, Type *Ty2) const;\n\n  /// Return true if the SCEV is a scAddRecExpr or it contains\n  /// scAddRecExpr. The result will be cached in HasRecMap.\n  bool containsAddRecurrence(const SCEV *S);\n\n  /// Erase Value from ValueExprMap and ExprValueMap.\n  void eraseValueFromMap(Value *V);\n\n  /// Return a SCEV expression for the full generality of the specified\n  /// expression.\n  const SCEV *getSCEV(Value *V);\n\n  const SCEV *getConstant(ConstantInt *V);\n  const SCEV *getConstant(const APInt &Val);\n  const SCEV *getConstant(Type *Ty, uint64_t V, bool isSigned = false);\n  const SCEV *getPtrToIntExpr(const SCEV *Op, Type *Ty, unsigned Depth = 0);\n  const SCEV *getTruncateExpr(const SCEV *Op, Type *Ty, unsigned Depth = 0);\n  const SCEV *getZeroExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth = 0);\n  const SCEV *getSignExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth = 0);\n  const SCEV *getAnyExtendExpr(const SCEV *Op, Type *Ty);\n  const SCEV *getAddExpr(SmallVectorImpl<const SCEV *> &Ops,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0);\n  const SCEV *getAddExpr(const SCEV *LHS, const SCEV *RHS,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0) {\n    SmallVector<const SCEV *, 2> Ops = {LHS, RHS};\n    return getAddExpr(Ops, Flags, Depth);\n  }\n  const SCEV *getAddExpr(const SCEV *Op0, const SCEV *Op1, const SCEV *Op2,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0) {\n    SmallVector<const SCEV *, 3> Ops = {Op0, Op1, Op2};\n    return getAddExpr(Ops, Flags, Depth);\n  }\n  const SCEV *getMulExpr(SmallVectorImpl<const SCEV *> &Ops,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0);\n  const SCEV *getMulExpr(const SCEV *LHS, const SCEV *RHS,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0) {\n    SmallVector<const SCEV *, 2> Ops = {LHS, RHS};\n    return getMulExpr(Ops, Flags, Depth);\n  }\n  const SCEV *getMulExpr(const SCEV *Op0, const SCEV *Op1, const SCEV *Op2,\n                         SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                         unsigned Depth = 0) {\n    SmallVector<const SCEV *, 3> Ops = {Op0, Op1, Op2};\n    return getMulExpr(Ops, Flags, Depth);\n  }\n  const SCEV *getUDivExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getUDivExactExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getURemExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getAddRecExpr(const SCEV *Start, const SCEV *Step, const Loop *L,\n                            SCEV::NoWrapFlags Flags);\n  const SCEV *getAddRecExpr(SmallVectorImpl<const SCEV *> &Operands,\n                            const Loop *L, SCEV::NoWrapFlags Flags);\n  const SCEV *getAddRecExpr(const SmallVectorImpl<const SCEV *> &Operands,\n                            const Loop *L, SCEV::NoWrapFlags Flags) {\n    SmallVector<const SCEV *, 4> NewOp(Operands.begin(), Operands.end());\n    return getAddRecExpr(NewOp, L, Flags);\n  }\n\n  /// Checks if \\p SymbolicPHI can be rewritten as an AddRecExpr under some\n  /// Predicates. If successful return these <AddRecExpr, Predicates>;\n  /// The function is intended to be called from PSCEV (the caller will decide\n  /// whether to actually add the predicates and carry out the rewrites).\n  Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\n  createAddRecFromPHIWithCasts(const SCEVUnknown *SymbolicPHI);\n\n  /// Returns an expression for a GEP\n  ///\n  /// \\p GEP The GEP. The indices contained in the GEP itself are ignored,\n  /// instead we use IndexExprs.\n  /// \\p IndexExprs The expressions for the indices.\n  const SCEV *getGEPExpr(GEPOperator *GEP,\n                         const SmallVectorImpl<const SCEV *> &IndexExprs);\n  const SCEV *getAbsExpr(const SCEV *Op, bool IsNSW);\n  const SCEV *getSignumExpr(const SCEV *Op);\n  const SCEV *getMinMaxExpr(SCEVTypes Kind,\n                            SmallVectorImpl<const SCEV *> &Operands);\n  const SCEV *getSMaxExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getSMaxExpr(SmallVectorImpl<const SCEV *> &Operands);\n  const SCEV *getUMaxExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getUMaxExpr(SmallVectorImpl<const SCEV *> &Operands);\n  const SCEV *getSMinExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getSMinExpr(SmallVectorImpl<const SCEV *> &Operands);\n  const SCEV *getUMinExpr(const SCEV *LHS, const SCEV *RHS);\n  const SCEV *getUMinExpr(SmallVectorImpl<const SCEV *> &Operands);\n  const SCEV *getUnknown(Value *V);\n  const SCEV *getCouldNotCompute();\n\n  /// Return a SCEV for the constant 0 of a specific type.\n  const SCEV *getZero(Type *Ty) { return getConstant(Ty, 0); }\n\n  /// Return a SCEV for the constant 1 of a specific type.\n  const SCEV *getOne(Type *Ty) { return getConstant(Ty, 1); }\n\n  /// Return a SCEV for the constant -1 of a specific type.\n  const SCEV *getMinusOne(Type *Ty) {\n    return getConstant(Ty, -1, /*isSigned=*/true);\n  }\n\n  /// Return an expression for sizeof ScalableTy that is type IntTy, where\n  /// ScalableTy is a scalable vector type.\n  const SCEV *getSizeOfScalableVectorExpr(Type *IntTy,\n                                          ScalableVectorType *ScalableTy);\n\n  /// Return an expression for the alloc size of AllocTy that is type IntTy\n  const SCEV *getSizeOfExpr(Type *IntTy, Type *AllocTy);\n\n  /// Return an expression for the store size of StoreTy that is type IntTy\n  const SCEV *getStoreSizeOfExpr(Type *IntTy, Type *StoreTy);\n\n  /// Return an expression for offsetof on the given field with type IntTy\n  const SCEV *getOffsetOfExpr(Type *IntTy, StructType *STy, unsigned FieldNo);\n\n  /// Return the SCEV object corresponding to -V.\n  const SCEV *getNegativeSCEV(const SCEV *V,\n                              SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap);\n\n  /// Return the SCEV object corresponding to ~V.\n  const SCEV *getNotSCEV(const SCEV *V);\n\n  /// Return LHS-RHS.  Minus is represented in SCEV as A+B*-1.\n  const SCEV *getMinusSCEV(const SCEV *LHS, const SCEV *RHS,\n                           SCEV::NoWrapFlags Flags = SCEV::FlagAnyWrap,\n                           unsigned Depth = 0);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type.  If the type must be extended, it is zero extended.\n  const SCEV *getTruncateOrZeroExtend(const SCEV *V, Type *Ty,\n                                      unsigned Depth = 0);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type.  If the type must be extended, it is sign extended.\n  const SCEV *getTruncateOrSignExtend(const SCEV *V, Type *Ty,\n                                      unsigned Depth = 0);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type.  If the type must be extended, it is zero extended.  The\n  /// conversion must not be narrowing.\n  const SCEV *getNoopOrZeroExtend(const SCEV *V, Type *Ty);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type.  If the type must be extended, it is sign extended.  The\n  /// conversion must not be narrowing.\n  const SCEV *getNoopOrSignExtend(const SCEV *V, Type *Ty);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type. If the type must be extended, it is extended with\n  /// unspecified bits. The conversion must not be narrowing.\n  const SCEV *getNoopOrAnyExtend(const SCEV *V, Type *Ty);\n\n  /// Return a SCEV corresponding to a conversion of the input value to the\n  /// specified type.  The conversion must not be widening.\n  const SCEV *getTruncateOrNoop(const SCEV *V, Type *Ty);\n\n  /// Promote the operands to the wider of the types using zero-extension, and\n  /// then perform a umax operation with them.\n  const SCEV *getUMaxFromMismatchedTypes(const SCEV *LHS, const SCEV *RHS);\n\n  /// Promote the operands to the wider of the types using zero-extension, and\n  /// then perform a umin operation with them.\n  const SCEV *getUMinFromMismatchedTypes(const SCEV *LHS, const SCEV *RHS);\n\n  /// Promote the operands to the wider of the types using zero-extension, and\n  /// then perform a umin operation with them. N-ary function.\n  const SCEV *getUMinFromMismatchedTypes(SmallVectorImpl<const SCEV *> &Ops);\n\n  /// Transitively follow the chain of pointer-type operands until reaching a\n  /// SCEV that does not have a single pointer operand. This returns a\n  /// SCEVUnknown pointer for well-formed pointer-type expressions, but corner\n  /// cases do exist.\n  const SCEV *getPointerBase(const SCEV *V);\n\n  /// Return a SCEV expression for the specified value at the specified scope\n  /// in the program.  The L value specifies a loop nest to evaluate the\n  /// expression at, where null is the top-level or a specified loop is\n  /// immediately inside of the loop.\n  ///\n  /// This method can be used to compute the exit value for a variable defined\n  /// in a loop by querying what the value will hold in the parent loop.\n  ///\n  /// In the case that a relevant loop exit value cannot be computed, the\n  /// original value V is returned.\n  const SCEV *getSCEVAtScope(const SCEV *S, const Loop *L);\n\n  /// This is a convenience function which does getSCEVAtScope(getSCEV(V), L).\n  const SCEV *getSCEVAtScope(Value *V, const Loop *L);\n\n  /// Test whether entry to the loop is protected by a conditional between LHS\n  /// and RHS.  This is used to help avoid max expressions in loop trip\n  /// counts, and to eliminate casts.\n  bool isLoopEntryGuardedByCond(const Loop *L, ICmpInst::Predicate Pred,\n                                const SCEV *LHS, const SCEV *RHS);\n\n  /// Test whether entry to the basic block is protected by a conditional\n  /// between LHS and RHS.\n  bool isBasicBlockEntryGuardedByCond(const BasicBlock *BB,\n                                      ICmpInst::Predicate Pred, const SCEV *LHS,\n                                      const SCEV *RHS);\n\n  /// Test whether the backedge of the loop is protected by a conditional\n  /// between LHS and RHS.  This is used to eliminate casts.\n  bool isLoopBackedgeGuardedByCond(const Loop *L, ICmpInst::Predicate Pred,\n                                   const SCEV *LHS, const SCEV *RHS);\n\n  /// Returns the maximum trip count of the loop if it is a single-exit\n  /// loop and we can compute a small maximum for that loop.\n  ///\n  /// Implemented in terms of the \\c getSmallConstantTripCount overload with\n  /// the single exiting block passed to it. See that routine for details.\n  unsigned getSmallConstantTripCount(const Loop *L);\n\n  /// Returns the maximum trip count of this loop as a normal unsigned\n  /// value. Returns 0 if the trip count is unknown or not constant. This\n  /// \"trip count\" assumes that control exits via ExitingBlock. More\n  /// precisely, it is the number of times that control may reach ExitingBlock\n  /// before taking the branch. For loops with multiple exits, it may not be\n  /// the number times that the loop header executes if the loop exits\n  /// prematurely via another branch.\n  unsigned getSmallConstantTripCount(const Loop *L,\n                                     const BasicBlock *ExitingBlock);\n\n  /// Returns the upper bound of the loop trip count as a normal unsigned\n  /// value.\n  /// Returns 0 if the trip count is unknown or not constant.\n  unsigned getSmallConstantMaxTripCount(const Loop *L);\n\n  /// Returns the largest constant divisor of the trip count of the\n  /// loop if it is a single-exit loop and we can compute a small maximum for\n  /// that loop.\n  ///\n  /// Implemented in terms of the \\c getSmallConstantTripMultiple overload with\n  /// the single exiting block passed to it. See that routine for details.\n  unsigned getSmallConstantTripMultiple(const Loop *L);\n\n  /// Returns the largest constant divisor of the trip count of this loop as a\n  /// normal unsigned value, if possible. This means that the actual trip\n  /// count is always a multiple of the returned value (don't forget the trip\n  /// count could very well be zero as well!). As explained in the comments\n  /// for getSmallConstantTripCount, this assumes that control exits the loop\n  /// via ExitingBlock.\n  unsigned getSmallConstantTripMultiple(const Loop *L,\n                                        const BasicBlock *ExitingBlock);\n\n  /// The terms \"backedge taken count\" and \"exit count\" are used\n  /// interchangeably to refer to the number of times the backedge of a loop \n  /// has executed before the loop is exited.\n  enum ExitCountKind {\n    /// An expression exactly describing the number of times the backedge has\n    /// executed when a loop is exited.\n    Exact,\n    /// A constant which provides an upper bound on the exact trip count.\n    ConstantMaximum,\n    /// An expression which provides an upper bound on the exact trip count.\n    SymbolicMaximum,\n  };\n\n  /// Return the number of times the backedge executes before the given exit\n  /// would be taken; if not exactly computable, return SCEVCouldNotCompute. \n  /// For a single exit loop, this value is equivelent to the result of\n  /// getBackedgeTakenCount.  The loop is guaranteed to exit (via *some* exit)\n  /// before the backedge is executed (ExitCount + 1) times.  Note that there\n  /// is no guarantee about *which* exit is taken on the exiting iteration.\n  const SCEV *getExitCount(const Loop *L, const BasicBlock *ExitingBlock,\n                           ExitCountKind Kind = Exact);\n\n  /// If the specified loop has a predictable backedge-taken count, return it,\n  /// otherwise return a SCEVCouldNotCompute object. The backedge-taken count is\n  /// the number of times the loop header will be branched to from within the\n  /// loop, assuming there are no abnormal exists like exception throws. This is\n  /// one less than the trip count of the loop, since it doesn't count the first\n  /// iteration, when the header is branched to from outside the loop.\n  ///\n  /// Note that it is not valid to call this method on a loop without a\n  /// loop-invariant backedge-taken count (see\n  /// hasLoopInvariantBackedgeTakenCount).\n  const SCEV *getBackedgeTakenCount(const Loop *L, ExitCountKind Kind = Exact);\n\n  /// Similar to getBackedgeTakenCount, except it will add a set of\n  /// SCEV predicates to Predicates that are required to be true in order for\n  /// the answer to be correct. Predicates can be checked with run-time\n  /// checks and can be used to perform loop versioning.\n  const SCEV *getPredicatedBackedgeTakenCount(const Loop *L,\n                                              SCEVUnionPredicate &Predicates);\n\n  /// When successful, this returns a SCEVConstant that is greater than or equal\n  /// to (i.e. a \"conservative over-approximation\") of the value returend by\n  /// getBackedgeTakenCount.  If such a value cannot be computed, it returns the\n  /// SCEVCouldNotCompute object.\n  const SCEV *getConstantMaxBackedgeTakenCount(const Loop *L) {\n    return getBackedgeTakenCount(L, ConstantMaximum);\n  }\n\n  /// When successful, this returns a SCEV that is greater than or equal\n  /// to (i.e. a \"conservative over-approximation\") of the value returend by\n  /// getBackedgeTakenCount.  If such a value cannot be computed, it returns the\n  /// SCEVCouldNotCompute object.\n  const SCEV *getSymbolicMaxBackedgeTakenCount(const Loop *L) {\n    return getBackedgeTakenCount(L, SymbolicMaximum);\n  }\n\n  /// Return true if the backedge taken count is either the value returned by\n  /// getConstantMaxBackedgeTakenCount or zero.\n  bool isBackedgeTakenCountMaxOrZero(const Loop *L);\n\n  /// Return true if the specified loop has an analyzable loop-invariant\n  /// backedge-taken count.\n  bool hasLoopInvariantBackedgeTakenCount(const Loop *L);\n\n  // This method should be called by the client when it made any change that\n  // would invalidate SCEV's answers, and the client wants to remove all loop\n  // information held internally by ScalarEvolution. This is intended to be used\n  // when the alternative to forget a loop is too expensive (i.e. large loop\n  // bodies).\n  void forgetAllLoops();\n\n  /// This method should be called by the client when it has changed a loop in\n  /// a way that may effect ScalarEvolution's ability to compute a trip count,\n  /// or if the loop is deleted.  This call is potentially expensive for large\n  /// loop bodies.\n  void forgetLoop(const Loop *L);\n\n  // This method invokes forgetLoop for the outermost loop of the given loop\n  // \\p L, making ScalarEvolution forget about all this subtree. This needs to\n  // be done whenever we make a transform that may affect the parameters of the\n  // outer loop, such as exit counts for branches.\n  void forgetTopmostLoop(const Loop *L);\n\n  /// This method should be called by the client when it has changed a value\n  /// in a way that may effect its value, or which may disconnect it from a\n  /// def-use chain linking it to a loop.\n  void forgetValue(Value *V);\n\n  /// Called when the client has changed the disposition of values in\n  /// this loop.\n  ///\n  /// We don't have a way to invalidate per-loop dispositions. Clear and\n  /// recompute is simpler.\n  void forgetLoopDispositions(const Loop *L);\n\n  /// Determine the minimum number of zero bits that S is guaranteed to end in\n  /// (at every loop iteration).  It is, at the same time, the minimum number\n  /// of times S is divisible by 2.  For example, given {4,+,8} it returns 2.\n  /// If S is guaranteed to be 0, it returns the bitwidth of S.\n  uint32_t GetMinTrailingZeros(const SCEV *S);\n\n  /// Determine the unsigned range for a particular SCEV.\n  /// NOTE: This returns a copy of the reference returned by getRangeRef.\n  ConstantRange getUnsignedRange(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_UNSIGNED);\n  }\n\n  /// Determine the min of the unsigned range for a particular SCEV.\n  APInt getUnsignedRangeMin(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_UNSIGNED).getUnsignedMin();\n  }\n\n  /// Determine the max of the unsigned range for a particular SCEV.\n  APInt getUnsignedRangeMax(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_UNSIGNED).getUnsignedMax();\n  }\n\n  /// Determine the signed range for a particular SCEV.\n  /// NOTE: This returns a copy of the reference returned by getRangeRef.\n  ConstantRange getSignedRange(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_SIGNED);\n  }\n\n  /// Determine the min of the signed range for a particular SCEV.\n  APInt getSignedRangeMin(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_SIGNED).getSignedMin();\n  }\n\n  /// Determine the max of the signed range for a particular SCEV.\n  APInt getSignedRangeMax(const SCEV *S) {\n    return getRangeRef(S, HINT_RANGE_SIGNED).getSignedMax();\n  }\n\n  /// Test if the given expression is known to be negative.\n  bool isKnownNegative(const SCEV *S);\n\n  /// Test if the given expression is known to be positive.\n  bool isKnownPositive(const SCEV *S);\n\n  /// Test if the given expression is known to be non-negative.\n  bool isKnownNonNegative(const SCEV *S);\n\n  /// Test if the given expression is known to be non-positive.\n  bool isKnownNonPositive(const SCEV *S);\n\n  /// Test if the given expression is known to be non-zero.\n  bool isKnownNonZero(const SCEV *S);\n\n  /// Splits SCEV expression \\p S into two SCEVs. One of them is obtained from\n  /// \\p S by substitution of all AddRec sub-expression related to loop \\p L\n  /// with initial value of that SCEV. The second is obtained from \\p S by\n  /// substitution of all AddRec sub-expressions related to loop \\p L with post\n  /// increment of this AddRec in the loop \\p L. In both cases all other AddRec\n  /// sub-expressions (not related to \\p L) remain the same.\n  /// If the \\p S contains non-invariant unknown SCEV the function returns\n  /// CouldNotCompute SCEV in both values of std::pair.\n  /// For example, for SCEV S={0, +, 1}<L1> + {0, +, 1}<L2> and loop L=L1\n  /// the function returns pair:\n  /// first = {0, +, 1}<L2>\n  /// second = {1, +, 1}<L1> + {0, +, 1}<L2>\n  /// We can see that for the first AddRec sub-expression it was replaced with\n  /// 0 (initial value) for the first element and to {1, +, 1}<L1> (post\n  /// increment value) for the second one. In both cases AddRec expression\n  /// related to L2 remains the same.\n  std::pair<const SCEV *, const SCEV *> SplitIntoInitAndPostInc(const Loop *L,\n                                                                const SCEV *S);\n\n  /// We'd like to check the predicate on every iteration of the most dominated\n  /// loop between loops used in LHS and RHS.\n  /// To do this we use the following list of steps:\n  /// 1. Collect set S all loops on which either LHS or RHS depend.\n  /// 2. If S is non-empty\n  /// a. Let PD be the element of S which is dominated by all other elements.\n  /// b. Let E(LHS) be value of LHS on entry of PD.\n  ///    To get E(LHS), we should just take LHS and replace all AddRecs that are\n  ///    attached to PD on with their entry values.\n  ///    Define E(RHS) in the same way.\n  /// c. Let B(LHS) be value of L on backedge of PD.\n  ///    To get B(LHS), we should just take LHS and replace all AddRecs that are\n  ///    attached to PD on with their backedge values.\n  ///    Define B(RHS) in the same way.\n  /// d. Note that E(LHS) and E(RHS) are automatically available on entry of PD,\n  ///    so we can assert on that.\n  /// e. Return true if isLoopEntryGuardedByCond(Pred, E(LHS), E(RHS)) &&\n  ///                   isLoopBackedgeGuardedByCond(Pred, B(LHS), B(RHS))\n  bool isKnownViaInduction(ICmpInst::Predicate Pred, const SCEV *LHS,\n                           const SCEV *RHS);\n\n  /// Test if the given expression is known to satisfy the condition described\n  /// by Pred, LHS, and RHS.\n  bool isKnownPredicate(ICmpInst::Predicate Pred, const SCEV *LHS,\n                        const SCEV *RHS);\n\n  /// Test if the given expression is known to satisfy the condition described\n  /// by Pred, LHS, and RHS in the given Context.\n  bool isKnownPredicateAt(ICmpInst::Predicate Pred, const SCEV *LHS,\n                        const SCEV *RHS, const Instruction *Context);\n\n  /// Test if the condition described by Pred, LHS, RHS is known to be true on\n  /// every iteration of the loop of the recurrency LHS.\n  bool isKnownOnEveryIteration(ICmpInst::Predicate Pred,\n                               const SCEVAddRecExpr *LHS, const SCEV *RHS);\n\n  /// A predicate is said to be monotonically increasing if may go from being\n  /// false to being true as the loop iterates, but never the other way\n  /// around.  A predicate is said to be monotonically decreasing if may go\n  /// from being true to being false as the loop iterates, but never the other\n  /// way around.\n  enum MonotonicPredicateType {\n    MonotonicallyIncreasing,\n    MonotonicallyDecreasing\n  };\n\n  /// If, for all loop invariant X, the predicate \"LHS `Pred` X\" is\n  /// monotonically increasing or decreasing, returns\n  /// Some(MonotonicallyIncreasing) and Some(MonotonicallyDecreasing)\n  /// respectively. If we could not prove either of these facts, returns None.\n  Optional<MonotonicPredicateType>\n  getMonotonicPredicateType(const SCEVAddRecExpr *LHS,\n                            ICmpInst::Predicate Pred);\n\n  struct LoopInvariantPredicate {\n    ICmpInst::Predicate Pred;\n    const SCEV *LHS;\n    const SCEV *RHS;\n\n    LoopInvariantPredicate(ICmpInst::Predicate Pred, const SCEV *LHS,\n                           const SCEV *RHS)\n        : Pred(Pred), LHS(LHS), RHS(RHS) {}\n  };\n  /// If the result of the predicate LHS `Pred` RHS is loop invariant with\n  /// respect to L, return a LoopInvariantPredicate with LHS and RHS being\n  /// invariants, available at L's entry. Otherwise, return None.\n  Optional<LoopInvariantPredicate>\n  getLoopInvariantPredicate(ICmpInst::Predicate Pred, const SCEV *LHS,\n                            const SCEV *RHS, const Loop *L);\n\n  /// If the result of the predicate LHS `Pred` RHS is loop invariant with\n  /// respect to L at given Context during at least first MaxIter iterations,\n  /// return a LoopInvariantPredicate with LHS and RHS being invariants,\n  /// available at L's entry. Otherwise, return None. The predicate should be\n  /// the loop's exit condition.\n  Optional<LoopInvariantPredicate>\n  getLoopInvariantExitCondDuringFirstIterations(ICmpInst::Predicate Pred,\n                                                const SCEV *LHS,\n                                                const SCEV *RHS, const Loop *L,\n                                                const Instruction *Context,\n                                                const SCEV *MaxIter);\n\n  /// Simplify LHS and RHS in a comparison with predicate Pred. Return true\n  /// iff any changes were made. If the operands are provably equal or\n  /// unequal, LHS and RHS are set to the same value and Pred is set to either\n  /// ICMP_EQ or ICMP_NE.\n  bool SimplifyICmpOperands(ICmpInst::Predicate &Pred, const SCEV *&LHS,\n                            const SCEV *&RHS, unsigned Depth = 0);\n\n  /// Return the \"disposition\" of the given SCEV with respect to the given\n  /// loop.\n  LoopDisposition getLoopDisposition(const SCEV *S, const Loop *L);\n\n  /// Return true if the value of the given SCEV is unchanging in the\n  /// specified loop.\n  bool isLoopInvariant(const SCEV *S, const Loop *L);\n\n  /// Determine if the SCEV can be evaluated at loop's entry. It is true if it\n  /// doesn't depend on a SCEVUnknown of an instruction which is dominated by\n  /// the header of loop L.\n  bool isAvailableAtLoopEntry(const SCEV *S, const Loop *L);\n\n  /// Return true if the given SCEV changes value in a known way in the\n  /// specified loop.  This property being true implies that the value is\n  /// variant in the loop AND that we can emit an expression to compute the\n  /// value of the expression at any particular loop iteration.\n  bool hasComputableLoopEvolution(const SCEV *S, const Loop *L);\n\n  /// Return the \"disposition\" of the given SCEV with respect to the given\n  /// block.\n  BlockDisposition getBlockDisposition(const SCEV *S, const BasicBlock *BB);\n\n  /// Return true if elements that makes up the given SCEV dominate the\n  /// specified basic block.\n  bool dominates(const SCEV *S, const BasicBlock *BB);\n\n  /// Return true if elements that makes up the given SCEV properly dominate\n  /// the specified basic block.\n  bool properlyDominates(const SCEV *S, const BasicBlock *BB);\n\n  /// Test whether the given SCEV has Op as a direct or indirect operand.\n  bool hasOperand(const SCEV *S, const SCEV *Op) const;\n\n  /// Return the size of an element read or written by Inst.\n  const SCEV *getElementSize(Instruction *Inst);\n\n  /// Compute the array dimensions Sizes from the set of Terms extracted from\n  /// the memory access function of this SCEVAddRecExpr (second step of\n  /// delinearization).\n  void findArrayDimensions(SmallVectorImpl<const SCEV *> &Terms,\n                           SmallVectorImpl<const SCEV *> &Sizes,\n                           const SCEV *ElementSize);\n\n  void print(raw_ostream &OS) const;\n  void verify() const;\n  bool invalidate(Function &F, const PreservedAnalyses &PA,\n                  FunctionAnalysisManager::Invalidator &Inv);\n\n  /// Collect parametric terms occurring in step expressions (first step of\n  /// delinearization).\n  void collectParametricTerms(const SCEV *Expr,\n                              SmallVectorImpl<const SCEV *> &Terms);\n\n  /// Return in Subscripts the access functions for each dimension in Sizes\n  /// (third step of delinearization).\n  void computeAccessFunctions(const SCEV *Expr,\n                              SmallVectorImpl<const SCEV *> &Subscripts,\n                              SmallVectorImpl<const SCEV *> &Sizes);\n\n  /// Gathers the individual index expressions from a GEP instruction.\n  ///\n  /// This function optimistically assumes the GEP references into a fixed size\n  /// array. If this is actually true, this function returns a list of array\n  /// subscript expressions in \\p Subscripts and a list of integers describing\n  /// the size of the individual array dimensions in \\p Sizes. Both lists have\n  /// either equal length or the size list is one element shorter in case there\n  /// is no known size available for the outermost array dimension. Returns true\n  /// if successful and false otherwise.\n  bool getIndexExpressionsFromGEP(const GetElementPtrInst *GEP,\n                                  SmallVectorImpl<const SCEV *> &Subscripts,\n                                  SmallVectorImpl<int> &Sizes);\n\n  /// Split this SCEVAddRecExpr into two vectors of SCEVs representing the\n  /// subscripts and sizes of an array access.\n  ///\n  /// The delinearization is a 3 step process: the first two steps compute the\n  /// sizes of each subscript and the third step computes the access functions\n  /// for the delinearized array:\n  ///\n  /// 1. Find the terms in the step functions\n  /// 2. Compute the array size\n  /// 3. Compute the access function: divide the SCEV by the array size\n  ///    starting with the innermost dimensions found in step 2. The Quotient\n  ///    is the SCEV to be divided in the next step of the recursion. The\n  ///    Remainder is the subscript of the innermost dimension. Loop over all\n  ///    array dimensions computed in step 2.\n  ///\n  /// To compute a uniform array size for several memory accesses to the same\n  /// object, one can collect in step 1 all the step terms for all the memory\n  /// accesses, and compute in step 2 a unique array shape. This guarantees\n  /// that the array shape will be the same across all memory accesses.\n  ///\n  /// FIXME: We could derive the result of steps 1 and 2 from a description of\n  /// the array shape given in metadata.\n  ///\n  /// Example:\n  ///\n  /// A[][n][m]\n  ///\n  /// for i\n  ///   for j\n  ///     for k\n  ///       A[j+k][2i][5i] =\n  ///\n  /// The initial SCEV:\n  ///\n  /// A[{{{0,+,2*m+5}_i, +, n*m}_j, +, n*m}_k]\n  ///\n  /// 1. Find the different terms in the step functions:\n  /// -> [2*m, 5, n*m, n*m]\n  ///\n  /// 2. Compute the array size: sort and unique them\n  /// -> [n*m, 2*m, 5]\n  /// find the GCD of all the terms = 1\n  /// divide by the GCD and erase constant terms\n  /// -> [n*m, 2*m]\n  /// GCD = m\n  /// divide by GCD -> [n, 2]\n  /// remove constant terms\n  /// -> [n]\n  /// size of the array is A[unknown][n][m]\n  ///\n  /// 3. Compute the access function\n  /// a. Divide {{{0,+,2*m+5}_i, +, n*m}_j, +, n*m}_k by the innermost size m\n  /// Quotient: {{{0,+,2}_i, +, n}_j, +, n}_k\n  /// Remainder: {{{0,+,5}_i, +, 0}_j, +, 0}_k\n  /// The remainder is the subscript of the innermost array dimension: [5i].\n  ///\n  /// b. Divide Quotient: {{{0,+,2}_i, +, n}_j, +, n}_k by next outer size n\n  /// Quotient: {{{0,+,0}_i, +, 1}_j, +, 1}_k\n  /// Remainder: {{{0,+,2}_i, +, 0}_j, +, 0}_k\n  /// The Remainder is the subscript of the next array dimension: [2i].\n  ///\n  /// The subscript of the outermost dimension is the Quotient: [j+k].\n  ///\n  /// Overall, we have: A[][n][m], and the access function: A[j+k][2i][5i].\n  void delinearize(const SCEV *Expr, SmallVectorImpl<const SCEV *> &Subscripts,\n                   SmallVectorImpl<const SCEV *> &Sizes,\n                   const SCEV *ElementSize);\n\n  /// Return the DataLayout associated with the module this SCEV instance is\n  /// operating on.\n  const DataLayout &getDataLayout() const {\n    return F.getParent()->getDataLayout();\n  }\n\n  const SCEVPredicate *getEqualPredicate(const SCEV *LHS, const SCEV *RHS);\n\n  const SCEVPredicate *\n  getWrapPredicate(const SCEVAddRecExpr *AR,\n                   SCEVWrapPredicate::IncrementWrapFlags AddedFlags);\n\n  /// Re-writes the SCEV according to the Predicates in \\p A.\n  const SCEV *rewriteUsingPredicate(const SCEV *S, const Loop *L,\n                                    SCEVUnionPredicate &A);\n  /// Tries to convert the \\p S expression to an AddRec expression,\n  /// adding additional predicates to \\p Preds as required.\n  const SCEVAddRecExpr *convertSCEVToAddRecWithPredicates(\n      const SCEV *S, const Loop *L,\n      SmallPtrSetImpl<const SCEVPredicate *> &Preds);\n\n  /// Compute \\p LHS - \\p RHS and returns the result as an APInt if it is a\n  /// constant, and None if it isn't.\n  ///\n  /// This is intended to be a cheaper version of getMinusSCEV.  We can be\n  /// frugal here since we just bail out of actually constructing and\n  /// canonicalizing an expression in the cases where the result isn't going\n  /// to be a constant.\n  Optional<APInt> computeConstantDifference(const SCEV *LHS, const SCEV *RHS);\n\n  /// Update no-wrap flags of an AddRec. This may drop the cached info about\n  /// this AddRec (such as range info) in case if new flags may potentially\n  /// sharpen it.\n  void setNoWrapFlags(SCEVAddRecExpr *AddRec, SCEV::NoWrapFlags Flags);\n\n  /// Try to apply information from loop guards for \\p L to \\p Expr.\n  const SCEV *applyLoopGuards(const SCEV *Expr, const Loop *L);\n\nprivate:\n  /// A CallbackVH to arrange for ScalarEvolution to be notified whenever a\n  /// Value is deleted.\n  class SCEVCallbackVH final : public CallbackVH {\n    ScalarEvolution *SE;\n\n    void deleted() override;\n    void allUsesReplacedWith(Value *New) override;\n\n  public:\n    SCEVCallbackVH(Value *V, ScalarEvolution *SE = nullptr);\n  };\n\n  friend class SCEVCallbackVH;\n  friend class SCEVExpander;\n  friend class SCEVUnknown;\n\n  /// The function we are analyzing.\n  Function &F;\n\n  /// Does the module have any calls to the llvm.experimental.guard intrinsic\n  /// at all?  If this is false, we avoid doing work that will only help if\n  /// thare are guards present in the IR.\n  bool HasGuards;\n\n  /// The target library information for the target we are targeting.\n  TargetLibraryInfo &TLI;\n\n  /// The tracker for \\@llvm.assume intrinsics in this function.\n  AssumptionCache &AC;\n\n  /// The dominator tree.\n  DominatorTree &DT;\n\n  /// The loop information for the function we are currently analyzing.\n  LoopInfo &LI;\n\n  /// This SCEV is used to represent unknown trip counts and things.\n  std::unique_ptr<SCEVCouldNotCompute> CouldNotCompute;\n\n  /// The type for HasRecMap.\n  using HasRecMapType = DenseMap<const SCEV *, bool>;\n\n  /// This is a cache to record whether a SCEV contains any scAddRecExpr.\n  HasRecMapType HasRecMap;\n\n  /// The type for ExprValueMap.\n  using ValueOffsetPair = std::pair<Value *, ConstantInt *>;\n  using ExprValueMapType = DenseMap<const SCEV *, SetVector<ValueOffsetPair>>;\n\n  /// ExprValueMap -- This map records the original values from which\n  /// the SCEV expr is generated from.\n  ///\n  /// We want to represent the mapping as SCEV -> ValueOffsetPair instead\n  /// of SCEV -> Value:\n  /// Suppose we know S1 expands to V1, and\n  ///  S1 = S2 + C_a\n  ///  S3 = S2 + C_b\n  /// where C_a and C_b are different SCEVConstants. Then we'd like to\n  /// expand S3 as V1 - C_a + C_b instead of expanding S2 literally.\n  /// It is helpful when S2 is a complex SCEV expr.\n  ///\n  /// In order to do that, we represent ExprValueMap as a mapping from\n  /// SCEV to ValueOffsetPair. We will save both S1->{V1, 0} and\n  /// S2->{V1, C_a} into the map when we create SCEV for V1. When S3\n  /// is expanded, it will first expand S2 to V1 - C_a because of\n  /// S2->{V1, C_a} in the map, then expand S3 to V1 - C_a + C_b.\n  ///\n  /// Note: S->{V, Offset} in the ExprValueMap means S can be expanded\n  /// to V - Offset.\n  ExprValueMapType ExprValueMap;\n\n  /// The type for ValueExprMap.\n  using ValueExprMapType =\n      DenseMap<SCEVCallbackVH, const SCEV *, DenseMapInfo<Value *>>;\n\n  /// This is a cache of the values we have analyzed so far.\n  ValueExprMapType ValueExprMap;\n\n  /// Mark predicate values currently being processed by isImpliedCond.\n  SmallPtrSet<const Value *, 6> PendingLoopPredicates;\n\n  /// Mark SCEVUnknown Phis currently being processed by getRangeRef.\n  SmallPtrSet<const PHINode *, 6> PendingPhiRanges;\n\n  // Mark SCEVUnknown Phis currently being processed by isImpliedViaMerge.\n  SmallPtrSet<const PHINode *, 6> PendingMerges;\n\n  /// Set to true by isLoopBackedgeGuardedByCond when we're walking the set of\n  /// conditions dominating the backedge of a loop.\n  bool WalkingBEDominatingConds = false;\n\n  /// Set to true by isKnownPredicateViaSplitting when we're trying to prove a\n  /// predicate by splitting it into a set of independent predicates.\n  bool ProvingSplitPredicate = false;\n\n  /// Memoized values for the GetMinTrailingZeros\n  DenseMap<const SCEV *, uint32_t> MinTrailingZerosCache;\n\n  /// Return the Value set from which the SCEV expr is generated.\n  SetVector<ValueOffsetPair> *getSCEVValues(const SCEV *S);\n\n  /// Private helper method for the GetMinTrailingZeros method\n  uint32_t GetMinTrailingZerosImpl(const SCEV *S);\n\n  /// Information about the number of loop iterations for which a loop exit's\n  /// branch condition evaluates to the not-taken path.  This is a temporary\n  /// pair of exact and max expressions that are eventually summarized in\n  /// ExitNotTakenInfo and BackedgeTakenInfo.\n  struct ExitLimit {\n    const SCEV *ExactNotTaken; // The exit is not taken exactly this many times\n    const SCEV *MaxNotTaken; // The exit is not taken at most this many times\n\n    // Not taken either exactly MaxNotTaken or zero times\n    bool MaxOrZero = false;\n\n    /// A set of predicate guards for this ExitLimit. The result is only valid\n    /// if all of the predicates in \\c Predicates evaluate to 'true' at\n    /// run-time.\n    SmallPtrSet<const SCEVPredicate *, 4> Predicates;\n\n    void addPredicate(const SCEVPredicate *P) {\n      assert(!isa<SCEVUnionPredicate>(P) && \"Only add leaf predicates here!\");\n      Predicates.insert(P);\n    }\n\n    /// Construct either an exact exit limit from a constant, or an unknown\n    /// one from a SCEVCouldNotCompute.  No other types of SCEVs are allowed\n    /// as arguments and asserts enforce that internally.\n    /*implicit*/ ExitLimit(const SCEV *E);\n\n    ExitLimit(\n        const SCEV *E, const SCEV *M, bool MaxOrZero,\n        ArrayRef<const SmallPtrSetImpl<const SCEVPredicate *> *> PredSetList);\n\n    ExitLimit(const SCEV *E, const SCEV *M, bool MaxOrZero,\n              const SmallPtrSetImpl<const SCEVPredicate *> &PredSet);\n\n    ExitLimit(const SCEV *E, const SCEV *M, bool MaxOrZero);\n\n    /// Test whether this ExitLimit contains any computed information, or\n    /// whether it's all SCEVCouldNotCompute values.\n    bool hasAnyInfo() const {\n      return !isa<SCEVCouldNotCompute>(ExactNotTaken) ||\n             !isa<SCEVCouldNotCompute>(MaxNotTaken);\n    }\n\n    bool hasOperand(const SCEV *S) const;\n\n    /// Test whether this ExitLimit contains all information.\n    bool hasFullInfo() const {\n      return !isa<SCEVCouldNotCompute>(ExactNotTaken);\n    }\n  };\n\n  /// Information about the number of times a particular loop exit may be\n  /// reached before exiting the loop.\n  struct ExitNotTakenInfo {\n    PoisoningVH<BasicBlock> ExitingBlock;\n    const SCEV *ExactNotTaken;\n    const SCEV *MaxNotTaken;\n    std::unique_ptr<SCEVUnionPredicate> Predicate;\n\n    explicit ExitNotTakenInfo(PoisoningVH<BasicBlock> ExitingBlock,\n                              const SCEV *ExactNotTaken,\n                              const SCEV *MaxNotTaken,\n                              std::unique_ptr<SCEVUnionPredicate> Predicate)\n      : ExitingBlock(ExitingBlock), ExactNotTaken(ExactNotTaken),\n        MaxNotTaken(ExactNotTaken), Predicate(std::move(Predicate)) {}\n\n    bool hasAlwaysTruePredicate() const {\n      return !Predicate || Predicate->isAlwaysTrue();\n    }\n  };\n\n  /// Information about the backedge-taken count of a loop. This currently\n  /// includes an exact count and a maximum count.\n  ///\n  class BackedgeTakenInfo {\n    /// A list of computable exits and their not-taken counts.  Loops almost\n    /// never have more than one computable exit.\n    SmallVector<ExitNotTakenInfo, 1> ExitNotTaken;\n\n    /// Expression indicating the least constant maximum backedge-taken count of\n    /// the loop that is known, or a SCEVCouldNotCompute. This expression is\n    /// only valid if the redicates associated with all loop exits are true.\n    const SCEV *ConstantMax;\n\n    /// Indicating if \\c ExitNotTaken has an element for every exiting block in\n    /// the loop.\n    bool IsComplete;\n\n    /// Expression indicating the least maximum backedge-taken count of the loop\n    /// that is known, or a SCEVCouldNotCompute. Lazily computed on first query.\n    const SCEV *SymbolicMax = nullptr;\n\n    /// True iff the backedge is taken either exactly Max or zero times.\n    bool MaxOrZero = false;\n\n    bool isComplete() const { return IsComplete; }\n    const SCEV *getConstantMax() const { return ConstantMax; }\n\n  public:\n    BackedgeTakenInfo() : ConstantMax(nullptr), IsComplete(false) {}\n    BackedgeTakenInfo(BackedgeTakenInfo &&) = default;\n    BackedgeTakenInfo &operator=(BackedgeTakenInfo &&) = default;\n\n    using EdgeExitInfo = std::pair<BasicBlock *, ExitLimit>;\n\n    /// Initialize BackedgeTakenInfo from a list of exact exit counts.\n    BackedgeTakenInfo(ArrayRef<EdgeExitInfo> ExitCounts, bool IsComplete,\n                      const SCEV *ConstantMax, bool MaxOrZero);\n\n    /// Test whether this BackedgeTakenInfo contains any computed information,\n    /// or whether it's all SCEVCouldNotCompute values.\n    bool hasAnyInfo() const {\n      return !ExitNotTaken.empty() ||\n             !isa<SCEVCouldNotCompute>(getConstantMax());\n    }\n\n    /// Test whether this BackedgeTakenInfo contains complete information.\n    bool hasFullInfo() const { return isComplete(); }\n\n    /// Return an expression indicating the exact *backedge-taken*\n    /// count of the loop if it is known or SCEVCouldNotCompute\n    /// otherwise.  If execution makes it to the backedge on every\n    /// iteration (i.e. there are no abnormal exists like exception\n    /// throws and thread exits) then this is the number of times the\n    /// loop header will execute minus one.\n    ///\n    /// If the SCEV predicate associated with the answer can be different\n    /// from AlwaysTrue, we must add a (non null) Predicates argument.\n    /// The SCEV predicate associated with the answer will be added to\n    /// Predicates. A run-time check needs to be emitted for the SCEV\n    /// predicate in order for the answer to be valid.\n    ///\n    /// Note that we should always know if we need to pass a predicate\n    /// argument or not from the way the ExitCounts vector was computed.\n    /// If we allowed SCEV predicates to be generated when populating this\n    /// vector, this information can contain them and therefore a\n    /// SCEVPredicate argument should be added to getExact.\n    const SCEV *getExact(const Loop *L, ScalarEvolution *SE,\n                         SCEVUnionPredicate *Predicates = nullptr) const;\n\n    /// Return the number of times this loop exit may fall through to the back\n    /// edge, or SCEVCouldNotCompute. The loop is guaranteed not to exit via\n    /// this block before this number of iterations, but may exit via another\n    /// block.\n    const SCEV *getExact(const BasicBlock *ExitingBlock,\n                         ScalarEvolution *SE) const;\n\n    /// Get the constant max backedge taken count for the loop.\n    const SCEV *getConstantMax(ScalarEvolution *SE) const;\n\n    /// Get the constant max backedge taken count for the particular loop exit.\n    const SCEV *getConstantMax(const BasicBlock *ExitingBlock,\n                               ScalarEvolution *SE) const;\n\n    /// Get the symbolic max backedge taken count for the loop.\n    const SCEV *getSymbolicMax(const Loop *L, ScalarEvolution *SE);\n\n    /// Return true if the number of times this backedge is taken is either the\n    /// value returned by getConstantMax or zero.\n    bool isConstantMaxOrZero(ScalarEvolution *SE) const;\n\n    /// Return true if any backedge taken count expressions refer to the given\n    /// subexpression.\n    bool hasOperand(const SCEV *S, ScalarEvolution *SE) const;\n\n    /// Invalidate this result and free associated memory.\n    void clear();\n  };\n\n  /// Cache the backedge-taken count of the loops for this function as they\n  /// are computed.\n  DenseMap<const Loop *, BackedgeTakenInfo> BackedgeTakenCounts;\n\n  /// Cache the predicated backedge-taken count of the loops for this\n  /// function as they are computed.\n  DenseMap<const Loop *, BackedgeTakenInfo> PredicatedBackedgeTakenCounts;\n\n  /// This map contains entries for all of the PHI instructions that we\n  /// attempt to compute constant evolutions for.  This allows us to avoid\n  /// potentially expensive recomputation of these properties.  An instruction\n  /// maps to null if we are unable to compute its exit value.\n  DenseMap<PHINode *, Constant *> ConstantEvolutionLoopExitValue;\n\n  /// This map contains entries for all the expressions that we attempt to\n  /// compute getSCEVAtScope information for, which can be expensive in\n  /// extreme cases.\n  DenseMap<const SCEV *, SmallVector<std::pair<const Loop *, const SCEV *>, 2>>\n      ValuesAtScopes;\n\n  /// Memoized computeLoopDisposition results.\n  DenseMap<const SCEV *,\n           SmallVector<PointerIntPair<const Loop *, 2, LoopDisposition>, 2>>\n      LoopDispositions;\n\n  struct LoopProperties {\n    /// Set to true if the loop contains no instruction that can have side\n    /// effects (i.e. via throwing an exception, volatile or atomic access).\n    bool HasNoAbnormalExits;\n\n    /// Set to true if the loop contains no instruction that can abnormally exit\n    /// the loop (i.e. via throwing an exception, by terminating the thread\n    /// cleanly or by infinite looping in a called function).  Strictly\n    /// speaking, the last one is not leaving the loop, but is identical to\n    /// leaving the loop for reasoning about undefined behavior.\n    bool HasNoSideEffects;\n  };\n\n  /// Cache for \\c getLoopProperties.\n  DenseMap<const Loop *, LoopProperties> LoopPropertiesCache;\n\n  /// Return a \\c LoopProperties instance for \\p L, creating one if necessary.\n  LoopProperties getLoopProperties(const Loop *L);\n\n  bool loopHasNoSideEffects(const Loop *L) {\n    return getLoopProperties(L).HasNoSideEffects;\n  }\n\n  bool loopHasNoAbnormalExits(const Loop *L) {\n    return getLoopProperties(L).HasNoAbnormalExits;\n  }\n\n  /// Compute a LoopDisposition value.\n  LoopDisposition computeLoopDisposition(const SCEV *S, const Loop *L);\n\n  /// Memoized computeBlockDisposition results.\n  DenseMap<\n      const SCEV *,\n      SmallVector<PointerIntPair<const BasicBlock *, 2, BlockDisposition>, 2>>\n      BlockDispositions;\n\n  /// Compute a BlockDisposition value.\n  BlockDisposition computeBlockDisposition(const SCEV *S, const BasicBlock *BB);\n\n  /// Memoized results from getRange\n  DenseMap<const SCEV *, ConstantRange> UnsignedRanges;\n\n  /// Memoized results from getRange\n  DenseMap<const SCEV *, ConstantRange> SignedRanges;\n\n  /// Used to parameterize getRange\n  enum RangeSignHint { HINT_RANGE_UNSIGNED, HINT_RANGE_SIGNED };\n\n  /// Set the memoized range for the given SCEV.\n  const ConstantRange &setRange(const SCEV *S, RangeSignHint Hint,\n                                ConstantRange CR) {\n    DenseMap<const SCEV *, ConstantRange> &Cache =\n        Hint == HINT_RANGE_UNSIGNED ? UnsignedRanges : SignedRanges;\n\n    auto Pair = Cache.try_emplace(S, std::move(CR));\n    if (!Pair.second)\n      Pair.first->second = std::move(CR);\n    return Pair.first->second;\n  }\n\n  /// Determine the range for a particular SCEV.\n  /// NOTE: This returns a reference to an entry in a cache. It must be\n  /// copied if its needed for longer.\n  const ConstantRange &getRangeRef(const SCEV *S, RangeSignHint Hint);\n\n  /// Determines the range for the affine SCEVAddRecExpr {\\p Start,+,\\p Stop}.\n  /// Helper for \\c getRange.\n  ConstantRange getRangeForAffineAR(const SCEV *Start, const SCEV *Stop,\n                                    const SCEV *MaxBECount, unsigned BitWidth);\n\n  /// Determines the range for the affine non-self-wrapping SCEVAddRecExpr {\\p\n  /// Start,+,\\p Stop}<nw>.\n  ConstantRange getRangeForAffineNoSelfWrappingAR(const SCEVAddRecExpr *AddRec,\n                                                  const SCEV *MaxBECount,\n                                                  unsigned BitWidth,\n                                                  RangeSignHint SignHint);\n\n  /// Try to compute a range for the affine SCEVAddRecExpr {\\p Start,+,\\p\n  /// Stop} by \"factoring out\" a ternary expression from the add recurrence.\n  /// Helper called by \\c getRange.\n  ConstantRange getRangeViaFactoring(const SCEV *Start, const SCEV *Stop,\n                                     const SCEV *MaxBECount, unsigned BitWidth);\n\n  /// We know that there is no SCEV for the specified value.  Analyze the\n  /// expression.\n  const SCEV *createSCEV(Value *V);\n\n  /// Provide the special handling we need to analyze PHI SCEVs.\n  const SCEV *createNodeForPHI(PHINode *PN);\n\n  /// Helper function called from createNodeForPHI.\n  const SCEV *createAddRecFromPHI(PHINode *PN);\n\n  /// A helper function for createAddRecFromPHI to handle simple cases.\n  const SCEV *createSimpleAffineAddRec(PHINode *PN, Value *BEValueV,\n                                            Value *StartValueV);\n\n  /// Helper function called from createNodeForPHI.\n  const SCEV *createNodeFromSelectLikePHI(PHINode *PN);\n\n  /// Provide special handling for a select-like instruction (currently this\n  /// is either a select instruction or a phi node).  \\p I is the instruction\n  /// being processed, and it is assumed equivalent to \"Cond ? TrueVal :\n  /// FalseVal\".\n  const SCEV *createNodeForSelectOrPHI(Instruction *I, Value *Cond,\n                                       Value *TrueVal, Value *FalseVal);\n\n  /// Provide the special handling we need to analyze GEP SCEVs.\n  const SCEV *createNodeForGEP(GEPOperator *GEP);\n\n  /// Implementation code for getSCEVAtScope; called at most once for each\n  /// SCEV+Loop pair.\n  const SCEV *computeSCEVAtScope(const SCEV *S, const Loop *L);\n\n  /// This looks up computed SCEV values for all instructions that depend on\n  /// the given instruction and removes them from the ValueExprMap map if they\n  /// reference SymName. This is used during PHI resolution.\n  void forgetSymbolicName(Instruction *I, const SCEV *SymName);\n\n  /// Return the BackedgeTakenInfo for the given loop, lazily computing new\n  /// values if the loop hasn't been analyzed yet. The returned result is\n  /// guaranteed not to be predicated.\n  BackedgeTakenInfo &getBackedgeTakenInfo(const Loop *L);\n\n  /// Similar to getBackedgeTakenInfo, but will add predicates as required\n  /// with the purpose of returning complete information.\n  const BackedgeTakenInfo &getPredicatedBackedgeTakenInfo(const Loop *L);\n\n  /// Compute the number of times the specified loop will iterate.\n  /// If AllowPredicates is set, we will create new SCEV predicates as\n  /// necessary in order to return an exact answer.\n  BackedgeTakenInfo computeBackedgeTakenCount(const Loop *L,\n                                              bool AllowPredicates = false);\n\n  /// Compute the number of times the backedge of the specified loop will\n  /// execute if it exits via the specified block. If AllowPredicates is set,\n  /// this call will try to use a minimal set of SCEV predicates in order to\n  /// return an exact answer.\n  ExitLimit computeExitLimit(const Loop *L, BasicBlock *ExitingBlock,\n                             bool AllowPredicates = false);\n\n  /// Compute the number of times the backedge of the specified loop will\n  /// execute if its exit condition were a conditional branch of ExitCond.\n  ///\n  /// \\p ControlsExit is true if ExitCond directly controls the exit\n  /// branch. In this case, we can assume that the loop exits only if the\n  /// condition is true and can infer that failing to meet the condition prior\n  /// to integer wraparound results in undefined behavior.\n  ///\n  /// If \\p AllowPredicates is set, this call will try to use a minimal set of\n  /// SCEV predicates in order to return an exact answer.\n  ExitLimit computeExitLimitFromCond(const Loop *L, Value *ExitCond,\n                                     bool ExitIfTrue, bool ControlsExit,\n                                     bool AllowPredicates = false);\n\n  /// Return a symbolic upper bound for the backedge taken count of the loop.\n  /// This is more general than getConstantMaxBackedgeTakenCount as it returns\n  /// an arbitrary expression as opposed to only constants.\n  const SCEV *computeSymbolicMaxBackedgeTakenCount(const Loop *L);\n\n  // Helper functions for computeExitLimitFromCond to avoid exponential time\n  // complexity.\n\n  class ExitLimitCache {\n    // It may look like we need key on the whole (L, ExitIfTrue, ControlsExit,\n    // AllowPredicates) tuple, but recursive calls to\n    // computeExitLimitFromCondCached from computeExitLimitFromCondImpl only\n    // vary the in \\c ExitCond and \\c ControlsExit parameters.  We remember the\n    // initial values of the other values to assert our assumption.\n    SmallDenseMap<PointerIntPair<Value *, 1>, ExitLimit> TripCountMap;\n\n    const Loop *L;\n    bool ExitIfTrue;\n    bool AllowPredicates;\n\n  public:\n    ExitLimitCache(const Loop *L, bool ExitIfTrue, bool AllowPredicates)\n        : L(L), ExitIfTrue(ExitIfTrue), AllowPredicates(AllowPredicates) {}\n\n    Optional<ExitLimit> find(const Loop *L, Value *ExitCond, bool ExitIfTrue,\n                             bool ControlsExit, bool AllowPredicates);\n\n    void insert(const Loop *L, Value *ExitCond, bool ExitIfTrue,\n                bool ControlsExit, bool AllowPredicates, const ExitLimit &EL);\n  };\n\n  using ExitLimitCacheTy = ExitLimitCache;\n\n  ExitLimit computeExitLimitFromCondCached(ExitLimitCacheTy &Cache,\n                                           const Loop *L, Value *ExitCond,\n                                           bool ExitIfTrue,\n                                           bool ControlsExit,\n                                           bool AllowPredicates);\n  ExitLimit computeExitLimitFromCondImpl(ExitLimitCacheTy &Cache, const Loop *L,\n                                         Value *ExitCond, bool ExitIfTrue,\n                                         bool ControlsExit,\n                                         bool AllowPredicates);\n  Optional<ScalarEvolution::ExitLimit>\n  computeExitLimitFromCondFromBinOp(ExitLimitCacheTy &Cache, const Loop *L,\n                                    Value *ExitCond, bool ExitIfTrue,\n                                    bool ControlsExit, bool AllowPredicates);\n\n  /// Compute the number of times the backedge of the specified loop will\n  /// execute if its exit condition were a conditional branch of the ICmpInst\n  /// ExitCond and ExitIfTrue. If AllowPredicates is set, this call will try\n  /// to use a minimal set of SCEV predicates in order to return an exact\n  /// answer.\n  ExitLimit computeExitLimitFromICmp(const Loop *L, ICmpInst *ExitCond,\n                                     bool ExitIfTrue,\n                                     bool IsSubExpr,\n                                     bool AllowPredicates = false);\n\n  /// Compute the number of times the backedge of the specified loop will\n  /// execute if its exit condition were a switch with a single exiting case\n  /// to ExitingBB.\n  ExitLimit computeExitLimitFromSingleExitSwitch(const Loop *L,\n                                                 SwitchInst *Switch,\n                                                 BasicBlock *ExitingBB,\n                                                 bool IsSubExpr);\n\n  /// Given an exit condition of 'icmp op load X, cst', try to see if we can\n  /// compute the backedge-taken count.\n  ExitLimit computeLoadConstantCompareExitLimit(LoadInst *LI, Constant *RHS,\n                                                const Loop *L,\n                                                ICmpInst::Predicate p);\n\n  /// Compute the exit limit of a loop that is controlled by a\n  /// \"(IV >> 1) != 0\" type comparison.  We cannot compute the exact trip\n  /// count in these cases (since SCEV has no way of expressing them), but we\n  /// can still sometimes compute an upper bound.\n  ///\n  /// Return an ExitLimit for a loop whose backedge is guarded by `LHS Pred\n  /// RHS`.\n  ExitLimit computeShiftCompareExitLimit(Value *LHS, Value *RHS, const Loop *L,\n                                         ICmpInst::Predicate Pred);\n\n  /// If the loop is known to execute a constant number of times (the\n  /// condition evolves only from constants), try to evaluate a few iterations\n  /// of the loop until we get the exit condition gets a value of ExitWhen\n  /// (true or false).  If we cannot evaluate the exit count of the loop,\n  /// return CouldNotCompute.\n  const SCEV *computeExitCountExhaustively(const Loop *L, Value *Cond,\n                                           bool ExitWhen);\n\n  /// Return the number of times an exit condition comparing the specified\n  /// value to zero will execute.  If not computable, return CouldNotCompute.\n  /// If AllowPredicates is set, this call will try to use a minimal set of\n  /// SCEV predicates in order to return an exact answer.\n  ExitLimit howFarToZero(const SCEV *V, const Loop *L, bool IsSubExpr,\n                         bool AllowPredicates = false);\n\n  /// Return the number of times an exit condition checking the specified\n  /// value for nonzero will execute.  If not computable, return\n  /// CouldNotCompute.\n  ExitLimit howFarToNonZero(const SCEV *V, const Loop *L);\n\n  /// Return the number of times an exit condition containing the specified\n  /// less-than comparison will execute.  If not computable, return\n  /// CouldNotCompute.\n  ///\n  /// \\p isSigned specifies whether the less-than is signed.\n  ///\n  /// \\p ControlsExit is true when the LHS < RHS condition directly controls\n  /// the branch (loops exits only if condition is true). In this case, we can\n  /// use NoWrapFlags to skip overflow checks.\n  ///\n  /// If \\p AllowPredicates is set, this call will try to use a minimal set of\n  /// SCEV predicates in order to return an exact answer.\n  ExitLimit howManyLessThans(const SCEV *LHS, const SCEV *RHS, const Loop *L,\n                             bool isSigned, bool ControlsExit,\n                             bool AllowPredicates = false);\n\n  ExitLimit howManyGreaterThans(const SCEV *LHS, const SCEV *RHS, const Loop *L,\n                                bool isSigned, bool IsSubExpr,\n                                bool AllowPredicates = false);\n\n  /// Return a predecessor of BB (which may not be an immediate predecessor)\n  /// which has exactly one successor from which BB is reachable, or null if\n  /// no such block is found.\n  std::pair<const BasicBlock *, const BasicBlock *>\n  getPredecessorWithUniqueSuccessorForBB(const BasicBlock *BB) const;\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the given FoundCondValue value evaluates to true in given\n  /// Context. If Context is nullptr, then the found predicate is true\n  /// everywhere. LHS and FoundLHS may have different type width.\n  bool isImpliedCond(ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,\n                     const Value *FoundCondValue, bool Inverse,\n                     const Instruction *Context = nullptr);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the given FoundCondValue value evaluates to true in given\n  /// Context. If Context is nullptr, then the found predicate is true\n  /// everywhere. LHS and FoundLHS must have same type width.\n  bool isImpliedCondBalancedTypes(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                  const SCEV *RHS,\n                                  ICmpInst::Predicate FoundPred,\n                                  const SCEV *FoundLHS, const SCEV *FoundRHS,\n                                  const Instruction *Context);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by FoundPred, FoundLHS, FoundRHS is\n  /// true in given Context. If Context is nullptr, then the found predicate is\n  /// true everywhere.\n  bool isImpliedCond(ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,\n                     ICmpInst::Predicate FoundPred, const SCEV *FoundLHS,\n                     const SCEV *FoundRHS,\n                     const Instruction *Context = nullptr);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true in given Context. If Context is nullptr, then the found predicate is\n  /// true everywhere.\n  bool isImpliedCondOperands(ICmpInst::Predicate Pred, const SCEV *LHS,\n                             const SCEV *RHS, const SCEV *FoundLHS,\n                             const SCEV *FoundRHS,\n                             const Instruction *Context = nullptr);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true. Here LHS is an operation that includes FoundLHS as one of its\n  /// arguments.\n  bool isImpliedViaOperations(ICmpInst::Predicate Pred,\n                              const SCEV *LHS, const SCEV *RHS,\n                              const SCEV *FoundLHS, const SCEV *FoundRHS,\n                              unsigned Depth = 0);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true.\n  /// Use only simple non-recursive types of checks, such as range analysis etc.\n  bool isKnownViaNonRecursiveReasoning(ICmpInst::Predicate Pred,\n                                       const SCEV *LHS, const SCEV *RHS);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true.\n  bool isImpliedCondOperandsHelper(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                   const SCEV *RHS, const SCEV *FoundLHS,\n                                   const SCEV *FoundRHS);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true.  Utility function used by isImpliedCondOperands.  Tries to get\n  /// cases like \"X `sgt` 0 => X - 1 `sgt` -1\".\n  bool isImpliedCondOperandsViaRanges(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                      const SCEV *RHS, const SCEV *FoundLHS,\n                                      const SCEV *FoundRHS);\n\n  /// Return true if the condition denoted by \\p LHS \\p Pred \\p RHS is implied\n  /// by a call to @llvm.experimental.guard in \\p BB.\n  bool isImpliedViaGuard(const BasicBlock *BB, ICmpInst::Predicate Pred,\n                         const SCEV *LHS, const SCEV *RHS);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true.\n  ///\n  /// This routine tries to rule out certain kinds of integer overflow, and\n  /// then tries to reason about arithmetic properties of the predicates.\n  bool isImpliedCondOperandsViaNoOverflow(ICmpInst::Predicate Pred,\n                                          const SCEV *LHS, const SCEV *RHS,\n                                          const SCEV *FoundLHS,\n                                          const SCEV *FoundRHS);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true.\n  ///\n  /// This routine tries to weaken the known condition basing on fact that\n  /// FoundLHS is an AddRec.\n  bool isImpliedCondOperandsViaAddRecStart(ICmpInst::Predicate Pred,\n                                           const SCEV *LHS, const SCEV *RHS,\n                                           const SCEV *FoundLHS,\n                                           const SCEV *FoundRHS,\n                                           const Instruction *Context);\n\n  /// Test whether the condition described by Pred, LHS, and RHS is true\n  /// whenever the condition described by Pred, FoundLHS, and FoundRHS is\n  /// true.\n  ///\n  /// This routine tries to figure out predicate for Phis which are SCEVUnknown\n  /// if it is true for every possible incoming value from their respective\n  /// basic blocks.\n  bool isImpliedViaMerge(ICmpInst::Predicate Pred,\n                         const SCEV *LHS, const SCEV *RHS,\n                         const SCEV *FoundLHS, const SCEV *FoundRHS,\n                         unsigned Depth);\n\n  /// If we know that the specified Phi is in the header of its containing\n  /// loop, we know the loop executes a constant number of times, and the PHI\n  /// node is just a recurrence involving constants, fold it.\n  Constant *getConstantEvolutionLoopExitValue(PHINode *PN, const APInt &BEs,\n                                              const Loop *L);\n\n  /// Test if the given expression is known to satisfy the condition described\n  /// by Pred and the known constant ranges of LHS and RHS.\n  bool isKnownPredicateViaConstantRanges(ICmpInst::Predicate Pred,\n                                         const SCEV *LHS, const SCEV *RHS);\n\n  /// Try to prove the condition described by \"LHS Pred RHS\" by ruling out\n  /// integer overflow.\n  ///\n  /// For instance, this will return true for \"A s< (A + C)<nsw>\" if C is\n  /// positive.\n  bool isKnownPredicateViaNoOverflow(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                     const SCEV *RHS);\n\n  /// Try to split Pred LHS RHS into logical conjunctions (and's) and try to\n  /// prove them individually.\n  bool isKnownPredicateViaSplitting(ICmpInst::Predicate Pred, const SCEV *LHS,\n                                    const SCEV *RHS);\n\n  /// Try to match the Expr as \"(L + R)<Flags>\".\n  bool splitBinaryAdd(const SCEV *Expr, const SCEV *&L, const SCEV *&R,\n                      SCEV::NoWrapFlags &Flags);\n\n  /// Drop memoized information computed for S.\n  void forgetMemoizedResults(const SCEV *S);\n\n  /// Return an existing SCEV for V if there is one, otherwise return nullptr.\n  const SCEV *getExistingSCEV(Value *V);\n\n  /// Return false iff given SCEV contains a SCEVUnknown with NULL value-\n  /// pointer.\n  bool checkValidity(const SCEV *S) const;\n\n  /// Return true if `ExtendOpTy`({`Start`,+,`Step`}) can be proved to be\n  /// equal to {`ExtendOpTy`(`Start`),+,`ExtendOpTy`(`Step`)}.  This is\n  /// equivalent to proving no signed (resp. unsigned) wrap in\n  /// {`Start`,+,`Step`} if `ExtendOpTy` is `SCEVSignExtendExpr`\n  /// (resp. `SCEVZeroExtendExpr`).\n  template <typename ExtendOpTy>\n  bool proveNoWrapByVaryingStart(const SCEV *Start, const SCEV *Step,\n                                 const Loop *L);\n\n  /// Try to prove NSW or NUW on \\p AR relying on ConstantRange manipulation.\n  SCEV::NoWrapFlags proveNoWrapViaConstantRanges(const SCEVAddRecExpr *AR);\n\n  /// Try to prove NSW on \\p AR by proving facts about conditions known  on\n  /// entry and backedge.\n  SCEV::NoWrapFlags proveNoSignedWrapViaInduction(const SCEVAddRecExpr *AR);\n\n  /// Try to prove NUW on \\p AR by proving facts about conditions known on\n  /// entry and backedge.\n  SCEV::NoWrapFlags proveNoUnsignedWrapViaInduction(const SCEVAddRecExpr *AR);\n\n  Optional<MonotonicPredicateType>\n  getMonotonicPredicateTypeImpl(const SCEVAddRecExpr *LHS,\n                                ICmpInst::Predicate Pred);\n\n  /// Return SCEV no-wrap flags that can be proven based on reasoning about\n  /// how poison produced from no-wrap flags on this value (e.g. a nuw add)\n  /// would trigger undefined behavior on overflow.\n  SCEV::NoWrapFlags getNoWrapFlagsFromUB(const Value *V);\n\n  /// Return true if the SCEV corresponding to \\p I is never poison.  Proving\n  /// this is more complex than proving that just \\p I is never poison, since\n  /// SCEV commons expressions across control flow, and you can have cases\n  /// like:\n  ///\n  ///   idx0 = a + b;\n  ///   ptr[idx0] = 100;\n  ///   if (<condition>) {\n  ///     idx1 = a +nsw b;\n  ///     ptr[idx1] = 200;\n  ///   }\n  ///\n  /// where the SCEV expression (+ a b) is guaranteed to not be poison (and\n  /// hence not sign-overflow) only if \"<condition>\" is true.  Since both\n  /// `idx0` and `idx1` will be mapped to the same SCEV expression, (+ a b),\n  /// it is not okay to annotate (+ a b) with <nsw> in the above example.\n  bool isSCEVExprNeverPoison(const Instruction *I);\n\n  /// This is like \\c isSCEVExprNeverPoison but it specifically works for\n  /// instructions that will get mapped to SCEV add recurrences.  Return true\n  /// if \\p I will never generate poison under the assumption that \\p I is an\n  /// add recurrence on the loop \\p L.\n  bool isAddRecNeverPoison(const Instruction *I, const Loop *L);\n\n  /// Similar to createAddRecFromPHI, but with the additional flexibility of\n  /// suggesting runtime overflow checks in case casts are encountered.\n  /// If successful, the analysis records that for this loop, \\p SymbolicPHI,\n  /// which is the UnknownSCEV currently representing the PHI, can be rewritten\n  /// into an AddRec, assuming some predicates; The function then returns the\n  /// AddRec and the predicates as a pair, and caches this pair in\n  /// PredicatedSCEVRewrites.\n  /// If the analysis is not successful, a mapping from the \\p SymbolicPHI to\n  /// itself (with no predicates) is recorded, and a nullptr with an empty\n  /// predicates vector is returned as a pair.\n  Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\n  createAddRecFromPHIWithCastsImpl(const SCEVUnknown *SymbolicPHI);\n\n  /// Compute the backedge taken count knowing the interval difference, the\n  /// stride and presence of the equality in the comparison.\n  const SCEV *computeBECount(const SCEV *Delta, const SCEV *Stride,\n                             bool Equality);\n\n  /// Compute the maximum backedge count based on the range of values\n  /// permitted by Start, End, and Stride. This is for loops of the form\n  /// {Start, +, Stride} LT End.\n  ///\n  /// Precondition: the induction variable is known to be positive.  We *don't*\n  /// assert these preconditions so please be careful.\n  const SCEV *computeMaxBECountForLT(const SCEV *Start, const SCEV *Stride,\n                                     const SCEV *End, unsigned BitWidth,\n                                     bool IsSigned);\n\n  /// Verify if an linear IV with positive stride can overflow when in a\n  /// less-than comparison, knowing the invariant term of the comparison,\n  /// the stride and the knowledge of NSW/NUW flags on the recurrence.\n  bool doesIVOverflowOnLT(const SCEV *RHS, const SCEV *Stride, bool IsSigned,\n                          bool NoWrap);\n\n  /// Verify if an linear IV with negative stride can overflow when in a\n  /// greater-than comparison, knowing the invariant term of the comparison,\n  /// the stride and the knowledge of NSW/NUW flags on the recurrence.\n  bool doesIVOverflowOnGT(const SCEV *RHS, const SCEV *Stride, bool IsSigned,\n                          bool NoWrap);\n\n  /// Get add expr already created or create a new one.\n  const SCEV *getOrCreateAddExpr(ArrayRef<const SCEV *> Ops,\n                                 SCEV::NoWrapFlags Flags);\n\n  /// Get mul expr already created or create a new one.\n  const SCEV *getOrCreateMulExpr(ArrayRef<const SCEV *> Ops,\n                                 SCEV::NoWrapFlags Flags);\n\n  // Get addrec expr already created or create a new one.\n  const SCEV *getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops,\n                                    const Loop *L, SCEV::NoWrapFlags Flags);\n\n  /// Return x if \\p Val is f(x) where f is a 1-1 function.\n  const SCEV *stripInjectiveFunctions(const SCEV *Val) const;\n\n  /// Find all of the loops transitively used in \\p S, and fill \\p LoopsUsed.\n  /// A loop is considered \"used\" by an expression if it contains\n  /// an add rec on said loop.\n  void getUsedLoops(const SCEV *S, SmallPtrSetImpl<const Loop *> &LoopsUsed);\n\n  /// Find all of the loops transitively used in \\p S, and update \\c LoopUsers\n  /// accordingly.\n  void addToLoopUseLists(const SCEV *S);\n\n  /// Try to match the pattern generated by getURemExpr(A, B). If successful,\n  /// Assign A and B to LHS and RHS, respectively.\n  bool matchURem(const SCEV *Expr, const SCEV *&LHS, const SCEV *&RHS);\n\n  /// Look for a SCEV expression with type `SCEVType` and operands `Ops` in\n  /// `UniqueSCEVs`.\n  ///\n  /// The first component of the returned tuple is the SCEV if found and null\n  /// otherwise.  The second component is the `FoldingSetNodeID` that was\n  /// constructed to look up the SCEV and the third component is the insertion\n  /// point.\n  std::tuple<SCEV *, FoldingSetNodeID, void *>\n  findExistingSCEVInCache(SCEVTypes SCEVType, ArrayRef<const SCEV *> Ops);\n\n  FoldingSet<SCEV> UniqueSCEVs;\n  FoldingSet<SCEVPredicate> UniquePreds;\n  BumpPtrAllocator SCEVAllocator;\n\n  /// This maps loops to a list of SCEV expressions that (transitively) use said\n  /// loop.\n  DenseMap<const Loop *, SmallVector<const SCEV *, 4>> LoopUsers;\n\n  /// Cache tentative mappings from UnknownSCEVs in a Loop, to a SCEV expression\n  /// they can be rewritten into under certain predicates.\n  DenseMap<std::pair<const SCEVUnknown *, const Loop *>,\n           std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>\n      PredicatedSCEVRewrites;\n\n  /// The head of a linked list of all SCEVUnknown values that have been\n  /// allocated. This is used by releaseMemory to locate them all and call\n  /// their destructors.\n  SCEVUnknown *FirstUnknown = nullptr;\n};\n\n/// Analysis pass that exposes the \\c ScalarEvolution for a function.\nclass ScalarEvolutionAnalysis\n    : public AnalysisInfoMixin<ScalarEvolutionAnalysis> {\n  friend AnalysisInfoMixin<ScalarEvolutionAnalysis>;\n\n  static AnalysisKey Key;\n\npublic:\n  using Result = ScalarEvolution;\n\n  ScalarEvolution run(Function &F, FunctionAnalysisManager &AM);\n};\n\n/// Verifier pass for the \\c ScalarEvolutionAnalysis results.\nclass ScalarEvolutionVerifierPass\n    : public PassInfoMixin<ScalarEvolutionVerifierPass> {\npublic:\n  PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);\n};\n\n/// Printer pass for the \\c ScalarEvolutionAnalysis results.\nclass ScalarEvolutionPrinterPass\n    : public PassInfoMixin<ScalarEvolutionPrinterPass> {\n  raw_ostream &OS;\n\npublic:\n  explicit ScalarEvolutionPrinterPass(raw_ostream &OS) : OS(OS) {}\n\n  PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);\n};\n\nclass ScalarEvolutionWrapperPass : public FunctionPass {\n  std::unique_ptr<ScalarEvolution> SE;\n\npublic:\n  static char ID;\n\n  ScalarEvolutionWrapperPass();\n\n  ScalarEvolution &getSE() { return *SE; }\n  const ScalarEvolution &getSE() const { return *SE; }\n\n  bool runOnFunction(Function &F) override;\n  void releaseMemory() override;\n  void getAnalysisUsage(AnalysisUsage &AU) const override;\n  void print(raw_ostream &OS, const Module * = nullptr) const override;\n  void verifyAnalysis() const override;\n};\n\n/// An interface layer with SCEV used to manage how we see SCEV expressions\n/// for values in the context of existing predicates. We can add new\n/// predicates, but we cannot remove them.\n///\n/// This layer has multiple purposes:\n///   - provides a simple interface for SCEV versioning.\n///   - guarantees that the order of transformations applied on a SCEV\n///     expression for a single Value is consistent across two different\n///     getSCEV calls. This means that, for example, once we've obtained\n///     an AddRec expression for a certain value through expression\n///     rewriting, we will continue to get an AddRec expression for that\n///     Value.\n///   - lowers the number of expression rewrites.\nclass PredicatedScalarEvolution {\npublic:\n  PredicatedScalarEvolution(ScalarEvolution &SE, Loop &L);\n\n  const SCEVUnionPredicate &getUnionPredicate() const;\n\n  /// Returns the SCEV expression of V, in the context of the current SCEV\n  /// predicate.  The order of transformations applied on the expression of V\n  /// returned by ScalarEvolution is guaranteed to be preserved, even when\n  /// adding new predicates.\n  const SCEV *getSCEV(Value *V);\n\n  /// Get the (predicated) backedge count for the analyzed loop.\n  const SCEV *getBackedgeTakenCount();\n\n  /// Adds a new predicate.\n  void addPredicate(const SCEVPredicate &Pred);\n\n  /// Attempts to produce an AddRecExpr for V by adding additional SCEV\n  /// predicates. If we can't transform the expression into an AddRecExpr we\n  /// return nullptr and not add additional SCEV predicates to the current\n  /// context.\n  const SCEVAddRecExpr *getAsAddRec(Value *V);\n\n  /// Proves that V doesn't overflow by adding SCEV predicate.\n  void setNoOverflow(Value *V, SCEVWrapPredicate::IncrementWrapFlags Flags);\n\n  /// Returns true if we've proved that V doesn't wrap by means of a SCEV\n  /// predicate.\n  bool hasNoOverflow(Value *V, SCEVWrapPredicate::IncrementWrapFlags Flags);\n\n  /// Returns the ScalarEvolution analysis used.\n  ScalarEvolution *getSE() const { return &SE; }\n\n  /// We need to explicitly define the copy constructor because of FlagsMap.\n  PredicatedScalarEvolution(const PredicatedScalarEvolution &);\n\n  /// Print the SCEV mappings done by the Predicated Scalar Evolution.\n  /// The printed text is indented by \\p Depth.\n  void print(raw_ostream &OS, unsigned Depth) const;\n\n  /// Check if \\p AR1 and \\p AR2 are equal, while taking into account\n  /// Equal predicates in Preds.\n  bool areAddRecsEqualWithPreds(const SCEVAddRecExpr *AR1,\n                                const SCEVAddRecExpr *AR2) const;\n\nprivate:\n  /// Increments the version number of the predicate.  This needs to be called\n  /// every time the SCEV predicate changes.\n  void updateGeneration();\n\n  /// Holds a SCEV and the version number of the SCEV predicate used to\n  /// perform the rewrite of the expression.\n  using RewriteEntry = std::pair<unsigned, const SCEV *>;\n\n  /// Maps a SCEV to the rewrite result of that SCEV at a certain version\n  /// number. If this number doesn't match the current Generation, we will\n  /// need to do a rewrite. To preserve the transformation order of previous\n  /// rewrites, we will rewrite the previous result instead of the original\n  /// SCEV.\n  DenseMap<const SCEV *, RewriteEntry> RewriteMap;\n\n  /// Records what NoWrap flags we've added to a Value *.\n  ValueMap<Value *, SCEVWrapPredicate::IncrementWrapFlags> FlagsMap;\n\n  /// The ScalarEvolution analysis.\n  ScalarEvolution &SE;\n\n  /// The analyzed Loop.\n  const Loop &L;\n\n  /// The SCEVPredicate that forms our context. We will rewrite all\n  /// expressions assuming that this predicate true.\n  SCEVUnionPredicate Preds;\n\n  /// Marks the version of the SCEV predicate used. When rewriting a SCEV\n  /// expression we mark it with the version of the predicate. We use this to\n  /// figure out if the predicate has changed from the last rewrite of the\n  /// SCEV. If so, we need to perform a new rewrite.\n  unsigned Generation = 0;\n\n  /// The backedge taken count.\n  const SCEV *BackedgeCount = nullptr;\n};\n\n} // end namespace llvm\n\n#endif // LLVM_ANALYSIS_SCALAREVOLUTION_H\n"}, "3": {"id": 3, "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolutionExpressions.h", "content": "//===- llvm/Analysis/ScalarEvolutionExpressions.h - SCEV Exprs --*- C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This file defines the classes used to represent and build scalar expressions.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef LLVM_ANALYSIS_SCALAREVOLUTIONEXPRESSIONS_H\n#define LLVM_ANALYSIS_SCALAREVOLUTIONEXPRESSIONS_H\n\n#include \"llvm/ADT/DenseMap.h\"\n#include \"llvm/ADT/FoldingSet.h\"\n#include \"llvm/ADT/SmallPtrSet.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/ADT/iterator_range.h\"\n#include \"llvm/Analysis/ScalarEvolution.h\"\n#include \"llvm/IR/Constants.h\"\n#include \"llvm/IR/Value.h\"\n#include \"llvm/IR/ValueHandle.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/ErrorHandling.h\"\n#include <cassert>\n#include <cstddef>\n\nnamespace llvm {\n\nclass APInt;\nclass Constant;\nclass ConstantRange;\nclass Loop;\nclass Type;\n\n  enum SCEVTypes : unsigned short {\n    // These should be ordered in terms of increasing complexity to make the\n    // folders simpler.\n    scConstant, scTruncate, scZeroExtend, scSignExtend, scAddExpr, scMulExpr,\n    scUDivExpr, scAddRecExpr, scUMaxExpr, scSMaxExpr, scUMinExpr, scSMinExpr,\n    scPtrToInt, scUnknown, scCouldNotCompute\n  };\n\n  /// This class represents a constant integer value.\n  class SCEVConstant : public SCEV {\n    friend class ScalarEvolution;\n\n    ConstantInt *V;\n\n    SCEVConstant(const FoldingSetNodeIDRef ID, ConstantInt *v) :\n      SCEV(ID, scConstant, 1), V(v) {}\n\n  public:\n    ConstantInt *getValue() const { return V; }\n    const APInt &getAPInt() const { return getValue()->getValue(); }\n\n    Type *getType() const { return V->getType(); }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scConstant;\n    }\n  };\n\n  inline unsigned short computeExpressionSize(ArrayRef<const SCEV *> Args) {\n    APInt Size(16, 1);\n    for (auto *Arg : Args)\n      Size = Size.uadd_sat(APInt(16, Arg->getExpressionSize()));\n    return (unsigned short)Size.getZExtValue();\n  }\n\n  /// This is the base class for unary cast operator classes.\n  class SCEVCastExpr : public SCEV {\n  protected:\n    std::array<const SCEV *, 1> Operands;\n    Type *Ty;\n\n    SCEVCastExpr(const FoldingSetNodeIDRef ID, SCEVTypes SCEVTy, const SCEV *op,\n                 Type *ty);\n\n  public:\n    const SCEV *getOperand() const { return Operands[0]; }\n    const SCEV *getOperand(unsigned i) const {\n      assert(i == 0 && \"Operand index out of range!\");\n      return Operands[0];\n    }\n    using op_iterator = std::array<const SCEV *, 1>::const_iterator;\n    using op_range = iterator_range<op_iterator>;\n\n    op_range operands() const {\n      return make_range(Operands.begin(), Operands.end());\n    }\n    size_t getNumOperands() const { return 1; }\n    Type *getType() const { return Ty; }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scPtrToInt || S->getSCEVType() == scTruncate ||\n             S->getSCEVType() == scZeroExtend ||\n             S->getSCEVType() == scSignExtend;\n    }\n  };\n\n  /// This class represents a cast from a pointer to a pointer-sized integer\n  /// value.\n  class SCEVPtrToIntExpr : public SCEVCastExpr {\n    friend class ScalarEvolution;\n\n    SCEVPtrToIntExpr(const FoldingSetNodeIDRef ID, const SCEV *Op, Type *ITy);\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scPtrToInt;\n    }\n  };\n\n  /// This is the base class for unary integral cast operator classes.\n  class SCEVIntegralCastExpr : public SCEVCastExpr {\n  protected:\n    SCEVIntegralCastExpr(const FoldingSetNodeIDRef ID, SCEVTypes SCEVTy,\n                         const SCEV *op, Type *ty);\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scTruncate ||\n             S->getSCEVType() == scZeroExtend ||\n             S->getSCEVType() == scSignExtend;\n    }\n  };\n\n  /// This class represents a truncation of an integer value to a\n  /// smaller integer value.\n  class SCEVTruncateExpr : public SCEVIntegralCastExpr {\n    friend class ScalarEvolution;\n\n    SCEVTruncateExpr(const FoldingSetNodeIDRef ID,\n                     const SCEV *op, Type *ty);\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scTruncate;\n    }\n  };\n\n  /// This class represents a zero extension of a small integer value\n  /// to a larger integer value.\n  class SCEVZeroExtendExpr : public SCEVIntegralCastExpr {\n    friend class ScalarEvolution;\n\n    SCEVZeroExtendExpr(const FoldingSetNodeIDRef ID,\n                       const SCEV *op, Type *ty);\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scZeroExtend;\n    }\n  };\n\n  /// This class represents a sign extension of a small integer value\n  /// to a larger integer value.\n  class SCEVSignExtendExpr : public SCEVIntegralCastExpr {\n    friend class ScalarEvolution;\n\n    SCEVSignExtendExpr(const FoldingSetNodeIDRef ID,\n                       const SCEV *op, Type *ty);\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scSignExtend;\n    }\n  };\n\n  /// This node is a base class providing common functionality for\n  /// n'ary operators.\n  class SCEVNAryExpr : public SCEV {\n  protected:\n    // Since SCEVs are immutable, ScalarEvolution allocates operand\n    // arrays with its SCEVAllocator, so this class just needs a simple\n    // pointer rather than a more elaborate vector-like data structure.\n    // This also avoids the need for a non-trivial destructor.\n    const SCEV *const *Operands;\n    size_t NumOperands;\n\n    SCEVNAryExpr(const FoldingSetNodeIDRef ID, enum SCEVTypes T,\n                 const SCEV *const *O, size_t N)\n        : SCEV(ID, T, computeExpressionSize(makeArrayRef(O, N))), Operands(O),\n          NumOperands(N) {}\n\n  public:\n    size_t getNumOperands() const { return NumOperands; }\n\n    const SCEV *getOperand(unsigned i) const {\n      assert(i < NumOperands && \"Operand index out of range!\");\n      return Operands[i];\n    }\n\n    using op_iterator = const SCEV *const *;\n    using op_range = iterator_range<op_iterator>;\n\n    op_iterator op_begin() const { return Operands; }\n    op_iterator op_end() const { return Operands + NumOperands; }\n    op_range operands() const {\n      return make_range(op_begin(), op_end());\n    }\n\n    Type *getType() const { return getOperand(0)->getType(); }\n\n    NoWrapFlags getNoWrapFlags(NoWrapFlags Mask = NoWrapMask) const {\n      return (NoWrapFlags)(SubclassData & Mask);\n    }\n\n    bool hasNoUnsignedWrap() const {\n      return getNoWrapFlags(FlagNUW) != FlagAnyWrap;\n    }\n\n    bool hasNoSignedWrap() const {\n      return getNoWrapFlags(FlagNSW) != FlagAnyWrap;\n    }\n\n    bool hasNoSelfWrap() const {\n      return getNoWrapFlags(FlagNW) != FlagAnyWrap;\n    }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scAddExpr || S->getSCEVType() == scMulExpr ||\n             S->getSCEVType() == scSMaxExpr || S->getSCEVType() == scUMaxExpr ||\n             S->getSCEVType() == scSMinExpr || S->getSCEVType() == scUMinExpr ||\n             S->getSCEVType() == scAddRecExpr;\n    }\n  };\n\n  /// This node is the base class for n'ary commutative operators.\n  class SCEVCommutativeExpr : public SCEVNAryExpr {\n  protected:\n    SCEVCommutativeExpr(const FoldingSetNodeIDRef ID,\n                        enum SCEVTypes T, const SCEV *const *O, size_t N)\n      : SCEVNAryExpr(ID, T, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scAddExpr || S->getSCEVType() == scMulExpr ||\n             S->getSCEVType() == scSMaxExpr || S->getSCEVType() == scUMaxExpr ||\n             S->getSCEVType() == scSMinExpr || S->getSCEVType() == scUMinExpr;\n    }\n\n    /// Set flags for a non-recurrence without clearing previously set flags.\n    void setNoWrapFlags(NoWrapFlags Flags) {\n      SubclassData |= Flags;\n    }\n  };\n\n  /// This node represents an addition of some number of SCEVs.\n  class SCEVAddExpr : public SCEVCommutativeExpr {\n    friend class ScalarEvolution;\n\n    Type *Ty;\n\n    SCEVAddExpr(const FoldingSetNodeIDRef ID, const SCEV *const *O, size_t N)\n        : SCEVCommutativeExpr(ID, scAddExpr, O, N) {\n      auto *FirstPointerTypedOp = find_if(operands(), [](const SCEV *Op) {\n        return Op->getType()->isPointerTy();\n      });\n      if (FirstPointerTypedOp != operands().end())\n        Ty = (*FirstPointerTypedOp)->getType();\n      else\n        Ty = getOperand(0)->getType();\n    }\n\n  public:\n    Type *getType() const { return Ty; }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scAddExpr;\n    }\n  };\n\n  /// This node represents multiplication of some number of SCEVs.\n  class SCEVMulExpr : public SCEVCommutativeExpr {\n    friend class ScalarEvolution;\n\n    SCEVMulExpr(const FoldingSetNodeIDRef ID,\n                const SCEV *const *O, size_t N)\n      : SCEVCommutativeExpr(ID, scMulExpr, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scMulExpr;\n    }\n  };\n\n  /// This class represents a binary unsigned division operation.\n  class SCEVUDivExpr : public SCEV {\n    friend class ScalarEvolution;\n\n    std::array<const SCEV *, 2> Operands;\n\n    SCEVUDivExpr(const FoldingSetNodeIDRef ID, const SCEV *lhs, const SCEV *rhs)\n        : SCEV(ID, scUDivExpr, computeExpressionSize({lhs, rhs})) {\n        Operands[0] = lhs;\n        Operands[1] = rhs;\n      }\n\n  public:\n    const SCEV *getLHS() const { return Operands[0]; }\n    const SCEV *getRHS() const { return Operands[1]; }\n    size_t getNumOperands() const { return 2; }\n    const SCEV *getOperand(unsigned i) const {\n      assert((i == 0 || i == 1) && \"Operand index out of range!\");\n      return i == 0 ? getLHS() : getRHS();\n    }\n\n    using op_iterator = std::array<const SCEV *, 2>::const_iterator;\n    using op_range = iterator_range<op_iterator>;\n    op_range operands() const {\n      return make_range(Operands.begin(), Operands.end());\n    }\n\n    Type *getType() const {\n      // In most cases the types of LHS and RHS will be the same, but in some\n      // crazy cases one or the other may be a pointer. ScalarEvolution doesn't\n      // depend on the type for correctness, but handling types carefully can\n      // avoid extra casts in the SCEVExpander. The LHS is more likely to be\n      // a pointer type than the RHS, so use the RHS' type here.\n      return getRHS()->getType();\n    }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scUDivExpr;\n    }\n  };\n\n  /// This node represents a polynomial recurrence on the trip count\n  /// of the specified loop.  This is the primary focus of the\n  /// ScalarEvolution framework; all the other SCEV subclasses are\n  /// mostly just supporting infrastructure to allow SCEVAddRecExpr\n  /// expressions to be created and analyzed.\n  ///\n  /// All operands of an AddRec are required to be loop invariant.\n  ///\n  class SCEVAddRecExpr : public SCEVNAryExpr {\n    friend class ScalarEvolution;\n\n    const Loop *L;\n\n    SCEVAddRecExpr(const FoldingSetNodeIDRef ID,\n                   const SCEV *const *O, size_t N, const Loop *l)\n      : SCEVNAryExpr(ID, scAddRecExpr, O, N), L(l) {}\n\n  public:\n    const SCEV *getStart() const { return Operands[0]; }\n    const Loop *getLoop() const { return L; }\n\n    /// Constructs and returns the recurrence indicating how much this\n    /// expression steps by.  If this is a polynomial of degree N, it\n    /// returns a chrec of degree N-1.  We cannot determine whether\n    /// the step recurrence has self-wraparound.\n    const SCEV *getStepRecurrence(ScalarEvolution &SE) const {\n      if (isAffine()) return getOperand(1);\n      return SE.getAddRecExpr(SmallVector<const SCEV *, 3>(op_begin()+1,\n                                                           op_end()),\n                              getLoop(), FlagAnyWrap);\n    }\n\n    /// Return true if this represents an expression A + B*x where A\n    /// and B are loop invariant values.\n    bool isAffine() const {\n      // We know that the start value is invariant.  This expression is thus\n      // affine iff the step is also invariant.\n      return getNumOperands() == 2;\n    }\n\n    /// Return true if this represents an expression A + B*x + C*x^2\n    /// where A, B and C are loop invariant values.  This corresponds\n    /// to an addrec of the form {L,+,M,+,N}\n    bool isQuadratic() const {\n      return getNumOperands() == 3;\n    }\n\n    /// Set flags for a recurrence without clearing any previously set flags.\n    /// For AddRec, either NUW or NSW implies NW. Keep track of this fact here\n    /// to make it easier to propagate flags.\n    void setNoWrapFlags(NoWrapFlags Flags) {\n      if (Flags & (FlagNUW | FlagNSW))\n        Flags = ScalarEvolution::setFlags(Flags, FlagNW);\n      SubclassData |= Flags;\n    }\n\n    /// Return the value of this chain of recurrences at the specified\n    /// iteration number.\n    const SCEV *evaluateAtIteration(const SCEV *It, ScalarEvolution &SE) const;\n\n    /// Return the number of iterations of this loop that produce\n    /// values in the specified constant range.  Another way of\n    /// looking at this is that it returns the first iteration number\n    /// where the value is not in the condition, thus computing the\n    /// exit count.  If the iteration count can't be computed, an\n    /// instance of SCEVCouldNotCompute is returned.\n    const SCEV *getNumIterationsInRange(const ConstantRange &Range,\n                                        ScalarEvolution &SE) const;\n\n    /// Return an expression representing the value of this expression\n    /// one iteration of the loop ahead.\n    const SCEVAddRecExpr *getPostIncExpr(ScalarEvolution &SE) const;\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scAddRecExpr;\n    }\n  };\n\n  /// This node is the base class min/max selections.\n  class SCEVMinMaxExpr : public SCEVCommutativeExpr {\n    friend class ScalarEvolution;\n\n    static bool isMinMaxType(enum SCEVTypes T) {\n      return T == scSMaxExpr || T == scUMaxExpr || T == scSMinExpr ||\n             T == scUMinExpr;\n    }\n\n  protected:\n    /// Note: Constructing subclasses via this constructor is allowed\n    SCEVMinMaxExpr(const FoldingSetNodeIDRef ID, enum SCEVTypes T,\n                   const SCEV *const *O, size_t N)\n        : SCEVCommutativeExpr(ID, T, O, N) {\n      assert(isMinMaxType(T));\n      // Min and max never overflow\n      setNoWrapFlags((NoWrapFlags)(FlagNUW | FlagNSW));\n    }\n\n  public:\n    static bool classof(const SCEV *S) {\n      return isMinMaxType(S->getSCEVType());\n    }\n\n    static enum SCEVTypes negate(enum SCEVTypes T) {\n      switch (T) {\n      case scSMaxExpr:\n        return scSMinExpr;\n      case scSMinExpr:\n        return scSMaxExpr;\n      case scUMaxExpr:\n        return scUMinExpr;\n      case scUMinExpr:\n        return scUMaxExpr;\n      default:\n        llvm_unreachable(\"Not a min or max SCEV type!\");\n      }\n    }\n  };\n\n  /// This class represents a signed maximum selection.\n  class SCEVSMaxExpr : public SCEVMinMaxExpr {\n    friend class ScalarEvolution;\n\n    SCEVSMaxExpr(const FoldingSetNodeIDRef ID, const SCEV *const *O, size_t N)\n        : SCEVMinMaxExpr(ID, scSMaxExpr, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scSMaxExpr;\n    }\n  };\n\n  /// This class represents an unsigned maximum selection.\n  class SCEVUMaxExpr : public SCEVMinMaxExpr {\n    friend class ScalarEvolution;\n\n    SCEVUMaxExpr(const FoldingSetNodeIDRef ID, const SCEV *const *O, size_t N)\n        : SCEVMinMaxExpr(ID, scUMaxExpr, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scUMaxExpr;\n    }\n  };\n\n  /// This class represents a signed minimum selection.\n  class SCEVSMinExpr : public SCEVMinMaxExpr {\n    friend class ScalarEvolution;\n\n    SCEVSMinExpr(const FoldingSetNodeIDRef ID, const SCEV *const *O, size_t N)\n        : SCEVMinMaxExpr(ID, scSMinExpr, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scSMinExpr;\n    }\n  };\n\n  /// This class represents an unsigned minimum selection.\n  class SCEVUMinExpr : public SCEVMinMaxExpr {\n    friend class ScalarEvolution;\n\n    SCEVUMinExpr(const FoldingSetNodeIDRef ID, const SCEV *const *O, size_t N)\n        : SCEVMinMaxExpr(ID, scUMinExpr, O, N) {}\n\n  public:\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scUMinExpr;\n    }\n  };\n\n  /// This means that we are dealing with an entirely unknown SCEV\n  /// value, and only represent it as its LLVM Value.  This is the\n  /// \"bottom\" value for the analysis.\n  class SCEVUnknown final : public SCEV, private CallbackVH {\n    friend class ScalarEvolution;\n\n    /// The parent ScalarEvolution value. This is used to update the\n    /// parent's maps when the value associated with a SCEVUnknown is\n    /// deleted or RAUW'd.\n    ScalarEvolution *SE;\n\n    /// The next pointer in the linked list of all SCEVUnknown\n    /// instances owned by a ScalarEvolution.\n    SCEVUnknown *Next;\n\n    SCEVUnknown(const FoldingSetNodeIDRef ID, Value *V,\n                ScalarEvolution *se, SCEVUnknown *next) :\n      SCEV(ID, scUnknown, 1), CallbackVH(V), SE(se), Next(next) {}\n\n    // Implement CallbackVH.\n    void deleted() override;\n    void allUsesReplacedWith(Value *New) override;\n\n  public:\n    Value *getValue() const { return getValPtr(); }\n\n    /// @{\n    /// Test whether this is a special constant representing a type\n    /// size, alignment, or field offset in a target-independent\n    /// manner, and hasn't happened to have been folded with other\n    /// operations into something unrecognizable. This is mainly only\n    /// useful for pretty-printing and other situations where it isn't\n    /// absolutely required for these to succeed.\n    bool isSizeOf(Type *&AllocTy) const;\n    bool isAlignOf(Type *&AllocTy) const;\n    bool isOffsetOf(Type *&STy, Constant *&FieldNo) const;\n    /// @}\n\n    Type *getType() const { return getValPtr()->getType(); }\n\n    /// Methods for support type inquiry through isa, cast, and dyn_cast:\n    static bool classof(const SCEV *S) {\n      return S->getSCEVType() == scUnknown;\n    }\n  };\n\n  /// This class defines a simple visitor class that may be used for\n  /// various SCEV analysis purposes.\n  template<typename SC, typename RetVal=void>\n  struct SCEVVisitor {\n    RetVal visit(const SCEV *S) {\n      switch (S->getSCEVType()) {\n      case scConstant:\n        return ((SC*)this)->visitConstant((const SCEVConstant*)S);\n      case scPtrToInt:\n        return ((SC *)this)->visitPtrToIntExpr((const SCEVPtrToIntExpr *)S);\n      case scTruncate:\n        return ((SC*)this)->visitTruncateExpr((const SCEVTruncateExpr*)S);\n      case scZeroExtend:\n        return ((SC*)this)->visitZeroExtendExpr((const SCEVZeroExtendExpr*)S);\n      case scSignExtend:\n        return ((SC*)this)->visitSignExtendExpr((const SCEVSignExtendExpr*)S);\n      case scAddExpr:\n        return ((SC*)this)->visitAddExpr((const SCEVAddExpr*)S);\n      case scMulExpr:\n        return ((SC*)this)->visitMulExpr((const SCEVMulExpr*)S);\n      case scUDivExpr:\n        return ((SC*)this)->visitUDivExpr((const SCEVUDivExpr*)S);\n      case scAddRecExpr:\n        return ((SC*)this)->visitAddRecExpr((const SCEVAddRecExpr*)S);\n      case scSMaxExpr:\n        return ((SC*)this)->visitSMaxExpr((const SCEVSMaxExpr*)S);\n      case scUMaxExpr:\n        return ((SC*)this)->visitUMaxExpr((const SCEVUMaxExpr*)S);\n      case scSMinExpr:\n        return ((SC *)this)->visitSMinExpr((const SCEVSMinExpr *)S);\n      case scUMinExpr:\n        return ((SC *)this)->visitUMinExpr((const SCEVUMinExpr *)S);\n      case scUnknown:\n        return ((SC*)this)->visitUnknown((const SCEVUnknown*)S);\n      case scCouldNotCompute:\n        return ((SC*)this)->visitCouldNotCompute((const SCEVCouldNotCompute*)S);\n      }\n      llvm_unreachable(\"Unknown SCEV kind!\");\n    }\n\n    RetVal visitCouldNotCompute(const SCEVCouldNotCompute *S) {\n      llvm_unreachable(\"Invalid use of SCEVCouldNotCompute!\");\n    }\n  };\n\n  /// Visit all nodes in the expression tree using worklist traversal.\n  ///\n  /// Visitor implements:\n  ///   // return true to follow this node.\n  ///   bool follow(const SCEV *S);\n  ///   // return true to terminate the search.\n  ///   bool isDone();\n  template<typename SV>\n  class SCEVTraversal {\n    SV &Visitor;\n    SmallVector<const SCEV *, 8> Worklist;\n    SmallPtrSet<const SCEV *, 8> Visited;\n\n    void push(const SCEV *S) {\n      if (Visited.insert(S).second && Visitor.follow(S))\n        Worklist.push_back(S);\n    }\n\n  public:\n    SCEVTraversal(SV& V): Visitor(V) {}\n\n    void visitAll(const SCEV *Root) {\n      push(Root);\n      while (!Worklist.empty() && !Visitor.isDone()) {\n        const SCEV *S = Worklist.pop_back_val();\n\n        switch (S->getSCEVType()) {\n        case scConstant:\n        case scUnknown:\n          continue;\n        case scPtrToInt:\n        case scTruncate:\n        case scZeroExtend:\n        case scSignExtend:\n          push(cast<SCEVCastExpr>(S)->getOperand());\n          continue;\n        case scAddExpr:\n        case scMulExpr:\n        case scSMaxExpr:\n        case scUMaxExpr:\n        case scSMinExpr:\n        case scUMinExpr:\n        case scAddRecExpr:\n          for (const auto *Op : cast<SCEVNAryExpr>(S)->operands())\n            push(Op);\n          continue;\n        case scUDivExpr: {\n          const SCEVUDivExpr *UDiv = cast<SCEVUDivExpr>(S);\n          push(UDiv->getLHS());\n          push(UDiv->getRHS());\n          continue;\n        }\n        case scCouldNotCompute:\n          llvm_unreachable(\"Attempt to use a SCEVCouldNotCompute object!\");\n        }\n        llvm_unreachable(\"Unknown SCEV kind!\");\n      }\n    }\n  };\n\n  /// Use SCEVTraversal to visit all nodes in the given expression tree.\n  template<typename SV>\n  void visitAll(const SCEV *Root, SV& Visitor) {\n    SCEVTraversal<SV> T(Visitor);\n    T.visitAll(Root);\n  }\n\n  /// Return true if any node in \\p Root satisfies the predicate \\p Pred.\n  template <typename PredTy>\n  bool SCEVExprContains(const SCEV *Root, PredTy Pred) {\n    struct FindClosure {\n      bool Found = false;\n      PredTy Pred;\n\n      FindClosure(PredTy Pred) : Pred(Pred) {}\n\n      bool follow(const SCEV *S) {\n        if (!Pred(S))\n          return true;\n\n        Found = true;\n        return false;\n      }\n\n      bool isDone() const { return Found; }\n    };\n\n    FindClosure FC(Pred);\n    visitAll(Root, FC);\n    return FC.Found;\n  }\n\n  /// This visitor recursively visits a SCEV expression and re-writes it.\n  /// The result from each visit is cached, so it will return the same\n  /// SCEV for the same input.\n  template<typename SC>\n  class SCEVRewriteVisitor : public SCEVVisitor<SC, const SCEV *> {\n  protected:\n    ScalarEvolution &SE;\n    // Memoize the result of each visit so that we only compute once for\n    // the same input SCEV. This is to avoid redundant computations when\n    // a SCEV is referenced by multiple SCEVs. Without memoization, this\n    // visit algorithm would have exponential time complexity in the worst\n    // case, causing the compiler to hang on certain tests.\n    DenseMap<const SCEV *, const SCEV *> RewriteResults;\n\n  public:\n    SCEVRewriteVisitor(ScalarEvolution &SE) : SE(SE) {}\n\n    const SCEV *visit(const SCEV *S) {\n      auto It = RewriteResults.find(S);\n      if (It != RewriteResults.end())\n        return It->second;\n      auto* Visited = SCEVVisitor<SC, const SCEV *>::visit(S);\n      auto Result = RewriteResults.try_emplace(S, Visited);\n      assert(Result.second && \"Should insert a new entry\");\n      return Result.first->second;\n    }\n\n    const SCEV *visitConstant(const SCEVConstant *Constant) {\n      return Constant;\n    }\n\n    const SCEV *visitPtrToIntExpr(const SCEVPtrToIntExpr *Expr) {\n      const SCEV *Operand = ((SC *)this)->visit(Expr->getOperand());\n      return Operand == Expr->getOperand()\n                 ? Expr\n                 : SE.getPtrToIntExpr(Operand, Expr->getType());\n    }\n\n    const SCEV *visitTruncateExpr(const SCEVTruncateExpr *Expr) {\n      const SCEV *Operand = ((SC*)this)->visit(Expr->getOperand());\n      return Operand == Expr->getOperand()\n                 ? Expr\n                 : SE.getTruncateExpr(Operand, Expr->getType());\n    }\n\n    const SCEV *visitZeroExtendExpr(const SCEVZeroExtendExpr *Expr) {\n      const SCEV *Operand = ((SC*)this)->visit(Expr->getOperand());\n      return Operand == Expr->getOperand()\n                 ? Expr\n                 : SE.getZeroExtendExpr(Operand, Expr->getType());\n    }\n\n    const SCEV *visitSignExtendExpr(const SCEVSignExtendExpr *Expr) {\n      const SCEV *Operand = ((SC*)this)->visit(Expr->getOperand());\n      return Operand == Expr->getOperand()\n                 ? Expr\n                 : SE.getSignExtendExpr(Operand, Expr->getType());\n    }\n\n    const SCEV *visitAddExpr(const SCEVAddExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC*)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getAddExpr(Operands);\n    }\n\n    const SCEV *visitMulExpr(const SCEVMulExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC*)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getMulExpr(Operands);\n    }\n\n    const SCEV *visitUDivExpr(const SCEVUDivExpr *Expr) {\n      auto *LHS = ((SC *)this)->visit(Expr->getLHS());\n      auto *RHS = ((SC *)this)->visit(Expr->getRHS());\n      bool Changed = LHS != Expr->getLHS() || RHS != Expr->getRHS();\n      return !Changed ? Expr : SE.getUDivExpr(LHS, RHS);\n    }\n\n    const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC*)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr\n                      : SE.getAddRecExpr(Operands, Expr->getLoop(),\n                                         Expr->getNoWrapFlags());\n    }\n\n    const SCEV *visitSMaxExpr(const SCEVSMaxExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC *)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getSMaxExpr(Operands);\n    }\n\n    const SCEV *visitUMaxExpr(const SCEVUMaxExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC*)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getUMaxExpr(Operands);\n    }\n\n    const SCEV *visitSMinExpr(const SCEVSMinExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC *)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getSMinExpr(Operands);\n    }\n\n    const SCEV *visitUMinExpr(const SCEVUMinExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      bool Changed = false;\n      for (auto *Op : Expr->operands()) {\n        Operands.push_back(((SC *)this)->visit(Op));\n        Changed |= Op != Operands.back();\n      }\n      return !Changed ? Expr : SE.getUMinExpr(Operands);\n    }\n\n    const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n      return Expr;\n    }\n\n    const SCEV *visitCouldNotCompute(const SCEVCouldNotCompute *Expr) {\n      return Expr;\n    }\n  };\n\n  using ValueToValueMap = DenseMap<const Value *, Value *>;\n  using ValueToSCEVMapTy = DenseMap<const Value *, const SCEV *>;\n\n  /// The SCEVParameterRewriter takes a scalar evolution expression and updates\n  /// the SCEVUnknown components following the Map (Value -> SCEV).\n  class SCEVParameterRewriter : public SCEVRewriteVisitor<SCEVParameterRewriter> {\n  public:\n    static const SCEV *rewrite(const SCEV *Scev, ScalarEvolution &SE,\n                               ValueToSCEVMapTy &Map) {\n      SCEVParameterRewriter Rewriter(SE, Map);\n      return Rewriter.visit(Scev);\n    }\n\n    SCEVParameterRewriter(ScalarEvolution &SE, ValueToSCEVMapTy &M)\n        : SCEVRewriteVisitor(SE), Map(M) {}\n\n    const SCEV *visitUnknown(const SCEVUnknown *Expr) {\n      auto I = Map.find(Expr->getValue());\n      if (I == Map.end())\n        return Expr;\n      return I->second;\n    }\n\n  private:\n    ValueToSCEVMapTy &Map;\n  };\n\n  using LoopToScevMapT = DenseMap<const Loop *, const SCEV *>;\n\n  /// The SCEVLoopAddRecRewriter takes a scalar evolution expression and applies\n  /// the Map (Loop -> SCEV) to all AddRecExprs.\n  class SCEVLoopAddRecRewriter\n      : public SCEVRewriteVisitor<SCEVLoopAddRecRewriter> {\n  public:\n    SCEVLoopAddRecRewriter(ScalarEvolution &SE, LoopToScevMapT &M)\n        : SCEVRewriteVisitor(SE), Map(M) {}\n\n    static const SCEV *rewrite(const SCEV *Scev, LoopToScevMapT &Map,\n                               ScalarEvolution &SE) {\n      SCEVLoopAddRecRewriter Rewriter(SE, Map);\n      return Rewriter.visit(Scev);\n    }\n\n    const SCEV *visitAddRecExpr(const SCEVAddRecExpr *Expr) {\n      SmallVector<const SCEV *, 2> Operands;\n      for (const SCEV *Op : Expr->operands())\n        Operands.push_back(visit(Op));\n\n      const Loop *L = Expr->getLoop();\n      const SCEV *Res = SE.getAddRecExpr(Operands, L, Expr->getNoWrapFlags());\n\n      if (0 == Map.count(L))\n        return Res;\n\n      const SCEVAddRecExpr *Rec = cast<SCEVAddRecExpr>(Res);\n      return Rec->evaluateAtIteration(Map[L], SE);\n    }\n\n  private:\n    LoopToScevMapT &Map;\n  };\n\n} // end namespace llvm\n\n#endif // LLVM_ANALYSIS_SCALAREVOLUTIONEXPRESSIONS_H\n"}}, "reports": [{"events": [{"location": {"col": 24, "file": 2, "line": 3774}, "message": "the definition seen here"}, {"location": {"col": 9, "file": 1, "line": 499}, "message": "differing parameters are named here: ('Ty1', 'Ty2'), in definition: ('T1', 'T2')"}, {"location": {"col": 9, "file": 1, "line": 499}, "message": "function 'llvm::ScalarEvolution::getWiderType' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "3a06d65f279406f0e2f7391379d49d94", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 3506}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 579}, "message": "differing parameters are named here: ('Operands'), in definition: ('Ops')"}, {"location": {"col": 15, "file": 1, "line": 579}, "message": "function 'llvm::ScalarEvolution::getMinMaxExpr' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "2494836f8cc0281b8ddd2310c3577baf", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 3643}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 582}, "message": "differing parameters are named here: ('Operands'), in definition: ('Ops')"}, {"location": {"col": 15, "file": 1, "line": 582}, "message": "function 'llvm::ScalarEvolution::getSMaxExpr' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "a22e44723e6eab32908af45fcf274875", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 3652}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 584}, "message": "differing parameters are named here: ('Operands'), in definition: ('Ops')"}, {"location": {"col": 15, "file": 1, "line": 584}, "message": "function 'llvm::ScalarEvolution::getUMaxExpr' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "99cf5747d6ee7a166f0041e6a97942ab", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 3662}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 586}, "message": "differing parameters are named here: ('Operands'), in definition: ('Ops')"}, {"location": {"col": 15, "file": 1, "line": 586}, "message": "function 'llvm::ScalarEvolution::getSMinExpr' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "4b751b38606b5f338768457535a6bad9", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 3672}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 588}, "message": "differing parameters are named here: ('Operands'), in definition: ('Ops')"}, {"location": {"col": 15, "file": 1, "line": 588}, "message": "function 'llvm::ScalarEvolution::getUMinExpr' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "4cd09a00db72a6e1e74d10c3bf8d4086", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 8349}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 686}, "message": "differing parameters are named here: ('S'), in definition: ('V')"}, {"location": {"col": 15, "file": 1, "line": 686}, "message": "function 'llvm::ScalarEvolution::getSCEVAtScope' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "d2893192ff114b42a44e619365dc2251", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 18, "file": 2, "line": 6919}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 785}, "message": "differing parameters are named here: ('Predicates'), in definition: ('Preds')"}, {"location": {"col": 15, "file": 1, "line": 785}, "message": "function 'llvm::ScalarEvolution::getPredicatedBackedgeTakenCount' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "a6b9d6f4f8ab0ccb24ba6a3f078a2eea", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 12858}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 1158}, "message": "differing parameters are named here: ('A'), in definition: ('Preds')"}, {"location": {"col": 15, "file": 1, "line": 1158}, "message": "function 'llvm::ScalarEvolution::rewriteUsingPredicate' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "75f9f2fe624d565ecfdcf75850f1eaf1", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 34, "file": 2, "line": 10351}, "message": "the definition seen here"}, {"location": {"col": 19, "file": 1, "line": 1173}, "message": "differing parameters are named here: ('LHS', 'RHS'), in definition: ('More', 'Less')"}, {"location": {"col": 19, "file": 1, "line": 1173}, "message": "function 'llvm::ScalarEvolution::computeConstantDifference' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "7c1720f35635628fba056f19c9f7ab5e", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 39, "file": 2, "line": 11972}, "message": "the definition seen here"}, {"location": {"col": 10, "file": 1, "line": 1190}, "message": "differing parameters are named here: ('New'), in definition: ('V')"}, {"location": {"col": 10, "file": 1, "line": 1190}, "message": "function 'llvm::ScalarEvolution::SCEVCallbackVH::allUsesReplacedWith' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "796073291ef9e7665a00d4101dc75f96", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 37, "file": 2, "line": 7202}, "message": "the definition seen here"}, {"location": {"col": 17, "file": 1, "line": 1424}, "message": "differing parameters are named here: ('Predicates'), in definition: ('Preds')"}, {"location": {"col": 17, "file": 1, "line": 1424}, "message": "function 'llvm::ScalarEvolution::BackedgeTakenInfo::getExact' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "2aaa51d691321ddb1338b8f44c4af428", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 32, "file": 2, "line": 5973}, "message": "the definition seen here"}, {"location": {"col": 17, "file": 1, "line": 1548}, "message": "differing parameters are named here: ('Stop'), in definition: ('Step')"}, {"location": {"col": 17, "file": 1, "line": 1548}, "message": "function 'llvm::ScalarEvolution::getRangeForAffineAR' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "0ff565bae7a2a31b2a91ddca118e2ae3", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 32, "file": 2, "line": 6074}, "message": "the definition seen here"}, {"location": {"col": 17, "file": 1, "line": 1561}, "message": "differing parameters are named here: ('Stop'), in definition: ('Step')"}, {"location": {"col": 17, "file": 1, "line": 1561}, "message": "function 'llvm::ScalarEvolution::getRangeViaFactoring' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "d5c7f0ae36543a4f6c02d26651019a6a", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 8480}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 1593}, "message": "differing parameters are named here: ('S'), in definition: ('V')"}, {"location": {"col": 15, "file": 1, "line": 1593}, "message": "function 'llvm::ScalarEvolution::computeSCEVAtScope' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "234e40dbd48fc43e66c1fe4c6de3d564", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 23, "file": 2, "line": 4171}, "message": "the definition seen here"}, {"location": {"col": 8, "file": 1, "line": 1598}, "message": "differing parameters are named here: ('I'), in definition: ('PN')"}, {"location": {"col": 8, "file": 1, "line": 1598}, "message": "function 'llvm::ScalarEvolution::forgetSymbolicName' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "ddd4ed1989a0340a43fa26231c007995", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 18, "file": 2, "line": 7680}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 1, "line": 1688}, "message": "differing parameters are named here: ('IsSubExpr'), in definition: ('ControlsExit')"}, {"location": {"col": 13, "file": 1, "line": 1688}, "message": "function 'llvm::ScalarEvolution::computeExitLimitFromICmp' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "dbd1ac15d438f19c6be0f5d67baefe85", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 18, "file": 2, "line": 7779}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 1, "line": 1696}, "message": "differing parameters are named here: ('ExitingBB', 'IsSubExpr'), in definition: ('ExitingBlock', 'ControlsExit')"}, {"location": {"col": 13, "file": 1, "line": 1696}, "message": "function 'llvm::ScalarEvolution::computeExitLimitFromSingleExitSwitch' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "678c9ae34218798fb5ce7c7749c5dfc5", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 18, "file": 2, "line": 9047}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 1, "line": 1729}, "message": "differing parameters are named here: ('IsSubExpr'), in definition: ('ControlsExit')"}, {"location": {"col": 13, "file": 1, "line": 1729}, "message": "function 'llvm::ScalarEvolution::howFarToZero' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "ed416ef830f57c83c5dd5cef9d56cc50", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 18, "file": 2, "line": 11280}, "message": "the definition seen here"}, {"location": {"col": 13, "file": 1, "line": 1753}, "message": "differing parameters are named here: ('IsSubExpr'), in definition: ('ControlsExit')"}, {"location": {"col": 13, "file": 1, "line": 1753}, "message": "function 'llvm::ScalarEvolution::howManyGreaterThans' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "95a9f5c6e9b7d5323fe6de9cf6ba9a8a", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 11081}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 1974}, "message": "differing parameters are named here: ('Stride'), in definition: ('Step')"}, {"location": {"col": 15, "file": 1, "line": 1974}, "message": "function 'llvm::ScalarEvolution::computeBECount' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "bb1850e8e9d21dc0cec4e851d2dce908", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 30, "file": 2, "line": 8722}, "message": "the definition seen here"}, {"location": {"col": 15, "file": 1, "line": 2012}, "message": "differing parameters are named here: ('Val'), in definition: ('S')"}, {"location": {"col": 15, "file": 1, "line": 2012}, "message": "function 'llvm::ScalarEvolution::stripInjectiveFunctions' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolution.h", "reportHash": "659679a35b8036174afb6a1081b88d48", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}, {"events": [{"location": {"col": 19, "file": 2, "line": 570}, "message": "the definition seen here"}, {"location": {"col": 10, "file": 3, "line": 554}, "message": "differing parameters are named here: ('STy'), in definition: ('CTy')"}, {"location": {"col": 10, "file": 3, "line": 554}, "message": "function 'llvm::SCEVUnknown::isOffsetOf' has a definition with different parameter names"}], "macros": [], "notes": [], "path": "/home/vsts/work/1/llvm-project/llvm/include/llvm/Analysis/ScalarEvolutionExpressions.h", "reportHash": "914e90b6ae02194d030d16f32776e515", "checkerName": "readability-inconsistent-declaration-parameter-name", "reviewStatus": null, "severity": "STYLE"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
